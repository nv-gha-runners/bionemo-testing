{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>NVIDIA BioNeMo Framework is a collection of programming tools, libraries, and models for computational drug discovery. It accelerates the most time-consuming and costly stages of building and adapting biomolecular AI models by providing domain-specific, optimized models and tooling that are easily integrated into GPU-based computational resources for the fastest performance on the market. You can access BioNeMo Framework as a free community resource or learn more about getting an enterprise license for improved expert-level support at the BioNeMo homepage.</p> <ul> <li> <p> User Guide</p> <p>Install BioNeMo and set up your environment to start accelerating your bioinformatics workflows.</p> <p>Get Started</p> </li> <li> <p> API Reference</p> <p>Access comprehensive documentation on BioNeMo's sub-packages, functions, and classes.</p> <p>API Reference</p> </li> <li> <p> Models</p> <p>Explore detailed instructions and best practices for using BioNeMo models in your research.</p> <p>Explore Models</p> </li> <li> <p> Datasets</p> <p>Explore biomolecular datasets that come pre-packaged with the BioNeMo Framework.</p> <p>Explore Datasets</p> </li> </ul>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>User Guide</li> <li>Models</li> <li>Datasets</li> <li>API</li> </ul>"},{"location":"API_reference/","title":"API reference","text":"<p>The API reference contains detailed descriptions of all public functions and objects. It's the best place to look if you need information on a specific function.</p>"},{"location":"API_reference/bionemo/core/api/","title":"Api","text":""},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.ModelOutput","title":"<code>ModelOutput = TypeVar('ModelOutput', Tensor, list[Tensor], tuple[Tensor], dict[str, Tensor], covariant=True)</code>  <code>module-attribute</code>","text":"<p>A Model's forward pass may produce a tensor, multiple tensors, or named tensors.</p>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoModelConfig","title":"<code>BionemoModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType]</code>, <code>ABC</code></p> <p>An abstract class for model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoModelConfig(Generic[ModelType], ABC):\n    \"\"\"An abstract class for model configuration.\"\"\"\n\n    @abstractmethod\n    def configure_model(self, *args, **kwargs) -&gt; Model:\n        \"\"\"Configures the model.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoModelConfig.configure_model","title":"<code>configure_model(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Configures the model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef configure_model(self, *args, **kwargs) -&gt; Model:\n    \"\"\"Configures the model.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoTrainableModelConfig","title":"<code>BionemoTrainableModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType, LossType]</code>, <code>BionemoModelConfig[ModelType]</code></p> <p>An abstract class for trainable model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoTrainableModelConfig(Generic[ModelType, LossType], BionemoModelConfig[ModelType]):\n    \"\"\"An abstract class for trainable model configuration.\"\"\"\n\n    @abstractmethod\n    def get_loss_reduction_class(self) -&gt; Type[LossType]:\n        \"\"\"Returns the loss reduction class.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoTrainableModelConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>  <code>abstractmethod</code>","text":"<p>Returns the loss reduction class.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef get_loss_reduction_class(self) -&gt; Type[LossType]:\n    \"\"\"Returns the loss reduction class.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Protocol[ModelOutput]</code></p> <p>Lightweight interface for a model: must have a forward method.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class Model(Protocol[ModelOutput]):\n    \"\"\"Lightweight interface for a model: must have a forward method.\"\"\"\n\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        \"\"\"Prediction / forward-step for a model.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.Model.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Prediction / forward-step for a model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; ModelOutput:\n    \"\"\"Prediction / forward-step for a model.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/core/data/","title":"BioNeMo test data management","text":"<p>This library manages the downloading and caching of large or binary data files used in the documentation or test suite. These files should not be committed directly to the repo, and instead should be loaded at test-time when they are needed.</p> <p>We currently support two locations for test data or saved models:</p> SwiftStack <p>SwiftStack or <code>pbss</code> is an NVIDIA-internal, s3-compatible object store that allows for very large data and fast, parallel read/writes. Most critically, <code>pbss</code> can be uploaded to without legal approvals for dataset redistribution. These files will not be accessible by external collaborators.</p> NGC <p>NGC hosts containers, models, and resources, some of which require authentication and others that are generally available. This library uses the model and resource types to save test data and reference model weights. These items are accessible by external collaborators, but require legal approval before re-distributing test data.</p>"},{"location":"API_reference/bionemo/core/data/#loading-test-or-example-data","title":"Loading test or example data","text":"<p>Test data are specified via yaml files in <code>sub-packages/bionemo-testing/src/bionemo/testing/data/resources</code>. As an example, in <code>esm2.yaml</code>:</p> <pre><code>- tag: nv_650m:1.0\n  ngc: \"nvidia/clara/esm2nv650m:1.0\"\n  ngc_registry: model\n  pbss: \"s3://bionemo-ci/models/esm2nv_650M_converted.nemo\"\n  sha256: 1e38063cafa808306329428dd17ea6df78c9e5d6b3d2caf04237c555a1f131b7\n  owner: Farhad Ramezanghorbani &lt;farhadr@nvidia.com&gt;\n  description: &gt;\n    A pretrained 650M parameter ESM-2 model.\n    See https://ngc.nvidia.com/catalog/models/nvidia:clara:esm2nv650m.\n</code></pre> <p>To load these model weights during a test, use the load function with the filename and tag of the desired asset, which returns a path a the specified file:</p> <pre><code>path_to_my_checkpoint = load(\"esm2/nv_650m:1.0\")\nconfig = ESM2Config(nemo1_ckpt_path=path_to_my_checkpoint)\n</code></pre> <p>If this function is called without the data available on the local machine, it will be fetched from the default source (currently <code>pbss</code>.) Otherwise, it will return the cached directory. To download with NGC, pass <code>source=\"ngc\"</code> to load.</p>"},{"location":"API_reference/bionemo/core/data/#file-unpacking-andor-decompression","title":"File unpacking and/or decompression","text":"<p>All test artifacts are individual files. If a zip or tar archive is specified, it will be unpacked automatically, and the path to the directory will be returned via load. Compressed files ('gzip', 'bz2', or 'xz') are automatically decompressed before they are returned. The file's compression and/or archive format is determined based on the filename specified in the <code>pbss</code> URL.</p> <p>Files in NGC resources</p> <p>NGC resources are folders, i.e., they may contain multiple files per resource. load will only download the filename matching the stem of the <code>pbss</code> url. The same NGC resource can therefore be used to host multiple test assets that are used independently.</p>"},{"location":"API_reference/bionemo/core/data/#adding-new-test-assets","title":"Adding new test assets","text":"<p>To add new data, first ensure that the data is available from either NGC or <code>pbss</code>. Next, extend or create a new yaml file in <code>sub-packages/bionemo-testing/src/bionemo/testing/data/resources</code> with the required information. Owner emails must be provided for all assets. The description and <code>ngc</code> fields are currently optional. If the <code>sha256</code> is left unspecified, <code>pooch</code> will report the downloaded file's sha when loaded.</p> <p>Warning</p> <p>SHAs should be provided for all files to ensure the download completes correctly, and to invalidate caches if the files change.</p>"},{"location":"API_reference/bionemo/core/data/api/","title":"Api","text":""},{"location":"API_reference/bionemo/core/data/load/","title":"Load","text":""},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.NGCDownloader","title":"<code>NGCDownloader</code>  <code>dataclass</code>","text":"<p>A class to download files from NGC in a Pooch-compatible way.</p> <p>NGC downloads are typically structured as directories, while pooch expects a single file. This class downloads a single file from an NGC directory and moves it to the desired location.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>@dataclass\nclass NGCDownloader:\n    \"\"\"A class to download files from NGC in a Pooch-compatible way.\n\n    NGC downloads are typically structured as directories, while pooch expects a single file. This class\n    downloads a single file from an NGC directory and moves it to the desired location.\n    \"\"\"\n\n    filename: str\n    ngc_registry: Literal[\"model\", \"resource\"]\n\n    def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n        \"\"\"Download a file from NGC.\"\"\"\n        client = default_ngc_client()\n        nest_asyncio.apply()\n\n        download_fns = {\n            \"model\": client.registry.model.download_version,\n            \"resource\": client.registry.resource.download_version,\n        }\n\n        output_file = Path(output_file)\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n\n        # NGC seems to always download to a specific directory that we can't specify ourselves.\n        ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n        with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n            download_fns[self.ngc_registry](url, temp_dir, file_patterns=[self.filename])\n            shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.NGCDownloader.__call__","title":"<code>__call__(url, output_file, _)</code>","text":"<p>Download a file from NGC.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n    \"\"\"Download a file from NGC.\"\"\"\n    client = default_ngc_client()\n    nest_asyncio.apply()\n\n    download_fns = {\n        \"model\": client.registry.model.download_version,\n        \"resource\": client.registry.resource.download_version,\n    }\n\n    output_file = Path(output_file)\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n\n    # NGC seems to always download to a specific directory that we can't specify ourselves.\n    ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n    with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n        download_fns[self.ngc_registry](url, temp_dir, file_patterns=[self.filename])\n        shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load._get_processor","title":"<code>_get_processor(extension, unpack, decompress)</code>","text":"<p>Get the processor for a given file extension.</p> <p>If unpack and decompress are both None, the processor will be inferred from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The file extension.</p> required <code>unpack</code> <code>bool | None</code> <p>Whether to unpack the file.</p> required <code>decompress</code> <code>bool | None</code> <p>Whether to decompress the file.</p> required <p>Returns:</p> Type Description <p>A Pooch processor object.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def _get_processor(extension: str, unpack: bool | None, decompress: bool | None):\n    \"\"\"Get the processor for a given file extension.\n\n    If unpack and decompress are both None, the processor will be inferred from the file extension.\n\n    Args:\n        extension: The file extension.\n        unpack: Whether to unpack the file.\n        decompress: Whether to decompress the file.\n\n    Returns:\n        A Pooch processor object.\n    \"\"\"\n    if extension in {\".gz\", \".bz2\", \".xz\"} and decompress is None:\n        return pooch.Decompress()\n\n    elif extension in {\".tar\", \".tar.gz\"} and unpack is None:\n        return pooch.Untar()\n\n    elif extension == \".zip\" and unpack is None:\n        return pooch.Unzip()\n\n    else:\n        return None\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load._s3_download","title":"<code>_s3_download(url, output_file, _)</code>","text":"<p>Download a file from PBSS.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def _s3_download(url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n    \"\"\"Download a file from PBSS.\"\"\"\n    # Parse S3 URL to get bucket and key\n    parts = url.replace(\"s3://\", \"\").split(\"/\")\n    bucket = parts[0]\n    key = \"/\".join(parts[1:])\n\n    with contextlib.closing(default_pbss_client()) as s3:\n        object_size = s3.head_object(Bucket=bucket, Key=key)[\"ContentLength\"]\n        progress_bar = tqdm(total=object_size, unit=\"B\", unit_scale=True, desc=url)\n\n        # Define callback\n        def progress_callback(bytes_transferred):\n            progress_bar.update(bytes_transferred)\n\n        # Download file from S3\n        s3.download_file(bucket, key, output_file, Callback=progress_callback)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.default_ngc_client","title":"<code>default_ngc_client(use_guest_if_api_key_invalid=True)</code>","text":"<p>Create a default NGC client.</p> <p>This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_ngc_client(use_guest_if_api_key_invalid: bool = True) -&gt; \"ngcsdk.Client\":\n    \"\"\"Create a default NGC client.\n\n    This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.\n    \"\"\"\n    import ngcsdk\n\n    client = ngcsdk.Client()\n\n    try:\n        client.configure()\n\n    except ValueError as e:\n        if use_guest_if_api_key_invalid:\n            logger.error(f\"Error configuring NGC client: {e}, signing in as guest.\")\n            client = ngcsdk.Client(\"no-apikey\")\n            client.configure(\n                api_key=\"no-apikey\",  # pragma: allowlist secret\n                org_name=\"no-org\",\n                team_name=\"no-team\",\n                ace_name=\"no-ace\",\n            )\n\n        else:\n            raise\n\n    return client\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.default_pbss_client","title":"<code>default_pbss_client()</code>","text":"<p>Create a default S3 client for PBSS.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_pbss_client():\n    \"\"\"Create a default S3 client for PBSS.\"\"\"\n    import boto3\n\n    retry_config = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"})\n    return boto3.client(\"s3\", endpoint_url=\"https://pbss.s8k.io\", config=retry_config)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.entrypoint","title":"<code>entrypoint()</code>","text":"<p>Allows a user to get a specific artifact from the command line.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def entrypoint():\n    \"\"\"Allows a user to get a specific artifact from the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Retrieve the local path to the requested artifact name or list resources.\"\n    )\n\n    # Create mutually exclusive group\n    group = parser.add_mutually_exclusive_group(required=True)\n\n    # Add the argument for artifact name, which is required if --list-resources is not used\n    group.add_argument(\"artifact_name\", type=str, nargs=\"?\", help=\"Name of the artifact\")\n\n    # Add the --list-resources option\n    group.add_argument(\n        \"--list-resources\", action=\"store_true\", default=False, help=\"List all available artifacts and then exit.\"\n    )\n\n    # Add the --source option\n    parser.add_argument(\n        \"--source\",\n        type=str,\n        choices=[\"pbss\", \"ngc\"],\n        default=\"ngc\",\n        help='Backend to use, Internal NVIDIA users can set this to \"pbss\".',\n    )\n\n    parser.add_argument(\n        \"--all\",\n        action=\"store_true\",\n        default=False,\n        help=\"Download all resources. Ignores all other options.\",\n    )\n    args = parser.parse_args()\n    maybe_error = main(\n        download_all=args.all,\n        list_resources=args.list_resources,\n        artifact_name=args.artifact_name,\n        source=args.source,\n    )\n    if maybe_error is not None:\n        parser.error(maybe_error)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.load","title":"<code>load(model_or_data_tag, source=DEFAULT_SOURCE, resources=None, cache_dir=None)</code>","text":"<p>Download a resource from PBSS or NGC.</p> <p>Parameters:</p> Name Type Description Default <code>model_or_data_tag</code> <code>str</code> <p>A pointer to the desired resource. Must be a key in the resources dictionary.</p> required <code>source</code> <code>SourceOptions</code> <p>Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".</p> <code>DEFAULT_SOURCE</code> <code>resources</code> <code>dict[str, Resource] | None</code> <p>A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)</p> <code>None</code> <code>cache_dir</code> <code>Path | None</code> <p>The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the desired tag was not found, or if an NGC url was requested but not provided.</p> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object pointing either at the downloaded file, or at a decompressed folder containing the</p> <code>Path</code> <p>file(s).</p> <p>Examples:</p> <p>For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:</p> <pre><code>&gt;&gt;&gt; load(\"filename/tag\")\nPosixPath(/tmp/bionemo/downloaded-file-name)\n</code></pre> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def load(\n    model_or_data_tag: str,\n    source: SourceOptions = DEFAULT_SOURCE,\n    resources: dict[str, Resource] | None = None,\n    cache_dir: Path | None = None,\n) -&gt; Path:\n    \"\"\"Download a resource from PBSS or NGC.\n\n    Args:\n        model_or_data_tag: A pointer to the desired resource. Must be a key in the resources dictionary.\n        source: Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".\n        resources: A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)\n        cache_dir: The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)\n\n    Raises:\n        ValueError: If the desired tag was not found, or if an NGC url was requested but not provided.\n\n    Returns:\n        A Path object pointing either at the downloaded file, or at a decompressed folder containing the\n        file(s).\n\n    Examples:\n        For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:\n        &gt;&gt;&gt; load(\"filename/tag\")\n        PosixPath(/tmp/bionemo/downloaded-file-name)\n    \"\"\"\n    if resources is None:\n        resources = get_all_resources()\n\n    if cache_dir is None:\n        cache_dir = BIONEMO_CACHE_DIR\n\n    if model_or_data_tag not in resources:\n        raise ValueError(f\"Resource '{model_or_data_tag}' not found.\")\n\n    if source == \"ngc\" and resources[model_or_data_tag].ngc is None:\n        raise ValueError(f\"Resource '{model_or_data_tag}' does not have an NGC URL.\")\n\n    resource = resources[model_or_data_tag]\n    filename = str(resource.pbss).split(\"/\")[-1]\n\n    extension = \"\".join(Path(filename).suffixes)\n    processor = _get_processor(extension, resource.unpack, resource.decompress)\n\n    if source == \"pbss\":\n        download_fn = _s3_download\n        url = resource.pbss\n\n    elif source == \"ngc\":\n        assert resource.ngc_registry is not None\n        download_fn = NGCDownloader(filename=filename, ngc_registry=resource.ngc_registry)\n        url = resource.ngc\n\n    else:\n        raise ValueError(f\"Source '{source}' not supported.\")\n\n    download = pooch.retrieve(\n        url=str(url),\n        fname=f\"{resource.sha256}-{filename}\",\n        known_hash=resource.sha256,\n        path=cache_dir,\n        downloader=download_fn,\n        processor=processor,\n    )\n\n    # Pooch by default returns a list of unpacked files if they unpack a zipped or tarred directory. Instead of that, we\n    # just want the unpacked, parent folder.\n    if isinstance(download, list):\n        return Path(processor.extract_dir)  # type: ignore\n\n    else:\n        return Path(download)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.main","title":"<code>main(download_all, list_resources, artifact_name, source)</code>","text":"<p>Main download script logic: parameters are 1:1 with CLI flags. Returns string describing error on failure.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def main(\n    download_all: bool, list_resources: bool, artifact_name: str, source: Literal[\"pbss\", \"ngc\"]\n) -&gt; Optional[str]:\n    \"\"\"Main download script logic: parameters are 1:1 with CLI flags. Returns string describing error on failure.\"\"\"\n    if download_all:\n        print(\"Downloading all resources:\", file=sys.stderr)\n        print_resources(output_source=sys.stderr)\n        print(\"-\" * 80, file=sys.stderr)\n\n        resource_to_local: dict[str, Path] = {}\n        for resource_name in tqdm(\n            sorted(get_all_resources()),\n            desc=\"Downloading Resources\",\n        ):\n            with contextlib.redirect_stdout(sys.stderr):\n                local_path = load(resource_name, source=source)\n            resource_to_local[resource_name] = local_path\n\n        print(\"-\" * 80, file=sys.stderr)\n        print(\"All resources downloaded:\", file=sys.stderr)\n        for resource_name, local_path in sorted(resource_to_local.items()):\n            print(f\"  {resource_name}: {str(local_path.absolute())}\", file=sys.stderr)\n\n    elif list_resources:\n        print_resources(output_source=sys.stdout)\n\n    elif artifact_name is not None and len(artifact_name) &gt; 0:\n        # Get the local path for the provided artifact name\n        with contextlib.redirect_stdout(sys.stderr):\n            local_path = load(artifact_name, source=source)\n\n        # Print the result =&gt; CLI use assumes that we can get the single downloaded resource's path on STDOUT\n        print(str(local_path.absolute()))\n\n    else:\n        return \"You must provide an artifact name if --list-resources or --all is not set!\"\n</code></pre>"},{"location":"API_reference/bionemo/core/data/load/#bionemo.core.data.load.print_resources","title":"<code>print_resources(*, output_source=sys.stdout)</code>","text":"<p>Prints all available downloadable resources &amp; their sources to STDOUT.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def print_resources(*, output_source: TextIO = sys.stdout) -&gt; None:\n    \"\"\"Prints all available downloadable resources &amp; their sources to STDOUT.\"\"\"\n    print(\"#resource_name\\tsource_options\", file=output_source)\n    for resource_name, resource in sorted(get_all_resources().items()):\n        sources = []\n        if resource.ngc is not None:\n            sources.append(\"ngc\")\n        if resource.pbss is not None:\n            sources.append(\"pbss\")\n        print(f\"{resource_name}\\t{','.join(sources)}\", file=output_source)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/","title":"Multi epoch dataset","text":""},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex","title":"<code>EpochIndex</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A tuple that contains both the current epoch and index for multi-epoch training.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class EpochIndex(NamedTuple):\n    \"\"\"A tuple that contains both the current epoch and index for multi-epoch training.\"\"\"\n\n    epoch: int\n    \"\"\"An integer representing the current epoch.\"\"\"\n\n    idx: int\n    \"\"\"An integer representing the index within the current epoch.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex.epoch","title":"<code>epoch</code>  <code>instance-attribute</code>","text":"<p>An integer representing the current epoch.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex.idx","title":"<code>idx</code>  <code>instance-attribute</code>","text":"<p>An integer representing the index within the current epoch.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.IdentityMultiEpochDatasetWrapper","title":"<code>IdentityMultiEpochDatasetWrapper</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MultiEpochDatasetWrapper[T, T]</code></p> <p>An implementation of the <code>MultiEpochDatasetWrapper</code> that does not apply any transformations.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class IdentityMultiEpochDatasetWrapper(MultiEpochDatasetWrapper[T, T]):\n    \"\"\"An implementation of the `MultiEpochDatasetWrapper` that does not apply any transformations.\"\"\"\n\n    def apply_transform(self, sample: T, index: EpochIndex) -&gt; T:\n        \"\"\"Return the sample as is.\"\"\"\n        del index  # Unused.\n        return sample\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.IdentityMultiEpochDatasetWrapper.apply_transform","title":"<code>apply_transform(sample, index)</code>","text":"<p>Return the sample as is.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def apply_transform(self, sample: T, index: EpochIndex) -&gt; T:\n    \"\"\"Return the sample as is.\"\"\"\n    del index  # Unused.\n    return sample\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDataset","title":"<code>MultiEpochDataset</code>","text":"<p>               Bases: <code>Protocol[T_co]</code></p> <p>A protocol for datasets for multi-epoch training in Megatron-LM.</p> <p>Dataset determinism in Megatron-LM</p> <p>In megatron training, the sampler and dataset objects are used to ensure consistent data loading across model-parallel ranks. For datasets to work with megatron training, they must return exactly the same data for every call to <code>__getitem__</code> with the same index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class MultiEpochDataset(Protocol[T_co]):\n    \"\"\"A protocol for datasets for multi-epoch training in Megatron-LM.\n\n    !!! important \"Dataset determinism in Megatron-LM\"\n        In megatron training, the sampler and dataset objects are used to ensure consistent data loading across\n        model-parallel ranks. For datasets to work with megatron training, they must return exactly the same data for\n        every call to `__getitem__` with the same index.\n    \"\"\"\n\n    def __getitem__(self, index: EpochIndex) -&gt; T_co:  # noqa: D105\n        ...\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler","title":"<code>MultiEpochDatasetResampler</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset[T_co]</code></p> <p>A dataset wrapper class that converts the sequential sampling from Megatron-LM to epoch-based sampling.</p> <p>Either <code>num_epochs</code> or <code>num_samples</code> should be provided. If neither are provided, the dataset will use a single epoch. If <code>num_epochs</code> is given, the resampled dataset will have <code>len(dataset) * num_epochs</code> samples. If <code>num_samples</code> the resampled dataset will have <code>num_samples</code> samples. For <code>num_samples</code>, the dataset will be repeated for multiple epochs until the desired number of samples is reached (with the final epoch being truncated).</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@dataclass\nclass MultiEpochDatasetResampler(Dataset[T_co]):\n    \"\"\"A dataset wrapper class that converts the sequential sampling from Megatron-LM to epoch-based sampling.\n\n    Either `num_epochs` or `num_samples` should be provided. If neither are provided, the dataset will use a single\n    epoch. If `num_epochs` is given, the resampled dataset will have `len(dataset) * num_epochs` samples. If\n    `num_samples` the resampled dataset will have `num_samples` samples. For `num_samples`, the dataset will be repeated\n    for multiple epochs until the desired number of samples is reached (with the final epoch being truncated).\n    \"\"\"\n\n    dataset: MultiEpochDataset[T_co]\n    \"\"\"The dataset to resample. Must support indexing with an `EpochIndex`.\"\"\"\n\n    num_epochs: int | None = None\n    \"\"\"The total number of epochs. The length of the resampled dataset will be len(dataset) * num_epochs.\"\"\"\n\n    num_samples: int | None = None\n    \"\"\"The total number of samples to draw.\n\n    The number of epochs will be determined by the number of samples and the length of the dataset.\n    \"\"\"\n\n    shuffle: bool = True\n    \"\"\"Whether to shuffle the samples in the dataset each epoch.\"\"\"\n\n    seed: int = 42  # type: ignore\n    \"\"\"A random seed for reproducibility.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Pre-shuffle each epoch's samples.\"\"\"\n        if self.num_epochs is None and self.num_samples is None:\n            self.num_epochs = 1\n        elif self.num_epochs is not None and self.num_samples is not None:\n            raise ValueError(\"Only one of num_epochs and num_samples should be provided.\")\n\n        if self.num_epochs is None and self.num_samples is not None:\n            self.num_epochs = math.ceil(self.num_samples / len(self.dataset))\n\n        elif self.num_samples is None and self.num_epochs is not None:\n            self.num_samples = len(self.dataset) * self.num_epochs\n\n        # Type guard statements, the above if/elif block should ensure these are not None.\n        assert self.num_epochs is not None\n        assert self.num_samples is not None\n\n        if self.num_epochs &lt; 1:\n            raise ValueError(\"num_epochs must be at least 1.\")\n\n        rng = np.random.default_rng(self.seed)\n\n        # Initialize a vector of random seeds so that each epoch is shuffled differently.\n        self.epoch_seeds = rng.integers(0, np.iinfo(np.int32).max, size=self.num_epochs)\n\n    def __getitem__(self, index: int) -&gt; T_co:\n        \"\"\"Get the sample at the given index.\"\"\"\n        if index not in range(len(self)):\n            raise IndexError(f\"Index {index} out of bounds for dataset of length {len(self)}.\")\n        return self.dataset[self._global_index_to_permuted_local_index(index)]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the resampled dataset.\"\"\"\n        return self.num_samples  # type: ignore\n\n    def _global_index_to_permuted_local_index(self, index: int) -&gt; EpochIndex:\n        \"\"\"Convert a global index to an epoch index.\"\"\"\n        epoch = index // len(self.dataset)\n        idx = index % len(self.dataset)\n        if self.shuffle:\n            idx = permute(idx, len(self.dataset), self.epoch_seeds[epoch])\n        return EpochIndex(epoch, idx)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.dataset","title":"<code>dataset</code>  <code>instance-attribute</code>","text":"<p>The dataset to resample. Must support indexing with an <code>EpochIndex</code>.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.num_epochs","title":"<code>num_epochs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The total number of epochs. The length of the resampled dataset will be len(dataset) * num_epochs.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.num_samples","title":"<code>num_samples = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The total number of samples to draw.</p> <p>The number of epochs will be determined by the number of samples and the length of the dataset.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.seed","title":"<code>seed = 42</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A random seed for reproducibility.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.shuffle","title":"<code>shuffle = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to shuffle the samples in the dataset each epoch.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get the sample at the given index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T_co:\n    \"\"\"Get the sample at the given index.\"\"\"\n    if index not in range(len(self)):\n        raise IndexError(f\"Index {index} out of bounds for dataset of length {len(self)}.\")\n    return self.dataset[self._global_index_to_permuted_local_index(index)]\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the resampled dataset.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the resampled dataset.\"\"\"\n    return self.num_samples  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Pre-shuffle each epoch's samples.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Pre-shuffle each epoch's samples.\"\"\"\n    if self.num_epochs is None and self.num_samples is None:\n        self.num_epochs = 1\n    elif self.num_epochs is not None and self.num_samples is not None:\n        raise ValueError(\"Only one of num_epochs and num_samples should be provided.\")\n\n    if self.num_epochs is None and self.num_samples is not None:\n        self.num_epochs = math.ceil(self.num_samples / len(self.dataset))\n\n    elif self.num_samples is None and self.num_epochs is not None:\n        self.num_samples = len(self.dataset) * self.num_epochs\n\n    # Type guard statements, the above if/elif block should ensure these are not None.\n    assert self.num_epochs is not None\n    assert self.num_samples is not None\n\n    if self.num_epochs &lt; 1:\n        raise ValueError(\"num_epochs must be at least 1.\")\n\n    rng = np.random.default_rng(self.seed)\n\n    # Initialize a vector of random seeds so that each epoch is shuffled differently.\n    self.epoch_seeds = rng.integers(0, np.iinfo(np.int32).max, size=self.num_epochs)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler._global_index_to_permuted_local_index","title":"<code>_global_index_to_permuted_local_index(index)</code>","text":"<p>Convert a global index to an epoch index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def _global_index_to_permuted_local_index(self, index: int) -&gt; EpochIndex:\n    \"\"\"Convert a global index to an epoch index.\"\"\"\n    epoch = index // len(self.dataset)\n    idx = index % len(self.dataset)\n    if self.shuffle:\n        idx = permute(idx, len(self.dataset), self.epoch_seeds[epoch])\n    return EpochIndex(epoch, idx)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper","title":"<code>MultiEpochDatasetWrapper</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset[U_co]</code>, <code>Generic[T, U_co]</code>, <code>ABC</code></p> <p>A wrapper to convert a standard pytorch dataset into one that supports multi-epoch megatron training.</p> <p>The underlying dataset's getitem method must be deterministic, i.e. it must return the same data for the same index every time it is called. If there are any non-deterministic operations, they should be moved to the <code>apply_transform</code> method. This method must also be deterministic for every (epoch, index) pair, but it can use the epoch to implement data augmentation each epoch.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@dataclass\nclass MultiEpochDatasetWrapper(Dataset[U_co], Generic[T, U_co], ABC):\n    \"\"\"A wrapper to convert a standard pytorch dataset into one that supports multi-epoch megatron training.\n\n    The underlying dataset's __getitem__ method must be deterministic, i.e. it must return the same data for the same\n    index every time it is called. If there are any non-deterministic operations, they should be moved to the\n    `apply_transform` method. This method must also be deterministic for every (epoch, index) pair, but it can use\n    the epoch to implement data augmentation each epoch.\n    \"\"\"\n\n    dataset: SizedDataset[T]\n    \"\"\"A deterministic dataset that supports indexing with an integer index.\"\"\"\n\n    @abstractmethod\n    def apply_transform(self, sample: T, index: EpochIndex) -&gt; U_co:\n        \"\"\"Apply any transformations to the sample for the given epoch.\"\"\"\n        raise NotImplementedError\n\n    def __getitem__(self, index: EpochIndex) -&gt; U_co:\n        \"\"\"Get the sample at the given epoch and index.\"\"\"\n        return self.apply_transform(self.dataset[index.idx], index)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.dataset","title":"<code>dataset</code>  <code>instance-attribute</code>","text":"<p>A deterministic dataset that supports indexing with an integer index.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get the sample at the given epoch and index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __getitem__(self, index: EpochIndex) -&gt; U_co:\n    \"\"\"Get the sample at the given epoch and index.\"\"\"\n    return self.apply_transform(self.dataset[index.idx], index)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.apply_transform","title":"<code>apply_transform(sample, index)</code>  <code>abstractmethod</code>","text":"<p>Apply any transformations to the sample for the given epoch.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@abstractmethod\ndef apply_transform(self, sample: T, index: EpochIndex) -&gt; U_co:\n    \"\"\"Apply any transformations to the sample for the given epoch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.SizedDataset","title":"<code>SizedDataset</code>","text":"<p>               Bases: <code>Protocol[T_co]</code></p> <p>A protocol for integer-indexed datasets that have a fixed length.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class SizedDataset(Protocol[T_co]):\n    \"\"\"A protocol for integer-indexed datasets that have a fixed length.\"\"\"\n\n    def __getitem__(self, index: int) -&gt; T_co:  # noqa: D105\n        ...\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/data/permute/","title":"Permute","text":""},{"location":"API_reference/bionemo/core/data/permute/#bionemo.core.data.permute.permute","title":"<code>permute(index, length, seed)</code>","text":"<p>Index into a permuted array with constant space and time complexity.</p> <p>This function permutes an index <code>i</code> into a range <code>[0, l)</code> using a hash function. See https://afnan.io/posts/2019-04-05-explaining-the-hashed-permutation/ for more details and \"Correlated Multi-Jittered Sampling\" by Andrew Kensler for the original algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index to permute.</p> required <code>length</code> <code>int</code> <p>The range of the permuted index.</p> required <code>seed</code> <code>int</code> <p>The permutation seed.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The permuted index in range(0, length).</p> Source code in <code>bionemo/core/data/permute.py</code> <pre><code>def permute(index: int, length: int, seed: int) -&gt; int:\n    \"\"\"Index into a permuted array with constant space and time complexity.\n\n    This function permutes an index `i` into a range `[0, l)` using a hash function. See\n    https://afnan.io/posts/2019-04-05-explaining-the-hashed-permutation/ for more details and\n    \"Correlated Multi-Jittered Sampling\" by Andrew Kensler for the original algorithm.\n\n    Args:\n        index: The index to permute.\n        length: The range of the permuted index.\n        seed: The permutation seed.\n\n    Returns:\n        The permuted index in range(0, length).\n    \"\"\"\n    if length &lt;= 1:\n        raise ValueError(\"The length of the permuted range must be greater than 1.\")\n\n    if index not in range(length):\n        raise ValueError(\"The index to permute must be in the range [0, l).\")\n\n    if seed &lt; 0:\n        raise ValueError(\"The permutation seed must be greater than or equal to 0.\")\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        w = length - 1\n        w |= w &gt;&gt; 1\n        w |= w &gt;&gt; 2\n        w |= w &gt;&gt; 4\n        w |= w &gt;&gt; 8\n        w |= w &gt;&gt; 16\n\n        while True:\n            index ^= seed\n            index *= 0xE170893D\n            index ^= seed &gt;&gt; 16\n            index ^= (index &amp; w) &gt;&gt; 4\n            index ^= seed &gt;&gt; 8\n            index *= 0x0929EB3F\n            index ^= seed &gt;&gt; 23\n            index ^= (index &amp; w) &gt;&gt; 1\n            index *= 1 | seed &gt;&gt; 27\n            index *= 0x6935FA69\n            index ^= (index &amp; w) &gt;&gt; 11\n            index *= 0x74DCB303\n            index ^= (index &amp; w) &gt;&gt; 2\n            index *= 0x9E501CC3\n            index ^= (index &amp; w) &gt;&gt; 2\n            index *= 0xC860A3DF\n            index &amp;= w\n            if index &lt; length:\n                break\n\n    return (index + seed) % length\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/","title":"Resamplers","text":""},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset","title":"<code>PRNGResampleDataset</code>","text":"<p>               Bases: <code>Dataset[T_co]</code></p> <p>A thread-safe dataset shuffler that uses a pseudo-random number generator (PRNG) to shuffle the dataset.</p> <p>PRNGResampleDataset shuffles a given dataset using a pseudo-random number generator (PRNG). This allows for reproducible shuffling by controlling the random seed, while not ever storing the list of indices in memory. It works by generating random indices assuming that the requesting function asks for them sequentially. Although random lookups are supported, random lookups will involve recomputing state which is slow, and involves linearly advancing from 0 if the last requested index was greater than or equal to this requested index. This should work well with the megatron sampler which is sequential. It handles skipped lookups as will happen with multiple workers by not generating those numbers.</p> <p>Prefer bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler</p> <p>This class performs sampling with replacement of an underlying dataset. It is recommended to use the epoch-based sampling provided by <code>bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler</code> instead, which ensures that each sample is seen exactly once per epoch. This dataset is useful for cases where the dataset is too large for the shuffled list of indices to fit in memory and exhaustive sampling is not required.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>class PRNGResampleDataset(Dataset[T_co]):\n    \"\"\"A thread-safe dataset shuffler that uses a pseudo-random number generator (PRNG) to shuffle the dataset.\n\n    PRNGResampleDataset shuffles a given dataset using a pseudo-random number generator (PRNG). This allows for\n    reproducible shuffling by controlling the random seed, while not ever storing the list of indices in memory. It\n    works by generating random indices assuming that the requesting function asks for them sequentially. Although random\n    lookups are supported, random lookups will involve recomputing state which is slow, and involves linearly advancing\n    from 0 if the last requested index was greater than or equal to this requested index. This should work well with the\n    megatron sampler which is sequential. It handles skipped lookups as will happen with multiple workers by not\n    generating those numbers.\n\n    !!! warning \"Prefer bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler\"\n\n        This class performs sampling with replacement of an underlying dataset. It is recommended to use the epoch-based\n        sampling provided by `bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler` instead, which ensures\n        that each sample is seen exactly once per epoch. This dataset is useful for cases where the dataset is too large\n        for the shuffled list of indices to fit in memory and exhaustive sampling is not required.\n    \"\"\"\n\n    def __init__(self, dataset: Dataset[T_co], seed: int = 42, num_samples: Optional[int] = None):\n        \"\"\"Initializes the PRNGResampleDataset.\n\n        Args:\n            dataset: The dataset to be shuffled.\n            seed: The seed value for the PRNG. Default is 42.\n            num_samples: The number of samples to draw from the dataset.\n                If None, the length of the dataset is used. Default is None.\n        \"\"\"\n        self.initial_seed = seed\n        self.rng = random.Random(seed)\n        self.dataset_len = len(dataset)  # type: ignore\n        self.num_samples = num_samples if num_samples is not None else len(dataset)\n        self.dataset = dataset\n        # Store the last accessed index. On this first pass this is initialized to infinity, which will trigger a reset since\n        #  index - inf &lt; 0 for all values of index. This will lead to `self.advance_state(index)` being called which will advance\n        #  the state to the correct starting index. The last_index will be then be replaced by `index` in that case and the algorithm\n        #  will proceed normally.\n        self.last_index: Union[int, math.inf] = math.inf\n        self.last_rand_index: Optional[int] = None\n\n    def rand_idx(self) -&gt; int:\n        \"\"\"Generates a random index within the range of the dataset size.\"\"\"\n        return self.rng.randint(0, self.dataset_len - 1)\n\n    def advance_state(self, num_to_advance: int):\n        \"\"\"Advances the PRNG state by generating n_to_advance random indices.\n\n        Args:\n            num_to_advance: The number of random state steps to advance.\n        \"\"\"\n        for _ in range(num_to_advance):\n            self.rand_idx()\n\n    def __getitem__(self, index: int) -&gt; T_co:\n        \"\"\"Returns the item from the dataset at the specified index.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            The item from the dataset at the specified index.\n\n        Note:\n            If the requested index is before the last accessed index, the PRNG state is reset to the initial seed\n            and advanced to the correct state. This is less efficient than advancing forward.\n        \"\"\"\n        idx_diff = index - self.last_index\n        if idx_diff &lt; 0:\n            # We need to go backwards (or it is the first call), which involves resetting to the initial seed and\n            #   then advancing to just before the correct index, which is accomplished with `range(index)`.\n            self.rng = random.Random(self.initial_seed)\n            self.advance_state(index)\n        elif idx_diff == 0:\n            # If the index is the same as the last index, we can just return the last random index that was generated.\n            #  no state needs to be updated in this case so just return.\n            return self.dataset[self.last_rand_index]\n        else:\n            # We need to advance however many steps were skipped since the last call. Since i+1 - i = 1, we need to advance\n            #  by `idx_diff - 1` to accomodate for skipped indices.\n            self.advance_state(idx_diff - 1)\n        self.last_index = index\n        self.last_rand_index = (\n            self.rand_idx()\n        )  # store the last index called incase the user wants to requrest this index again.\n        return self.dataset[self.last_rand_index]  # Advances state by 1\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n        return self.num_samples\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns the item from the dataset at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>T_co</code> <p>The item from the dataset at the specified index.</p> Note <p>If the requested index is before the last accessed index, the PRNG state is reset to the initial seed and advanced to the correct state. This is less efficient than advancing forward.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T_co:\n    \"\"\"Returns the item from the dataset at the specified index.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        The item from the dataset at the specified index.\n\n    Note:\n        If the requested index is before the last accessed index, the PRNG state is reset to the initial seed\n        and advanced to the correct state. This is less efficient than advancing forward.\n    \"\"\"\n    idx_diff = index - self.last_index\n    if idx_diff &lt; 0:\n        # We need to go backwards (or it is the first call), which involves resetting to the initial seed and\n        #   then advancing to just before the correct index, which is accomplished with `range(index)`.\n        self.rng = random.Random(self.initial_seed)\n        self.advance_state(index)\n    elif idx_diff == 0:\n        # If the index is the same as the last index, we can just return the last random index that was generated.\n        #  no state needs to be updated in this case so just return.\n        return self.dataset[self.last_rand_index]\n    else:\n        # We need to advance however many steps were skipped since the last call. Since i+1 - i = 1, we need to advance\n        #  by `idx_diff - 1` to accomodate for skipped indices.\n        self.advance_state(idx_diff - 1)\n    self.last_index = index\n    self.last_rand_index = (\n        self.rand_idx()\n    )  # store the last index called incase the user wants to requrest this index again.\n    return self.dataset[self.last_rand_index]  # Advances state by 1\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__init__","title":"<code>__init__(dataset, seed=42, num_samples=None)</code>","text":"<p>Initializes the PRNGResampleDataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[T_co]</code> <p>The dataset to be shuffled.</p> required <code>seed</code> <code>int</code> <p>The seed value for the PRNG. Default is 42.</p> <code>42</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the dataset. If None, the length of the dataset is used. Default is None.</p> <code>None</code> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __init__(self, dataset: Dataset[T_co], seed: int = 42, num_samples: Optional[int] = None):\n    \"\"\"Initializes the PRNGResampleDataset.\n\n    Args:\n        dataset: The dataset to be shuffled.\n        seed: The seed value for the PRNG. Default is 42.\n        num_samples: The number of samples to draw from the dataset.\n            If None, the length of the dataset is used. Default is None.\n    \"\"\"\n    self.initial_seed = seed\n    self.rng = random.Random(seed)\n    self.dataset_len = len(dataset)  # type: ignore\n    self.num_samples = num_samples if num_samples is not None else len(dataset)\n    self.dataset = dataset\n    # Store the last accessed index. On this first pass this is initialized to infinity, which will trigger a reset since\n    #  index - inf &lt; 0 for all values of index. This will lead to `self.advance_state(index)` being called which will advance\n    #  the state to the correct starting index. The last_index will be then be replaced by `index` in that case and the algorithm\n    #  will proceed normally.\n    self.last_index: Union[int, math.inf] = math.inf\n    self.last_rand_index: Optional[int] = None\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples in the dataset.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.advance_state","title":"<code>advance_state(num_to_advance)</code>","text":"<p>Advances the PRNG state by generating n_to_advance random indices.</p> <p>Parameters:</p> Name Type Description Default <code>num_to_advance</code> <code>int</code> <p>The number of random state steps to advance.</p> required Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def advance_state(self, num_to_advance: int):\n    \"\"\"Advances the PRNG state by generating n_to_advance random indices.\n\n    Args:\n        num_to_advance: The number of random state steps to advance.\n    \"\"\"\n    for _ in range(num_to_advance):\n        self.rand_idx()\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.rand_idx","title":"<code>rand_idx()</code>","text":"<p>Generates a random index within the range of the dataset size.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def rand_idx(self) -&gt; int:\n    \"\"\"Generates a random index within the range of the dataset size.\"\"\"\n    return self.rng.randint(0, self.dataset_len - 1)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resource/","title":"Resource","text":""},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class that represents a remote resource for downloading and caching test data.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>class Resource(pydantic.BaseModel):\n    \"\"\"Class that represents a remote resource for downloading and caching test data.\"\"\"\n\n    model_config = pydantic.ConfigDict(use_attribute_docstrings=True)\n\n    tag: Annotated[str, pydantic.StringConstraints(pattern=r\"^[^/]*/[^/]*$\")]  # Only slash between filename and tag.\n    \"\"\"A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").\"\"\"\n\n    ngc: Annotated[str, pydantic.AfterValidator(_validate_ngc_resource)] | None = None\n    \"\"\"The NGC URL for the resource.\n\n    Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.\n    \"\"\"\n\n    ngc_registry: Literal[\"model\", \"resource\"] | None = None\n    \"\"\"The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.\"\"\"\n\n    pbss: Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(allowed_schemes=[\"s3\"])]\n    \"\"\"The PBSS (NVIDIA-internal) URL of the resource.\"\"\"\n\n    sha256: str | None\n    \"\"\"The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).\"\"\"\n\n    owner: pydantic.NameEmail\n    \"\"\"The owner or primary point of contact for the resource, in the format \"Name &lt;email&gt;\".\"\"\"\n\n    description: str | None = None\n    \"\"\"A description of the file(s).\"\"\"\n\n    unpack: Literal[False, None] = None\n    \"\"\"Whether the resource should be unpacked after download. If None, will defer to the file extension.\"\"\"\n\n    decompress: Literal[False, None] = None\n    \"\"\"Whether the resource should be decompressed after download. If None, will defer to the file extension.\"\"\"\n\n    @pydantic.model_validator(mode=\"after\")\n    def _validate_ngc_registry(self):\n        if self.ngc and not self.ngc_registry:\n            raise ValueError(f\"ngc_registry must be provided if ngc is not None: {self.tag}\")\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.decompress","title":"<code>decompress = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be decompressed after download. If None, will defer to the file extension.</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A description of the file(s).</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.ngc","title":"<code>ngc = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC URL for the resource.</p> <p>Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.ngc_registry","title":"<code>ngc_registry = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.owner","title":"<code>owner</code>  <code>instance-attribute</code>","text":"<p>The owner or primary point of contact for the resource, in the format \"Name \"."},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.pbss","title":"<code>pbss</code>  <code>instance-attribute</code>","text":"<p>The PBSS (NVIDIA-internal) URL of the resource.</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.sha256","title":"<code>sha256</code>  <code>instance-attribute</code>","text":"<p>The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.tag","title":"<code>tag</code>  <code>instance-attribute</code>","text":"<p>A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.Resource.unpack","title":"<code>unpack = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be unpacked after download. If None, will defer to the file extension.</p>"},{"location":"API_reference/bionemo/core/data/resource/#bionemo.core.data.resource.get_all_resources","title":"<code>get_all_resources(resource_path=None)</code>  <code>cached</code>","text":"<p>Return a dictionary of all resources.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>@functools.cache\ndef get_all_resources(resource_path: Path | None = None) -&gt; dict[str, Resource]:\n    \"\"\"Return a dictionary of all resources.\"\"\"\n    if not resource_path:\n        resource_path = Path(files(\"bionemo.core.data\").joinpath(\"resources\"))  # type: ignore\n\n    resources_files = itertools.chain(resource_path.glob(\"*.yaml\"), resource_path.glob(\"*.yml\"))\n\n    all_resources = [resource for file in resources_files for resource in _parse_resource_file(file)]\n\n    resource_list = pydantic.TypeAdapter(list[Resource]).validate_python(all_resources)\n    resource_dict = {resource.tag: resource for resource in resource_list}\n\n    if len(resource_dict) != len(resource_list):\n        # Show the # of and which ones are duplicated so that a user can begin debugging and resolve the issue.\n        tag_counts = Counter([resource.tag for resource in resource_list])\n        raise ValueError(f\"Duplicate resource tags found!: {[tag for tag, count in tag_counts.items() if count &gt; 1]}\")\n\n    return resource_dict\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/","title":"Config","text":""},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.LossType","title":"<code>LossType = TypeVar('LossType')</code>  <code>module-attribute</code>","text":"<p>Stand-in for a loss function; no constraints.</p>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.ModelOutput","title":"<code>ModelOutput = TypeVar('ModelOutput', Tensor, list[Tensor], tuple[Tensor], dict[str, Tensor], covariant=True)</code>  <code>module-attribute</code>","text":"<p>A Model's forward pass may produce a tensor, multiple tensors, or named tensors.</p>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.ModelType","title":"<code>ModelType = TypeVar('ModelType', bound=Model)</code>  <code>module-attribute</code>","text":"<p>Generic type for things that have a forward pass.</p>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoModelConfig","title":"<code>BionemoModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType]</code>, <code>ABC</code></p> <p>An abstract class for model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoModelConfig(Generic[ModelType], ABC):\n    \"\"\"An abstract class for model configuration.\"\"\"\n\n    @abstractmethod\n    def configure_model(self, *args, **kwargs) -&gt; Model:\n        \"\"\"Configures the model.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoModelConfig.configure_model","title":"<code>configure_model(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Configures the model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef configure_model(self, *args, **kwargs) -&gt; Model:\n    \"\"\"Configures the model.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoTrainableModelConfig","title":"<code>BionemoTrainableModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType, LossType]</code>, <code>BionemoModelConfig[ModelType]</code></p> <p>An abstract class for trainable model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoTrainableModelConfig(Generic[ModelType, LossType], BionemoModelConfig[ModelType]):\n    \"\"\"An abstract class for trainable model configuration.\"\"\"\n\n    @abstractmethod\n    def get_loss_reduction_class(self) -&gt; Type[LossType]:\n        \"\"\"Returns the loss reduction class.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoTrainableModelConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>  <code>abstractmethod</code>","text":"<p>Returns the loss reduction class.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef get_loss_reduction_class(self) -&gt; Type[LossType]:\n    \"\"\"Returns the loss reduction class.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Protocol[ModelOutput]</code></p> <p>Lightweight interface for a model: must have a forward method.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class Model(Protocol[ModelOutput]):\n    \"\"\"Lightweight interface for a model: must have a forward method.\"\"\"\n\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        \"\"\"Prediction / forward-step for a model.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.Model.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Prediction / forward-step for a model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; ModelOutput:\n    \"\"\"Prediction / forward-step for a model.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/batching_utils/","title":"Batching utils","text":""},{"location":"API_reference/bionemo/core/utils/batching_utils/#bionemo.core.utils.batching_utils.pad_token_ids","title":"<code>pad_token_ids(token_ids, padding_value=0, padding_len=None, pad_size_divisible_by=1, **convert_to_kwargs)</code>","text":"<p>Pads token ids with padding value, and return the padded tokens and the corresponding mask.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>Union[List[int], List[Tensor]]</code> <p>List of token ids or tensors</p> required <code>padding_value</code> <code>int</code> <p>Value to pad with. Defaults to 0.</p> <code>0</code> <code>padding_len</code> <code>Optional[int]</code> <p>Max length of the padded token ids. Defaults to None.</p> <code>None</code> <code>pad_size_divisible_by</code> <code>int</code> <p>Pad the length of the token ids to be divisible by this number. Defaults to 1.</p> <code>1</code> <code>**convert_to_kwargs</code> <p>Passed directly to tensor.to(**kwargs) if provided</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[List[int], List[int]]: Padded token ids and mask</p> Source code in <code>bionemo/core/utils/batching_utils.py</code> <pre><code>def pad_token_ids(\n    token_ids: Union[List[int], List[torch.Tensor]],\n    padding_value: int = 0,\n    padding_len: Optional[int] = None,\n    pad_size_divisible_by: int = 1,\n    **convert_to_kwargs,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Pads token ids with padding value, and return the padded tokens and the corresponding mask.\n\n    Args:\n        token_ids: List of token ids or tensors\n        padding_value: Value to pad with. Defaults to 0.\n        padding_len: Max length of the padded token ids. Defaults to None.\n        pad_size_divisible_by: Pad the length of the token ids to be divisible by this number. Defaults to 1.\n        **convert_to_kwargs: Passed directly to tensor.to(**kwargs) if provided\n\n    Returns:\n        Tuple[List[int], List[int]]: Padded token ids and mask\n    \"\"\"\n    lengths = torch.tensor([len(s) for s in token_ids])\n    if padding_len is None:\n        padding_len = lengths.max()\n\n    # make padding divisible by pad_size_divisible_by\n    if pad_size_divisible_by &gt; 1:\n        padding_len = int(math.ceil(padding_len / pad_size_divisible_by) * pad_size_divisible_by)\n\n    # build mask\n    mask = torch.arange(padding_len)[None, :] &lt; lengths[:, None]\n\n    # make sure all sequences are pytorch tensors\n    token_ids = [torch.tensor(s) if not torch.is_tensor(s) else s for s in token_ids]\n    # pad sequences\n    masked_token_ids = torch.nn.utils.rnn.pad_sequence(token_ids, batch_first=True, padding_value=padding_value)\n\n    # convert to desired device\n    if len(convert_to_kwargs):\n        mask = mask.to(**convert_to_kwargs)\n        masked_token_ids = masked_token_ids.to(**convert_to_kwargs)\n\n    # Further pad the sequences to the fixed maximum length, if necessary\n    if masked_token_ids.size(1) &lt; padding_len:\n        padding_size = padding_len - masked_token_ids.size(1)\n        masked_token_ids = torch.nn.functional.pad(masked_token_ids, [0, padding_size], value=padding_value)\n\n    return masked_token_ids, mask\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/dtypes/","title":"Dtypes","text":""},{"location":"API_reference/bionemo/core/utils/dtypes/#bionemo.core.utils.dtypes.get_autocast_dtype","title":"<code>get_autocast_dtype(precision)</code>","text":"<p>Returns the torch dtype corresponding to the given precision.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>PrecisionTypes</code> <p>The precision type.</p> required <p>Returns:</p> Type Description <code>dtype</code> <p>torch.dtype: The torch dtype corresponding to the given precision.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the precision is not supported.</p> Source code in <code>bionemo/core/utils/dtypes.py</code> <pre><code>def get_autocast_dtype(precision: PrecisionTypes) -&gt; torch.dtype:\n    \"\"\"Returns the torch dtype corresponding to the given precision.\n\n    Args:\n        precision: The precision type.\n\n    Returns:\n        torch.dtype: The torch dtype corresponding to the given precision.\n\n    Raises:\n        ValueError: If the precision is not supported.\n    \"\"\"\n    # TODO move this to a utilities folder, or find/import the function that does this in NeMo\n    if precision == \"fp16\":\n        return torch.float16\n    elif precision == \"bf16\":\n        return torch.bfloat16\n    elif precision == \"fp32\":\n        return torch.float32\n    elif precision == \"16-mixed\":\n        return torch.float16\n    elif precision == \"fp16-mixed\":\n        return torch.float16\n    elif precision == \"bf16-mixed\":\n        return torch.bfloat16\n    elif precision == \"fp32-mixed\":\n        return torch.float32\n    elif precision == 16:\n        return torch.float16\n    elif precision == 32:\n        return torch.float32\n    else:\n        raise ValueError(f\"Unsupported precision: {precision}\")\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/random_utils/","title":"Random utils","text":""},{"location":"API_reference/bionemo/core/utils/random_utils/#bionemo.core.utils.random_utils.get_seed_from_rng","title":"<code>get_seed_from_rng(rng, dtype=np.int64)</code>","text":"<p>Generates a deterministic random seed from an existing random generator.</p> <p>This is useful in particular because setting the torch seed doesn't want to accept a tuple of numbers, we we often do in initializing a numpy random generator with epoch, index, and global seeds.</p> <p>Used to seed a torch random generator from a numpy random generator.</p> Source code in <code>bionemo/core/utils/random_utils.py</code> <pre><code>def get_seed_from_rng(rng: np.random.Generator, dtype: Type[np.signedinteger] = np.int64) -&gt; int:\n    \"\"\"Generates a deterministic random seed from an existing random generator.\n\n    This is useful in particular because setting the torch seed doesn't want to accept a tuple of numbers, we we often\n    do in initializing a numpy random generator with epoch, index, and global seeds.\n\n    Used to seed a torch random generator from a numpy random generator.\n    \"\"\"\n    return int(rng.integers(np.iinfo(dtype).max))\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/random_utils/#bionemo.core.utils.random_utils.random_numpy_context","title":"<code>random_numpy_context(seed=42)</code>","text":"<p>Context manager for setting numpy random state.</p> <p>The state is saved on entry and restored on exit to what it was. This way you can run code that needs random state in a <code>with</code> context using this function, and get back to whatever state was there before. This is useful for testing where you don't want the random state from one test to impact other tests.</p> Example <p>import numpy as np from bionemo.core.utils.random_utils import random_numpy_context ori_state = np.random.get_state() with random_numpy_context(45):     np.random.randint(5) # this will change the state new_state = np.random.get_state() assert ori_state == new_state</p> Source code in <code>bionemo/core/utils/random_utils.py</code> <pre><code>@contextmanager\ndef random_numpy_context(seed: int = 42) -&gt; Iterator[None]:\n    \"\"\"Context manager for setting numpy random state.\n\n    The state is saved on entry and restored on exit to what it was. This way you can run code that needs random state\n    in a `with` context using this function, and get back to whatever state was there before. This is useful for testing\n    where you don't want the random state from one test to impact other tests.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from bionemo.core.utils.random_utils import random_numpy_context\n        &gt;&gt;&gt; ori_state = np.random.get_state()\n        &gt;&gt;&gt; with random_numpy_context(45):\n            np.random.randint(5) # this will change the state\n        &gt;&gt;&gt; new_state = np.random.get_state()\n        &gt;&gt;&gt; assert ori_state == new_state\n    \"\"\"\n    state = np.random.get_state()  # just fail if this fails\n    try:\n        np.random.seed(seed)\n        yield\n    finally:\n        np.random.set_state(state)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/","title":"Api","text":""},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Config","title":"<code>ESM2Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig</code>, <code>IOMixinWithGettersSetters</code></p> <p>Configuration class for ESM2 model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2Config(ESM2GenericConfig, iom.IOMixinWithGettersSetters):\n    \"\"\"Configuration class for ESM2 model.\"\"\"\n\n    model_cls: Type[ESM2Model] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2GenericConfig","title":"<code>ESM2GenericConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[ESM2ModelT, MegatronLossType]</code></p> <p>Configuration class for ESM2 model.</p> <p>Attributes:</p> Name Type Description <code>num_layers</code> <code>int</code> <p>Number of layers in the model.</p> <code>hidden_size</code> <code>int</code> <p>Hidden size of the model.</p> <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads in the model.</p> <code>ffn_hidden_size</code> <code>int</code> <p>Hidden size of the feed-forward network.</p> <code>hidden_dropout</code> <code>float</code> <p>Dropout rate for hidden layers.</p> <code>attention_dropout</code> <code>float</code> <p>Dropout rate for attention layers.</p> <code>apply_residual_connection_post_layernorm</code> <code>bool</code> <p>Whether to apply residual connection after layer normalization.</p> <code>layernorm_epsilon</code> <code>float</code> <p>Epsilon value for layer normalization.</p> <code>layernorm_zero_centered_gamma</code> <code>float</code> <p>Whether to zero-center the gamma parameter in layer normalization.</p> <code>activation_func</code> <code>Callable</code> <p>Activation function used in the model.</p> <code>init_method_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>apply_query_key_layer_scaling</code> <code>float</code> <p>Whether to apply scaling to query and key layers.</p> <code>masked_softmax_fusion</code> <code>bool</code> <p>Whether to use a kernel that fuses attention softmax with its mask.</p> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>Whether to share embeddings and output weights.</p> <code>enable_autocast</code> <code>bool</code> <p>Whether to enable autocast for mixed precision.</p> <code>biobert_spec_option</code> <code>BiobertSpecOption</code> <p>BiobertSpecOption for the model.</p> <code>position_embedding_type</code> <code>PositionEmbeddingKinds</code> <p>Type of position embedding used in the model.</p> <code>seq_length</code> <code>int</code> <p>Length of the input sequence.</p> <code>make_vocab_size_divisible_by</code> <code>int</code> <p>Make the vocabulary size divisible by this value.</p> <code>token_dropout</code> <code>bool</code> <p>Whether to apply token dropout.</p> <code>use_attention_mask</code> <code>bool</code> <p>Whether to use attention mask.</p> <code>use_esm_attention</code> <code>bool</code> <p>Whether to use ESM attention.</p> <code>attention_softmax_in_fp32</code> <code>bool</code> <p>Whether to use fp32 for attention softmax.</p> <code>optimizer_fn</code> <code>Optional[Callable[[MegatronBioBertModel], Optimizer]]</code> <p>Optional optimizer function for the model.</p> <code>parallel_output</code> <code>bool</code> <p>Whether to use parallel output.</p> <code>rotary_base</code> <code>int</code> <p>Base value for rotary positional encoding.</p> <code>rotary_percent</code> <code>float</code> <p>Percentage of rotary positional encoding.</p> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length.</p> <code>get_attention_mask_from_fusion</code> <code>bool</code> <p>Whether to get attention mask from fusion.</p> <code>nemo1_ckpt_path</code> <code>str | None</code> <p>Path to NEMO1 checkpoint.</p> <code>return_only_hidden_states</code> <code>bool</code> <p>Whether to return only hidden states.</p> <code>loss_reduction_class</code> <code>Type[MegatronLossType]</code> <p>Loss reduction class for the model. Default to BERTMLMLossWithReduction.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2GenericConfig(BioBertConfig[ESM2ModelT, MegatronLossType]):\n    \"\"\"Configuration class for ESM2 model.\n\n    Attributes:\n        num_layers: Number of layers in the model.\n        hidden_size: Hidden size of the model.\n        num_attention_heads: Number of attention heads in the model.\n        ffn_hidden_size: Hidden size of the feed-forward network.\n        hidden_dropout: Dropout rate for hidden layers.\n        attention_dropout: Dropout rate for attention layers.\n        apply_residual_connection_post_layernorm: Whether to apply residual connection after layer normalization.\n        layernorm_epsilon: Epsilon value for layer normalization.\n        layernorm_zero_centered_gamma: Whether to zero-center the gamma parameter in layer normalization.\n        activation_func: Activation function used in the model.\n        init_method_std: Standard deviation for weight initialization.\n        apply_query_key_layer_scaling: Whether to apply scaling to query and key layers.\n        masked_softmax_fusion: Whether to use a kernel that fuses attention softmax with its mask.\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        share_embeddings_and_output_weights: Whether to share embeddings and output weights.\n        enable_autocast: Whether to enable autocast for mixed precision.\n        biobert_spec_option: BiobertSpecOption for the model.\n        position_embedding_type: Type of position embedding used in the model.\n        seq_length: Length of the input sequence.\n        make_vocab_size_divisible_by: Make the vocabulary size divisible by this value.\n        token_dropout: Whether to apply token dropout.\n        use_attention_mask: Whether to use attention mask.\n        use_esm_attention: Whether to use ESM attention.\n        attention_softmax_in_fp32: Whether to use fp32 for attention softmax.\n        optimizer_fn: Optional optimizer function for the model.\n        parallel_output: Whether to use parallel output.\n        rotary_base: Base value for rotary positional encoding.\n        rotary_percent: Percentage of rotary positional encoding.\n        seq_len_interpolation_factor: Interpolation factor for sequence length.\n        get_attention_mask_from_fusion: Whether to get attention mask from fusion.\n        nemo1_ckpt_path: Path to NEMO1 checkpoint.\n        return_only_hidden_states: Whether to return only hidden states.\n        loss_reduction_class: Loss reduction class for the model. Default to BERTMLMLossWithReduction.\n    \"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[ESM2ModelT] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n    num_attention_heads: int = 20\n    ffn_hidden_size: int = 4 * 1280  # Transformer FFN hidden size. Usually 4 * hidden_size.\n    hidden_dropout: float = 0  # ESM2 removes dropout from hidden layers and attention\n    attention_dropout: float = 0.0  # ESM2 does not use attention dropout\n    apply_residual_connection_post_layernorm: bool = False  # TODO: farhadr False is new default, True was BERT pub.\n    layernorm_epsilon: float = 1.0e-5\n    bias_activation_fusion: bool = True  # True degrades accuracy slightly, but is faster.\n    activation_func: Callable = F.gelu  # esm_gelu_func  # ESM2 MLP\n    init_method_std: float = 0.02\n    softmax_scale: float = 1.0\n\n    # embedding\n    token_dropout: bool = True\n    use_attention_mask: bool = True\n\n    # core attention\n    use_esm_attention: bool = False  # Skip ESM2 custom attention for TE acceleration. Still passes golden value test.\n    attention_softmax_in_fp32: bool = False\n    normalize_attention_scores: bool = False\n\n    # From megatron.core.models.gpt.bert_model.GPTModel\n    fp16_lm_cross_entropy: bool = False  # Move the cross entropy unreduced loss calculation for lm head to fp16\n    parallel_output: bool = True\n    share_embeddings_and_output_weights: bool = True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: PositionEmbeddingKinds = \"rope\"  # ESM2 uses relative positional encoding 'ROPE' to extrapolate to longer sequences unseen during training\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec\n\n    optimizer_fn: Optional[Callable[[MegatronBioBertModel], Optimizer]] = None\n    # TODO (@skothenhill,@georgea) update to use the nemo2 checkpoint mixins\n    #  support HF (requires weight interleaving on qkv layer) and nemo1 checkpoints ideally.\n    nemo1_ckpt_path: str | None = None\n    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n    #  self.override_parent_fields will be loaded from the checkpoint and override those values here.\n    initial_ckpt_path: str | None = None\n    # TODO (@jstjohn) come up with a cleaner way in the biobert module to return user requested\n    #  things as part of the workflow for inference and fine-tuning.\n    return_embeddings: bool = False\n    include_embeddings: bool = False\n    include_input_ids: bool = False\n    skip_logits: bool = False\n    return_only_hidden_states: bool = False  # return logits\n\n    def __post_init__(self):\n        # TODO, as a validator?\n        \"\"\"Check configuration compatibility.\"\"\"\n        # reset moe_token_dispatcher_type when variable_seq_lengths is True.\n        # must be performed before super().__post_init__()\n        if self.variable_seq_lengths and self.moe_token_dispatcher_type in [\"allgather\", \"alltoall_seq\"]:\n            logging.warning(\n                \"MoE token dispatcher type 'allgather' and 'alltoall_seq' are not supported with variable sequence lengths. Setting moe_token_dispatcher_type to 'alltoall'.\"\n            )\n            self.moe_token_dispatcher_type = \"alltoall\"\n\n        # reset apply_query_key_layer_scaling based on biobert_spec_option\n        super().__post_init__()\n        if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            self.apply_query_key_layer_scaling = False\n        elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n            logging.warning(\n                \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n            )\n            self.apply_query_key_layer_scaling = True\n        else:\n            raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2GenericConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check configuration compatibility.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __post_init__(self):\n    # TODO, as a validator?\n    \"\"\"Check configuration compatibility.\"\"\"\n    # reset moe_token_dispatcher_type when variable_seq_lengths is True.\n    # must be performed before super().__post_init__()\n    if self.variable_seq_lengths and self.moe_token_dispatcher_type in [\"allgather\", \"alltoall_seq\"]:\n        logging.warning(\n            \"MoE token dispatcher type 'allgather' and 'alltoall_seq' are not supported with variable sequence lengths. Setting moe_token_dispatcher_type to 'alltoall'.\"\n        )\n        self.moe_token_dispatcher_type = \"alltoall\"\n\n    # reset apply_query_key_layer_scaling based on biobert_spec_option\n    super().__post_init__()\n    if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n        self.apply_query_key_layer_scaling = False\n    elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n        logging.warning(\n            \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n        )\n        self.apply_query_key_layer_scaling = True\n    else:\n        raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Model","title":"<code>ESM2Model</code>","text":"<p>               Bases: <code>MegatronBioBertModel</code></p> <p>ESM2 Transformer language model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>class ESM2Model(MegatronBioBertModel):\n    \"\"\"ESM2 Transformer language model.\"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        num_tokentypes: int,\n        transformer_layer_spec: spec_utils.ModuleSpec,\n        vocab_size: int,\n        max_sequence_length: int,\n        tokenizer: Optional[BioNeMoESMTokenizer] = None,\n        pre_process: bool = True,\n        post_process: bool = True,\n        fp16_lm_cross_entropy: bool = False,\n        parallel_output: bool = True,\n        share_embeddings_and_output_weights: bool = False,\n        position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n        rotary_percent: float = 1.0,\n        seq_len_interpolation_factor: Optional[float] = None,\n        add_binary_head: bool = True,\n        return_embeddings: bool = False,\n        include_embeddings: bool = False,\n        include_input_ids: bool = False,\n        use_full_attention_mask: bool = False,\n        include_hiddens: bool = False,\n        skip_logits: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2 model.\n\n        Args:\n            config (TransformerConfig): transformer config\n            num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n            transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n            vocab_size (int): vocabulary size\n            max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n            tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n            pre_process (bool): Include embedding layer (used with pipeline parallelism)\n            post_process (bool): Include an output layer (used with pipeline parallelism)\n            fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n            parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n            share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n            position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n                Defaults is 'learned_absolute'.\n            rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n                Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n            seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n            add_binary_head (bool): Whether to add a binary head. Defaults to True.\n            return_embeddings (bool): Whether to return embeddings. Defaults to False.\n            include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n            include_input_ids (bool): Whether to include input_ids in the output dictionary. Defaults to False.\n            use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n            include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n            skip_logits (bool): Skip writing the token logits in output dict\n        \"\"\"\n        super(MegatronBioBertModel, self).__init__(config=config)\n        self.post_process = post_process\n        self.add_binary_head = add_binary_head\n        if return_embeddings:\n            assert self.post_process, \"only return embeddings on the last pipeline stage\"\n        # `b` = batch, `s` = sequence.\n        # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n        #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n        self.use_full_attention_mask = use_full_attention_mask\n        self.config: TransformerConfig = config\n        self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.pre_process = pre_process\n        self.post_process = post_process\n        self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n        self.parallel_output = parallel_output\n        self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n        self.position_embedding_type = position_embedding_type\n        self.add_binary_head = add_binary_head\n        self.return_embeddings = return_embeddings\n        self.include_embeddings = include_embeddings\n        self.include_hiddens = include_hiddens\n        self.include_input_ids = include_input_ids\n        self.skip_logits = skip_logits\n\n        # megatron core pipelining currently depends on model type\n        self.model_type = ModelType.encoder_or_decoder\n\n        # Embeddings.\n        if self.pre_process:\n            self.register_buffer(\n                \"bert_position_id_tensor\",\n                torch.arange(max_sequence_length, dtype=torch.long, requires_grad=False).unsqueeze(0),\n                persistent=False,\n            )\n            # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n            # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n            # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n            self.embedding = ESM2Embedding(\n                config=self.config,\n                vocab_size=self.vocab_size,\n                max_sequence_length=self.max_sequence_length,\n                position_embedding_type=position_embedding_type,\n                num_tokentypes=num_tokentypes,\n                # ESM2 NEW ARGS\n                token_dropout=self.config.token_dropout,\n                use_attention_mask=self.config.use_attention_mask,\n                mask_token_id=tokenizer.mask_token_id,\n            )\n\n        if self.position_embedding_type == \"rope\":\n            self.rotary_pos_emb = RotaryEmbedding(\n                kv_channels=self.config.kv_channels,\n                rotary_percent=rotary_percent,\n                rotary_interleaved=self.config.rotary_interleaved,\n                seq_len_interpolation_factor=seq_len_interpolation_factor,\n            )\n\n        # Transformer.\n        self.encoder = TransformerBlock(\n            config=self.config,\n            spec=self.transformer_layer_spec,\n            pre_process=self.pre_process,\n            post_process=self.post_process,\n        )\n\n        # Output\n        if post_process:\n            # TODO: Make sure you are passing in the mpu_vocab_size properly\n            self.lm_head = BertLMHead(\n                config.hidden_size,\n                config,\n            )\n\n            self.output_layer = tensor_parallel.ColumnParallelLinear(\n                config.hidden_size,\n                self.vocab_size,\n                config=config,\n                init_method=config.init_method,\n                bias=True,\n                skip_bias_add=False,\n                gather_output=not self.parallel_output,\n                skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n            )\n\n            self.binary_head = None\n            if self.add_binary_head:\n                # TODO: Shoudl switch this to TE ?\n                self.binary_head = get_linear_layer(\n                    config.hidden_size, 2, config.init_method, config.perform_initialization\n                )\n\n                self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n        if self.pre_process or self.post_process:\n            self.setup_embeddings_and_output_layer()\n\n    def embedding_forward(\n        self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n    ):\n        \"\"\"Forward pass of the embedding layer.\n\n        Args:\n            input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n            position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n            tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n            attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n        Returns:\n            Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n        \"\"\"\n        # ESM2 Customization: ESM2Embedding forward takes attention_mask\n        # in addition to the args required by LanguageModelEmbedding\n        return self.embedding(\n            input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n        )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Model.__init__","title":"<code>__init__(config, num_tokentypes, transformer_layer_spec, vocab_size, max_sequence_length, tokenizer=None, pre_process=True, post_process=True, fp16_lm_cross_entropy=False, parallel_output=True, share_embeddings_and_output_weights=False, position_embedding_type='learned_absolute', rotary_percent=1.0, seq_len_interpolation_factor=None, add_binary_head=True, return_embeddings=False, include_embeddings=False, include_input_ids=False, use_full_attention_mask=False, include_hiddens=False, skip_logits=False)</code>","text":"<p>Initialize the ESM2 model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>transformer config</p> required <code>num_tokentypes</code> <code>int</code> <p>Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.</p> required <code>transformer_layer_spec</code> <code>ModuleSpec</code> <p>Specifies module to use for transformer layers</p> required <code>vocab_size</code> <code>int</code> <p>vocabulary size</p> required <code>max_sequence_length</code> <code>int</code> <p>maximum size of sequence. This is used for positional embedding</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>optional tokenizer object (currently only used in the constructor of ESM2Model)</p> <code>None</code> <code>pre_process</code> <code>bool</code> <p>Include embedding layer (used with pipeline parallelism)</p> <code>True</code> <code>post_process</code> <code>bool</code> <p>Include an output layer (used with pipeline parallelism)</p> <code>True</code> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>False</code> <code>parallel_output</code> <code>bool</code> <p>Do not gather the outputs, keep them split across tensor parallel ranks</p> <code>True</code> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>When True, input embeddings and output logit weights are shared. Defaults to False.</p> <code>False</code> <code>position_embedding_type</code> <code>string</code> <p>Position embedding type. Options ['learned_absolute', 'rope']. Defaults is 'learned_absolute'.</p> <code>'learned_absolute'</code> <code>rotary_percent</code> <code>float</code> <p>Percent of rotary dimension to use for rotary position embeddings. Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.</p> <code>1.0</code> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length. Defaults to None.</p> <code>None</code> <code>add_binary_head</code> <code>bool</code> <p>Whether to add a binary head. Defaults to True.</p> <code>True</code> <code>return_embeddings</code> <code>bool</code> <p>Whether to return embeddings. Defaults to False.</p> <code>False</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the output dictionary. Defaults to False.</p> <code>False</code> <code>include_input_ids</code> <code>bool</code> <p>Whether to include input_ids in the output dictionary. Defaults to False.</p> <code>False</code> <code>use_full_attention_mask</code> <code>bool</code> <p>Whether to use full attention mask. Defaults to False.</p> <code>False</code> <code>include_hiddens</code> <code>bool</code> <p>Whether to include hidden states in the output dictionary. Defaults to False.</p> <code>False</code> <code>skip_logits</code> <code>bool</code> <p>Skip writing the token logits in output dict</p> <code>False</code> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    num_tokentypes: int,\n    transformer_layer_spec: spec_utils.ModuleSpec,\n    vocab_size: int,\n    max_sequence_length: int,\n    tokenizer: Optional[BioNeMoESMTokenizer] = None,\n    pre_process: bool = True,\n    post_process: bool = True,\n    fp16_lm_cross_entropy: bool = False,\n    parallel_output: bool = True,\n    share_embeddings_and_output_weights: bool = False,\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n    rotary_percent: float = 1.0,\n    seq_len_interpolation_factor: Optional[float] = None,\n    add_binary_head: bool = True,\n    return_embeddings: bool = False,\n    include_embeddings: bool = False,\n    include_input_ids: bool = False,\n    use_full_attention_mask: bool = False,\n    include_hiddens: bool = False,\n    skip_logits: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the ESM2 model.\n\n    Args:\n        config (TransformerConfig): transformer config\n        num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n        vocab_size (int): vocabulary size\n        max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n        tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n        pre_process (bool): Include embedding layer (used with pipeline parallelism)\n        post_process (bool): Include an output layer (used with pipeline parallelism)\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n        share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n        position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n            Defaults is 'learned_absolute'.\n        rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n            Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n        seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n        add_binary_head (bool): Whether to add a binary head. Defaults to True.\n        return_embeddings (bool): Whether to return embeddings. Defaults to False.\n        include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n        include_input_ids (bool): Whether to include input_ids in the output dictionary. Defaults to False.\n        use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n        include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n        skip_logits (bool): Skip writing the token logits in output dict\n    \"\"\"\n    super(MegatronBioBertModel, self).__init__(config=config)\n    self.post_process = post_process\n    self.add_binary_head = add_binary_head\n    if return_embeddings:\n        assert self.post_process, \"only return embeddings on the last pipeline stage\"\n    # `b` = batch, `s` = sequence.\n    # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n    #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n    self.use_full_attention_mask = use_full_attention_mask\n    self.config: TransformerConfig = config\n    self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n    self.vocab_size = vocab_size\n    self.max_sequence_length = max_sequence_length\n    self.pre_process = pre_process\n    self.post_process = post_process\n    self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n    self.parallel_output = parallel_output\n    self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n    self.position_embedding_type = position_embedding_type\n    self.add_binary_head = add_binary_head\n    self.return_embeddings = return_embeddings\n    self.include_embeddings = include_embeddings\n    self.include_hiddens = include_hiddens\n    self.include_input_ids = include_input_ids\n    self.skip_logits = skip_logits\n\n    # megatron core pipelining currently depends on model type\n    self.model_type = ModelType.encoder_or_decoder\n\n    # Embeddings.\n    if self.pre_process:\n        self.register_buffer(\n            \"bert_position_id_tensor\",\n            torch.arange(max_sequence_length, dtype=torch.long, requires_grad=False).unsqueeze(0),\n            persistent=False,\n        )\n        # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n        # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n        # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n        self.embedding = ESM2Embedding(\n            config=self.config,\n            vocab_size=self.vocab_size,\n            max_sequence_length=self.max_sequence_length,\n            position_embedding_type=position_embedding_type,\n            num_tokentypes=num_tokentypes,\n            # ESM2 NEW ARGS\n            token_dropout=self.config.token_dropout,\n            use_attention_mask=self.config.use_attention_mask,\n            mask_token_id=tokenizer.mask_token_id,\n        )\n\n    if self.position_embedding_type == \"rope\":\n        self.rotary_pos_emb = RotaryEmbedding(\n            kv_channels=self.config.kv_channels,\n            rotary_percent=rotary_percent,\n            rotary_interleaved=self.config.rotary_interleaved,\n            seq_len_interpolation_factor=seq_len_interpolation_factor,\n        )\n\n    # Transformer.\n    self.encoder = TransformerBlock(\n        config=self.config,\n        spec=self.transformer_layer_spec,\n        pre_process=self.pre_process,\n        post_process=self.post_process,\n    )\n\n    # Output\n    if post_process:\n        # TODO: Make sure you are passing in the mpu_vocab_size properly\n        self.lm_head = BertLMHead(\n            config.hidden_size,\n            config,\n        )\n\n        self.output_layer = tensor_parallel.ColumnParallelLinear(\n            config.hidden_size,\n            self.vocab_size,\n            config=config,\n            init_method=config.init_method,\n            bias=True,\n            skip_bias_add=False,\n            gather_output=not self.parallel_output,\n            skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n        )\n\n        self.binary_head = None\n        if self.add_binary_head:\n            # TODO: Shoudl switch this to TE ?\n            self.binary_head = get_linear_layer(\n                config.hidden_size, 2, config.init_method, config.perform_initialization\n            )\n\n            self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n    if self.pre_process or self.post_process:\n        self.setup_embeddings_and_output_layer()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Model.embedding_forward","title":"<code>embedding_forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Forward pass of the embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, sequence_length) containing the input IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the position IDs.</p> required <code>tokentype_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def embedding_forward(\n    self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n):\n    \"\"\"Forward pass of the embedding layer.\n\n    Args:\n        input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n        position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n        tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n        attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n    \"\"\"\n    # ESM2 Customization: ESM2Embedding forward takes attention_mask\n    # in addition to the args required by LanguageModelEmbedding\n    return self.embedding(\n        input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule","title":"<code>ESMDataModule</code>","text":"<p>               Bases: <code>MegatronDataModule</code></p> <p>LightningDataModule wrapper of <code>ESMDataset</code>.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>class ESMDataModule(MegatronDataModule):\n    \"\"\"LightningDataModule wrapper of `ESMDataset`.\"\"\"\n\n    def __init__(\n        self,\n        train_cluster_path: str | os.PathLike,\n        train_database_path: str | os.PathLike,\n        valid_cluster_path: str | os.PathLike,\n        valid_database_path: str | os.PathLike,\n        seed: int | None = 42,\n        min_seq_length: int | None = None,\n        max_seq_length: int = 1024,\n        micro_batch_size: int = 4,\n        global_batch_size: int = 8,\n        num_workers: int = 10,  # TODO(@jomitchell) can this be automatically set?\n        persistent_workers: bool = True,\n        pin_memory: bool = True,\n        rampup_batch_size: list[int] | None = None,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,\n        mask_random_prob: float = 0.1,\n        random_mask_strategy: dataset.RandomMaskStrategy = dataset.RandomMaskStrategy.ALL_TOKENS,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        dataloader_type: Literal[\"single\", \"cyclic\"] = \"single\",\n    ) -&gt; None:\n        \"\"\"Initialize the ESMDataModule.\n\n        Args:\n            train_cluster_path: A path to the parquet files containing UniRef90 training clusters.\n            train_database_path: A path to the sqlite file mapping UniRef90 cluster IDs to sequences.\n            valid_cluster_path: A path to the parquet files containing UniRef50 validation clusters.\n            valid_database_path: A path to the sqlite file mapping UniRef50 cluster IDs to sequences.\n            seed: Input random seed. If None, initializes randomly. Defaults to 42.\n            min_seq_length: Whether to pad sequences to a minimum length. If None, sequences are padded to the maximum\n                sequence length. Defaults to None.\n            max_seq_length: The maximum context length for the ESM transformer. Defaults to 1024.\n            micro_batch_size: Passed to MegatronDataSampler. Defaults to 4.\n            global_batch_size: Passed to MegatronDataSampler.. Defaults to 8.\n            num_workers: The number of workers for the pytorch Dataloaders. Defaults to 10.\n            persistent_workers: Whether to keep the workers alive between epochs. Defaults to True.\n            pin_memory: Whether to pin GPU memory in the pytorch Dataloaders. Defaults to True.\n            rampup_batch_size: Passed to MegatronDataSampler. Defaults to None.\n            mask_prob: The overall chance of masking a token and having it appear in the loss fn. Defaults to 0.15.\n            mask_token_prob: Percentage of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n            mask_random_prob: Percentage of masked tokens assigned to a random amino acid. Defaults to 0.1.\n            random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n            tokenizer: The ESM2 tokenizer. Defaults to the one returned by `tokenizer.get_tokenizer()`.\n            dataloader_type: The type of dataloader to use. Defaults to \"single\".\n        \"\"\"\n        super().__init__()\n        self._train_cluster_path = train_cluster_path\n        self._train_database_path = train_database_path\n        self._valid_cluster_path = valid_cluster_path\n        self._valid_database_path = valid_database_path\n        self._seed = seed\n        self._min_seq_length = min_seq_length if min_seq_length is not None else max_seq_length\n        self._max_seq_length = max_seq_length\n        self._mask_prob = mask_prob\n        self._mask_token_prob = mask_token_prob\n        self._mask_random_prob = mask_random_prob\n        self._random_mask_strategy = random_mask_strategy\n        self._tokenizer = tokenizer\n\n        self._micro_batch_size = micro_batch_size\n        self._num_workers = num_workers\n        self._persistent_workers = persistent_workers\n        self._pin_memory = pin_memory\n\n        self.data_sampler = MegatronDataSampler(\n            seq_len=max_seq_length,\n            micro_batch_size=micro_batch_size,\n            global_batch_size=global_batch_size,\n            dataloader_type=dataloader_type,  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n            rampup_batch_size=rampup_batch_size,\n        )\n\n    @property\n    def tokenizer(self) -&gt; tokenizer.BioNeMoESMTokenizer:\n        \"\"\"Returns the tokenizer.\"\"\"\n        return self._tokenizer\n\n    def setup(self, stage: str = \"\") -&gt; None:\n        \"\"\"Setup the ESMDataModule.\n\n        Args:\n            stage: Unused.\n\n        Raises:\n            RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n        \"\"\"\n        del stage  # Unused.\n\n        if not hasattr(self, \"trainer\") or self.trainer is None:\n            raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n        if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n            logging.warning(\n                \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n                \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n            )\n\n        max_train_steps = self.trainer.max_steps\n        if max_train_steps &lt;= 0:\n            raise RuntimeError(\"Please specify trainer.max_steps\")\n\n        # Create training dataset\n        num_train_samples = int(\n            max_train_steps * self.data_sampler.global_batch_size\n        )  # training data requires upsampling (multiply by max_train_steps) on single MegatronPretrainingRandomSampler\n        self._train_ds = dataset.create_train_dataset(\n            cluster_file=self._train_cluster_path,\n            db_path=self._train_database_path,\n            total_samples=num_train_samples,\n            seed=self._seed,\n            max_seq_length=self._max_seq_length,\n            mask_prob=self._mask_prob,\n            mask_token_prob=self._mask_token_prob,\n            mask_random_prob=self._mask_random_prob,\n            random_mask_strategy=self._random_mask_strategy,\n            tokenizer=self._tokenizer,\n        )\n\n        # Create validation dataset\n        val_clusters = dataset.create_valid_clusters(self._valid_cluster_path)\n        if self.trainer.limit_val_batches == 0:  # disable validation\n            logging.info(\"Skip creating validation dataset because trainer.limit_val_batches=0.\")\n        else:\n            num_val_samples = infer_num_samples(\n                limit_batches=self.trainer.limit_val_batches,\n                num_samples_in_dataset=len(val_clusters),\n                global_batch_size=self.data_sampler.global_batch_size,\n                stage=\"val\",\n            )\n            self._valid_ds = dataset.create_valid_dataset(\n                clusters=self._valid_cluster_path,\n                db_path=self._valid_database_path,\n                total_samples=num_val_samples,\n                seed=self._seed,\n                max_seq_length=self._max_seq_length,\n                mask_prob=self._mask_prob,\n                mask_token_prob=self._mask_token_prob,\n                mask_random_prob=self._mask_random_prob,\n                random_mask_strategy=self._random_mask_strategy,\n                tokenizer=self._tokenizer,\n            )\n\n        assert (\n            hasattr(self, \"trainer\") and self.trainer is not None\n        ), \"Setup should be completed when trainer and config are attached.\"\n\n    def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n        \"\"\"Create dataloader for train, validation, and test stages.\n\n        Args:\n            dataset: The dataset to create the dataloader for.\n            mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n            **kwargs: Additional arguments to pass to the dataloader.\n        \"\"\"\n        self.update_init_global_step()\n        assert self._tokenizer.pad_token_id is not None, \"Tokenizer must have a pad token id.\"\n\n        return WrappedDataLoader(\n            mode=mode,\n            dataset=dataset,\n            num_workers=self._num_workers,\n            pin_memory=self._pin_memory,\n            persistent_workers=self._persistent_workers,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=self._tokenizer.pad_token_id,\n                min_length=self._min_seq_length,\n                max_length=self._max_seq_length,\n            ),\n            **kwargs,\n        )\n\n    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n        \"\"\"Returns the dataloader for training data.\"\"\"\n        return self._create_dataloader(self._train_ds, mode=\"train\")\n\n    def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Returns the dataloader for validation data.\"\"\"\n        return self._create_dataloader(self._valid_ds, mode=\"validation\")\n\n    def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Raises a not implemented error.\"\"\"\n        raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.tokenizer","title":"<code>tokenizer</code>  <code>property</code>","text":"<p>Returns the tokenizer.</p>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.__init__","title":"<code>__init__(train_cluster_path, train_database_path, valid_cluster_path, valid_database_path, seed=42, min_seq_length=None, max_seq_length=1024, micro_batch_size=4, global_batch_size=8, num_workers=10, persistent_workers=True, pin_memory=True, rampup_batch_size=None, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=dataset.RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer(), dataloader_type='single')</code>","text":"<p>Initialize the ESMDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>train_cluster_path</code> <code>str | PathLike</code> <p>A path to the parquet files containing UniRef90 training clusters.</p> required <code>train_database_path</code> <code>str | PathLike</code> <p>A path to the sqlite file mapping UniRef90 cluster IDs to sequences.</p> required <code>valid_cluster_path</code> <code>str | PathLike</code> <p>A path to the parquet files containing UniRef50 validation clusters.</p> required <code>valid_database_path</code> <code>str | PathLike</code> <p>A path to the sqlite file mapping UniRef50 cluster IDs to sequences.</p> required <code>seed</code> <code>int | None</code> <p>Input random seed. If None, initializes randomly. Defaults to 42.</p> <code>42</code> <code>min_seq_length</code> <code>int | None</code> <p>Whether to pad sequences to a minimum length. If None, sequences are padded to the maximum sequence length. Defaults to None.</p> <code>None</code> <code>max_seq_length</code> <code>int</code> <p>The maximum context length for the ESM transformer. Defaults to 1024.</p> <code>1024</code> <code>micro_batch_size</code> <code>int</code> <p>Passed to MegatronDataSampler. Defaults to 4.</p> <code>4</code> <code>global_batch_size</code> <code>int</code> <p>Passed to MegatronDataSampler.. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>The number of workers for the pytorch Dataloaders. Defaults to 10.</p> <code>10</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to keep the workers alive between epochs. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin GPU memory in the pytorch Dataloaders. Defaults to True.</p> <code>True</code> <code>rampup_batch_size</code> <code>list[int] | None</code> <p>Passed to MegatronDataSampler. Defaults to None.</p> <code>None</code> <code>mask_prob</code> <code>float</code> <p>The overall chance of masking a token and having it appear in the loss fn. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Percentage of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Percentage of masked tokens assigned to a random amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> <code>ALL_TOKENS</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The ESM2 tokenizer. Defaults to the one returned by <code>tokenizer.get_tokenizer()</code>.</p> <code>get_tokenizer()</code> <code>dataloader_type</code> <code>Literal['single', 'cyclic']</code> <p>The type of dataloader to use. Defaults to \"single\".</p> <code>'single'</code> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def __init__(\n    self,\n    train_cluster_path: str | os.PathLike,\n    train_database_path: str | os.PathLike,\n    valid_cluster_path: str | os.PathLike,\n    valid_database_path: str | os.PathLike,\n    seed: int | None = 42,\n    min_seq_length: int | None = None,\n    max_seq_length: int = 1024,\n    micro_batch_size: int = 4,\n    global_batch_size: int = 8,\n    num_workers: int = 10,  # TODO(@jomitchell) can this be automatically set?\n    persistent_workers: bool = True,\n    pin_memory: bool = True,\n    rampup_batch_size: list[int] | None = None,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: dataset.RandomMaskStrategy = dataset.RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    dataloader_type: Literal[\"single\", \"cyclic\"] = \"single\",\n) -&gt; None:\n    \"\"\"Initialize the ESMDataModule.\n\n    Args:\n        train_cluster_path: A path to the parquet files containing UniRef90 training clusters.\n        train_database_path: A path to the sqlite file mapping UniRef90 cluster IDs to sequences.\n        valid_cluster_path: A path to the parquet files containing UniRef50 validation clusters.\n        valid_database_path: A path to the sqlite file mapping UniRef50 cluster IDs to sequences.\n        seed: Input random seed. If None, initializes randomly. Defaults to 42.\n        min_seq_length: Whether to pad sequences to a minimum length. If None, sequences are padded to the maximum\n            sequence length. Defaults to None.\n        max_seq_length: The maximum context length for the ESM transformer. Defaults to 1024.\n        micro_batch_size: Passed to MegatronDataSampler. Defaults to 4.\n        global_batch_size: Passed to MegatronDataSampler.. Defaults to 8.\n        num_workers: The number of workers for the pytorch Dataloaders. Defaults to 10.\n        persistent_workers: Whether to keep the workers alive between epochs. Defaults to True.\n        pin_memory: Whether to pin GPU memory in the pytorch Dataloaders. Defaults to True.\n        rampup_batch_size: Passed to MegatronDataSampler. Defaults to None.\n        mask_prob: The overall chance of masking a token and having it appear in the loss fn. Defaults to 0.15.\n        mask_token_prob: Percentage of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Percentage of masked tokens assigned to a random amino acid. Defaults to 0.1.\n        random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n        tokenizer: The ESM2 tokenizer. Defaults to the one returned by `tokenizer.get_tokenizer()`.\n        dataloader_type: The type of dataloader to use. Defaults to \"single\".\n    \"\"\"\n    super().__init__()\n    self._train_cluster_path = train_cluster_path\n    self._train_database_path = train_database_path\n    self._valid_cluster_path = valid_cluster_path\n    self._valid_database_path = valid_database_path\n    self._seed = seed\n    self._min_seq_length = min_seq_length if min_seq_length is not None else max_seq_length\n    self._max_seq_length = max_seq_length\n    self._mask_prob = mask_prob\n    self._mask_token_prob = mask_token_prob\n    self._mask_random_prob = mask_random_prob\n    self._random_mask_strategy = random_mask_strategy\n    self._tokenizer = tokenizer\n\n    self._micro_batch_size = micro_batch_size\n    self._num_workers = num_workers\n    self._persistent_workers = persistent_workers\n    self._pin_memory = pin_memory\n\n    self.data_sampler = MegatronDataSampler(\n        seq_len=max_seq_length,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        dataloader_type=dataloader_type,  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n        rampup_batch_size=rampup_batch_size,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule._create_dataloader","title":"<code>_create_dataloader(dataset, mode, **kwargs)</code>","text":"<p>Create dataloader for train, validation, and test stages.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to create the dataloader for.</p> required <code>mode</code> <code>Mode</code> <p>Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).</p> required <code>**kwargs</code> <p>Additional arguments to pass to the dataloader.</p> <code>{}</code> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n    \"\"\"Create dataloader for train, validation, and test stages.\n\n    Args:\n        dataset: The dataset to create the dataloader for.\n        mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n        **kwargs: Additional arguments to pass to the dataloader.\n    \"\"\"\n    self.update_init_global_step()\n    assert self._tokenizer.pad_token_id is not None, \"Tokenizer must have a pad token id.\"\n\n    return WrappedDataLoader(\n        mode=mode,\n        dataset=dataset,\n        num_workers=self._num_workers,\n        pin_memory=self._pin_memory,\n        persistent_workers=self._persistent_workers,\n        collate_fn=functools.partial(\n            collate.bert_padding_collate_fn,\n            padding_value=self._tokenizer.pad_token_id,\n            min_length=self._min_seq_length,\n            max_length=self._max_seq_length,\n        ),\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.setup","title":"<code>setup(stage='')</code>","text":"<p>Setup the ESMDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Unused.</p> <code>''</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the trainer is not attached, or if the trainer's max_steps is not set.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def setup(self, stage: str = \"\") -&gt; None:\n    \"\"\"Setup the ESMDataModule.\n\n    Args:\n        stage: Unused.\n\n    Raises:\n        RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n    \"\"\"\n    del stage  # Unused.\n\n    if not hasattr(self, \"trainer\") or self.trainer is None:\n        raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n    if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n        logging.warning(\n            \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n            \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n        )\n\n    max_train_steps = self.trainer.max_steps\n    if max_train_steps &lt;= 0:\n        raise RuntimeError(\"Please specify trainer.max_steps\")\n\n    # Create training dataset\n    num_train_samples = int(\n        max_train_steps * self.data_sampler.global_batch_size\n    )  # training data requires upsampling (multiply by max_train_steps) on single MegatronPretrainingRandomSampler\n    self._train_ds = dataset.create_train_dataset(\n        cluster_file=self._train_cluster_path,\n        db_path=self._train_database_path,\n        total_samples=num_train_samples,\n        seed=self._seed,\n        max_seq_length=self._max_seq_length,\n        mask_prob=self._mask_prob,\n        mask_token_prob=self._mask_token_prob,\n        mask_random_prob=self._mask_random_prob,\n        random_mask_strategy=self._random_mask_strategy,\n        tokenizer=self._tokenizer,\n    )\n\n    # Create validation dataset\n    val_clusters = dataset.create_valid_clusters(self._valid_cluster_path)\n    if self.trainer.limit_val_batches == 0:  # disable validation\n        logging.info(\"Skip creating validation dataset because trainer.limit_val_batches=0.\")\n    else:\n        num_val_samples = infer_num_samples(\n            limit_batches=self.trainer.limit_val_batches,\n            num_samples_in_dataset=len(val_clusters),\n            global_batch_size=self.data_sampler.global_batch_size,\n            stage=\"val\",\n        )\n        self._valid_ds = dataset.create_valid_dataset(\n            clusters=self._valid_cluster_path,\n            db_path=self._valid_database_path,\n            total_samples=num_val_samples,\n            seed=self._seed,\n            max_seq_length=self._max_seq_length,\n            mask_prob=self._mask_prob,\n            mask_token_prob=self._mask_token_prob,\n            mask_random_prob=self._mask_random_prob,\n            random_mask_strategy=self._random_mask_strategy,\n            tokenizer=self._tokenizer,\n        )\n\n    assert (\n        hasattr(self, \"trainer\") and self.trainer is not None\n    ), \"Setup should be completed when trainer and config are attached.\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Raises a not implemented error.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Raises a not implemented error.\"\"\"\n    raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the dataloader for training data.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n    \"\"\"Returns the dataloader for training data.\"\"\"\n    return self._create_dataloader(self._train_ds, mode=\"train\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the dataloader for validation data.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Returns the dataloader for validation data.\"\"\"\n    return self._create_dataloader(self._valid_ds, mode=\"validation\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/","title":"Dataset","text":""},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset","title":"<code>ESMMaskedResidueDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for ESM pretraining that implements cluster sampling of UniRef50 and UniRef90 sequences.</p> <p>Megatron-LM expects the input datasets to be indexable, and for the output of the dataset for a given index to be deterministic. In cluster sampling, this can be tricky, since we need to perform weighted sampling over UniRef50 clusters.</p> <p>Here, the getitem(i) returns a randomly sampled UniRef90 sequence from the i % len(dataset) UniRef50 cluster, with i controlling the random seed used for selecting the UniRef90 sequence and performing the masking.</p> <p>Multi-epoch training</p> <p>Currently, this class owns the logic for upsampling proteins for multi-epoch training by directly passing a total_samples that's larger than the number of clusters provided. This is done because megatron training assumes that <code>dataset[i]</code> will always return the exact same tensors in distributed training. Because the we want to vary mask patterns and cluster sampling each time a given cluster is sampled, we create our own pseudo-epochs inside the dataset itself. Eventually we'd like to move away from this paradigm and allow multi-epoch training to vary the dataset's random state through a callback, and allow megatron samplers to handle the epoch-to-epoch shuffling of sample order.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>class ESMMaskedResidueDataset(Dataset):\n    \"\"\"Dataset class for ESM pretraining that implements cluster sampling of UniRef50 and UniRef90 sequences.\n\n    Megatron-LM expects the input datasets to be indexable, and for the output of the dataset for a given index to be\n    deterministic. In cluster sampling, this can be tricky, since we need to perform weighted sampling over UniRef50\n    clusters.\n\n    Here, the getitem(i) returns a randomly sampled UniRef90 sequence from the i % len(dataset) UniRef50 cluster, with i\n    controlling the random seed used for selecting the UniRef90 sequence and performing the masking.\n\n    !!! note \"Multi-epoch training\"\n\n        Currently, this class owns the logic for upsampling proteins for multi-epoch training by directly passing a\n        total_samples that's larger than the number of clusters provided. This is done because megatron training assumes\n        that `dataset[i]` will always return the exact same tensors in distributed training. Because the we want to vary\n        mask patterns and cluster sampling each time a given cluster is sampled, we create our own pseudo-epochs inside\n        the dataset itself. Eventually we'd like to move away from this paradigm and allow multi-epoch training to vary\n        the dataset's random state through a callback, and allow megatron samplers to handle the epoch-to-epoch\n        shuffling of sample order.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        protein_dataset: Dataset,\n        clusters: Sequence[Sequence[str]],\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n        max_seq_length: int = 1024,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,\n        mask_random_prob: float = 0.1,\n        random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    ) -&gt; None:\n        \"\"\"Initializes the dataset.\n\n        Args:\n            protein_dataset: Dataset containing protein sequences, indexed by UniRef90 ids.\n            clusters: UniRef90 ids for all training sequences, bucketed by UniRef50 cluster. Alternatively for\n                validation, this can also just a list of UniRef50 ids, with each entry being a length-1 list with a\n                single UniRef50 id.\n            total_samples: Total number of samples to draw from the dataset.\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n                that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n                generated.\n            max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n            mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n            mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n            mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n            random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n            tokenizer: The input ESM tokenizer. Defaults to the standard ESM tokenizer.\n        \"\"\"\n        self.protein_dataset = protein_dataset\n        self.clusters = clusters\n        self.seed = seed\n        self.max_seq_length = max_seq_length\n        self.random_mask_strategy = random_mask_strategy\n\n        if tokenizer.mask_token_id is None:\n            raise ValueError(\"Tokenizer does not have a mask token.\")\n\n        self.mask_config = masking.BertMaskConfig(\n            tokenizer=tokenizer,\n            random_tokens=range(len(tokenizer.all_tokens))\n            if self.random_mask_strategy == RandomMaskStrategy.ALL_TOKENS\n            else range(4, 24),\n            mask_prob=mask_prob,\n            mask_token_prob=mask_token_prob,\n            random_token_prob=mask_random_prob,\n        )\n\n        self.tokenizer = tokenizer\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of clusters, which constitutes a single epoch.\"\"\"\n        return len(self.clusters)\n\n    def __getitem__(self, index: EpochIndex) -&gt; BertSample:\n        \"\"\"Deterministically masks and returns a protein sequence from the dataset.\n\n        This method samples from the i % len(dataset) cluster from the input clusters list. Random draws of the same\n        cluster can be achieved by calling this method with i + len(dataset), i.e., wrapping around the dataset length.\n\n        Args:\n            index: The current epoch and the index of the cluster to sample.\n\n        Returns:\n            A (possibly-truncated), masked protein sequence with CLS and EOS tokens and associated mask fields.\n        \"\"\"\n        # Initialize a random number generator with a seed that is a combination of the dataset seed, epoch, and index.\n        rng = np.random.default_rng([self.seed, index.epoch, index.idx])\n        if not len(self.clusters[index.idx]):\n            raise ValueError(f\"Cluster {index.idx} is empty.\")\n\n        sequence_id = rng.choice(self.clusters[index.idx])\n        sequence = self.protein_dataset[sequence_id]\n\n        # We don't want special tokens before we pass the input to the masking function; we add these in the collate_fn.\n        tokenized_sequence = self._tokenize(sequence)\n        cropped_sequence = _random_crop(tokenized_sequence, self.max_seq_length, rng)\n\n        # Get a single integer seed for torch from our rng, since the index tuple is hard to pass directly to torch.\n        torch_seed = random_utils.get_seed_from_rng(rng)\n        masked_sequence, labels, loss_mask = masking.apply_bert_pretraining_mask(\n            tokenized_sequence=cropped_sequence,  # type: ignore\n            random_seed=torch_seed,\n            mask_config=self.mask_config,\n        )\n\n        return {\n            \"text\": masked_sequence,\n            \"types\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(masked_sequence, dtype=torch.int64),\n            \"labels\": labels,\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n        }\n\n    def _tokenize(self, sequence: str) -&gt; torch.Tensor:\n        \"\"\"Tokenize a protein sequence.\n\n        Args:\n            sequence: The protein sequence.\n\n        Returns:\n            The tokenized sequence.\n        \"\"\"\n        tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n        return tensor.flatten()  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Deterministically masks and returns a protein sequence from the dataset.</p> <p>This method samples from the i % len(dataset) cluster from the input clusters list. Random draws of the same cluster can be achieved by calling this method with i + len(dataset), i.e., wrapping around the dataset length.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>EpochIndex</code> <p>The current epoch and the index of the cluster to sample.</p> required <p>Returns:</p> Type Description <code>BertSample</code> <p>A (possibly-truncated), masked protein sequence with CLS and EOS tokens and associated mask fields.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __getitem__(self, index: EpochIndex) -&gt; BertSample:\n    \"\"\"Deterministically masks and returns a protein sequence from the dataset.\n\n    This method samples from the i % len(dataset) cluster from the input clusters list. Random draws of the same\n    cluster can be achieved by calling this method with i + len(dataset), i.e., wrapping around the dataset length.\n\n    Args:\n        index: The current epoch and the index of the cluster to sample.\n\n    Returns:\n        A (possibly-truncated), masked protein sequence with CLS and EOS tokens and associated mask fields.\n    \"\"\"\n    # Initialize a random number generator with a seed that is a combination of the dataset seed, epoch, and index.\n    rng = np.random.default_rng([self.seed, index.epoch, index.idx])\n    if not len(self.clusters[index.idx]):\n        raise ValueError(f\"Cluster {index.idx} is empty.\")\n\n    sequence_id = rng.choice(self.clusters[index.idx])\n    sequence = self.protein_dataset[sequence_id]\n\n    # We don't want special tokens before we pass the input to the masking function; we add these in the collate_fn.\n    tokenized_sequence = self._tokenize(sequence)\n    cropped_sequence = _random_crop(tokenized_sequence, self.max_seq_length, rng)\n\n    # Get a single integer seed for torch from our rng, since the index tuple is hard to pass directly to torch.\n    torch_seed = random_utils.get_seed_from_rng(rng)\n    masked_sequence, labels, loss_mask = masking.apply_bert_pretraining_mask(\n        tokenized_sequence=cropped_sequence,  # type: ignore\n        random_seed=torch_seed,\n        mask_config=self.mask_config,\n    )\n\n    return {\n        \"text\": masked_sequence,\n        \"types\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n        \"attention_mask\": torch.ones_like(masked_sequence, dtype=torch.int64),\n        \"labels\": labels,\n        \"loss_mask\": loss_mask,\n        \"is_random\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n    }\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset.__init__","title":"<code>__init__(protein_dataset, clusters, seed=np.random.SeedSequence().entropy, max_seq_length=1024, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>protein_dataset</code> <code>Dataset</code> <p>Dataset containing protein sequences, indexed by UniRef90 ids.</p> required <code>clusters</code> <code>Sequence[Sequence[str]]</code> <p>UniRef90 ids for all training sequences, bucketed by UniRef50 cluster. Alternatively for validation, this can also just a list of UniRef50 ids, with each entry being a length-1 list with a single UniRef50 id.</p> required <code>total_samples</code> <p>Total number of samples to draw from the dataset.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> <code>max_seq_length</code> <code>int</code> <p>Crop long sequences to a maximum of this length, including BOS and EOS tokens.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>The overall probability a token is included in the loss function. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Proportion of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> <code>ALL_TOKENS</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The input ESM tokenizer. Defaults to the standard ESM tokenizer.</p> <code>get_tokenizer()</code> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __init__(\n    self,\n    protein_dataset: Dataset,\n    clusters: Sequence[Sequence[str]],\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    max_seq_length: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n) -&gt; None:\n    \"\"\"Initializes the dataset.\n\n    Args:\n        protein_dataset: Dataset containing protein sequences, indexed by UniRef90 ids.\n        clusters: UniRef90 ids for all training sequences, bucketed by UniRef50 cluster. Alternatively for\n            validation, this can also just a list of UniRef50 ids, with each entry being a length-1 list with a\n            single UniRef50 id.\n        total_samples: Total number of samples to draw from the dataset.\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n            that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n            generated.\n        max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n        mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n        mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n        random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n        tokenizer: The input ESM tokenizer. Defaults to the standard ESM tokenizer.\n    \"\"\"\n    self.protein_dataset = protein_dataset\n    self.clusters = clusters\n    self.seed = seed\n    self.max_seq_length = max_seq_length\n    self.random_mask_strategy = random_mask_strategy\n\n    if tokenizer.mask_token_id is None:\n        raise ValueError(\"Tokenizer does not have a mask token.\")\n\n    self.mask_config = masking.BertMaskConfig(\n        tokenizer=tokenizer,\n        random_tokens=range(len(tokenizer.all_tokens))\n        if self.random_mask_strategy == RandomMaskStrategy.ALL_TOKENS\n        else range(4, 24),\n        mask_prob=mask_prob,\n        mask_token_prob=mask_token_prob,\n        random_token_prob=mask_random_prob,\n    )\n\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of clusters, which constitutes a single epoch.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of clusters, which constitutes a single epoch.\"\"\"\n    return len(self.clusters)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset._tokenize","title":"<code>_tokenize(sequence)</code>","text":"<p>Tokenize a protein sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>str</code> <p>The protein sequence.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tokenized sequence.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def _tokenize(self, sequence: str) -&gt; torch.Tensor:\n    \"\"\"Tokenize a protein sequence.\n\n    Args:\n        sequence: The protein sequence.\n\n    Returns:\n        The tokenized sequence.\n    \"\"\"\n    tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n    return tensor.flatten()  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset","title":"<code>ProteinSQLiteDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for protein sequences stored in a SQLite database.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>class ProteinSQLiteDataset(Dataset):\n    \"\"\"Dataset for protein sequences stored in a SQLite database.\"\"\"\n\n    def __init__(self, db_path: str | os.PathLike):\n        \"\"\"Initializes the dataset.\n\n        Args:\n            db_path: Path to the SQLite database.\n        \"\"\"\n        self.conn = sqlite3.connect(str(db_path))\n        self.cursor = self.conn.cursor()\n        self._len = None\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of proteins in the dataset.\n\n        Returns:\n            Number of proteins in the dataset.\n        \"\"\"\n        if self._len is None:\n            self.cursor.execute(\"SELECT COUNT(*) FROM protein\")\n            self._len = int(self.cursor.fetchone()[0])\n        return self._len\n\n    def __getitem__(self, idx: str) -&gt; str:\n        \"\"\"Returns the sequence of a protein at a given index.\n\n        TODO: This method may want to support batched indexing for improved performance.\n\n        Args:\n            idx: An identifier for the protein sequence. For training data, these are UniRef90 IDs, while for validation\n                data, they are UniRef50 IDs.\n\n        Returns:\n            The protein sequence as a string.\n        \"\"\"\n        if not isinstance(idx, str):\n            raise TypeError(f\"Expected string, got {type(idx)}: {idx}.\")\n\n        self.cursor.execute(\"SELECT sequence FROM protein WHERE id = ?\", (idx,))\n        return self.cursor.fetchone()[0]\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the sequence of a protein at a given index.</p> <p>TODO: This method may want to support batched indexing for improved performance.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>str</code> <p>An identifier for the protein sequence. For training data, these are UniRef90 IDs, while for validation data, they are UniRef50 IDs.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The protein sequence as a string.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __getitem__(self, idx: str) -&gt; str:\n    \"\"\"Returns the sequence of a protein at a given index.\n\n    TODO: This method may want to support batched indexing for improved performance.\n\n    Args:\n        idx: An identifier for the protein sequence. For training data, these are UniRef90 IDs, while for validation\n            data, they are UniRef50 IDs.\n\n    Returns:\n        The protein sequence as a string.\n    \"\"\"\n    if not isinstance(idx, str):\n        raise TypeError(f\"Expected string, got {type(idx)}: {idx}.\")\n\n    self.cursor.execute(\"SELECT sequence FROM protein WHERE id = ?\", (idx,))\n    return self.cursor.fetchone()[0]\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset.__init__","title":"<code>__init__(db_path)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str | PathLike</code> <p>Path to the SQLite database.</p> required Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __init__(self, db_path: str | os.PathLike):\n    \"\"\"Initializes the dataset.\n\n    Args:\n        db_path: Path to the SQLite database.\n    \"\"\"\n    self.conn = sqlite3.connect(str(db_path))\n    self.cursor = self.conn.cursor()\n    self._len = None\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of proteins in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of proteins in the dataset.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of proteins in the dataset.\n\n    Returns:\n        Number of proteins in the dataset.\n    \"\"\"\n    if self._len is None:\n        self.cursor.execute(\"SELECT COUNT(*) FROM protein\")\n        self._len = int(self.cursor.fetchone()[0])\n    return self._len\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.RandomMaskStrategy","title":"<code>RandomMaskStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for different random masking strategies.</p> <p>In ESM2 pretraining, 15% of all tokens are masked and among which 10% are replaced with a random token. This class controls the set of random tokens to choose from.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>class RandomMaskStrategy(str, Enum):\n    \"\"\"Enum for different random masking strategies.\n\n    In ESM2 pretraining, 15% of all tokens are masked and among which 10% are replaced with a random token. This class controls the set of random tokens to choose from.\n\n    \"\"\"\n\n    AMINO_ACIDS_ONLY = \"amino_acids_only\"\n    \"\"\"Mask only with amino acid tokens.\"\"\"\n\n    ALL_TOKENS = \"all_tokens\"\n    \"\"\"Mask with all tokens in the tokenizer, including special tokens, padding and non-canonical amino acid tokens.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.RandomMaskStrategy.ALL_TOKENS","title":"<code>ALL_TOKENS = 'all_tokens'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mask with all tokens in the tokenizer, including special tokens, padding and non-canonical amino acid tokens.</p>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.RandomMaskStrategy.AMINO_ACIDS_ONLY","title":"<code>AMINO_ACIDS_ONLY = 'amino_acids_only'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mask only with amino acid tokens.</p>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset._random_crop","title":"<code>_random_crop(s, crop_length, rng)</code>","text":"<p>Randomly crops a input to a maximum length.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def _random_crop(s: _T, crop_length: int, rng: np.random.Generator) -&gt; _T:\n    \"\"\"Randomly crops a input to a maximum length.\"\"\"\n    if crop_length &gt;= len(s):\n        return s\n\n    start_index = rng.integers(0, len(s) - crop_length)\n    return s[start_index : start_index + crop_length]\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.create_train_dataset","title":"<code>create_train_dataset(cluster_file, db_path, total_samples, seed, max_seq_length=1024, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Creates a training dataset for ESM pretraining.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_file</code> <code>str | PathLike</code> <p>Path to the cluster file. The file should contain a \"ur90_id\" column, where each row contains a list of UniRef90 ids for a single UniRef50 cluster.</p> required <code>db_path</code> <code>str | PathLike</code> <p>Path to the SQLite database.</p> required <code>total_samples</code> <code>int</code> <p>Total number of samples to draw from the dataset.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>max_seq_length</code> <code>int</code> <p>Crop long sequences to a maximum of this length, including BOS and EOS tokens.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>The overall probability a token is included in the loss function. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Proportion of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> <code>ALL_TOKENS</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The input ESM tokenizer. Defaults to the standard ESM tokenizer.</p> <code>get_tokenizer()</code> <p>Returns:</p> Type Description <p>A dataset for ESM pretraining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the cluster file does not exist, the database file does not exist, or the cluster file does not contain a \"ur90_id\" column.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def create_train_dataset(\n    cluster_file: str | os.PathLike,\n    db_path: str | os.PathLike,\n    total_samples: int,\n    seed: int,\n    max_seq_length: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n):\n    \"\"\"Creates a training dataset for ESM pretraining.\n\n    Args:\n        cluster_file: Path to the cluster file. The file should contain a \"ur90_id\" column, where each row contains a\n            list of UniRef90 ids for a single UniRef50 cluster.\n        db_path: Path to the SQLite database.\n        total_samples: Total number of samples to draw from the dataset.\n        seed: Random seed for reproducibility.\n        max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n        mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n        mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n        random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n        tokenizer: The input ESM tokenizer. Defaults to the standard ESM tokenizer.\n\n    Returns:\n        A dataset for ESM pretraining.\n\n    Raises:\n        ValueError: If the cluster file does not exist, the database file does not exist, or the cluster file does not\n            contain a \"ur90_id\" column.\n    \"\"\"\n    if not Path(cluster_file).exists():\n        raise ValueError(f\"Cluster file {cluster_file} not found.\")\n\n    if not Path(db_path).exists():\n        raise ValueError(f\"Database file {db_path} not found.\")\n\n    cluster_df = pd.read_parquet(cluster_file)\n    if \"ur90_id\" not in cluster_df.columns:\n        raise ValueError(f\"Training cluster file must contain a 'ur90_id' column. Found columns {cluster_df.columns}.\")\n\n    protein_dataset = ProteinSQLiteDataset(db_path)\n    masked_cluster_dataset = ESMMaskedResidueDataset(\n        protein_dataset=protein_dataset,\n        clusters=cluster_df[\"ur90_id\"],\n        seed=seed,\n        max_seq_length=max_seq_length,\n        mask_prob=mask_prob,\n        mask_token_prob=mask_token_prob,\n        mask_random_prob=mask_random_prob,\n        random_mask_strategy=random_mask_strategy,\n        tokenizer=tokenizer,\n    )\n\n    return MultiEpochDatasetResampler(masked_cluster_dataset, num_samples=total_samples, shuffle=True, seed=seed)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.create_valid_clusters","title":"<code>create_valid_clusters(cluster_file)</code>","text":"<p>Create a pandas series of UniRef50 cluster IDs from a cluster parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_file</code> <code>str | PathLike</code> <p>Path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A pandas series of UniRef50 cluster IDs.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def create_valid_clusters(cluster_file: str | os.PathLike) -&gt; pd.Series:\n    \"\"\"Create a pandas series of UniRef50 cluster IDs from a cluster parquet file.\n\n    Args:\n        cluster_file: Path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50\n        IDs, with one UniRef50 ID per row.\n\n    Returns:\n        A pandas series of UniRef50 cluster IDs.\n    \"\"\"\n    if not Path(cluster_file).exists():\n        raise ValueError(f\"Cluster file {cluster_file} not found.\")\n\n    cluster_df = pd.read_parquet(cluster_file)\n    if \"ur50_id\" not in cluster_df.columns:\n        raise ValueError(\n            f\"Validation cluster file must contain a 'ur50_id' column. Found columns {cluster_df.columns}.\"\n        )\n    clusters = cluster_df[\"ur50_id\"].apply(lambda x: [x])\n    return clusters\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.create_valid_dataset","title":"<code>create_valid_dataset(clusters, db_path, seed, total_samples=None, max_seq_length=1024, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Creates a validation dataset for ESM pretraining.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_file</code> <p>Clusters as pd.Series, or path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50 IDs, with one UniRef50 ID per row.</p> required <code>db_path</code> <code>str | PathLike</code> <p>Path to the SQLite database.</p> required <code>total_samples</code> <code>int | None</code> <p>Total number of samples to draw from the dataset.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>max_seq_length</code> <code>int</code> <p>Crop long sequences to a maximum of this length, including BOS and EOS tokens.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>The overall probability a token is included in the loss function. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Proportion of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_masking_strategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the cluster file does not exist, the database file does not exist, or the cluster file does not contain a \"ur50_id\" column.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def create_valid_dataset(  # noqa: D417\n    clusters: pd.Series | str | os.PathLike,\n    db_path: str | os.PathLike,\n    seed: int,\n    total_samples: int | None = None,\n    max_seq_length: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n):\n    \"\"\"Creates a validation dataset for ESM pretraining.\n\n    Args:\n        cluster_file: Clusters as pd.Series, or path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50\n            IDs, with one UniRef50 ID per row.\n        db_path: Path to the SQLite database.\n        total_samples: Total number of samples to draw from the dataset.\n        seed: Random seed for reproducibility.\n        max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n        mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n        mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n        random_masking_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n\n    Raises:\n        ValueError: If the cluster file does not exist, the database file does not exist, or the cluster file does not\n            contain a \"ur50_id\" column.\n    \"\"\"\n    if isinstance(clusters, (str, os.PathLike)):\n        clusters = create_valid_clusters(clusters)\n\n    elif not isinstance(clusters, pd.Series):\n        raise ValueError(f\"Clusters must be a pandas Series. Got {type(clusters)}.\")\n\n    if not Path(db_path).exists():\n        raise ValueError(f\"Database file {db_path} not found.\")\n\n    protein_dataset = ProteinSQLiteDataset(db_path)\n    masked_dataset = ESMMaskedResidueDataset(\n        protein_dataset=protein_dataset,\n        clusters=clusters,\n        seed=seed,\n        max_seq_length=max_seq_length,\n        mask_prob=mask_prob,\n        mask_token_prob=mask_token_prob,\n        mask_random_prob=mask_random_prob,\n        random_mask_strategy=random_mask_strategy,\n        tokenizer=tokenizer,\n    )\n\n    return MultiEpochDatasetResampler(masked_dataset, num_samples=total_samples, shuffle=True, seed=seed)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/tokenizer/","title":"Vendored tokenizer config for facebook/esm2_t33_650M_UR50D","text":"<p>This directory contains the output of</p> <pre><code>from transformers import AutoTokenizer\nAutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\").save_pretrained(\"...\")\n</code></pre> <p>for reproducible results and to reduce reliance on external API calls.</p>"},{"location":"API_reference/bionemo/esm2/model/convert/","title":"Convert","text":""},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert.HFESM2Importer","title":"<code>HFESM2Importer</code>","text":"<p>               Bases: <code>ModelConnector[AutoModelForMaskedLM, BionemoLightningModule]</code></p> <p>Converts a Hugging Face ESM-2 model to a NeMo ESM-2 model.</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>@io.model_importer(BionemoLightningModule, \"hf\")\nclass HFESM2Importer(io.ModelConnector[AutoModelForMaskedLM, BionemoLightningModule]):\n    \"\"\"Converts a Hugging Face ESM-2 model to a NeMo ESM-2 model.\"\"\"\n\n    def init(self) -&gt; BionemoLightningModule:\n        \"\"\"Initialize the converted model.\"\"\"\n        return biobert_lightning_module(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -&gt; Path:\n        \"\"\"Applies the transformation.\n\n        Largely inspired by\n        https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/hf-integration.html\n        \"\"\"\n        source = AutoModelForMaskedLM.from_pretrained(str(self), trust_remote_code=True, torch_dtype=\"auto\")\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        print(f\"Converted ESM-2 model to Nemo, model saved to {output_path}\")\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        \"\"\"Converting HF state dict to NeMo state dict.\"\"\"\n        mapping = {\n            # \"esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\": \"rotary_pos_emb.inv_freq\",\n            \"esm.encoder.layer.*.attention.output.dense.weight\": \"encoder.layers.*.self_attention.linear_proj.weight\",\n            \"esm.encoder.layer.*.attention.output.dense.bias\": \"encoder.layers.*.self_attention.linear_proj.bias\",\n            \"esm.encoder.layer.*.attention.LayerNorm.weight\": \"encoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"esm.encoder.layer.*.attention.LayerNorm.bias\": \"encoder.layers.*.self_attention.linear_qkv.layer_norm_bias\",\n            \"esm.encoder.layer.*.intermediate.dense.weight\": \"encoder.layers.*.mlp.linear_fc1.weight\",\n            \"esm.encoder.layer.*.intermediate.dense.bias\": \"encoder.layers.*.mlp.linear_fc1.bias\",\n            \"esm.encoder.layer.*.output.dense.weight\": \"encoder.layers.*.mlp.linear_fc2.weight\",\n            \"esm.encoder.layer.*.output.dense.bias\": \"encoder.layers.*.mlp.linear_fc2.bias\",\n            \"esm.encoder.layer.*.LayerNorm.weight\": \"encoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n            \"esm.encoder.layer.*.LayerNorm.bias\": \"encoder.layers.*.mlp.linear_fc1.layer_norm_bias\",\n            \"esm.encoder.emb_layer_norm_after.weight\": \"encoder.final_layernorm.weight\",\n            \"esm.encoder.emb_layer_norm_after.bias\": \"encoder.final_layernorm.bias\",\n            \"lm_head.dense.weight\": \"lm_head.dense.weight\",\n            \"lm_head.dense.bias\": \"lm_head.dense.bias\",\n            \"lm_head.layer_norm.weight\": \"lm_head.layer_norm.weight\",\n            \"lm_head.layer_norm.bias\": \"lm_head.layer_norm.bias\",\n        }\n\n        # lm_head.bias\n        return io.apply_transforms(\n            source,\n            target,\n            mapping=mapping,\n            transforms=[_pad_embeddings, _pad_bias, _import_qkv_weight, _import_qkv_bias],\n        )\n\n    @property\n    def tokenizer(self) -&gt; BioNeMoESMTokenizer:\n        \"\"\"We just have the one tokenizer for ESM-2.\"\"\"\n        return get_tokenizer()\n\n    @property\n    def config(self) -&gt; ESM2Config:\n        \"\"\"Returns the transformed ESM-2 config given the model tag.\"\"\"\n        source = HFAutoConfig.from_pretrained(str(self), trust_remote_code=True)\n        output = ESM2Config(\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=source.intermediate_size,\n            position_embedding_type=\"rope\",\n            num_attention_heads=source.num_attention_heads,\n            seq_length=source.max_position_embeddings,\n            fp16=(dtype_from_hf(source) == torch.float16),\n            bf16=(dtype_from_hf(source) == torch.bfloat16),\n            params_dtype=dtype_from_hf(source),\n        )\n\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert.HFESM2Importer.config","title":"<code>config</code>  <code>property</code>","text":"<p>Returns the transformed ESM-2 config given the model tag.</p>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert.HFESM2Importer.tokenizer","title":"<code>tokenizer</code>  <code>property</code>","text":"<p>We just have the one tokenizer for ESM-2.</p>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert.HFESM2Importer.apply","title":"<code>apply(output_path)</code>","text":"<p>Applies the transformation.</p> <p>Largely inspired by https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/hf-integration.html</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>def apply(self, output_path: Path) -&gt; Path:\n    \"\"\"Applies the transformation.\n\n    Largely inspired by\n    https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/hf-integration.html\n    \"\"\"\n    source = AutoModelForMaskedLM.from_pretrained(str(self), trust_remote_code=True, torch_dtype=\"auto\")\n    target = self.init()\n    trainer = self.nemo_setup(target)\n    self.convert_state(source, target)\n    self.nemo_save(output_path, trainer)\n\n    print(f\"Converted ESM-2 model to Nemo, model saved to {output_path}\")\n\n    teardown(trainer, target)\n    del trainer, target\n\n    return output_path\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert.HFESM2Importer.convert_state","title":"<code>convert_state(source, target)</code>","text":"<p>Converting HF state dict to NeMo state dict.</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>def convert_state(self, source, target):\n    \"\"\"Converting HF state dict to NeMo state dict.\"\"\"\n    mapping = {\n        # \"esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\": \"rotary_pos_emb.inv_freq\",\n        \"esm.encoder.layer.*.attention.output.dense.weight\": \"encoder.layers.*.self_attention.linear_proj.weight\",\n        \"esm.encoder.layer.*.attention.output.dense.bias\": \"encoder.layers.*.self_attention.linear_proj.bias\",\n        \"esm.encoder.layer.*.attention.LayerNorm.weight\": \"encoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n        \"esm.encoder.layer.*.attention.LayerNorm.bias\": \"encoder.layers.*.self_attention.linear_qkv.layer_norm_bias\",\n        \"esm.encoder.layer.*.intermediate.dense.weight\": \"encoder.layers.*.mlp.linear_fc1.weight\",\n        \"esm.encoder.layer.*.intermediate.dense.bias\": \"encoder.layers.*.mlp.linear_fc1.bias\",\n        \"esm.encoder.layer.*.output.dense.weight\": \"encoder.layers.*.mlp.linear_fc2.weight\",\n        \"esm.encoder.layer.*.output.dense.bias\": \"encoder.layers.*.mlp.linear_fc2.bias\",\n        \"esm.encoder.layer.*.LayerNorm.weight\": \"encoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n        \"esm.encoder.layer.*.LayerNorm.bias\": \"encoder.layers.*.mlp.linear_fc1.layer_norm_bias\",\n        \"esm.encoder.emb_layer_norm_after.weight\": \"encoder.final_layernorm.weight\",\n        \"esm.encoder.emb_layer_norm_after.bias\": \"encoder.final_layernorm.bias\",\n        \"lm_head.dense.weight\": \"lm_head.dense.weight\",\n        \"lm_head.dense.bias\": \"lm_head.dense.bias\",\n        \"lm_head.layer_norm.weight\": \"lm_head.layer_norm.weight\",\n        \"lm_head.layer_norm.bias\": \"lm_head.layer_norm.bias\",\n    }\n\n    # lm_head.bias\n    return io.apply_transforms(\n        source,\n        target,\n        mapping=mapping,\n        transforms=[_pad_embeddings, _pad_bias, _import_qkv_weight, _import_qkv_bias],\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert.HFESM2Importer.init","title":"<code>init()</code>","text":"<p>Initialize the converted model.</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>def init(self) -&gt; BionemoLightningModule:\n    \"\"\"Initialize the converted model.\"\"\"\n    return biobert_lightning_module(self.config, tokenizer=self.tokenizer)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert._import_qkv_bias","title":"<code>_import_qkv_bias(ctx, query, key, value)</code>","text":"<p>Pad the embedding layer to the new input dimension.</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>@io.state_transform(\n    source_key=(\n        \"esm.encoder.layer.*.attention.self.query.bias\",\n        \"esm.encoder.layer.*.attention.self.key.bias\",\n        \"esm.encoder.layer.*.attention.self.value.bias\",\n    ),\n    target_key=\"encoder.layers.*.self_attention.linear_qkv.bias\",\n)\ndef _import_qkv_bias(ctx: io.TransformCTX, query, key, value):\n    \"\"\"Pad the embedding layer to the new input dimension.\"\"\"\n    concat_biases = torch.cat((query, key, value), dim=0)\n    input_shape = concat_biases.size()\n    np = ctx.target.config.num_attention_heads\n    # transpose biases\n    # [num_splits_model_parallel * attention head size * #attention heads]\n    # --&gt; [attention head size * num_splits_model_parallel * #attention heads]\n    concat_biases = concat_biases.view(3, np, -1)\n    concat_biases = concat_biases.transpose(0, 1).contiguous()\n    concat_biases = concat_biases.view(*input_shape)\n    return concat_biases\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert._import_qkv_weight","title":"<code>_import_qkv_weight(ctx, query, key, value)</code>","text":"<p>Pad the embedding layer to the new input dimension.</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>@io.state_transform(\n    source_key=(\n        \"esm.encoder.layer.*.attention.self.query.weight\",\n        \"esm.encoder.layer.*.attention.self.key.weight\",\n        \"esm.encoder.layer.*.attention.self.value.weight\",\n    ),\n    target_key=\"encoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv_weight(ctx: io.TransformCTX, query, key, value):\n    \"\"\"Pad the embedding layer to the new input dimension.\"\"\"\n    concat_weights = torch.cat((query, key, value), dim=0)\n    input_shape = concat_weights.size()\n    np = ctx.target.config.num_attention_heads\n    # transpose weights\n    # [sequence length, batch size, num_splits_model_parallel * attention head size * #attention heads]\n    # --&gt; [sequence length, batch size, attention head size * num_splits_model_parallel * #attention heads]\n    concat_weights = concat_weights.view(3, np, -1, query.size()[-1])\n    concat_weights = concat_weights.transpose(0, 1).contiguous()\n    concat_weights = concat_weights.view(*input_shape)\n    return concat_weights\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert._pad_bias","title":"<code>_pad_bias(ctx, source_bias)</code>","text":"<p>Pad the embedding layer to the new input dimension.</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>@io.state_transform(\n    source_key=\"lm_head.bias\",\n    target_key=\"output_layer.bias\",\n)\ndef _pad_bias(ctx: io.TransformCTX, source_bias):\n    \"\"\"Pad the embedding layer to the new input dimension.\"\"\"\n    nemo_embedding_dimension = ctx.target.config.make_vocab_size_divisible_by\n    hf_embedding_dimension = source_bias.size(0)\n    output_bias = torch.zeros(nemo_embedding_dimension, dtype=source_bias.dtype, device=source_bias.device)\n    output_bias[:hf_embedding_dimension] = source_bias\n    return output_bias\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/convert/#bionemo.esm2.model.convert._pad_embeddings","title":"<code>_pad_embeddings(ctx, source_embed)</code>","text":"<p>Pad the embedding layer to the new input dimension.</p> Source code in <code>bionemo/esm2/model/convert.py</code> <pre><code>@io.state_transform(\n    source_key=\"esm.embeddings.word_embeddings.weight\",\n    target_key=\"embedding.word_embeddings.weight\",\n)\ndef _pad_embeddings(ctx: io.TransformCTX, source_embed):\n    \"\"\"Pad the embedding layer to the new input dimension.\"\"\"\n    nemo_embedding_dimension = ctx.target.config.make_vocab_size_divisible_by\n    hf_embedding_dimension = source_embed.size(0)\n    num_padding_rows = nemo_embedding_dimension - hf_embedding_dimension\n    padding_rows = torch.zeros(num_padding_rows, source_embed.size(1))\n    return torch.cat((source_embed, padding_rows), dim=0)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/embedding/","title":"Embedding","text":""},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding","title":"<code>ESM2Embedding</code>","text":"<p>               Bases: <code>LanguageModelEmbedding</code></p> <p>ESM2 Embedding with custom logic for attention masking and token dropout.</p> Source code in <code>bionemo/esm2/model/embedding.py</code> <pre><code>class ESM2Embedding(LanguageModelEmbedding):\n    \"\"\"ESM2 Embedding with custom logic for attention masking and token dropout.\"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        vocab_size: int,\n        max_sequence_length: int,\n        position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"rope\",\n        num_tokentypes: int = 0,\n        # ESM2 NEW ARGS\n        token_dropout: bool = True,\n        use_attention_mask: bool = True,\n        mask_token_id: Optional[int] = torch.nan,\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2 Embedding module.\"\"\"\n        super().__init__(\n            config=config,\n            vocab_size=vocab_size,\n            max_sequence_length=max_sequence_length,\n            position_embedding_type=position_embedding_type,\n            num_tokentypes=num_tokentypes,\n        )\n        self.token_dropout = token_dropout\n        self.use_attention_mask = use_attention_mask\n        self.mask_token_id = mask_token_id\n\n    @property\n    def dtype(self) -&gt; torch.dtype:\n        \"\"\"The dtype of the embedding weights.\"\"\"\n        return self.word_embeddings.weight.dtype\n\n    def _apply_esm2_customization(\n        self, word_embeddings: Tensor, input_ids: Tensor, attention_mask: Tensor\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"ESM2 customization for attention masking and token dropout.\n\n        Args:\n            word_embeddings (Tensor[float]): The input tokens. Shape: [b, s, h]\n            input_ids (Tensor[int]): The input tokens. Shape: [b, s]\n            attention_mask (Tensor[bool]): attention mask. Shape: [b, s]\n\n        Returns:\n            Tuple[Tensor, Tensor]: (Updated embeddings, embedding mask) Shape: ([b, s, h], [b, s])\n        \"\"\"\n        embeddings_mask = None\n        if attention_mask is not None and (self.token_dropout or self.use_attention_mask):\n            embeddings_mask = attention_mask\n\n        if embeddings_mask is not None and self.token_dropout:\n            word_embeddings = word_embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n            src_lengths = embeddings_mask.sum(-1)\n            mask_ratio_observed = (input_ids == self.mask_token_id).sum(-1).to(self.dtype) / src_lengths\n\n            scale_factor = (1 - ESM2_MASK_RATIO_TRAIN) / (1 - mask_ratio_observed)[:, None, None]\n            word_embeddings = (word_embeddings * scale_factor).to(word_embeddings.dtype)\n        if embeddings_mask is not None and self.use_attention_mask:\n            word_embeddings = (word_embeddings * embeddings_mask.unsqueeze(-1)).to(word_embeddings.dtype)\n        return word_embeddings, embeddings_mask\n\n    def forward(\n        self,\n        input_ids: Tensor,\n        position_ids: Tensor,\n        tokentype_ids: Optional[int] = None,\n        attention_mask: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Forward pass of the embedding module.\n\n        Args:\n            input_ids (Tensor): The input tokens. Shape: [b, s]\n            position_ids (Tensor): The position id's used to calculate position embeddings. Shape: [b, s]\n            tokentype_ids (int, optional): The token type ids. Used when args.bert_binary_head is set to True. Defaults to None\n            attention_mask (Tensor): attention mask. Shape: [b, s]\n\n        Returns:\n            Tensor: The output embeddings\n        \"\"\"\n        word_embeddings = self.word_embeddings(input_ids)  # [b, s, h]\n\n        # ESM2 Customization\n        word_embeddings, embeddings_mask = self._apply_esm2_customization(word_embeddings, input_ids, attention_mask)\n\n        if self.add_position_embedding:\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings = word_embeddings + position_embeddings\n        else:\n            embeddings = word_embeddings\n\n        # ESM2 Customization: include attention masking from ESM2\n        if embeddings_mask is not None and self.use_attention_mask:\n            embeddings = (embeddings * embeddings_mask.unsqueeze(-1)).to(embeddings.dtype)\n\n        # Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].\n        embeddings = embeddings.transpose(0, 1).contiguous()\n\n        if tokentype_ids is not None:\n            if self.tokentype_embeddings is None:\n                raise ValueError(\"tokentype_embedding is needed to process tokentype_ids\")\n            # [b s h] -&gt; [s b h] (So that it can be added with embeddings)\n            tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(1, 0, 2)\n            embeddings = embeddings + tokentype_embedding\n        else:\n            assert self.tokentype_embeddings is None\n\n        # If the input flag for fp32 residual connection is set, convert for float.\n        if self.config.fp32_residual_connection:\n            embeddings = embeddings.float()\n\n        # Dropout.\n        if self.config.sequence_parallel:\n            embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)\n            # `scatter_to_sequence_parallel_region` returns a view, which prevents\n            # the original tensor from being garbage collected. Clone to facilitate GC.\n            # Has a small runtime cost (~0.5%).\n            if self.config.clone_scatter_output_in_embedding:\n                embeddings = embeddings.clone()\n            with tensor_parallel.get_cuda_rng_tracker().fork():\n                embeddings = self.embedding_dropout(embeddings)\n        else:\n            embeddings = self.embedding_dropout(embeddings)\n\n        return embeddings\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>The dtype of the embedding weights.</p>"},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding.__init__","title":"<code>__init__(config, vocab_size, max_sequence_length, position_embedding_type='rope', num_tokentypes=0, token_dropout=True, use_attention_mask=True, mask_token_id=torch.nan)</code>","text":"<p>Initialize the ESM2 Embedding module.</p> Source code in <code>bionemo/esm2/model/embedding.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    vocab_size: int,\n    max_sequence_length: int,\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"rope\",\n    num_tokentypes: int = 0,\n    # ESM2 NEW ARGS\n    token_dropout: bool = True,\n    use_attention_mask: bool = True,\n    mask_token_id: Optional[int] = torch.nan,\n) -&gt; None:\n    \"\"\"Initialize the ESM2 Embedding module.\"\"\"\n    super().__init__(\n        config=config,\n        vocab_size=vocab_size,\n        max_sequence_length=max_sequence_length,\n        position_embedding_type=position_embedding_type,\n        num_tokentypes=num_tokentypes,\n    )\n    self.token_dropout = token_dropout\n    self.use_attention_mask = use_attention_mask\n    self.mask_token_id = mask_token_id\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding._apply_esm2_customization","title":"<code>_apply_esm2_customization(word_embeddings, input_ids, attention_mask)</code>","text":"<p>ESM2 customization for attention masking and token dropout.</p> <p>Parameters:</p> Name Type Description Default <code>word_embeddings</code> <code>Tensor[float]</code> <p>The input tokens. Shape: [b, s, h]</p> required <code>input_ids</code> <code>Tensor[int]</code> <p>The input tokens. Shape: [b, s]</p> required <code>attention_mask</code> <code>Tensor[bool]</code> <p>attention mask. Shape: [b, s]</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[Tensor, Tensor]: (Updated embeddings, embedding mask) Shape: ([b, s, h], [b, s])</p> Source code in <code>bionemo/esm2/model/embedding.py</code> <pre><code>def _apply_esm2_customization(\n    self, word_embeddings: Tensor, input_ids: Tensor, attention_mask: Tensor\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"ESM2 customization for attention masking and token dropout.\n\n    Args:\n        word_embeddings (Tensor[float]): The input tokens. Shape: [b, s, h]\n        input_ids (Tensor[int]): The input tokens. Shape: [b, s]\n        attention_mask (Tensor[bool]): attention mask. Shape: [b, s]\n\n    Returns:\n        Tuple[Tensor, Tensor]: (Updated embeddings, embedding mask) Shape: ([b, s, h], [b, s])\n    \"\"\"\n    embeddings_mask = None\n    if attention_mask is not None and (self.token_dropout or self.use_attention_mask):\n        embeddings_mask = attention_mask\n\n    if embeddings_mask is not None and self.token_dropout:\n        word_embeddings = word_embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n        src_lengths = embeddings_mask.sum(-1)\n        mask_ratio_observed = (input_ids == self.mask_token_id).sum(-1).to(self.dtype) / src_lengths\n\n        scale_factor = (1 - ESM2_MASK_RATIO_TRAIN) / (1 - mask_ratio_observed)[:, None, None]\n        word_embeddings = (word_embeddings * scale_factor).to(word_embeddings.dtype)\n    if embeddings_mask is not None and self.use_attention_mask:\n        word_embeddings = (word_embeddings * embeddings_mask.unsqueeze(-1)).to(word_embeddings.dtype)\n    return word_embeddings, embeddings_mask\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding.forward","title":"<code>forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Forward pass of the embedding module.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tokens. Shape: [b, s]</p> required <code>position_ids</code> <code>Tensor</code> <p>The position id's used to calculate position embeddings. Shape: [b, s]</p> required <code>tokentype_ids</code> <code>int</code> <p>The token type ids. Used when args.bert_binary_head is set to True. Defaults to None</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>attention mask. Shape: [b, s]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output embeddings</p> Source code in <code>bionemo/esm2/model/embedding.py</code> <pre><code>def forward(\n    self,\n    input_ids: Tensor,\n    position_ids: Tensor,\n    tokentype_ids: Optional[int] = None,\n    attention_mask: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the embedding module.\n\n    Args:\n        input_ids (Tensor): The input tokens. Shape: [b, s]\n        position_ids (Tensor): The position id's used to calculate position embeddings. Shape: [b, s]\n        tokentype_ids (int, optional): The token type ids. Used when args.bert_binary_head is set to True. Defaults to None\n        attention_mask (Tensor): attention mask. Shape: [b, s]\n\n    Returns:\n        Tensor: The output embeddings\n    \"\"\"\n    word_embeddings = self.word_embeddings(input_ids)  # [b, s, h]\n\n    # ESM2 Customization\n    word_embeddings, embeddings_mask = self._apply_esm2_customization(word_embeddings, input_ids, attention_mask)\n\n    if self.add_position_embedding:\n        position_embeddings = self.position_embeddings(position_ids)\n        embeddings = word_embeddings + position_embeddings\n    else:\n        embeddings = word_embeddings\n\n    # ESM2 Customization: include attention masking from ESM2\n    if embeddings_mask is not None and self.use_attention_mask:\n        embeddings = (embeddings * embeddings_mask.unsqueeze(-1)).to(embeddings.dtype)\n\n    # Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].\n    embeddings = embeddings.transpose(0, 1).contiguous()\n\n    if tokentype_ids is not None:\n        if self.tokentype_embeddings is None:\n            raise ValueError(\"tokentype_embedding is needed to process tokentype_ids\")\n        # [b s h] -&gt; [s b h] (So that it can be added with embeddings)\n        tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(1, 0, 2)\n        embeddings = embeddings + tokentype_embedding\n    else:\n        assert self.tokentype_embeddings is None\n\n    # If the input flag for fp32 residual connection is set, convert for float.\n    if self.config.fp32_residual_connection:\n        embeddings = embeddings.float()\n\n    # Dropout.\n    if self.config.sequence_parallel:\n        embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)\n        # `scatter_to_sequence_parallel_region` returns a view, which prevents\n        # the original tensor from being garbage collected. Clone to facilitate GC.\n        # Has a small runtime cost (~0.5%).\n        if self.config.clone_scatter_output_in_embedding:\n            embeddings = embeddings.clone()\n        with tensor_parallel.get_cuda_rng_tracker().fork():\n            embeddings = self.embedding_dropout(embeddings)\n    else:\n        embeddings = self.embedding_dropout(embeddings)\n\n    return embeddings\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/","title":"Model","text":""},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Config","title":"<code>ESM2Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig</code>, <code>IOMixinWithGettersSetters</code></p> <p>Configuration class for ESM2 model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2Config(ESM2GenericConfig, iom.IOMixinWithGettersSetters):\n    \"\"\"Configuration class for ESM2 model.\"\"\"\n\n    model_cls: Type[ESM2Model] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2GenericConfig","title":"<code>ESM2GenericConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[ESM2ModelT, MegatronLossType]</code></p> <p>Configuration class for ESM2 model.</p> <p>Attributes:</p> Name Type Description <code>num_layers</code> <code>int</code> <p>Number of layers in the model.</p> <code>hidden_size</code> <code>int</code> <p>Hidden size of the model.</p> <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads in the model.</p> <code>ffn_hidden_size</code> <code>int</code> <p>Hidden size of the feed-forward network.</p> <code>hidden_dropout</code> <code>float</code> <p>Dropout rate for hidden layers.</p> <code>attention_dropout</code> <code>float</code> <p>Dropout rate for attention layers.</p> <code>apply_residual_connection_post_layernorm</code> <code>bool</code> <p>Whether to apply residual connection after layer normalization.</p> <code>layernorm_epsilon</code> <code>float</code> <p>Epsilon value for layer normalization.</p> <code>layernorm_zero_centered_gamma</code> <code>float</code> <p>Whether to zero-center the gamma parameter in layer normalization.</p> <code>activation_func</code> <code>Callable</code> <p>Activation function used in the model.</p> <code>init_method_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>apply_query_key_layer_scaling</code> <code>float</code> <p>Whether to apply scaling to query and key layers.</p> <code>masked_softmax_fusion</code> <code>bool</code> <p>Whether to use a kernel that fuses attention softmax with its mask.</p> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>Whether to share embeddings and output weights.</p> <code>enable_autocast</code> <code>bool</code> <p>Whether to enable autocast for mixed precision.</p> <code>biobert_spec_option</code> <code>BiobertSpecOption</code> <p>BiobertSpecOption for the model.</p> <code>position_embedding_type</code> <code>PositionEmbeddingKinds</code> <p>Type of position embedding used in the model.</p> <code>seq_length</code> <code>int</code> <p>Length of the input sequence.</p> <code>make_vocab_size_divisible_by</code> <code>int</code> <p>Make the vocabulary size divisible by this value.</p> <code>token_dropout</code> <code>bool</code> <p>Whether to apply token dropout.</p> <code>use_attention_mask</code> <code>bool</code> <p>Whether to use attention mask.</p> <code>use_esm_attention</code> <code>bool</code> <p>Whether to use ESM attention.</p> <code>attention_softmax_in_fp32</code> <code>bool</code> <p>Whether to use fp32 for attention softmax.</p> <code>optimizer_fn</code> <code>Optional[Callable[[MegatronBioBertModel], Optimizer]]</code> <p>Optional optimizer function for the model.</p> <code>parallel_output</code> <code>bool</code> <p>Whether to use parallel output.</p> <code>rotary_base</code> <code>int</code> <p>Base value for rotary positional encoding.</p> <code>rotary_percent</code> <code>float</code> <p>Percentage of rotary positional encoding.</p> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length.</p> <code>get_attention_mask_from_fusion</code> <code>bool</code> <p>Whether to get attention mask from fusion.</p> <code>nemo1_ckpt_path</code> <code>str | None</code> <p>Path to NEMO1 checkpoint.</p> <code>return_only_hidden_states</code> <code>bool</code> <p>Whether to return only hidden states.</p> <code>loss_reduction_class</code> <code>Type[MegatronLossType]</code> <p>Loss reduction class for the model. Default to BERTMLMLossWithReduction.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2GenericConfig(BioBertConfig[ESM2ModelT, MegatronLossType]):\n    \"\"\"Configuration class for ESM2 model.\n\n    Attributes:\n        num_layers: Number of layers in the model.\n        hidden_size: Hidden size of the model.\n        num_attention_heads: Number of attention heads in the model.\n        ffn_hidden_size: Hidden size of the feed-forward network.\n        hidden_dropout: Dropout rate for hidden layers.\n        attention_dropout: Dropout rate for attention layers.\n        apply_residual_connection_post_layernorm: Whether to apply residual connection after layer normalization.\n        layernorm_epsilon: Epsilon value for layer normalization.\n        layernorm_zero_centered_gamma: Whether to zero-center the gamma parameter in layer normalization.\n        activation_func: Activation function used in the model.\n        init_method_std: Standard deviation for weight initialization.\n        apply_query_key_layer_scaling: Whether to apply scaling to query and key layers.\n        masked_softmax_fusion: Whether to use a kernel that fuses attention softmax with its mask.\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        share_embeddings_and_output_weights: Whether to share embeddings and output weights.\n        enable_autocast: Whether to enable autocast for mixed precision.\n        biobert_spec_option: BiobertSpecOption for the model.\n        position_embedding_type: Type of position embedding used in the model.\n        seq_length: Length of the input sequence.\n        make_vocab_size_divisible_by: Make the vocabulary size divisible by this value.\n        token_dropout: Whether to apply token dropout.\n        use_attention_mask: Whether to use attention mask.\n        use_esm_attention: Whether to use ESM attention.\n        attention_softmax_in_fp32: Whether to use fp32 for attention softmax.\n        optimizer_fn: Optional optimizer function for the model.\n        parallel_output: Whether to use parallel output.\n        rotary_base: Base value for rotary positional encoding.\n        rotary_percent: Percentage of rotary positional encoding.\n        seq_len_interpolation_factor: Interpolation factor for sequence length.\n        get_attention_mask_from_fusion: Whether to get attention mask from fusion.\n        nemo1_ckpt_path: Path to NEMO1 checkpoint.\n        return_only_hidden_states: Whether to return only hidden states.\n        loss_reduction_class: Loss reduction class for the model. Default to BERTMLMLossWithReduction.\n    \"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[ESM2ModelT] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n    num_attention_heads: int = 20\n    ffn_hidden_size: int = 4 * 1280  # Transformer FFN hidden size. Usually 4 * hidden_size.\n    hidden_dropout: float = 0  # ESM2 removes dropout from hidden layers and attention\n    attention_dropout: float = 0.0  # ESM2 does not use attention dropout\n    apply_residual_connection_post_layernorm: bool = False  # TODO: farhadr False is new default, True was BERT pub.\n    layernorm_epsilon: float = 1.0e-5\n    bias_activation_fusion: bool = True  # True degrades accuracy slightly, but is faster.\n    activation_func: Callable = F.gelu  # esm_gelu_func  # ESM2 MLP\n    init_method_std: float = 0.02\n    softmax_scale: float = 1.0\n\n    # embedding\n    token_dropout: bool = True\n    use_attention_mask: bool = True\n\n    # core attention\n    use_esm_attention: bool = False  # Skip ESM2 custom attention for TE acceleration. Still passes golden value test.\n    attention_softmax_in_fp32: bool = False\n    normalize_attention_scores: bool = False\n\n    # From megatron.core.models.gpt.bert_model.GPTModel\n    fp16_lm_cross_entropy: bool = False  # Move the cross entropy unreduced loss calculation for lm head to fp16\n    parallel_output: bool = True\n    share_embeddings_and_output_weights: bool = True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: PositionEmbeddingKinds = \"rope\"  # ESM2 uses relative positional encoding 'ROPE' to extrapolate to longer sequences unseen during training\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec\n\n    optimizer_fn: Optional[Callable[[MegatronBioBertModel], Optimizer]] = None\n    # TODO (@skothenhill,@georgea) update to use the nemo2 checkpoint mixins\n    #  support HF (requires weight interleaving on qkv layer) and nemo1 checkpoints ideally.\n    nemo1_ckpt_path: str | None = None\n    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n    #  self.override_parent_fields will be loaded from the checkpoint and override those values here.\n    initial_ckpt_path: str | None = None\n    # TODO (@jstjohn) come up with a cleaner way in the biobert module to return user requested\n    #  things as part of the workflow for inference and fine-tuning.\n    return_embeddings: bool = False\n    include_embeddings: bool = False\n    include_input_ids: bool = False\n    skip_logits: bool = False\n    return_only_hidden_states: bool = False  # return logits\n\n    def __post_init__(self):\n        # TODO, as a validator?\n        \"\"\"Check configuration compatibility.\"\"\"\n        # reset moe_token_dispatcher_type when variable_seq_lengths is True.\n        # must be performed before super().__post_init__()\n        if self.variable_seq_lengths and self.moe_token_dispatcher_type in [\"allgather\", \"alltoall_seq\"]:\n            logging.warning(\n                \"MoE token dispatcher type 'allgather' and 'alltoall_seq' are not supported with variable sequence lengths. Setting moe_token_dispatcher_type to 'alltoall'.\"\n            )\n            self.moe_token_dispatcher_type = \"alltoall\"\n\n        # reset apply_query_key_layer_scaling based on biobert_spec_option\n        super().__post_init__()\n        if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            self.apply_query_key_layer_scaling = False\n        elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n            logging.warning(\n                \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n            )\n            self.apply_query_key_layer_scaling = True\n        else:\n            raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2GenericConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check configuration compatibility.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __post_init__(self):\n    # TODO, as a validator?\n    \"\"\"Check configuration compatibility.\"\"\"\n    # reset moe_token_dispatcher_type when variable_seq_lengths is True.\n    # must be performed before super().__post_init__()\n    if self.variable_seq_lengths and self.moe_token_dispatcher_type in [\"allgather\", \"alltoall_seq\"]:\n        logging.warning(\n            \"MoE token dispatcher type 'allgather' and 'alltoall_seq' are not supported with variable sequence lengths. Setting moe_token_dispatcher_type to 'alltoall'.\"\n        )\n        self.moe_token_dispatcher_type = \"alltoall\"\n\n    # reset apply_query_key_layer_scaling based on biobert_spec_option\n    super().__post_init__()\n    if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n        self.apply_query_key_layer_scaling = False\n    elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n        logging.warning(\n            \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n        )\n        self.apply_query_key_layer_scaling = True\n    else:\n        raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Model","title":"<code>ESM2Model</code>","text":"<p>               Bases: <code>MegatronBioBertModel</code></p> <p>ESM2 Transformer language model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>class ESM2Model(MegatronBioBertModel):\n    \"\"\"ESM2 Transformer language model.\"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        num_tokentypes: int,\n        transformer_layer_spec: spec_utils.ModuleSpec,\n        vocab_size: int,\n        max_sequence_length: int,\n        tokenizer: Optional[BioNeMoESMTokenizer] = None,\n        pre_process: bool = True,\n        post_process: bool = True,\n        fp16_lm_cross_entropy: bool = False,\n        parallel_output: bool = True,\n        share_embeddings_and_output_weights: bool = False,\n        position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n        rotary_percent: float = 1.0,\n        seq_len_interpolation_factor: Optional[float] = None,\n        add_binary_head: bool = True,\n        return_embeddings: bool = False,\n        include_embeddings: bool = False,\n        include_input_ids: bool = False,\n        use_full_attention_mask: bool = False,\n        include_hiddens: bool = False,\n        skip_logits: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2 model.\n\n        Args:\n            config (TransformerConfig): transformer config\n            num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n            transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n            vocab_size (int): vocabulary size\n            max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n            tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n            pre_process (bool): Include embedding layer (used with pipeline parallelism)\n            post_process (bool): Include an output layer (used with pipeline parallelism)\n            fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n            parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n            share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n            position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n                Defaults is 'learned_absolute'.\n            rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n                Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n            seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n            add_binary_head (bool): Whether to add a binary head. Defaults to True.\n            return_embeddings (bool): Whether to return embeddings. Defaults to False.\n            include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n            include_input_ids (bool): Whether to include input_ids in the output dictionary. Defaults to False.\n            use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n            include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n            skip_logits (bool): Skip writing the token logits in output dict\n        \"\"\"\n        super(MegatronBioBertModel, self).__init__(config=config)\n        self.post_process = post_process\n        self.add_binary_head = add_binary_head\n        if return_embeddings:\n            assert self.post_process, \"only return embeddings on the last pipeline stage\"\n        # `b` = batch, `s` = sequence.\n        # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n        #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n        self.use_full_attention_mask = use_full_attention_mask\n        self.config: TransformerConfig = config\n        self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.pre_process = pre_process\n        self.post_process = post_process\n        self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n        self.parallel_output = parallel_output\n        self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n        self.position_embedding_type = position_embedding_type\n        self.add_binary_head = add_binary_head\n        self.return_embeddings = return_embeddings\n        self.include_embeddings = include_embeddings\n        self.include_hiddens = include_hiddens\n        self.include_input_ids = include_input_ids\n        self.skip_logits = skip_logits\n\n        # megatron core pipelining currently depends on model type\n        self.model_type = ModelType.encoder_or_decoder\n\n        # Embeddings.\n        if self.pre_process:\n            self.register_buffer(\n                \"bert_position_id_tensor\",\n                torch.arange(max_sequence_length, dtype=torch.long, requires_grad=False).unsqueeze(0),\n                persistent=False,\n            )\n            # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n            # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n            # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n            self.embedding = ESM2Embedding(\n                config=self.config,\n                vocab_size=self.vocab_size,\n                max_sequence_length=self.max_sequence_length,\n                position_embedding_type=position_embedding_type,\n                num_tokentypes=num_tokentypes,\n                # ESM2 NEW ARGS\n                token_dropout=self.config.token_dropout,\n                use_attention_mask=self.config.use_attention_mask,\n                mask_token_id=tokenizer.mask_token_id,\n            )\n\n        if self.position_embedding_type == \"rope\":\n            self.rotary_pos_emb = RotaryEmbedding(\n                kv_channels=self.config.kv_channels,\n                rotary_percent=rotary_percent,\n                rotary_interleaved=self.config.rotary_interleaved,\n                seq_len_interpolation_factor=seq_len_interpolation_factor,\n            )\n\n        # Transformer.\n        self.encoder = TransformerBlock(\n            config=self.config,\n            spec=self.transformer_layer_spec,\n            pre_process=self.pre_process,\n            post_process=self.post_process,\n        )\n\n        # Output\n        if post_process:\n            # TODO: Make sure you are passing in the mpu_vocab_size properly\n            self.lm_head = BertLMHead(\n                config.hidden_size,\n                config,\n            )\n\n            self.output_layer = tensor_parallel.ColumnParallelLinear(\n                config.hidden_size,\n                self.vocab_size,\n                config=config,\n                init_method=config.init_method,\n                bias=True,\n                skip_bias_add=False,\n                gather_output=not self.parallel_output,\n                skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n            )\n\n            self.binary_head = None\n            if self.add_binary_head:\n                # TODO: Shoudl switch this to TE ?\n                self.binary_head = get_linear_layer(\n                    config.hidden_size, 2, config.init_method, config.perform_initialization\n                )\n\n                self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n        if self.pre_process or self.post_process:\n            self.setup_embeddings_and_output_layer()\n\n    def embedding_forward(\n        self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n    ):\n        \"\"\"Forward pass of the embedding layer.\n\n        Args:\n            input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n            position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n            tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n            attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n        Returns:\n            Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n        \"\"\"\n        # ESM2 Customization: ESM2Embedding forward takes attention_mask\n        # in addition to the args required by LanguageModelEmbedding\n        return self.embedding(\n            input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n        )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Model.__init__","title":"<code>__init__(config, num_tokentypes, transformer_layer_spec, vocab_size, max_sequence_length, tokenizer=None, pre_process=True, post_process=True, fp16_lm_cross_entropy=False, parallel_output=True, share_embeddings_and_output_weights=False, position_embedding_type='learned_absolute', rotary_percent=1.0, seq_len_interpolation_factor=None, add_binary_head=True, return_embeddings=False, include_embeddings=False, include_input_ids=False, use_full_attention_mask=False, include_hiddens=False, skip_logits=False)</code>","text":"<p>Initialize the ESM2 model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>transformer config</p> required <code>num_tokentypes</code> <code>int</code> <p>Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.</p> required <code>transformer_layer_spec</code> <code>ModuleSpec</code> <p>Specifies module to use for transformer layers</p> required <code>vocab_size</code> <code>int</code> <p>vocabulary size</p> required <code>max_sequence_length</code> <code>int</code> <p>maximum size of sequence. This is used for positional embedding</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>optional tokenizer object (currently only used in the constructor of ESM2Model)</p> <code>None</code> <code>pre_process</code> <code>bool</code> <p>Include embedding layer (used with pipeline parallelism)</p> <code>True</code> <code>post_process</code> <code>bool</code> <p>Include an output layer (used with pipeline parallelism)</p> <code>True</code> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>False</code> <code>parallel_output</code> <code>bool</code> <p>Do not gather the outputs, keep them split across tensor parallel ranks</p> <code>True</code> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>When True, input embeddings and output logit weights are shared. Defaults to False.</p> <code>False</code> <code>position_embedding_type</code> <code>string</code> <p>Position embedding type. Options ['learned_absolute', 'rope']. Defaults is 'learned_absolute'.</p> <code>'learned_absolute'</code> <code>rotary_percent</code> <code>float</code> <p>Percent of rotary dimension to use for rotary position embeddings. Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.</p> <code>1.0</code> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length. Defaults to None.</p> <code>None</code> <code>add_binary_head</code> <code>bool</code> <p>Whether to add a binary head. Defaults to True.</p> <code>True</code> <code>return_embeddings</code> <code>bool</code> <p>Whether to return embeddings. Defaults to False.</p> <code>False</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the output dictionary. Defaults to False.</p> <code>False</code> <code>include_input_ids</code> <code>bool</code> <p>Whether to include input_ids in the output dictionary. Defaults to False.</p> <code>False</code> <code>use_full_attention_mask</code> <code>bool</code> <p>Whether to use full attention mask. Defaults to False.</p> <code>False</code> <code>include_hiddens</code> <code>bool</code> <p>Whether to include hidden states in the output dictionary. Defaults to False.</p> <code>False</code> <code>skip_logits</code> <code>bool</code> <p>Skip writing the token logits in output dict</p> <code>False</code> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    num_tokentypes: int,\n    transformer_layer_spec: spec_utils.ModuleSpec,\n    vocab_size: int,\n    max_sequence_length: int,\n    tokenizer: Optional[BioNeMoESMTokenizer] = None,\n    pre_process: bool = True,\n    post_process: bool = True,\n    fp16_lm_cross_entropy: bool = False,\n    parallel_output: bool = True,\n    share_embeddings_and_output_weights: bool = False,\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n    rotary_percent: float = 1.0,\n    seq_len_interpolation_factor: Optional[float] = None,\n    add_binary_head: bool = True,\n    return_embeddings: bool = False,\n    include_embeddings: bool = False,\n    include_input_ids: bool = False,\n    use_full_attention_mask: bool = False,\n    include_hiddens: bool = False,\n    skip_logits: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the ESM2 model.\n\n    Args:\n        config (TransformerConfig): transformer config\n        num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n        vocab_size (int): vocabulary size\n        max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n        tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n        pre_process (bool): Include embedding layer (used with pipeline parallelism)\n        post_process (bool): Include an output layer (used with pipeline parallelism)\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n        share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n        position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n            Defaults is 'learned_absolute'.\n        rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n            Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n        seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n        add_binary_head (bool): Whether to add a binary head. Defaults to True.\n        return_embeddings (bool): Whether to return embeddings. Defaults to False.\n        include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n        include_input_ids (bool): Whether to include input_ids in the output dictionary. Defaults to False.\n        use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n        include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n        skip_logits (bool): Skip writing the token logits in output dict\n    \"\"\"\n    super(MegatronBioBertModel, self).__init__(config=config)\n    self.post_process = post_process\n    self.add_binary_head = add_binary_head\n    if return_embeddings:\n        assert self.post_process, \"only return embeddings on the last pipeline stage\"\n    # `b` = batch, `s` = sequence.\n    # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n    #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n    self.use_full_attention_mask = use_full_attention_mask\n    self.config: TransformerConfig = config\n    self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n    self.vocab_size = vocab_size\n    self.max_sequence_length = max_sequence_length\n    self.pre_process = pre_process\n    self.post_process = post_process\n    self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n    self.parallel_output = parallel_output\n    self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n    self.position_embedding_type = position_embedding_type\n    self.add_binary_head = add_binary_head\n    self.return_embeddings = return_embeddings\n    self.include_embeddings = include_embeddings\n    self.include_hiddens = include_hiddens\n    self.include_input_ids = include_input_ids\n    self.skip_logits = skip_logits\n\n    # megatron core pipelining currently depends on model type\n    self.model_type = ModelType.encoder_or_decoder\n\n    # Embeddings.\n    if self.pre_process:\n        self.register_buffer(\n            \"bert_position_id_tensor\",\n            torch.arange(max_sequence_length, dtype=torch.long, requires_grad=False).unsqueeze(0),\n            persistent=False,\n        )\n        # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n        # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n        # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n        self.embedding = ESM2Embedding(\n            config=self.config,\n            vocab_size=self.vocab_size,\n            max_sequence_length=self.max_sequence_length,\n            position_embedding_type=position_embedding_type,\n            num_tokentypes=num_tokentypes,\n            # ESM2 NEW ARGS\n            token_dropout=self.config.token_dropout,\n            use_attention_mask=self.config.use_attention_mask,\n            mask_token_id=tokenizer.mask_token_id,\n        )\n\n    if self.position_embedding_type == \"rope\":\n        self.rotary_pos_emb = RotaryEmbedding(\n            kv_channels=self.config.kv_channels,\n            rotary_percent=rotary_percent,\n            rotary_interleaved=self.config.rotary_interleaved,\n            seq_len_interpolation_factor=seq_len_interpolation_factor,\n        )\n\n    # Transformer.\n    self.encoder = TransformerBlock(\n        config=self.config,\n        spec=self.transformer_layer_spec,\n        pre_process=self.pre_process,\n        post_process=self.post_process,\n    )\n\n    # Output\n    if post_process:\n        # TODO: Make sure you are passing in the mpu_vocab_size properly\n        self.lm_head = BertLMHead(\n            config.hidden_size,\n            config,\n        )\n\n        self.output_layer = tensor_parallel.ColumnParallelLinear(\n            config.hidden_size,\n            self.vocab_size,\n            config=config,\n            init_method=config.init_method,\n            bias=True,\n            skip_bias_add=False,\n            gather_output=not self.parallel_output,\n            skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n        )\n\n        self.binary_head = None\n        if self.add_binary_head:\n            # TODO: Shoudl switch this to TE ?\n            self.binary_head = get_linear_layer(\n                config.hidden_size, 2, config.init_method, config.perform_initialization\n            )\n\n            self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n    if self.pre_process or self.post_process:\n        self.setup_embeddings_and_output_layer()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Model.embedding_forward","title":"<code>embedding_forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Forward pass of the embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, sequence_length) containing the input IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the position IDs.</p> required <code>tokentype_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def embedding_forward(\n    self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n):\n    \"\"\"Forward pass of the embedding layer.\n\n    Args:\n        input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n        position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n        tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n        attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n    \"\"\"\n    # ESM2 Customization: ESM2Embedding forward takes attention_mask\n    # in addition to the args required by LanguageModelEmbedding\n    return self.embedding(\n        input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.esm_gelu_func","title":"<code>esm_gelu_func(x)</code>","text":"<p>ESM2-specific gelu implementation from the original ESM repo.</p> <p>Warning</p> <p>Using F.gelu yields subtly wrong results, but only when used in combination with bias_activation_fusion=True This variant will not allow you to use bias_activation_fusion=True, which may be the only accuracy benefit over a native F.gelu.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of any given dimension</p> required Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@torch.compile\ndef esm_gelu_func(x: Tensor) -&gt; Tensor:\n    \"\"\"ESM2-specific gelu implementation from the original ESM repo.\n\n    !!! warning\n\n        Using F.gelu yields subtly wrong results, but only when used in combination with bias_activation_fusion=True\n        This variant will not allow you to use bias_activation_fusion=True, which may be the only accuracy benefit over\n        a native F.gelu.\n\n    Args:\n        x: input tensor of any given dimension\n    \"\"\"\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule","title":"<code>ESM2FineTuneDataModule</code>","text":"<p>               Bases: <code>MegatronDataModule</code></p> <p>A PyTorch Lightning DataModule for fine-tuning ESM2 models.</p> <p>This DataModule is designed to handle the data preparation and loading for fine-tuning ESM2 models. It provides a flexible way to create and manage datasets, data loaders, and sampling strategies.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>class ESM2FineTuneDataModule(MegatronDataModule):\n    \"\"\"A PyTorch Lightning DataModule for fine-tuning ESM2 models.\n\n    This DataModule is designed to handle the data preparation and loading for fine-tuning ESM2 models.\n    It provides a flexible way to create and manage datasets, data loaders, and sampling strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dataset: DATASET_TYPES = None,\n        valid_dataset: DATASET_TYPES = None,\n        predict_dataset: DATASET_TYPES = None,\n        seed: int = 42,\n        min_seq_length: int | None = None,\n        max_seq_length: int = 1024,\n        micro_batch_size: int = 4,\n        global_batch_size: int = 8,\n        num_workers: int = 2,\n        persistent_workers: bool = True,\n        pin_memory: bool = True,\n        rampup_batch_size: list[int] | None = None,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2FineTuneDataModule.\n\n        Args:\n            train_dataset: The training dataset.\n            valid_dataset: The validation dataset.\n            predict_dataset: The prediction dataset. Should not be set together with train/valid datasets\n            seed: The random seed to use for shuffling the datasets. Defaults to 42.\n            min_seq_length: The minimum sequence length for the datasets. Defaults to None.\n            max_seq_length: The maximum sequence length for the datasets. Defaults to 1024.\n            micro_batch_size: The micro-batch size for the data loader. Defaults to 4.\n            global_batch_size: The global batch size for the data loader. Defaults to 8.\n            num_workers: The number of worker processes for the data loader. Defaults to 10.\n            persistent_workers: Whether to persist the worker processes. Defaults to True.\n            pin_memory: Whether to pin the data in memory. Defaults to True.\n            rampup_batch_size: The batch size ramp-up schedule. Defaults to None.\n            tokenizer: The tokenizer to use for tokenization. Defaults to the BioNeMoESMTokenizer.\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__()\n        self.train_dataset = train_dataset\n        self.valid_dataset = valid_dataset\n        self.predict_dataset = predict_dataset\n        if predict_dataset is not None:\n            assert train_dataset is None, \"Datamodule expects either trin/valid dataset or predict dataset\"\n        self._seed = seed\n        self._min_seq_length = min_seq_length\n        self._max_seq_length = max_seq_length\n        self._tokenizer = tokenizer\n\n        self._micro_batch_size = micro_batch_size\n        self._num_workers = num_workers\n        self._persistent_workers = persistent_workers\n        self._pin_memory = pin_memory\n\n        self.data_sampler = MegatronDataSampler(\n            seq_len=max_seq_length,\n            micro_batch_size=micro_batch_size,\n            global_batch_size=global_batch_size,\n            dataloader_type=\"single\",  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n            rampup_batch_size=rampup_batch_size,\n            output_log=predict_dataset is None,  # logging does not work with predict step\n        )\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Setup the ESMDataModule.\n\n        Args:\n            stage: Unused.\n\n        Raises:\n            RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n        \"\"\"\n        del stage  # Unused.\n\n        if not hasattr(self, \"trainer\") or self.trainer is None:\n            raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n        if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n            logging.warning(\n                \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n                \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n            )\n\n        # Create training dataset\n        if self.train_dataset is not None:\n            # If classification task, ensure consistent label vocabulary across splits\n            if hasattr(self.train_dataset, \"label_tokenizer\"):\n                self.valid_dataset.label_tokenizer = self.train_dataset.label_tokenizer\n\n            max_train_steps = self.trainer.max_steps\n            if max_train_steps &lt;= 0:\n                raise RuntimeError(\"Please specify trainer.max_steps\")\n\n            num_train_samples = int(max_train_steps * self.data_sampler.global_batch_size)\n            self._train_ds = self._create_epoch_based_dataset(self.train_dataset, num_train_samples)\n\n        # Create validation dataset\n        if self.valid_dataset is not None and self.trainer.limit_val_batches != 0:\n            num_val_samples = infer_num_samples(\n                limit_batches=self.trainer.limit_val_batches,\n                num_samples_in_dataset=len(self.valid_dataset),\n                global_batch_size=self.data_sampler.global_batch_size,\n                stage=\"val\",\n            )\n            self._valid_ds = self._create_epoch_based_dataset(self.valid_dataset, num_val_samples)\n\n        assert (\n            hasattr(self, \"trainer\") and self.trainer is not None\n        ), \"Setup should be completed when trainer and config are attached.\"\n\n    def _create_epoch_based_dataset(\n        self,\n        dataset: InMemoryPerTokenValueDataset | InMemorySingleValueDataset,\n        total_samples: int,\n    ):\n        return MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(dataset),\n            num_samples=total_samples,\n            shuffle=self.predict_dataset is None,\n            seed=self._seed,\n        )\n\n    def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n        \"\"\"Create dataloader for train, validation, and test stages.\n\n        Args:\n            dataset: The dataset to create the dataloader for.\n            mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n            **kwargs: Additional arguments to pass to the dataloader.\n        \"\"\"\n        if mode not in [\"predict\", \"test\"]:\n            self.update_init_global_step()\n        assert self._tokenizer.pad_token_id is not None, \"Tokenizer must have a pad token id.\"\n\n        return WrappedDataLoader(\n            mode=mode,\n            dataset=dataset,\n            num_workers=self._num_workers,\n            pin_memory=self._pin_memory,\n            persistent_workers=self._persistent_workers,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=self._tokenizer.pad_token_id,\n                min_length=self._min_seq_length,\n                max_length=self._max_seq_length,\n            ),\n            **kwargs,\n        )\n\n    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n        \"\"\"Returns the dataloader for training data.\"\"\"\n        assert self._train_ds is not None, \"train_dataset is not provided to ESM2FineTuneDataModule\"\n        return self._create_dataloader(self._train_ds, mode=\"train\")\n\n    def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Returns the dataloader for validation data.\"\"\"\n        assert self._valid_ds is not None, \"valid_dataset is not provided to ESM2FineTuneDataModule\"\n        return self._create_dataloader(self._valid_ds, mode=\"validation\")\n\n    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Returns the dataloader for prediction data.\"\"\"\n        assert self.predict_dataset is not None, \"predict_dataset is not provided to ESM2FineTuneDataModule\"\n        return self._create_dataloader(self.predict_dataset, mode=\"predict\")\n\n    def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Raises a not implemented error.\"\"\"\n        raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.__init__","title":"<code>__init__(train_dataset=None, valid_dataset=None, predict_dataset=None, seed=42, min_seq_length=None, max_seq_length=1024, micro_batch_size=4, global_batch_size=8, num_workers=2, persistent_workers=True, pin_memory=True, rampup_batch_size=None, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Initialize the ESM2FineTuneDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>DATASET_TYPES</code> <p>The training dataset.</p> <code>None</code> <code>valid_dataset</code> <code>DATASET_TYPES</code> <p>The validation dataset.</p> <code>None</code> <code>predict_dataset</code> <code>DATASET_TYPES</code> <p>The prediction dataset. Should not be set together with train/valid datasets</p> <code>None</code> <code>seed</code> <code>int</code> <p>The random seed to use for shuffling the datasets. Defaults to 42.</p> <code>42</code> <code>min_seq_length</code> <code>int | None</code> <p>The minimum sequence length for the datasets. Defaults to None.</p> <code>None</code> <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length for the datasets. Defaults to 1024.</p> <code>1024</code> <code>micro_batch_size</code> <code>int</code> <p>The micro-batch size for the data loader. Defaults to 4.</p> <code>4</code> <code>global_batch_size</code> <code>int</code> <p>The global batch size for the data loader. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>The number of worker processes for the data loader. Defaults to 10.</p> <code>2</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to persist the worker processes. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin the data in memory. Defaults to True.</p> <code>True</code> <code>rampup_batch_size</code> <code>list[int] | None</code> <p>The batch size ramp-up schedule. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use for tokenization. Defaults to the BioNeMoESMTokenizer.</p> <code>get_tokenizer()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def __init__(\n    self,\n    train_dataset: DATASET_TYPES = None,\n    valid_dataset: DATASET_TYPES = None,\n    predict_dataset: DATASET_TYPES = None,\n    seed: int = 42,\n    min_seq_length: int | None = None,\n    max_seq_length: int = 1024,\n    micro_batch_size: int = 4,\n    global_batch_size: int = 8,\n    num_workers: int = 2,\n    persistent_workers: bool = True,\n    pin_memory: bool = True,\n    rampup_batch_size: list[int] | None = None,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n) -&gt; None:\n    \"\"\"Initialize the ESM2FineTuneDataModule.\n\n    Args:\n        train_dataset: The training dataset.\n        valid_dataset: The validation dataset.\n        predict_dataset: The prediction dataset. Should not be set together with train/valid datasets\n        seed: The random seed to use for shuffling the datasets. Defaults to 42.\n        min_seq_length: The minimum sequence length for the datasets. Defaults to None.\n        max_seq_length: The maximum sequence length for the datasets. Defaults to 1024.\n        micro_batch_size: The micro-batch size for the data loader. Defaults to 4.\n        global_batch_size: The global batch size for the data loader. Defaults to 8.\n        num_workers: The number of worker processes for the data loader. Defaults to 10.\n        persistent_workers: Whether to persist the worker processes. Defaults to True.\n        pin_memory: Whether to pin the data in memory. Defaults to True.\n        rampup_batch_size: The batch size ramp-up schedule. Defaults to None.\n        tokenizer: The tokenizer to use for tokenization. Defaults to the BioNeMoESMTokenizer.\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__()\n    self.train_dataset = train_dataset\n    self.valid_dataset = valid_dataset\n    self.predict_dataset = predict_dataset\n    if predict_dataset is not None:\n        assert train_dataset is None, \"Datamodule expects either trin/valid dataset or predict dataset\"\n    self._seed = seed\n    self._min_seq_length = min_seq_length\n    self._max_seq_length = max_seq_length\n    self._tokenizer = tokenizer\n\n    self._micro_batch_size = micro_batch_size\n    self._num_workers = num_workers\n    self._persistent_workers = persistent_workers\n    self._pin_memory = pin_memory\n\n    self.data_sampler = MegatronDataSampler(\n        seq_len=max_seq_length,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        dataloader_type=\"single\",  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n        rampup_batch_size=rampup_batch_size,\n        output_log=predict_dataset is None,  # logging does not work with predict step\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule._create_dataloader","title":"<code>_create_dataloader(dataset, mode, **kwargs)</code>","text":"<p>Create dataloader for train, validation, and test stages.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to create the dataloader for.</p> required <code>mode</code> <code>Mode</code> <p>Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).</p> required <code>**kwargs</code> <p>Additional arguments to pass to the dataloader.</p> <code>{}</code> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n    \"\"\"Create dataloader for train, validation, and test stages.\n\n    Args:\n        dataset: The dataset to create the dataloader for.\n        mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n        **kwargs: Additional arguments to pass to the dataloader.\n    \"\"\"\n    if mode not in [\"predict\", \"test\"]:\n        self.update_init_global_step()\n    assert self._tokenizer.pad_token_id is not None, \"Tokenizer must have a pad token id.\"\n\n    return WrappedDataLoader(\n        mode=mode,\n        dataset=dataset,\n        num_workers=self._num_workers,\n        pin_memory=self._pin_memory,\n        persistent_workers=self._persistent_workers,\n        collate_fn=functools.partial(\n            collate.bert_padding_collate_fn,\n            padding_value=self._tokenizer.pad_token_id,\n            min_length=self._min_seq_length,\n            max_length=self._max_seq_length,\n        ),\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns the dataloader for prediction data.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Returns the dataloader for prediction data.\"\"\"\n    assert self.predict_dataset is not None, \"predict_dataset is not provided to ESM2FineTuneDataModule\"\n    return self._create_dataloader(self.predict_dataset, mode=\"predict\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Setup the ESMDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Unused.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the trainer is not attached, or if the trainer's max_steps is not set.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Setup the ESMDataModule.\n\n    Args:\n        stage: Unused.\n\n    Raises:\n        RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n    \"\"\"\n    del stage  # Unused.\n\n    if not hasattr(self, \"trainer\") or self.trainer is None:\n        raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n    if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n        logging.warning(\n            \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n            \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n        )\n\n    # Create training dataset\n    if self.train_dataset is not None:\n        # If classification task, ensure consistent label vocabulary across splits\n        if hasattr(self.train_dataset, \"label_tokenizer\"):\n            self.valid_dataset.label_tokenizer = self.train_dataset.label_tokenizer\n\n        max_train_steps = self.trainer.max_steps\n        if max_train_steps &lt;= 0:\n            raise RuntimeError(\"Please specify trainer.max_steps\")\n\n        num_train_samples = int(max_train_steps * self.data_sampler.global_batch_size)\n        self._train_ds = self._create_epoch_based_dataset(self.train_dataset, num_train_samples)\n\n    # Create validation dataset\n    if self.valid_dataset is not None and self.trainer.limit_val_batches != 0:\n        num_val_samples = infer_num_samples(\n            limit_batches=self.trainer.limit_val_batches,\n            num_samples_in_dataset=len(self.valid_dataset),\n            global_batch_size=self.data_sampler.global_batch_size,\n            stage=\"val\",\n        )\n        self._valid_ds = self._create_epoch_based_dataset(self.valid_dataset, num_val_samples)\n\n    assert (\n        hasattr(self, \"trainer\") and self.trainer is not None\n    ), \"Setup should be completed when trainer and config are attached.\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Raises a not implemented error.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Raises a not implemented error.\"\"\"\n    raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the dataloader for training data.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n    \"\"\"Returns the dataloader for training data.\"\"\"\n    assert self._train_ds is not None, \"train_dataset is not provided to ESM2FineTuneDataModule\"\n    return self._create_dataloader(self._train_ds, mode=\"train\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the dataloader for validation data.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Returns the dataloader for validation data.\"\"\"\n    assert self._valid_ds is not None, \"valid_dataset is not provided to ESM2FineTuneDataModule\"\n    return self._create_dataloader(self._valid_ds, mode=\"validation\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/","title":"Dataset","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryPerTokenValueDataset","title":"<code>InMemoryPerTokenValueDataset</code>","text":"<p>               Bases: <code>InMemoryProteinDataset</code></p> <p>An in-memory dataset of labeled strings, which are tokenized on demand.</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>class InMemoryPerTokenValueDataset(InMemoryProteinDataset):\n    \"\"\"An in-memory dataset of labeled strings, which are tokenized on demand.\"\"\"\n\n    def __init__(\n        self,\n        sequences: pd.Series,\n        labels: pd.Series,\n        task_type: Literal[\"classification\", \"regression\"] = \"classification\",\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        \"\"\"Initializes a dataset for per-token classification fine-tuning.\n\n        This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n        dataset sequences provided.\n\n        Args:\n            sequences (pd.Series): A pandas Series containing protein sequences.\n            labels (pd.Series, optional): A pandas Series containing labels. Defaults to None.\n            task_type (str): Fine-tuning task type. Defaults to classification. Regression per-token values are not supported.\n            tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n                that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n                generated.\n        \"\"\"\n        super().__init__(sequences, labels, task_type, tokenizer, seed)\n\n        self.task_type = task_type\n        if not task_type == \"classification\":\n            raise ValueError(f\"{task_type} task type is not supported with {self.__class__.__name__}\")\n\n        label_tokenizer = Label2IDTokenizer()\n        self.label_tokenizer = label_tokenizer.build_vocab(self.labels.sort_values(inplace=False).values)\n        self.label_cls_eos_id = MLM_LOSS_IGNORE_INDEX\n\n    def transform_label(self, label: str) -&gt; Tensor:\n        \"\"\"Transform the sequence label by tokenizing them.\n\n        This method tokenizes a sequence of labels into a tensor of tokens and adds CLS/EOS tokens.\n\n        Args:\n            label: label sequence to be transformed\n\n        Returns:\n            tokenized label\n        \"\"\"\n        tokenized_labels = torch.tensor(self.label_tokenizer.text_to_ids(label))\n\n        # for multi-class (mutually exclusive) classification with CrossEntropyLoss\n        cls_eos = torch.tensor([self.label_cls_eos_id], dtype=tokenized_labels.dtype)\n\n        # add cls / eos label ids with padding value -100 to have the same shape as tokenized_sequence\n        labels = torch.cat((cls_eos, tokenized_labels, cls_eos))\n        return labels\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryPerTokenValueDataset.__init__","title":"<code>__init__(sequences, labels, task_type='classification', tokenizer=tokenizer.get_tokenizer(), seed=np.random.SeedSequence().entropy)</code>","text":"<p>Initializes a dataset for per-token classification fine-tuning.</p> <p>This is an in-memory dataset that does not apply masking to the sequence. But keeps track of  in the dataset sequences provided. <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Series</code> <p>A pandas Series containing protein sequences.</p> required <code>labels</code> <code>Series</code> <p>A pandas Series containing labels. Defaults to None.</p> required <code>task_type</code> <code>str</code> <p>Fine-tuning task type. Defaults to classification. Regression per-token values are not supported.</p> <code>'classification'</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to tokenizer.get_tokenizer().</p> <code>get_tokenizer()</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def __init__(\n    self,\n    sequences: pd.Series,\n    labels: pd.Series,\n    task_type: Literal[\"classification\", \"regression\"] = \"classification\",\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n):\n    \"\"\"Initializes a dataset for per-token classification fine-tuning.\n\n    This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n    dataset sequences provided.\n\n    Args:\n        sequences (pd.Series): A pandas Series containing protein sequences.\n        labels (pd.Series, optional): A pandas Series containing labels. Defaults to None.\n        task_type (str): Fine-tuning task type. Defaults to classification. Regression per-token values are not supported.\n        tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n            that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n            generated.\n    \"\"\"\n    super().__init__(sequences, labels, task_type, tokenizer, seed)\n\n    self.task_type = task_type\n    if not task_type == \"classification\":\n        raise ValueError(f\"{task_type} task type is not supported with {self.__class__.__name__}\")\n\n    label_tokenizer = Label2IDTokenizer()\n    self.label_tokenizer = label_tokenizer.build_vocab(self.labels.sort_values(inplace=False).values)\n    self.label_cls_eos_id = MLM_LOSS_IGNORE_INDEX\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryPerTokenValueDataset.transform_label","title":"<code>transform_label(label)</code>","text":"<p>Transform the sequence label by tokenizing them.</p> <p>This method tokenizes a sequence of labels into a tensor of tokens and adds CLS/EOS tokens.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>label sequence to be transformed</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>tokenized label</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def transform_label(self, label: str) -&gt; Tensor:\n    \"\"\"Transform the sequence label by tokenizing them.\n\n    This method tokenizes a sequence of labels into a tensor of tokens and adds CLS/EOS tokens.\n\n    Args:\n        label: label sequence to be transformed\n\n    Returns:\n        tokenized label\n    \"\"\"\n    tokenized_labels = torch.tensor(self.label_tokenizer.text_to_ids(label))\n\n    # for multi-class (mutually exclusive) classification with CrossEntropyLoss\n    cls_eos = torch.tensor([self.label_cls_eos_id], dtype=tokenized_labels.dtype)\n\n    # add cls / eos label ids with padding value -100 to have the same shape as tokenized_sequence\n    labels = torch.cat((cls_eos, tokenized_labels, cls_eos))\n    return labels\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset","title":"<code>InMemoryProteinDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>An in-memory dataset that tokenize strings into BertSample instances.</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>class InMemoryProteinDataset(Dataset):\n    \"\"\"An in-memory dataset that tokenize strings into BertSample instances.\"\"\"\n\n    def __init__(\n        self,\n        sequences: pd.Series,\n        labels: pd.Series | None = None,\n        task_type: Literal[\"classification\", \"regression\", None] = None,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        \"\"\"Initializes a dataset of protein sequences.\n\n        This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n        dataset sequences provided.\n\n        Args:\n            sequences (pd.Series): A pandas Series containing protein sequences.\n            labels (pd.Series, optional): A pandas Series containing labels. Defaults to None.\n            task_type (str, optional): Fine-tuning task type. Defaults to None.\n            tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n                that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n                generated.\n        \"\"\"\n        self.sequences = sequences\n        self.labels = labels\n        self.task_type = task_type\n\n        self.seed = seed\n        self._len = len(self.sequences)\n        self.tokenizer = tokenizer\n\n    @classmethod\n    def from_csv(\n        cls,\n        csv_path: str | os.PathLike,\n        task_type: Literal[\"classification\", \"regression\", None] = None,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        ignore_labels: bool = False,\n        label_column: str = \"labels\",\n    ):\n        \"\"\"Class method to create a ProteinDataset instance from a CSV file.\n\n        Args:\n            csv_path: path to CSV file containing sequences and optionally labels column.\n            task_type (str, optional): Fine-tuning task type. Defaults to None.\n            tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n            ignore_labels (bool): ignore labels column if exist (to avoid reading labels during inference)\n            label_column (str): label column name in CSV file. Defaults to `labels`.\n        \"\"\"\n        df = pd.read_csv(csv_path)\n\n        # Validate presence of required columns\n        if \"sequences\" not in df.columns:\n            raise KeyError(\"The CSV must contain a 'sequences' column.\")\n\n        sequences = df[\"sequences\"]\n        labels = None\n        if not ignore_labels:\n            labels = df[label_column]\n\n        return cls(sequences, labels=labels, task_type=task_type, tokenizer=tokenizer)\n\n    def __len__(self) -&gt; int:\n        \"\"\"The size of the dataset.\"\"\"\n        return self._len\n\n    def __getitem__(self, index: int) -&gt; BertSample:\n        \"\"\"Obtains the BertSample at the given index.\"\"\"\n        sequence = self.sequences[index]\n        tokenized_sequence = self._tokenize(sequence)\n\n        label = tokenized_sequence if self.labels is None else self.transform_label(self.labels.iloc[index])\n        # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n        loss_mask = ~torch.isin(tokenized_sequence, Tensor(self.tokenizer.all_special_ids))\n\n        return {\n            \"text\": tokenized_sequence,\n            \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n            \"labels\": label,\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        }\n\n    def _tokenize(self, sequence: str) -&gt; Tensor:\n        \"\"\"Tokenize a protein sequence.\n\n        Args:\n            sequence: The protein sequence.\n\n        Returns:\n            The tokenized sequence.\n        \"\"\"\n        tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n        return tensor.flatten()  # type: ignore\n\n    def transform_label(self, label):\n        \"\"\"Transform the label.\n\n        This method should be implemented by subclass if label needs additional transformation.\n\n        Args:\n            label: label to be transformed\n\n        Returns:\n            transformed_label\n        \"\"\"\n        return label\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Obtains the BertSample at the given index.</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; BertSample:\n    \"\"\"Obtains the BertSample at the given index.\"\"\"\n    sequence = self.sequences[index]\n    tokenized_sequence = self._tokenize(sequence)\n\n    label = tokenized_sequence if self.labels is None else self.transform_label(self.labels.iloc[index])\n    # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n    loss_mask = ~torch.isin(tokenized_sequence, Tensor(self.tokenizer.all_special_ids))\n\n    return {\n        \"text\": tokenized_sequence,\n        \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n        \"labels\": label,\n        \"loss_mask\": loss_mask,\n        \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n    }\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset.__init__","title":"<code>__init__(sequences, labels=None, task_type=None, tokenizer=tokenizer.get_tokenizer(), seed=np.random.SeedSequence().entropy)</code>","text":"<p>Initializes a dataset of protein sequences.</p> <p>This is an in-memory dataset that does not apply masking to the sequence. But keeps track of  in the dataset sequences provided. <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Series</code> <p>A pandas Series containing protein sequences.</p> required <code>labels</code> <code>Series</code> <p>A pandas Series containing labels. Defaults to None.</p> <code>None</code> <code>task_type</code> <code>str</code> <p>Fine-tuning task type. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to tokenizer.get_tokenizer().</p> <code>get_tokenizer()</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def __init__(\n    self,\n    sequences: pd.Series,\n    labels: pd.Series | None = None,\n    task_type: Literal[\"classification\", \"regression\", None] = None,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n):\n    \"\"\"Initializes a dataset of protein sequences.\n\n    This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n    dataset sequences provided.\n\n    Args:\n        sequences (pd.Series): A pandas Series containing protein sequences.\n        labels (pd.Series, optional): A pandas Series containing labels. Defaults to None.\n        task_type (str, optional): Fine-tuning task type. Defaults to None.\n        tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n            that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n            generated.\n    \"\"\"\n    self.sequences = sequences\n    self.labels = labels\n    self.task_type = task_type\n\n    self.seed = seed\n    self._len = len(self.sequences)\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset.__len__","title":"<code>__len__()</code>","text":"<p>The size of the dataset.</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"The size of the dataset.\"\"\"\n    return self._len\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset._tokenize","title":"<code>_tokenize(sequence)</code>","text":"<p>Tokenize a protein sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>str</code> <p>The protein sequence.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tokenized sequence.</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def _tokenize(self, sequence: str) -&gt; Tensor:\n    \"\"\"Tokenize a protein sequence.\n\n    Args:\n        sequence: The protein sequence.\n\n    Returns:\n        The tokenized sequence.\n    \"\"\"\n    tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n    return tensor.flatten()  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset.from_csv","title":"<code>from_csv(csv_path, task_type=None, tokenizer=tokenizer.get_tokenizer(), ignore_labels=False, label_column='labels')</code>  <code>classmethod</code>","text":"<p>Class method to create a ProteinDataset instance from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str | PathLike</code> <p>path to CSV file containing sequences and optionally labels column.</p> required <code>task_type</code> <code>str</code> <p>Fine-tuning task type. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to tokenizer.get_tokenizer().</p> <code>get_tokenizer()</code> <code>ignore_labels</code> <code>bool</code> <p>ignore labels column if exist (to avoid reading labels during inference)</p> <code>False</code> <code>label_column</code> <code>str</code> <p>label column name in CSV file. Defaults to <code>labels</code>.</p> <code>'labels'</code> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    csv_path: str | os.PathLike,\n    task_type: Literal[\"classification\", \"regression\", None] = None,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    ignore_labels: bool = False,\n    label_column: str = \"labels\",\n):\n    \"\"\"Class method to create a ProteinDataset instance from a CSV file.\n\n    Args:\n        csv_path: path to CSV file containing sequences and optionally labels column.\n        task_type (str, optional): Fine-tuning task type. Defaults to None.\n        tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n        ignore_labels (bool): ignore labels column if exist (to avoid reading labels during inference)\n        label_column (str): label column name in CSV file. Defaults to `labels`.\n    \"\"\"\n    df = pd.read_csv(csv_path)\n\n    # Validate presence of required columns\n    if \"sequences\" not in df.columns:\n        raise KeyError(\"The CSV must contain a 'sequences' column.\")\n\n    sequences = df[\"sequences\"]\n    labels = None\n    if not ignore_labels:\n        labels = df[label_column]\n\n    return cls(sequences, labels=labels, task_type=task_type, tokenizer=tokenizer)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset.transform_label","title":"<code>transform_label(label)</code>","text":"<p>Transform the label.</p> <p>This method should be implemented by subclass if label needs additional transformation.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <p>label to be transformed</p> required <p>Returns:</p> Type Description <p>transformed_label</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def transform_label(self, label):\n    \"\"\"Transform the label.\n\n    This method should be implemented by subclass if label needs additional transformation.\n\n    Args:\n        label: label to be transformed\n\n    Returns:\n        transformed_label\n    \"\"\"\n    return label\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemorySingleValueDataset","title":"<code>InMemorySingleValueDataset</code>","text":"<p>               Bases: <code>InMemoryProteinDataset</code></p> <p>An in-memory dataset that tokenizes strings into BertSample instances.</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>class InMemorySingleValueDataset(InMemoryProteinDataset):\n    \"\"\"An in-memory dataset that tokenizes strings into BertSample instances.\"\"\"\n\n    def __init__(\n        self,\n        sequences: pd.Series,\n        labels: pd.Series,\n        task_type: Literal[\"classification\", \"regression\"] = \"regression\",\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        \"\"\"Initializes a dataset for single-value fine-tuning.\n\n        This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n        dataset sequences provided.\n\n        Args:\n            sequences (pd.Series): A pandas Series containing protein sequences.\n            labels (pd.Series, optional): A pandas Series containing labels. Defaults to None.\n            task_type (str): Fine-tuning task type. Defaults to regression.\n            tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n                that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n                generated.\n        \"\"\"\n        super().__init__(sequences, labels, task_type, tokenizer, seed)\n\n        self.task_type = task_type\n        if self.task_type == \"classification\":\n            label_tokenizer = Label2IDTokenizer()\n            self.label_tokenizer = label_tokenizer.build_vocab(\n                self.labels.sort_values(inplace=False).values.reshape(-1, 1)\n            )\n\n    def transform_label(self, label: float | str) -&gt; Tensor:\n        \"\"\"Transform the regression label.\n\n        Args:\n            label: single regression/classification value\n\n        Returns:\n            tokenized label\n        \"\"\"\n        if self.task_type == \"regression\":\n            return torch.tensor([label], dtype=torch.float)\n        elif self.task_type == \"classification\":\n            tokenized_label = torch.tensor(self.label_tokenizer.text_to_ids([label]))\n            return tokenized_label\n        else:\n            raise ValueError(f\"{self.task_type} task type is not supported with {self.__class__.__name__}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemorySingleValueDataset.__init__","title":"<code>__init__(sequences, labels, task_type='regression', tokenizer=tokenizer.get_tokenizer(), seed=np.random.SeedSequence().entropy)</code>","text":"<p>Initializes a dataset for single-value fine-tuning.</p> <p>This is an in-memory dataset that does not apply masking to the sequence. But keeps track of  in the dataset sequences provided. <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Series</code> <p>A pandas Series containing protein sequences.</p> required <code>labels</code> <code>Series</code> <p>A pandas Series containing labels. Defaults to None.</p> required <code>task_type</code> <code>str</code> <p>Fine-tuning task type. Defaults to regression.</p> <code>'regression'</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to tokenizer.get_tokenizer().</p> <code>get_tokenizer()</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def __init__(\n    self,\n    sequences: pd.Series,\n    labels: pd.Series,\n    task_type: Literal[\"classification\", \"regression\"] = \"regression\",\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n):\n    \"\"\"Initializes a dataset for single-value fine-tuning.\n\n    This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n    dataset sequences provided.\n\n    Args:\n        sequences (pd.Series): A pandas Series containing protein sequences.\n        labels (pd.Series, optional): A pandas Series containing labels. Defaults to None.\n        task_type (str): Fine-tuning task type. Defaults to regression.\n        tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n            that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n            generated.\n    \"\"\"\n    super().__init__(sequences, labels, task_type, tokenizer, seed)\n\n    self.task_type = task_type\n    if self.task_type == \"classification\":\n        label_tokenizer = Label2IDTokenizer()\n        self.label_tokenizer = label_tokenizer.build_vocab(\n            self.labels.sort_values(inplace=False).values.reshape(-1, 1)\n        )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/dataset/#bionemo.esm2.model.finetune.dataset.InMemorySingleValueDataset.transform_label","title":"<code>transform_label(label)</code>","text":"<p>Transform the regression label.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>float | str</code> <p>single regression/classification value</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>tokenized label</p> Source code in <code>bionemo/esm2/model/finetune/dataset.py</code> <pre><code>def transform_label(self, label: float | str) -&gt; Tensor:\n    \"\"\"Transform the regression label.\n\n    Args:\n        label: single regression/classification value\n\n    Returns:\n        tokenized label\n    \"\"\"\n    if self.task_type == \"regression\":\n        return torch.tensor([label], dtype=torch.float)\n    elif self.task_type == \"classification\":\n        tokenized_label = torch.tensor(self.label_tokenizer.text_to_ids([label]))\n        return tokenized_label\n    else:\n        raise ValueError(f\"{self.task_type} task type is not supported with {self.__class__.__name__}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/loss/","title":"Loss","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/loss/#bionemo.esm2.model.finetune.loss.ClassifierLossReduction","title":"<code>ClassifierLossReduction</code>","text":"<p>               Bases: <code>BERTMLMLossWithReduction</code></p> <p>A class for calculating the cross entropy loss of classification output.</p> <p>This class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/esm2/model/finetune/loss.py</code> <pre><code>class ClassifierLossReduction(BERTMLMLossWithReduction):\n    \"\"\"A class for calculating the cross entropy loss of classification output.\n\n    This class used for calculating the loss, and for logging the reduced loss across micro batches.\n    \"\"\"\n\n    def forward(\n        self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside classification head.\n\n        Returns:\n            A tuple where the loss tensor will be used for backpropagation and the dict will be passed to\n            the reduce method, which currently only works for logging.\n        \"\"\"\n        targets = batch[\"labels\"].squeeze()  # [b] or [b, s] for sequence-level or token-level classification\n\n        classification_output = forward_out[\"classification_output\"]  # [b, num_class] or [b, s, num_class]\n        # [b, s, num_class] -&gt; [b, num_class, s] to satisfy toke-level input dims for cross_entropy loss\n        if classification_output.dim() == 3:\n            classification_output = classification_output.permute(0, 2, 1)\n\n        loss_mask = batch[\"loss_mask\"]  # [b, s]\n\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            losses = torch.nn.functional.cross_entropy(classification_output, targets, reduction=\"none\")\n            # token-level losses may contain NaNs at masked locations. We use masked_select to filter out these NaNs\n            if classification_output.dim() == 3:\n                masked_loss = torch.masked_select(losses, loss_mask)\n                loss = masked_loss.sum() / loss_mask.sum()\n            else:\n                loss = losses.mean()  # sequence-level single value classification\n        else:\n            raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/loss/#bionemo.esm2.model.finetune.loss.ClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>the output of the forward method inside classification head.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple where the loss tensor will be used for backpropagation and the dict will be passed to</p> <code>PerTokenLossDict | SameSizeLossDict</code> <p>the reduce method, which currently only works for logging.</p> Source code in <code>bionemo/esm2/model/finetune/loss.py</code> <pre><code>def forward(\n    self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside classification head.\n\n    Returns:\n        A tuple where the loss tensor will be used for backpropagation and the dict will be passed to\n        the reduce method, which currently only works for logging.\n    \"\"\"\n    targets = batch[\"labels\"].squeeze()  # [b] or [b, s] for sequence-level or token-level classification\n\n    classification_output = forward_out[\"classification_output\"]  # [b, num_class] or [b, s, num_class]\n    # [b, s, num_class] -&gt; [b, num_class, s] to satisfy toke-level input dims for cross_entropy loss\n    if classification_output.dim() == 3:\n        classification_output = classification_output.permute(0, 2, 1)\n\n    loss_mask = batch[\"loss_mask\"]  # [b, s]\n\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        losses = torch.nn.functional.cross_entropy(classification_output, targets, reduction=\"none\")\n        # token-level losses may contain NaNs at masked locations. We use masked_select to filter out these NaNs\n        if classification_output.dim() == 3:\n            masked_loss = torch.masked_select(losses, loss_mask)\n            loss = masked_loss.sum() / loss_mask.sum()\n        else:\n            loss = losses.mean()  # sequence-level single value classification\n    else:\n        raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/loss/#bionemo.esm2.model.finetune.loss.ClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/esm2/model/finetune/loss.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/loss/#bionemo.esm2.model.finetune.loss.RegressorLossReduction","title":"<code>RegressorLossReduction</code>","text":"<p>               Bases: <code>BERTMLMLossWithReduction</code></p> <p>A class for calculating the MSE loss of regression output.</p> <p>This class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/esm2/model/finetune/loss.py</code> <pre><code>class RegressorLossReduction(BERTMLMLossWithReduction):\n    \"\"\"A class for calculating the MSE loss of regression output.\n\n    This class used for calculating the loss, and for logging the reduced loss across micro batches.\n    \"\"\"\n\n    def forward(\n        self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside classification head.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        regression_output = forward_out[\"regression_output\"]\n        targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            loss = torch.nn.functional.mse_loss(regression_output, targets)\n        else:\n            raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/loss/#bionemo.esm2.model.finetune.loss.RegressorLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>the output of the forward method inside classification head.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/esm2/model/finetune/loss.py</code> <pre><code>def forward(\n    self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside classification head.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    regression_output = forward_out[\"regression_output\"]\n    targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        loss = torch.nn.functional.mse_loss(regression_output, targets)\n    else:\n        raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/loss/#bionemo.esm2.model.finetune.loss.RegressorLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/esm2/model/finetune/loss.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/peft/","title":"Peft","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/peft/#bionemo.esm2.model.finetune.peft.ESM2LoRA","title":"<code>ESM2LoRA</code>","text":"<p>               Bases: <code>LoRA</code></p> <p>LoRA for the BioNeMo2 ESM Model.</p> Source code in <code>bionemo/esm2/model/finetune/peft.py</code> <pre><code>class ESM2LoRA(LoRA):\n    \"\"\"LoRA for the BioNeMo2 ESM Model.\"\"\"\n\n    def __call__(self, model: nn.Module) -&gt; nn.Module:\n        \"\"\"This method is called when the object is called as a function.\n\n        Args:\n            model: The input model.\n\n        Returns:\n            The modified model.\n        \"\"\"\n        fn.walk(model, self.selective_freeze)\n        fn.walk(model, self.transform)\n        return model\n\n    def selective_freeze(self, m: nn.Module, name=None, prefix=None):\n        \"\"\"Freezes specific modules in the given model.\n\n        Args:\n            m (nn.Module): The model to selectively freeze.\n            name (str): The name of the module to freeze. Valid values are \"encoder\" and \"embedding\".\n            prefix (str): The prefix of the module to freeze.\n\n        Returns:\n            nn.Module: The modified model with the specified modules frozen.\n\n        See Also:\n            nemo.collections.llm.fn.mixin.FNMixin\n        \"\"\"\n        if name in [\"encoder\", \"embedding\"]:\n            FNMixin.freeze(m)\n        return m\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/peft/#bionemo.esm2.model.finetune.peft.ESM2LoRA.__call__","title":"<code>__call__(model)</code>","text":"<p>This method is called when the object is called as a function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The input model.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The modified model.</p> Source code in <code>bionemo/esm2/model/finetune/peft.py</code> <pre><code>def __call__(self, model: nn.Module) -&gt; nn.Module:\n    \"\"\"This method is called when the object is called as a function.\n\n    Args:\n        model: The input model.\n\n    Returns:\n        The modified model.\n    \"\"\"\n    fn.walk(model, self.selective_freeze)\n    fn.walk(model, self.transform)\n    return model\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/peft/#bionemo.esm2.model.finetune.peft.ESM2LoRA.selective_freeze","title":"<code>selective_freeze(m, name=None, prefix=None)</code>","text":"<p>Freezes specific modules in the given model.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Module</code> <p>The model to selectively freeze.</p> required <code>name</code> <code>str</code> <p>The name of the module to freeze. Valid values are \"encoder\" and \"embedding\".</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix of the module to freeze.</p> <code>None</code> <p>Returns:</p> Type Description <p>nn.Module: The modified model with the specified modules frozen.</p> See Also <p>nemo.collections.llm.fn.mixin.FNMixin</p> Source code in <code>bionemo/esm2/model/finetune/peft.py</code> <pre><code>def selective_freeze(self, m: nn.Module, name=None, prefix=None):\n    \"\"\"Freezes specific modules in the given model.\n\n    Args:\n        m (nn.Module): The model to selectively freeze.\n        name (str): The name of the module to freeze. Valid values are \"encoder\" and \"embedding\".\n        prefix (str): The prefix of the module to freeze.\n\n    Returns:\n        nn.Module: The modified model with the specified modules frozen.\n\n    See Also:\n        nemo.collections.llm.fn.mixin.FNMixin\n    \"\"\"\n    if name in [\"encoder\", \"embedding\"]:\n        FNMixin.freeze(m)\n    return m\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/","title":"Sequence model","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.ESM2FineTuneSeqConfig","title":"<code>ESM2FineTuneSeqConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig[ESM2FineTuneSeqModel, BERTMLMLossWithReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>@dataclass\nclass ESM2FineTuneSeqConfig(\n    ESM2GenericConfig[ESM2FineTuneSeqModel, BERTMLMLossWithReduction], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ESM2FineTuneSeqModel] = ESM2FineTuneSeqModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    task_type: Literal[\"classification\", \"regression\"] = \"regression\"\n    encoder_frozen: bool = True  # freeze encoder parameters\n    mlp_ft_dropout: float = 0.25  # MLP layer dropout\n    mlp_hidden_size: int = 256\n    mlp_target_size: int = 1\n\n    def get_loss_reduction_class(self) -&gt; Type[BERTMLMLossWithReduction]:\n        \"\"\"Returns RegressorLossReduction class.\"\"\"\n        if self.task_type == \"regression\":\n            return RegressorLossReduction\n        elif self.task_type == \"classification\":\n            return ClassifierLossReduction\n        else:\n            raise ValueError(f\"Unsupported task_type: {self.task_type}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.ESM2FineTuneSeqConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Returns RegressorLossReduction class.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[BERTMLMLossWithReduction]:\n    \"\"\"Returns RegressorLossReduction class.\"\"\"\n    if self.task_type == \"regression\":\n        return RegressorLossReduction\n    elif self.task_type == \"classification\":\n        return ClassifierLossReduction\n    else:\n        raise ValueError(f\"Unsupported task_type: {self.task_type}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.ESM2FineTuneSeqModel","title":"<code>ESM2FineTuneSeqModel</code>","text":"<p>               Bases: <code>ESM2Model</code></p> <p>ESM2 model that is suitable for fine-tuning on downstream tasks.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>class ESM2FineTuneSeqModel(ESM2Model):\n    \"\"\"ESM2 model that is suitable for fine-tuning on downstream tasks.\"\"\"\n\n    def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n        \"\"\"Constructs an instance of the ESM2 model suitable for fine-tuning.\"\"\"\n        super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n\n        # freeze encoder parameters\n        if config.encoder_frozen:\n            for _, param in self.named_parameters():\n                param.requires_grad = False\n\n        self.include_embeddings_finetuning = (\n            include_embeddings  # this include_embeddings is for the final output of fine-tuning\n        )\n        # If post_process is True that means that we are at the last megatron parallelism stage and we can\n        #   apply the head.\n        if post_process:\n            self.task_type = config.task_type\n            # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n            self.head_name = f\"{self.task_type}_head\"  # Example: 'regression_head' or 'classification_head'\n            # Set the attribute dynamically\n            setattr(self, self.head_name, MegatronMLPHead(config))\n\n    def forward(self, *args, **kwargs) -&gt; BioBertOutput | Tensor:\n        \"\"\"Inference.\"\"\"\n        output = super().forward(*args, **kwargs)\n        # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n        if not self.post_process:\n            return output  # we are not at the last pipeline stage so just return what the parent has\n        # Double check that the output from the parent has everything we need to do prediction in this head.\n        if not isinstance(output, dict) or \"embeddings\" not in output:\n            raise ValueError(\n                f\"Expected to find 'embeddings' in the output, and output to be dictionary-like, found {output},\\n\"\n                \"Make sure include_embeddings=True in the call to super().__init__\"\n            )\n        # Get the embeddings from the parent output, and pull out the [CLS] token for this task\n        embeddings: Tensor = output[\"embeddings\"]\n        # Predict our 1d regression target\n        task_head = getattr(self, self.head_name)\n        output[f\"{self.task_type}_output\"] = task_head(embeddings)\n        if not self.include_embeddings_finetuning:\n            del output[\"embeddings\"]\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.ESM2FineTuneSeqModel.__init__","title":"<code>__init__(config, *args, post_process=True, include_embeddings=False, **kwargs)</code>","text":"<p>Constructs an instance of the ESM2 model suitable for fine-tuning.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n    \"\"\"Constructs an instance of the ESM2 model suitable for fine-tuning.\"\"\"\n    super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n\n    # freeze encoder parameters\n    if config.encoder_frozen:\n        for _, param in self.named_parameters():\n            param.requires_grad = False\n\n    self.include_embeddings_finetuning = (\n        include_embeddings  # this include_embeddings is for the final output of fine-tuning\n    )\n    # If post_process is True that means that we are at the last megatron parallelism stage and we can\n    #   apply the head.\n    if post_process:\n        self.task_type = config.task_type\n        # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n        self.head_name = f\"{self.task_type}_head\"  # Example: 'regression_head' or 'classification_head'\n        # Set the attribute dynamically\n        setattr(self, self.head_name, MegatronMLPHead(config))\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.ESM2FineTuneSeqModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; BioBertOutput | Tensor:\n    \"\"\"Inference.\"\"\"\n    output = super().forward(*args, **kwargs)\n    # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n    if not self.post_process:\n        return output  # we are not at the last pipeline stage so just return what the parent has\n    # Double check that the output from the parent has everything we need to do prediction in this head.\n    if not isinstance(output, dict) or \"embeddings\" not in output:\n        raise ValueError(\n            f\"Expected to find 'embeddings' in the output, and output to be dictionary-like, found {output},\\n\"\n            \"Make sure include_embeddings=True in the call to super().__init__\"\n        )\n    # Get the embeddings from the parent output, and pull out the [CLS] token for this task\n    embeddings: Tensor = output[\"embeddings\"]\n    # Predict our 1d regression target\n    task_head = getattr(self, self.head_name)\n    output[f\"{self.task_type}_output\"] = task_head(embeddings)\n    if not self.include_embeddings_finetuning:\n        del output[\"embeddings\"]\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.MegatronMLPHead","title":"<code>MegatronMLPHead</code>","text":"<p>               Bases: <code>MegatronModule</code></p> <p>An MLP class for sequence-level regression.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>class MegatronMLPHead(MegatronModule):\n    \"\"\"An MLP class for sequence-level regression.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config)\n\n        layer_sizes = [config.hidden_size, config.mlp_hidden_size, config.mlp_target_size]\n        self.linear_layers = torch.nn.ModuleList(\n            [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]  # noqa: RUF007\n        )\n        self.act = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=config.mlp_ft_dropout)\n\n    def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n        \"\"\"Inference.\"\"\"\n        # [b, s, h]\n        for layer in self.linear_layers[:-1]:\n            hidden_states = self.dropout(self.act(layer(hidden_states)))\n\n        output = self.linear_layers[-1](hidden_states)\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.MegatronMLPHead.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>def __init__(self, config: TransformerConfig):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config)\n\n    layer_sizes = [config.hidden_size, config.mlp_hidden_size, config.mlp_target_size]\n    self.linear_layers = torch.nn.ModuleList(\n        [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]  # noqa: RUF007\n    )\n    self.act = torch.nn.ReLU()\n    self.dropout = torch.nn.Dropout(p=config.mlp_ft_dropout)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/sequence_model/#bionemo.esm2.model.finetune.sequence_model.MegatronMLPHead.forward","title":"<code>forward(hidden_states)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/sequence_model.py</code> <pre><code>def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n    \"\"\"Inference.\"\"\"\n    # [b, s, h]\n    for layer in self.linear_layers[:-1]:\n        hidden_states = self.dropout(self.act(layer(hidden_states)))\n\n    output = self.linear_layers[-1](hidden_states)\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/","title":"Token model","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.ESM2FineTuneTokenConfig","title":"<code>ESM2FineTuneTokenConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig[ESM2FineTuneTokenModel, ClassifierLossReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>@dataclass\nclass ESM2FineTuneTokenConfig(\n    ESM2GenericConfig[ESM2FineTuneTokenModel, ClassifierLossReduction], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ESM2FineTuneTokenModel] = ESM2FineTuneTokenModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"classification_head\"])\n\n    task_type: Literal[\"classification\", \"regression\"] = \"classification\"\n    encoder_frozen: bool = True  # freeze encoder parameters\n    cnn_num_classes: int = 3  # number of classes in each label\n    cnn_dropout: float = 0.25\n    cnn_hidden_size: int = 32  # The number of output channels in the bottleneck layer of the convolution.\n\n    def get_loss_reduction_class(self) -&gt; Type[ClassifierLossReduction]:\n        \"\"\"The loss function type.\"\"\"\n        return ClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.ESM2FineTuneTokenConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>The loss function type.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[ClassifierLossReduction]:\n    \"\"\"The loss function type.\"\"\"\n    return ClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.ESM2FineTuneTokenModel","title":"<code>ESM2FineTuneTokenModel</code>","text":"<p>               Bases: <code>ESM2Model</code></p> <p>An ESM2 model that is suitable for fine tuning.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>class ESM2FineTuneTokenModel(ESM2Model):\n    \"\"\"An ESM2 model that is suitable for fine tuning.\"\"\"\n\n    def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n\n        # freeze encoder parameters\n        if config.encoder_frozen:\n            for _, param in self.named_parameters():\n                param.requires_grad = False\n\n        self.include_hiddens_finetuning = (\n            include_hiddens  # this include_hiddens is for the final output of fine-tuning\n        )\n        # If post_process is True that means that we are at the last megatron parallelism stage and we can\n        #   apply the head.\n        if post_process:\n            self.task_type = config.task_type\n            # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n            self.head_name = f\"{self.task_type}_head\"  # Example: 'regression_head' or 'classification_head'\n            setattr(self, self.head_name, MegatronConvNetHead(config))\n\n    def forward(self, *args, **kwargs) -&gt; Tensor | BioBertOutput:\n        \"\"\"Inference.\"\"\"\n        output = super().forward(*args, **kwargs)\n        # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n        if not self.post_process:\n            return output  # we are not at the last pipeline stage so just return what the parent has\n        # Double check that the output from the parent has everything we need to do prediction in this head.\n        if not isinstance(output, dict) or \"hidden_states\" not in output:\n            raise ValueError(\n                f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n                \"Make sure include_hiddens=True in the call to super().__init__\"\n            )\n        # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n        hidden_states: Tensor = output[\"hidden_states\"]\n        # Predict our 1d regression target\n        task_head = getattr(self, self.head_name)\n        output[f\"{self.task_type}_output\"] = task_head(hidden_states)\n        if not self.include_hiddens_finetuning:\n            del output[\"hidden_states\"]\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.ESM2FineTuneTokenModel.__init__","title":"<code>__init__(config, *args, include_hiddens=False, post_process=True, **kwargs)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n\n    # freeze encoder parameters\n    if config.encoder_frozen:\n        for _, param in self.named_parameters():\n            param.requires_grad = False\n\n    self.include_hiddens_finetuning = (\n        include_hiddens  # this include_hiddens is for the final output of fine-tuning\n    )\n    # If post_process is True that means that we are at the last megatron parallelism stage and we can\n    #   apply the head.\n    if post_process:\n        self.task_type = config.task_type\n        # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n        self.head_name = f\"{self.task_type}_head\"  # Example: 'regression_head' or 'classification_head'\n        setattr(self, self.head_name, MegatronConvNetHead(config))\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.ESM2FineTuneTokenModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; Tensor | BioBertOutput:\n    \"\"\"Inference.\"\"\"\n    output = super().forward(*args, **kwargs)\n    # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n    if not self.post_process:\n        return output  # we are not at the last pipeline stage so just return what the parent has\n    # Double check that the output from the parent has everything we need to do prediction in this head.\n    if not isinstance(output, dict) or \"hidden_states\" not in output:\n        raise ValueError(\n            f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n            \"Make sure include_hiddens=True in the call to super().__init__\"\n        )\n    # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n    hidden_states: Tensor = output[\"hidden_states\"]\n    # Predict our 1d regression target\n    task_head = getattr(self, self.head_name)\n    output[f\"{self.task_type}_output\"] = task_head(hidden_states)\n    if not self.include_hiddens_finetuning:\n        del output[\"hidden_states\"]\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.MegatronConvNetHead","title":"<code>MegatronConvNetHead</code>","text":"<p>               Bases: <code>MegatronModule</code></p> <p>A convolutional neural network class for residue-level classification.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>class MegatronConvNetHead(MegatronModule):\n    \"\"\"A convolutional neural network class for residue-level classification.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config)\n\n        self.finetune_model = torch.nn.Sequential(\n            torch.nn.Conv2d(config.hidden_size, config.cnn_hidden_size, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n            torch.nn.ReLU(),\n            torch.nn.Dropout(config.cnn_dropout),\n        )\n        # class_heads (torch.nn.ModuleList): A list of convolutional layers, each corresponding to a different class head.\n        # These are used for producing logits scores of varying sizes as specified in `output_sizes`.\n        self.class_heads = torch.nn.Conv2d(32, config.cnn_num_classes, kernel_size=(7, 1), padding=(3, 0))\n\n    def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n        \"\"\"Inference.\"\"\"\n        # [b, s, h] -&gt; [b, h, s, 1]\n        hidden_states = hidden_states.permute(0, 2, 1).unsqueeze(dim=-1)\n        hidden_states = self.finetune_model(hidden_states)  # [b, 32, s, 1]\n        output = self.class_heads(hidden_states).squeeze(dim=-1).permute(0, 2, 1)  # [b, s, output_size]\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.MegatronConvNetHead.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>def __init__(self, config: TransformerConfig):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config)\n\n    self.finetune_model = torch.nn.Sequential(\n        torch.nn.Conv2d(config.hidden_size, config.cnn_hidden_size, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n        torch.nn.ReLU(),\n        torch.nn.Dropout(config.cnn_dropout),\n    )\n    # class_heads (torch.nn.ModuleList): A list of convolutional layers, each corresponding to a different class head.\n    # These are used for producing logits scores of varying sizes as specified in `output_sizes`.\n    self.class_heads = torch.nn.Conv2d(32, config.cnn_num_classes, kernel_size=(7, 1), padding=(3, 0))\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/token_model/#bionemo.esm2.model.finetune.token_model.MegatronConvNetHead.forward","title":"<code>forward(hidden_states)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/token_model.py</code> <pre><code>def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n    \"\"\"Inference.\"\"\"\n    # [b, s, h] -&gt; [b, h, s, 1]\n    hidden_states = hidden_states.permute(0, 2, 1).unsqueeze(dim=-1)\n    hidden_states = self.finetune_model(hidden_states)  # [b, 32, s, 1]\n    output = self.class_heads(hidden_states).squeeze(dim=-1).permute(0, 2, 1)  # [b, s, output_size]\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/","title":"Config models","text":""},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ESM2DataConfig","title":"<code>ESM2DataConfig</code>","text":"<p>               Bases: <code>DataConfig[ESMDataModule]</code></p> <p>ESM2DataConfig is a configuration class for setting up the pre-training data module for ESM2.</p> <p>The ESM2DataModule implements the cluster oriented sampling method defined in the ESM2 publication.</p> <p>Attributes:</p> Name Type Description <code>train_cluster_path</code> <code>Path</code> <p>Path to the training cluster data.</p> <code>train_database_path</code> <code>Path</code> <p>Path to the training database.</p> <code>valid_cluster_path</code> <code>Path</code> <p>Path to the validation cluster data.</p> <code>valid_database_path</code> <code>Path</code> <p>Path to the validation database.</p> <code>micro_batch_size</code> <code>int</code> <p>Size of the micro-batch. Default is 8.</p> <code>result_dir</code> <code>str</code> <p>Directory to store results. Default is \"./results\".</p> <code>min_seq_length</code> <code>int</code> <p>Minimum sequence length. Default is 128.</p> <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Default is 128.</p> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Strategy for random masking. Default is RandomMaskStrategy.ALL_TOKENS.</p> <code>num_dataset_workers</code> <code>int</code> <p>Number of workers for the dataset. Default is 0.</p> <p>Methods:</p> Name Description <code>construct_data_module</code> <p>int) -&gt; ESMDataModule: Constructs and returns an ESMDataModule instance with the provided global batch size.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>class ESM2DataConfig(DataConfig[ESMDataModule]):\n    \"\"\"ESM2DataConfig is a configuration class for setting up the pre-training data module for ESM2.\n\n    The ESM2DataModule implements the cluster oriented sampling method defined in the ESM2 publication.\n\n    Attributes:\n        train_cluster_path (Path): Path to the training cluster data.\n        train_database_path (Path): Path to the training database.\n        valid_cluster_path (Path): Path to the validation cluster data.\n        valid_database_path (Path): Path to the validation database.\n        micro_batch_size (int): Size of the micro-batch. Default is 8.\n        result_dir (str): Directory to store results. Default is \"./results\".\n        min_seq_length (int): Minimum sequence length. Default is 128.\n        max_seq_length (int): Maximum sequence length. Default is 128.\n        random_mask_strategy (RandomMaskStrategy): Strategy for random masking. Default is RandomMaskStrategy.ALL_TOKENS.\n        num_dataset_workers (int): Number of workers for the dataset. Default is 0.\n\n    Methods:\n        construct_data_module(global_batch_size: int) -&gt; ESMDataModule:\n            Constructs and returns an ESMDataModule instance with the provided global batch size.\n    \"\"\"\n\n    train_cluster_path: Path\n    train_database_path: Path\n    valid_cluster_path: Path\n    valid_database_path: Path\n\n    micro_batch_size: int = 8\n    result_dir: str | Path = \"./results\"\n    min_seq_length: int = 128\n    max_seq_length: int = 128\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS\n    num_dataset_workers: int = 0\n\n    @field_serializer(\n        \"train_cluster_path\", \"train_database_path\", \"valid_cluster_path\", \"valid_database_path\", \"result_dir\"\n    )\n    def serialize_paths(self, value: Path) -&gt; str:  # noqa: D102\n        return serialize_path_or_str(value)\n\n    @field_validator(\n        \"train_cluster_path\", \"train_database_path\", \"valid_cluster_path\", \"valid_database_path\", \"result_dir\"\n    )\n    def deserialize_paths(cls, value: str) -&gt; Path:  # noqa: D102\n        return deserialize_str_to_path(value)\n\n    @field_serializer(\"random_mask_strategy\")\n    def serialize_spec_option(self, value: RandomMaskStrategy) -&gt; str:  # noqa: D102\n        return value.value\n\n    @field_validator(\"random_mask_strategy\", mode=\"before\")\n    def deserialize_spec_option(cls, value: str) -&gt; RandomMaskStrategy:  # noqa: D102\n        return RandomMaskStrategy(value)\n\n    def construct_data_module(self, global_batch_size: int) -&gt; ESMDataModule:\n        \"\"\"Constructs and returns an ESMDataModule instance with the provided global batch size.\n\n        This method provides means for constructing the datamodule, any pre-requisites for the DataModule should be\n        aquired here. For example, tokenizers, preprocessing, may want to live in this method.\n\n        Args:\n            global_batch_size (int): Global batch size for the data module. Global batch size must be a function of\n                parallelism settings and the `micro_batch_size` attribute. Since the DataConfig has no ownership over\n                parallelism configuration, we expect someone higher up on the ownership chain to provide the value to\n                this method.\n\n        \"\"\"\n        tokenizer = get_tokenizer()\n        data = ESMDataModule(\n            train_cluster_path=self.train_cluster_path,\n            train_database_path=self.train_database_path,\n            valid_cluster_path=self.valid_cluster_path,\n            valid_database_path=self.valid_database_path,\n            global_batch_size=global_batch_size,\n            micro_batch_size=self.micro_batch_size,\n            min_seq_length=self.min_seq_length,\n            max_seq_length=self.max_seq_length,\n            num_workers=self.num_dataset_workers,\n            random_mask_strategy=self.random_mask_strategy,\n            tokenizer=tokenizer,\n        )\n        return data\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ESM2DataConfig.construct_data_module","title":"<code>construct_data_module(global_batch_size)</code>","text":"<p>Constructs and returns an ESMDataModule instance with the provided global batch size.</p> <p>This method provides means for constructing the datamodule, any pre-requisites for the DataModule should be aquired here. For example, tokenizers, preprocessing, may want to live in this method.</p> <p>Parameters:</p> Name Type Description Default <code>global_batch_size</code> <code>int</code> <p>Global batch size for the data module. Global batch size must be a function of parallelism settings and the <code>micro_batch_size</code> attribute. Since the DataConfig has no ownership over parallelism configuration, we expect someone higher up on the ownership chain to provide the value to this method.</p> required Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>def construct_data_module(self, global_batch_size: int) -&gt; ESMDataModule:\n    \"\"\"Constructs and returns an ESMDataModule instance with the provided global batch size.\n\n    This method provides means for constructing the datamodule, any pre-requisites for the DataModule should be\n    aquired here. For example, tokenizers, preprocessing, may want to live in this method.\n\n    Args:\n        global_batch_size (int): Global batch size for the data module. Global batch size must be a function of\n            parallelism settings and the `micro_batch_size` attribute. Since the DataConfig has no ownership over\n            parallelism configuration, we expect someone higher up on the ownership chain to provide the value to\n            this method.\n\n    \"\"\"\n    tokenizer = get_tokenizer()\n    data = ESMDataModule(\n        train_cluster_path=self.train_cluster_path,\n        train_database_path=self.train_database_path,\n        valid_cluster_path=self.valid_cluster_path,\n        valid_database_path=self.valid_database_path,\n        global_batch_size=global_batch_size,\n        micro_batch_size=self.micro_batch_size,\n        min_seq_length=self.min_seq_length,\n        max_seq_length=self.max_seq_length,\n        num_workers=self.num_dataset_workers,\n        random_mask_strategy=self.random_mask_strategy,\n        tokenizer=tokenizer,\n    )\n    return data\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig","title":"<code>ExposedESM2PretrainConfig</code>","text":"<p>               Bases: <code>ExposedModelConfig[ESM2Config]</code></p> <p>Configuration class for ESM2 pretraining with select exposed parameters.</p> <p>See the inherited ExposedModelConfig for attributes and methods from the base class. Use this class either as a template or extension for custom configurations. Importantly, these kinds of classes should do two things, select attributes to expose to the user, and provide validation and serialization any attributes.</p> <p>Attributes:</p> Name Type Description <code>use_esm_attention</code> <code>bool</code> <p>Flag to skip ESM2 custom attention for TE acceleration. Defaults to False.</p> <code>token_dropout</code> <code>bool</code> <p>Flag to enable token dropout. Defaults to True.</p> <code>normalize_attention_scores</code> <code>bool</code> <p>Flag to normalize attention scores. Defaults to False.</p> <code>variable_seq_lengths</code> <code>bool</code> <p>Flag to enable variable sequence lengths. Defaults to False.</p> <code>core_attention_override</code> <code>Optional[Type[Module]]</code> <p>Optional override for core attention module. Defaults to None.</p> <p>Methods:</p> Name Description <code>restrict_biobert_spec_to_esm2</code> <p>BiobertSpecOption) -&gt; BiobertSpecOption: Validates the BiobertSpecOption to ensure it is compatible with ESM2.</p> <code>serialize_core_attention_override</code> <p>Optional[Type[torch.nn.Module]]) -&gt; Optional[str]: Serializes the core attention override module to a string.</p> <code>validate_core_attention_override</code> <p>Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.</p> <code>validate_and_set_attention_and_scaling</code> <p>Validates and sets the attention and scaling parameters based on the biobert_spec_option.</p> <code>model_validator</code> <p>MainConfig) -&gt; MainConfig: Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.</p> <code>model_class</code> <p>Returns the model class associated with this configuration.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>class ExposedESM2PretrainConfig(ExposedModelConfig[ESM2Config]):\n    \"\"\"Configuration class for ESM2 pretraining with select exposed parameters.\n\n    See the inherited ExposedModelConfig for attributes and methods from the base class. Use this class either\n    as a template or extension for custom configurations. Importantly, these kinds of classes should do two things,\n    select attributes to expose to the user, and provide validation and serialization any attributes.\n\n    Attributes:\n        use_esm_attention (bool): Flag to skip ESM2 custom attention for TE acceleration. Defaults to False.\n        token_dropout (bool): Flag to enable token dropout. Defaults to True.\n        normalize_attention_scores (bool): Flag to normalize attention scores. Defaults to False.\n        variable_seq_lengths (bool): Flag to enable variable sequence lengths. Defaults to False.\n        core_attention_override (Optional[Type[torch.nn.Module]]): Optional override for core attention module. Defaults to None.\n\n    Methods:\n        restrict_biobert_spec_to_esm2(cls, biobert_spec_option: BiobertSpecOption) -&gt; BiobertSpecOption:\n            Validates the BiobertSpecOption to ensure it is compatible with ESM2.\n        serialize_core_attention_override(self, value: Optional[Type[torch.nn.Module]]) -&gt; Optional[str]:\n            Serializes the core attention override module to a string.\n        validate_core_attention_override(cls, value):\n            Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.\n        validate_and_set_attention_and_scaling(self):\n            Validates and sets the attention and scaling parameters based on the biobert_spec_option.\n        model_validator(self, global_cfg: MainConfig) -&gt; MainConfig:\n            Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.\n        model_class(self) -&gt; Type[ESM2Config]:\n            Returns the model class associated with this configuration.\n    \"\"\"\n\n    use_esm_attention: bool = False  # Skip ESM2 custom attention for TE acceleration. Still passes golden value test.\n    token_dropout: bool = True\n    normalize_attention_scores: bool = False\n    variable_seq_lengths: bool = False\n    core_attention_override: Type[torch.nn.Module] | None = None\n\n    @field_serializer(\"core_attention_override\")\n    def serialize_core_attention_override(self, value: Optional[Type[torch.nn.Module]]) -&gt; Optional[str]:\n        \"\"\"Serializes the core attention override module to a string.\"\"\"\n        if value is None:\n            return None\n        return f\"{value.__module__}.{value.__name__}\"\n\n    @field_validator(\"core_attention_override\", mode=\"before\")\n    def validate_core_attention_override(cls, value):\n        \"\"\"Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.\"\"\"\n        if value is None:\n            return None\n        if isinstance(value, str):\n            module_name, class_name = value.rsplit(\".\", 1)\n            try:\n                module = importlib.import_module(module_name)\n                cls = getattr(module, class_name)\n                if not issubclass(cls, torch.nn.Module):\n                    raise ValueError(f\"{cls} is not a subclass of torch.nn.Module\")\n                return cls\n            except (ImportError, AttributeError):\n                raise ValueError(f\"Cannot import {value}\")\n        return value\n\n    @model_validator(mode=\"after\")\n    def validate_and_set_attention_and_scaling(self):\n        \"\"\"Validates and sets the attention and scaling parameters based on the biobert_spec_option.\"\"\"\n        logging.info(\n            \"Mutating apply_query_key_layer_scaling and core_attention_override based on biobert_spec_option..\"\n        )\n        if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            self.apply_query_key_layer_scaling = False\n        elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n            logging.warning(\n                \"BiobertSpecOption.esm2_bert_layer_local_spec is deprecated. \"\n                \"Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n            )\n            self.apply_query_key_layer_scaling = True\n        return self\n\n    def model_validator(self, global_cfg: MainConfig) -&gt; MainConfig:\n        \"\"\"Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.\n\n        The global validator acts on the MainConfig, this couples together the ESM2DataConfig with ESM2PretrainingConfig.\n        Additionally, it provides validation for sequence length and parallelism settings.\n\n        Args:\n            global_cfg (MainConfig): The global configuration object.\n        \"\"\"\n        global_cfg = super().model_validator(global_cfg)\n        # Need to ensure that at the least we have access to min_seq_length and max_seq_length\n        if not isinstance(global_cfg.data_config, ESM2DataConfig):\n            raise TypeError(f\"ESM2PretrainConfig requires ESM2DataConfig, got {global_cfg.data_config=}\")\n\n        pipeline_model_parallel_size, tensor_model_parallel_size = (\n            global_cfg.parallel_config.pipeline_model_parallel_size,\n            global_cfg.parallel_config.tensor_model_parallel_size,\n        )\n        min_seq_length, max_seq_length = global_cfg.data_config.min_seq_length, global_cfg.data_config.max_seq_length\n        assert (\n            self.variable_seq_lengths\n            == (pipeline_model_parallel_size * tensor_model_parallel_size &gt; 1 and min_seq_length != max_seq_length)\n        ), \"Must set variable_seq_lengths to True when min_seq_length != max_seq_length under pipeline or tensor parallelism.\"\n        return global_cfg\n\n    def model_class(self) -&gt; Type[ESM2Config]:\n        \"\"\"Returns the model class associated with this configuration.\"\"\"\n        return ESM2Config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.model_class","title":"<code>model_class()</code>","text":"<p>Returns the model class associated with this configuration.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>def model_class(self) -&gt; Type[ESM2Config]:\n    \"\"\"Returns the model class associated with this configuration.\"\"\"\n    return ESM2Config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.model_validator","title":"<code>model_validator(global_cfg)</code>","text":"<p>Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.</p> <p>The global validator acts on the MainConfig, this couples together the ESM2DataConfig with ESM2PretrainingConfig. Additionally, it provides validation for sequence length and parallelism settings.</p> <p>Parameters:</p> Name Type Description Default <code>global_cfg</code> <code>MainConfig</code> <p>The global configuration object.</p> required Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>def model_validator(self, global_cfg: MainConfig) -&gt; MainConfig:\n    \"\"\"Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.\n\n    The global validator acts on the MainConfig, this couples together the ESM2DataConfig with ESM2PretrainingConfig.\n    Additionally, it provides validation for sequence length and parallelism settings.\n\n    Args:\n        global_cfg (MainConfig): The global configuration object.\n    \"\"\"\n    global_cfg = super().model_validator(global_cfg)\n    # Need to ensure that at the least we have access to min_seq_length and max_seq_length\n    if not isinstance(global_cfg.data_config, ESM2DataConfig):\n        raise TypeError(f\"ESM2PretrainConfig requires ESM2DataConfig, got {global_cfg.data_config=}\")\n\n    pipeline_model_parallel_size, tensor_model_parallel_size = (\n        global_cfg.parallel_config.pipeline_model_parallel_size,\n        global_cfg.parallel_config.tensor_model_parallel_size,\n    )\n    min_seq_length, max_seq_length = global_cfg.data_config.min_seq_length, global_cfg.data_config.max_seq_length\n    assert (\n        self.variable_seq_lengths\n        == (pipeline_model_parallel_size * tensor_model_parallel_size &gt; 1 and min_seq_length != max_seq_length)\n    ), \"Must set variable_seq_lengths to True when min_seq_length != max_seq_length under pipeline or tensor parallelism.\"\n    return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.serialize_core_attention_override","title":"<code>serialize_core_attention_override(value)</code>","text":"<p>Serializes the core attention override module to a string.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>@field_serializer(\"core_attention_override\")\ndef serialize_core_attention_override(self, value: Optional[Type[torch.nn.Module]]) -&gt; Optional[str]:\n    \"\"\"Serializes the core attention override module to a string.\"\"\"\n    if value is None:\n        return None\n    return f\"{value.__module__}.{value.__name__}\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.validate_and_set_attention_and_scaling","title":"<code>validate_and_set_attention_and_scaling()</code>","text":"<p>Validates and sets the attention and scaling parameters based on the biobert_spec_option.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_and_set_attention_and_scaling(self):\n    \"\"\"Validates and sets the attention and scaling parameters based on the biobert_spec_option.\"\"\"\n    logging.info(\n        \"Mutating apply_query_key_layer_scaling and core_attention_override based on biobert_spec_option..\"\n    )\n    if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n        self.apply_query_key_layer_scaling = False\n    elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n        logging.warning(\n            \"BiobertSpecOption.esm2_bert_layer_local_spec is deprecated. \"\n            \"Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n        )\n        self.apply_query_key_layer_scaling = True\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.validate_core_attention_override","title":"<code>validate_core_attention_override(value)</code>","text":"<p>Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>@field_validator(\"core_attention_override\", mode=\"before\")\ndef validate_core_attention_override(cls, value):\n    \"\"\"Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.\"\"\"\n    if value is None:\n        return None\n    if isinstance(value, str):\n        module_name, class_name = value.rsplit(\".\", 1)\n        try:\n            module = importlib.import_module(module_name)\n            cls = getattr(module, class_name)\n            if not issubclass(cls, torch.nn.Module):\n                raise ValueError(f\"{cls} is not a subclass of torch.nn.Module\")\n            return cls\n        except (ImportError, AttributeError):\n            raise ValueError(f\"Cannot import {value}\")\n    return value\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/main/","title":"Main","text":""},{"location":"API_reference/bionemo/esm2/run/recipes/","title":"Recipes","text":""},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.ESM2Recipes","title":"<code>ESM2Recipes</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pre-baked recipes for ESM2.</p> <p>THIS PYDANTIC MODEL IS NOT MEANT FOR SERIALIZATION. Only used to facilitate argparse. Each recipe should take <code>args</code> as the only argument. We use partials so we can provide this information at runtime. Add new recipes to this model.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>class ESM2Recipes(BaseModel):\n    \"\"\"Pre-baked recipes for ESM2.\n\n    THIS PYDANTIC MODEL IS NOT MEANT FOR SERIALIZATION. Only used to facilitate argparse. Each recipe should take `args`\n    as the only argument. We use partials so we can provide this information at runtime. Add new recipes to this model.\n    \"\"\"\n\n    # Use partials so we can still parameterize the recipes from the CLI (e.g. data paths.)\n    esm2_tiny_test_recipe: Callable[[argparse.Namespace], MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]] = (\n        partial(esm2_tiny_test_recipe)\n    )\n    esm2_8m_recipe: Callable[[argparse.Namespace], MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]] = partial(\n        esm2_8m_recipe\n    )\n    esm2_650m_recipe: Callable[[argparse.Namespace], MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]] = partial(\n        esm2_650m_recipe\n    )\n    esm2_3b_recipe: Callable[[argparse.Namespace], MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]] = partial(\n        esm2_3b_recipe\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.default_adam_optimizer_with_cosine_annealing_recipe","title":"<code>default_adam_optimizer_with_cosine_annealing_recipe(max_steps=None)</code>","text":"<p>Default optimizer scheduler config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def default_adam_optimizer_with_cosine_annealing_recipe(max_steps: Optional[int] = None) -&gt; OptimizerSchedulerConfig:\n    \"\"\"Default optimizer scheduler config for ESM2.\"\"\"\n    return OptimizerSchedulerConfig(max_steps=max_steps)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_experiment_config","title":"<code>esm2_3b_experiment_config(result_dir)</code>","text":"<p>Experiment config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2 650m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=50,\n        result_dir=result_dir,\n        experiment_name=\"esm2-3b-pretraining\",\n        # TODO should this be exposed?\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_model_config","title":"<code>esm2_3b_model_config(initial_ckpt_path=None)</code>","text":"<p>Model config for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_model_config(initial_ckpt_path=None) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 3b.\"\"\"\n    return ExposedESM2PretrainConfig(\n        num_layers=36,\n        hidden_size=2560,\n        ffn_hidden_size=2560 * 4,\n        num_attention_heads=40,\n        seq_length=1024,\n        biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n        initial_ckpt_path=initial_ckpt_path,\n        get_attention_mask_from_fusion=True,\n        params_dtype=\"bf16-mixed\",\n        pipeline_dtype=\"bf16-mixed\",\n        autocast_dtype=\"bf16-mixed\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_parallel_config","title":"<code>esm2_3b_parallel_config()</code>","text":"<p>Parallel config for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Parallel config for ESM2 3b.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=2,\n        pipeline_model_parallel_size=1,\n        # TODO: is this correct?\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        # NOTE assumes 8xGPU node. Can always edit the config.\n        num_devices=8,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_recipe","title":"<code>esm2_3b_recipe(args)</code>","text":"<p>Recipe for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_recipe(args) -&gt; MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]:\n    \"\"\"Recipe for ESM2 3b.\"\"\"\n    return MainConfig(\n        data_config=esm2_base_data_config(args),\n        parallel_config=esm2_3b_parallel_config(),\n        training_config=esm2_base_training_config(max_steps=args.max_steps),  # no changes for 8m\n        bionemo_model_config=esm2_3b_model_config(args.initial_ckpt_path),\n        optim_config=esm2_base_optimizer_scheduler_config(max_steps=args.scheduler_max_steps),  # no changes for 8m\n        experiment_config=esm2_3b_experiment_config(args.result_dir),\n        wandb_config=esm2_3b_wandb_config(),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_wandb_config","title":"<code>esm2_3b_wandb_config()</code>","text":"<p>Wandb config for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for ESM2 3b.\"\"\"\n    return WandbConfig(\n        entity=\"esm2-3b_pretraining\",\n        project=\"esm2-3b_pretraining\",\n        group=\"esm2-3b\",\n        tags=[\"esm2-650m\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_experiment_config","title":"<code>esm2_650m_experiment_config(result_dir)</code>","text":"<p>Experiment config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2 650m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=50,\n        result_dir=result_dir,\n        experiment_name=\"esm2-650m-pretraining\",\n        # TODO should this be exposed?\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_model_config","title":"<code>esm2_650m_model_config(initial_ckpt_path=None)</code>","text":"<p>Model config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_model_config(initial_ckpt_path=None) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 650m.\"\"\"\n    return ExposedESM2PretrainConfig(\n        num_layers=33,\n        hidden_size=1280,\n        ffn_hidden_size=1280 * 4,\n        seq_length=1024,\n        num_attention_heads=20,\n        biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n        initial_ckpt_path=initial_ckpt_path,\n        get_attention_mask_from_fusion=True,\n        params_dtype=\"bf16-mixed\",\n        pipeline_dtype=\"bf16-mixed\",\n        autocast_dtype=\"bf16-mixed\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_recipe","title":"<code>esm2_650m_recipe(args)</code>","text":"<p>Recipe for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_recipe(args) -&gt; MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]:\n    \"\"\"Recipe for ESM2 650m.\"\"\"\n    return MainConfig(\n        data_config=esm2_base_data_config(args),\n        parallel_config=esm2_base_parallel_config(),\n        training_config=esm2_base_training_config(max_steps=args.max_steps),  # no changes for 8m\n        bionemo_model_config=esm2_650m_model_config(args.initial_ckpt_path),\n        optim_config=esm2_base_optimizer_scheduler_config(max_steps=args.scheduler_max_steps),  # no changes for 8m\n        experiment_config=esm2_650m_experiment_config(args.result_dir),\n        wandb_config=esm2_650m_wandb_config(),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_wandb_config","title":"<code>esm2_650m_wandb_config()</code>","text":"<p>Wandb config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for ESM2 650m.\"\"\"\n    return WandbConfig(\n        entity=\"esm2-650m_pretraining\",\n        project=\"esm2-650m_pretraining\",\n        group=\"esm2-650m\",\n        tags=[\"esm2\", \"pretraining\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_experiment_config","title":"<code>esm2_8m_experiment_config(result_dir)</code>","text":"<p>Experiment config for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2 8m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=50,  # default set in previous script.\n        result_dir=result_dir,\n        experiment_name=\"esm2-8m-pretraining\",\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_model_config","title":"<code>esm2_8m_model_config(initial_ckpt_path=None)</code>","text":"<p>Model config for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_model_config(initial_ckpt_path=None) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 8m.\"\"\"\n    return ExposedESM2PretrainConfig(\n        num_layers=6,\n        hidden_size=320,\n        ffn_hidden_size=320 * 4,\n        num_attention_heads=20,\n        seq_length=1024,\n        biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n        initial_ckpt_path=initial_ckpt_path,\n        get_attention_mask_from_fusion=True,\n        params_dtype=\"bf16-mixed\",\n        pipeline_dtype=\"bf16-mixed\",\n        autocast_dtype=\"bf16-mixed\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_recipe","title":"<code>esm2_8m_recipe(args)</code>","text":"<p>Recipe for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_recipe(args) -&gt; MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]:\n    \"\"\"Recipe for ESM2 8m.\"\"\"\n    return MainConfig(\n        data_config=esm2_base_data_config(args),\n        parallel_config=esm2_base_parallel_config(),\n        training_config=esm2_base_training_config(max_steps=args.max_steps),  # no changes for 8m\n        bionemo_model_config=esm2_8m_model_config(args.initial_ckpt_path),\n        optim_config=esm2_base_optimizer_scheduler_config(max_steps=args.scheduler_max_steps),  # no changes for 8m\n        experiment_config=esm2_8m_experiment_config(args.result_dir),\n        wandb_config=esm2_8m_wandb_config(),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_wandb_config","title":"<code>esm2_8m_wandb_config()</code>","text":"<p>Wandb config for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for ESM2 8m.\"\"\"\n    wandb_config = WandbConfig(\n        entity=\"esm2-8m_pretraining\",\n        project=\"esm2-8m_pretraining\",\n        group=\"esm2-8m\",\n        tags=[\"esm2\", \"pretraining\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n    return wandb_config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_data_config","title":"<code>esm2_base_data_config(args)</code>","text":"<p>Base data config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_data_config(args) -&gt; ESM2DataConfig:\n    \"\"\"Base data config for ESM2.\"\"\"\n    data_config = ESM2DataConfig(\n        min_seq_length=1024,\n        max_seq_length=1024,\n        micro_batch_size=1,\n        num_dataset_workers=8,\n        train_cluster_path=args.train_cluster_path,\n        train_database_path=args.train_database_path,\n        valid_cluster_path=args.valid_cluster_path,\n        valid_database_path=args.valid_database_path,\n    )\n    return data_config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_optimizer_scheduler_config","title":"<code>esm2_base_optimizer_scheduler_config(max_steps=None)</code>","text":"<p>Base optimizer scheduler config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_optimizer_scheduler_config(max_steps: Optional[int] = None) -&gt; OptimizerSchedulerConfig:\n    \"\"\"Base optimizer scheduler config for ESM2.\"\"\"\n    return OptimizerSchedulerConfig(\n        optimizer=\"adam\",\n        lr=4e-4,\n        interval=\"step\",\n        monitor=\"val_loss\",\n        lr_scheduler=\"warmup_anneal\",\n        warmup_steps=2000,\n        max_steps=max_steps,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_parallel_config","title":"<code>esm2_base_parallel_config()</code>","text":"<p>Base parallel config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Base parallel config for ESM2.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        num_devices=1,\n        num_nodes=1,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_training_config","title":"<code>esm2_base_training_config(max_steps=500000)</code>","text":"<p>Base training config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_training_config(max_steps: int = 500000) -&gt; TrainingConfig:\n    \"\"\"Base training config for ESM2.\"\"\"\n    return TrainingConfig(\n        max_steps=max_steps,\n        limit_val_batches=1.0,\n        val_check_interval=10_000,\n        precision=\"bf16-mixed\",\n        include_perplexity=True,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_tiny_model_config","title":"<code>esm2_tiny_model_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec, variable_seq_lengths=False)</code>","text":"<p>Model config for ESM2 tiny, used for testing.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_tiny_model_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n    variable_seq_lengths: bool = False,\n) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 tiny, used for testing.\"\"\"\n    return ExposedESM2PretrainConfig(\n        seq_length=seq_length,\n        num_layers=2,\n        hidden_size=32,\n        num_attention_heads=2,\n        ffn_hidden_size=4 * 32,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        biobert_spec_option=biobert_spec_option,\n        get_attention_mask_from_fusion=True,\n        nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n        # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n        initial_ckpt_path=str(initial_ckpt_path) if initial_ckpt_path is not None else None,\n        variable_seq_lengths=variable_seq_lengths,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_tiny_test_recipe","title":"<code>esm2_tiny_test_recipe(args)</code>","text":"<p>Test recipe for ESM2 tiny, used for testing.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_tiny_test_recipe(args):\n    \"\"\"Test recipe for ESM2 tiny, used for testing.\"\"\"\n    parallel_config = simple_parallel_recipe()\n    training_config = tiny_train_config_recipe()\n\n    data_config = ESM2DataConfig(\n        min_seq_length=128,\n        max_seq_length=128,\n        micro_batch_size=2,\n        num_dataset_workers=1,\n        train_cluster_path=args.train_cluster_path,\n        train_database_path=args.train_database_path,\n        valid_cluster_path=args.valid_cluster_path,\n        valid_database_path=args.valid_database_path,\n    )\n    bionemo_model_config = esm2_tiny_model_config(\n        seq_length=data_config.max_seq_length, initial_ckpt_path=args.initial_ckpt_path\n    )\n\n    optim_config = default_adam_optimizer_with_cosine_annealing_recipe(max_steps=args.scheduler_max_steps)\n    experiment_config = experiment_config_recipe(args.result_dir)\n    wandb_config = WandbConfig(\n        project=\"bionemo2-demo\",\n        entity=\"nvidia\",\n        offline=True,\n        tags=[],\n        group=\"dev\",\n        id=\"dev\",\n        log_model=False,\n        anonymous=True,\n    )\n    main_config = MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.experiment_config_recipe","title":"<code>experiment_config_recipe(result_dir='./results')</code>","text":"<p>Experiment config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def experiment_config_recipe(result_dir=\"./results\") -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=result_dir,\n        experiment_name=\"default_experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"val_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.simple_parallel_recipe","title":"<code>simple_parallel_recipe(tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=1)</code>","text":"<p>Simple parallel recipe for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def simple_parallel_recipe(\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    num_devices: int = 1,\n    accumulate_grad_batches: int = 1,\n) -&gt; ParallelConfig:\n    \"\"\"Simple parallel recipe for ESM2.\"\"\"\n    assert (\n        num_devices &gt;= tensor_model_parallel_size * pipeline_model_parallel_size\n    ), \"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\"\n    return ParallelConfig(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        num_devices=num_devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.tiny_train_config_recipe","title":"<code>tiny_train_config_recipe()</code>","text":"<p>Tiny training config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def tiny_train_config_recipe() -&gt; TrainingConfig:\n    \"\"\"Tiny training config for ESM2.\"\"\"\n    return TrainingConfig(max_steps=10, limit_val_batches=2, val_check_interval=2)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/","title":"Index","text":""},{"location":"API_reference/bionemo/esm2/scripts/#esm2-scripts-directory","title":"ESM2 Scripts Directory","text":"<p>This is a collection for one-off scripts that can be ran through the command line. See the <code>[project.scripts]</code> section of the pyproject.toml file for how these are generated.</p>"},{"location":"API_reference/bionemo/esm2/scripts/finetune_esm2/","title":"Finetune esm2","text":""},{"location":"API_reference/bionemo/esm2/scripts/finetune_esm2/#bionemo.esm2.scripts.finetune_esm2.finetune_esm2_entrypoint","title":"<code>finetune_esm2_entrypoint()</code>","text":"<p>Entrypoint for running ESM2 finetuning.</p> Source code in <code>bionemo/esm2/scripts/finetune_esm2.py</code> <pre><code>def finetune_esm2_entrypoint():\n    \"\"\"Entrypoint for running ESM2 finetuning.\"\"\"\n    # 1. get arguments\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # to avoid padding for single value labels:\n    if args.min_seq_length is not None and args.datset_class is InMemorySingleValueDataset:\n        parser.error(\"Arguments --min-seq-length cannot be set when using InMemorySingleValueDataset.\")\n\n    # 2. Call pretrain with args\n    train_model(\n        train_data_path=args.train_data_path,\n        valid_data_path=args.valid_data_path,\n        num_nodes=args.num_nodes,\n        devices=args.num_gpus,\n        min_seq_length=args.min_seq_length,\n        max_seq_length=args.max_seq_length,\n        result_dir=args.result_dir,\n        wandb_entity=args.wandb_entity,\n        wandb_project=args.wandb_project,\n        wandb_tags=args.wandb_tags,\n        wandb_group=args.wandb_group,\n        wandb_id=args.wandb_id,\n        wandb_anonymous=args.wandb_anonymous,\n        wandb_log_model=args.wandb_log_model,\n        wandb_offline=args.wandb_offline,\n        num_steps=args.num_steps,\n        limit_val_batches=args.limit_val_batches,\n        val_check_interval=args.val_check_interval,\n        log_every_n_steps=args.log_every_n_steps,\n        num_dataset_workers=args.num_dataset_workers,\n        lr=args.lr,\n        micro_batch_size=args.micro_batch_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        tensor_model_parallel_size=args.tensor_model_parallel_size,\n        accumulate_grad_batches=args.accumulate_grad_batches,\n        precision=args.precision,\n        task_type=args.task_type,\n        encoder_frozen=args.encoder_frozen,\n        scale_lr_layer=args.scale_lr_layer,\n        lr_multiplier=args.lr_multiplier,\n        # single value classification / regression mlp\n        mlp_ft_dropout=args.mlp_ft_dropout,\n        mlp_hidden_size=args.mlp_hidden_size,\n        mlp_target_size=args.mlp_target_size,\n        # token-level classification cnn\n        cnn_dropout=args.cnn_dropout,\n        cnn_hidden_size=args.cnn_hidden_size,\n        cnn_num_classes=args.cnn_num_classes,\n        experiment_name=args.experiment_name,\n        resume_if_exists=args.resume_if_exists,\n        restore_from_checkpoint_path=args.restore_from_checkpoint_path,\n        save_last_checkpoint=args.save_last_checkpoint,\n        metric_to_monitor_for_checkpoints=args.metric_to_monitor_for_checkpoints,\n        save_top_k=args.save_top_k,\n        nsys_profiling=args.nsys_profiling,\n        nsys_start_step=args.nsys_start_step,\n        nsys_end_step=args.nsys_end_step,\n        nsys_ranks=args.nsys_ranks,\n        dataset_class=args.dataset_class,\n        config_class=args.config_class,\n        overlap_grad_reduce=args.overlap_grad_reduce,\n        overlap_param_gather=not args.no_overlap_param_gather,\n        average_in_collective=not args.no_average_in_collective,\n        grad_reduce_in_fp32=args.grad_reduce_in_fp32,\n        ckpt_async_save=not args.avoid_ckpt_async_save,\n        label_column=args.label_column,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/finetune_esm2/#bionemo.esm2.scripts.finetune_esm2.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the cli parser for this tool.</p> Source code in <code>bionemo/esm2/scripts/finetune_esm2.py</code> <pre><code>def get_parser():\n    \"\"\"Return the cli parser for this tool.\"\"\"\n    # TODO migrate to hydra config\n    # Parse the arguments and pull them out into local variables for ease of future refactor to a\n    #   config management system.\n    parser = argparse.ArgumentParser(description=\"Pretrain ESM2 with UR data.\")\n    parser.add_argument(\n        \"--train-data-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the train data CSV file\",\n    )\n    parser.add_argument(\n        \"--valid-data-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the valid data CSV file\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=get_args(PrecisionTypes),\n        required=False,\n        default=\"bf16-mixed\",\n        help=\"Precision type to use for training.\",\n    )\n    parser.add_argument(\n        \"--task-type\",\n        type=str,\n        choices=[\"regression\", \"classification\"],\n        required=True,\n        default=\"regression\",\n        help=\"Fine-tuning task type.\",\n    )\n    parser.add_argument(\n        \"--label-column\",\n        type=str,\n        required=False,\n        default=\"labels\",\n        help=\"Label column name in CSV datafile.\",\n    )\n    parser.add_argument(\n        \"--encoder-frozen\",\n        action=\"store_true\",\n        default=False,\n        help=\"Freeze the encoder parameters\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        required=False,\n        default=4e-4,\n        help=\"Learning rate for training. Default is 4e-4\",\n    )\n    parser.add_argument(\n        \"--scale-lr-layer\",\n        type=str,\n        required=False,\n        default=None,\n        help=\"Layer name for which we scale the lr by lr-multiplier\",\n    )\n    parser.add_argument(\n        \"--lr-multiplier\",\n        type=float,\n        required=False,\n        default=1.0,\n        help=\"Learning rate multiplier for layers with scale-lr-layer in their name\",\n    )\n    parser.add_argument(\n        \"--mlp-ft-dropout\",\n        type=float,\n        required=False,\n        default=0.25,\n        help=\"Dropout for single value classification / regression mlp. Default is 0.25\",\n    )\n    parser.add_argument(\n        \"--mlp-hidden-size\",\n        type=int,\n        required=False,\n        default=256,\n        help=\"Dimension of hidden layer in mlp task head. Default is 256\",\n    )\n    parser.add_argument(\n        \"--mlp-target-size\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Output dimension of the mlp task head. Set to 1 for regression and number of classes for classification tasks. Default is 1\",\n    )\n    parser.add_argument(\n        \"--cnn-dropout\",\n        type=float,\n        required=False,\n        default=0.25,\n        help=\"Dropout for token-level classification cnn. Default is 0.25\",\n    )\n    parser.add_argument(\n        \"--cnn-hidden-size\",\n        type=int,\n        required=False,\n        default=32,\n        help=\"Hidden dimension of cnn head. Default is 32\",\n    )\n    parser.add_argument(\n        \"--cnn-num-classes\",\n        type=int,\n        required=False,\n        default=3,\n        help=\"Number of classes for token-level classification cnn. Default is 3\",\n    )\n    parser.add_argument(\n        \"--create-tensorboard-logger\", action=\"store_true\", default=False, help=\"Create a tensorboard logger.\"\n    )\n    # FIXME (@skothenhill) figure out how checkpointing and resumption should work with the new nemo trainer\n    parser.add_argument(\n        \"--resume-if-exists\", action=\"store_true\", default=False, help=\"Resume training if a checkpoint exists.\"\n    )\n    parser.add_argument(\n        \"--result-dir\", type=Path, required=False, default=Path(\"./results\"), help=\"Path to the result directory.\"\n    )\n    parser.add_argument(\"--experiment-name\", type=str, required=False, default=\"esm2\", help=\"Name of the experiment.\")\n\n    parser.add_argument(\"--wandb-entity\", type=str, default=None, help=\"The team posting this run\")\n    parser.add_argument(\"--wandb-project\", type=str, default=None, help=\"Wandb project name \")\n    parser.add_argument(\"--wandb-tags\", nargs=\"+\", type=str, default=None, help=\"Tags associated with this run\")\n    parser.add_argument(\n        \"--wandb-group\", type=str, default=None, help=\"A unique string shared by all runs in a given group\"\n    )\n    parser.add_argument(\n        \"--wandb-id\", type=str, default=None, help=\"Sets the version, mainly used to resume a previous run\"\n    )\n    parser.add_argument(\n        \"--wandb-anonymous\", action=\"store_true\", help=\"Enable or explicitly disable anonymous logging\"\n    )\n    parser.add_argument(\n        \"--wandb-log-model\", action=\"store_true\", help=\"Save checkpoints in wandb dir to upload on W&amp;B servers\"\n    )\n    parser.add_argument(\"--wandb-offline\", action=\"store_true\", help=\"Use wandb in offline mode\")\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-nodes\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of nodes to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-steps\",\n        type=int,\n        required=False,\n        default=500000,\n        help=\"Number of steps to use for training. Default is 500000.\",\n    )\n    parser.add_argument(\n        \"--num-dataset-workers\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of workers to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--val-check-interval\",\n        type=int,\n        required=False,\n        default=10000,\n        help=\"Number of steps between validation. Default is 10000.\",\n    )\n    parser.add_argument(\n        \"--log-every-n-steps\",\n        type=int,\n        required=False,\n        help=\"Number of steps between logging. Default is 50.\",\n    )\n    parser.add_argument(\n        \"--min-seq-length\",\n        type=float_or_int_or_none,\n        required=False,\n        default=None,\n        help=\"Minimum sequence length. Sampled will be padded if less than this value. Set 'None' to unset minimum.\",\n    )\n    parser.add_argument(\n        \"--max-seq-length\",\n        type=int,\n        required=False,\n        default=1024,\n        help=\"Maximum sequence length. Samples will be truncated if exceeds this value.\",\n    )\n    parser.add_argument(\n        \"--limit-val-batches\",\n        type=float_or_int_or_none,\n        required=False,\n        default=2,\n        help=\"Number of global batches used for validation if int. Fraction of validation dataset if float. Default is 2.\",\n    )\n    parser.add_argument(\n        \"--micro-batch-size\",\n        type=int,\n        required=False,\n        default=64,\n        help=\"Micro-batch size. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--pipeline-model-parallel-size\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Pipeline model parallel size. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--tensor-model-parallel-size\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Tensor model parallel size. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--accumulate-grad-batches\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Gradient accumulation steps. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--save-last-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the last checkpoint.\",\n    )\n    parser.add_argument(\n        \"--metric-to-monitor-for-checkpoints\",\n        type=str,\n        required=False,\n        default=\"val_loss\",\n        help=\"The metric to monitor for checkpointing.\",\n    )\n    parser.add_argument(\n        \"--save-top-k\",\n        type=int,\n        required=False,\n        default=2,\n        help=\"Save the top k checkpoints.\",\n    )\n    parser.add_argument(\n        \"--restore-from-checkpoint-path\",\n        type=Path,\n        required=False,\n        default=None,\n        help=\"Path to the checkpoint directory to restore from. Will override `--resume-if-exists` when set.\",\n    )\n    parser.add_argument(\n        \"--nsys-profiling\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling output you must run the whole program with `nsys`. For example: \"\n        \" `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true --capture-range=cudaProfilerApi --capture-range-end=stop  [regular python command here]`\",\n    )\n    # start, end, rank\n    parser.add_argument(\n        \"--nsys-start-step\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Start nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--nsys-end-step\",\n        type=int,\n        required=False,\n        help=\"End nsys profiling after this step.\",\n    )\n    # rank as list of integers\n    parser.add_argument(\n        \"--nsys-ranks\",\n        type=int,\n        nargs=\"+\",\n        required=False,\n        default=[0],\n        help=\"Enable nsys profiling for these ranks.\",\n    )\n    # DDP config\n    parser.add_argument(\n        \"--overlap-grad-reduce\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--no-overlap-param-gather\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--no-average-in-collective\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--grad-reduce-in-fp32\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--avoid-ckpt-async-save\",\n        action=\"store_true\",\n        default=False,\n    )\n\n    config_class_options: Dict[str, Type[BioBertConfig]] = SUPPORTED_CONFIGS\n\n    def config_class_type(desc: str) -&gt; Type[BioBertConfig]:\n        try:\n            return config_class_options[desc]\n        except KeyError:\n            raise argparse.ArgumentTypeError(\n                f\"Do not recognize key {desc}, valid options are: {config_class_options.keys()}\"\n            )\n\n    parser.add_argument(\n        \"--config-class\",\n        type=config_class_type,\n        default=ESM2FineTuneSeqConfig,\n        help=\"Model configs link model classes with losses, and handle model initialization (including from a prior \"\n        \"checkpoint). This is how you can fine-tune a model. First train with one config class that points to one model \"\n        \"class and loss, then implement and provide an alternative config class that points to a variant of that model \"\n        \"and alternative loss. In the future this script should also provide similar support for picking different data \"\n        f\"modules for fine-tuning with different data types. Choices: {config_class_options.keys()}\",\n    )\n\n    dataset_class_options: Dict[str, Type[InMemoryProteinDataset]] = SUPPORTED_DATASETS\n\n    def dataset_class_type(desc: str) -&gt; Type[InMemoryProteinDataset]:\n        try:\n            return dataset_class_options[desc]\n        except KeyError:\n            raise argparse.ArgumentTypeError(\n                f\"Do not recognize key {desc}, valid options are: {dataset_class_options.keys()}\"\n            )\n\n    parser.add_argument(\n        \"--dataset-class\",\n        type=dataset_class_type,\n        default=InMemorySingleValueDataset,\n        help=f\"Dataset class name for finetuning. Choices: {config_class_options.keys()}\",\n    )\n    return parser\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/finetune_esm2/#bionemo.esm2.scripts.finetune_esm2.train_model","title":"<code>train_model(train_data_path, valid_data_path, num_nodes, devices, min_seq_length, max_seq_length, result_dir, num_steps, limit_val_batches, val_check_interval, log_every_n_steps, num_dataset_workers, lr, micro_batch_size, accumulate_grad_batches, experiment_name, resume_if_exists, precision, task_type='regression', encoder_frozen=False, scale_lr_layer=None, lr_multiplier=1.0, mlp_ft_dropout=0.25, mlp_hidden_size=256, mlp_target_size=1, cnn_dropout=0.25, cnn_hidden_size=32, cnn_num_classes=3, wandb_entity=None, wandb_project=None, wandb_offline=False, wandb_tags=None, wandb_group=None, wandb_id=None, wandb_anonymous=False, wandb_log_model=False, pipeline_model_parallel_size=1, tensor_model_parallel_size=1, create_tensorboard_logger=False, restore_from_checkpoint_path=None, save_last_checkpoint=True, metric_to_monitor_for_checkpoints='val_loss', save_top_k=2, nsys_profiling=False, nsys_start_step=0, nsys_end_step=None, nsys_ranks=[0], dataset_class=InMemorySingleValueDataset, config_class=ESM2FineTuneSeqConfig, metric_tracker=None, overlap_grad_reduce=False, overlap_param_gather=True, average_in_collective=True, grad_reduce_in_fp32=False, ckpt_async_save=True, label_column='labels')</code>","text":"<p>Train an ESM2 model on UR data.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_path</code> <code>Path</code> <p>path to train CSV</p> required <code>valid_data_path</code> <code>Path</code> <p>path to validation CSV</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes to run on</p> required <code>devices</code> <code>int</code> <p>number of devices</p> required <code>min_seq_length</code> <code>Optional[int]</code> <p>minimum sequence length</p> required <code>max_seq_length</code> <code>int</code> <p>maximum sequence length</p> required <code>result_dir</code> <code>Path</code> <p>directory to store results, logs and checkpoints</p> required <code>num_steps</code> <code>int</code> <p>number of steps to train the model for</p> required <code>limit_val_batches</code> <code>int</code> <p>limit the number of validation global batches to this many</p> required <code>val_check_interval</code> <code>int</code> <p>number of steps to periodically check the validation loss</p> required <code>log_every_n_steps</code> <code>Optional[int]</code> <p>log every n steps</p> required <code>num_dataset_workers</code> <code>int</code> <p>number of dataset workers</p> required <code>lr</code> <code>float</code> <p>learning rate</p> required <code>micro_batch_size</code> <code>int</code> <p>micro batch size, from this and parallelism settings we infer the global batch size</p> required <code>accumulate_grad_batches</code> <code>int</code> <p>number of batches to accumulate gradients for</p> required <code>experiment_name</code> <code>str</code> <p>experiment name, this is the name used for the wandb run, and the sub-directory of the result_dir that stores the logs and checkpoints.</p> required <code>resume_if_exists</code> <code>bool</code> <p>attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]</p> required <code>precision</code> <code>PrecisionTypes</code> <p>Precision type for training (e.g., float16, float32)</p> required <code>task_type</code> <code>Literal[classification, regression]</code> <p>Fine-tuning task type. Default is regression.</p> <code>'regression'</code> <code>encoder_frozen</code> <code>bool</code> <p>Freeze the encoder parameters. Default is False.</p> <code>False</code> <code>scale_lr_layer</code> <code>Optional[str]</code> <p>layer names for which the lr is scaled by lr_multiplier</p> <code>None</code> <code>lr_multiplier</code> <code>float</code> <p>lr multiplier for parameters in scale_lr_layer</p> <code>1.0</code> <code>mlp_ft_dropout</code> <code>float</code> <p>dropout for single value classification / regression mlp</p> <code>0.25</code> <code>mlp_hidden_size</code> <code>int</code> <p>dimension of hidden layer in mlp task head</p> <code>256</code> <code>mlp_target_size</code> <code>int</code> <p>(int): output dimension of the mlp task head (number of classes in classification tasks)</p> <code>1</code> <code>cnn_dropout</code> <code>float</code> <p>dropout for token-level classification cnn</p> <code>0.25</code> <code>cnn_hidden_size</code> <code>int</code> <p>hidden dimension of cnn head</p> <code>32</code> <code>cnn_num_classes</code> <code>int</code> <p>number of classes in token-level classification</p> <code>3</code> <code>wandb_entity</code> <code>Optional[str]</code> <p>The team posting this run (default: your username or your default team)</p> <code>None</code> <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project to which this run will belong</p> <code>None</code> <code>wandb_offline</code> <code>bool</code> <p>Run offline (data can be streamed later to wandb servers).</p> <code>False</code> <code>wandb_tags</code> <code>Optional[List[str]]</code> <p>Tags associated with this run</p> <code>None</code> <code>wandb_group</code> <code>Optional[str]</code> <p>A unique string shared by all runs in a given group</p> <code>None</code> <code>wandb_id</code> <code>Optional[str]</code> <p>Sets the version, mainly used to resume a previous run</p> <code>None</code> <code>wandb_anonymous</code> <code>Optional[bool]</code> <p>Enables or explicitly disables anonymous logging</p> <code>False</code> <code>wandb_log_model</code> <code>bool</code> <p>Save checkpoints in wandb dir to upload on W&amp;B servers</p> <code>False</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>pipeline model parallel size</p> <code>1</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>tensor model parallel size</p> <code>1</code> <code>create_tensorboard_logger</code> <code>bool</code> <p>create the tensorboard logger</p> <code>False</code> <code>restore_from_checkpoint_path</code> <code>Optional[str]</code> <p>If set, restores the model from the directory passed in. Expects the checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.</p> <code>None</code> <code>save_last_checkpoint</code> <code>bool</code> <p>whether to save the last checkpoint</p> <code>True</code> <code>metric_to_monitor_for_checkpoints</code> <code>str</code> <p>metric to monitor for checkpoints</p> <code>'val_loss'</code> <code>save_top_k</code> <code>int</code> <p>number of top checkpoints to save</p> <code>2</code> <code>nsys_profiling</code> <code>bool</code> <p>whether to enable nsys profiling</p> <code>False</code> <code>nsys_start_step</code> <code>int</code> <p>start step for nsys profiling</p> <code>0</code> <code>nsys_end_step</code> <code>Optional[int]</code> <p>end step for nsys profiling</p> <code>None</code> <code>nsys_ranks</code> <code>List[int]</code> <p>ranks for nsys profiling</p> <code>[0]</code> <code>dataset_class</code> <code>Type[InMemoryProteinDataset]</code> <p>The dataset class for loading the data from a CSV file</p> <code>InMemorySingleValueDataset</code> <code>config_class</code> <code>Type[BioBertConfig]</code> <p>The config class for configuring the model using checkpoint provided</p> <code>ESM2FineTuneSeqConfig</code> <code>metric_tracker</code> <code>Callback | None</code> <p>Optional callback to track metrics (used for testing)</p> <code>None</code> <code>overlap_grad_reduce</code> <code>bool</code> <p>overlap gradient reduction</p> <code>False</code> <code>overlap_param_gather</code> <code>bool</code> <p>overlap parameter gather</p> <code>True</code> <code>average_in_collective</code> <code>bool</code> <p>average in collective</p> <code>True</code> <code>grad_reduce_in_fp32</code> <code>bool</code> <p>gradient reduction in fp32</p> <code>False</code> <code>ckpt_async_save</code> <code>bool</code> <p>whether to save ckpt async. Set to False for federated learning</p> <code>True</code> <code>label_column</code> <code>str</code> <p>name of label column in CSV data file. Defaults to <code>labels</code>.</p> <code>'labels'</code> Source code in <code>bionemo/esm2/scripts/finetune_esm2.py</code> <pre><code>def train_model(\n    train_data_path: Path,\n    valid_data_path: Path,\n    num_nodes: int,\n    devices: int,\n    min_seq_length: Optional[int],\n    max_seq_length: int,\n    result_dir: Path,\n    num_steps: int,\n    limit_val_batches: int,\n    val_check_interval: int,\n    log_every_n_steps: Optional[int],\n    num_dataset_workers: int,\n    lr: float,\n    micro_batch_size: int,\n    accumulate_grad_batches: int,\n    experiment_name: str,\n    resume_if_exists: bool,\n    precision: PrecisionTypes,\n    task_type: str = \"regression\",\n    encoder_frozen: bool = False,\n    scale_lr_layer: Optional[str] = None,\n    lr_multiplier: float = 1.0,\n    # single value classification / regression mlp\n    mlp_ft_dropout: float = 0.25,\n    mlp_hidden_size: int = 256,\n    mlp_target_size: int = 1,\n    # token-level classification cnn\n    cnn_dropout: float = 0.25,\n    cnn_hidden_size: int = 32,\n    cnn_num_classes: int = 3,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_offline: bool = False,\n    wandb_tags: Optional[List[str]] = None,\n    wandb_group: Optional[str] = None,\n    wandb_id: Optional[str] = None,\n    wandb_anonymous: Optional[bool] = False,\n    wandb_log_model: bool = False,\n    pipeline_model_parallel_size: int = 1,\n    tensor_model_parallel_size: int = 1,\n    create_tensorboard_logger: bool = False,\n    restore_from_checkpoint_path: Optional[str] = None,\n    save_last_checkpoint: bool = True,\n    metric_to_monitor_for_checkpoints: str = \"val_loss\",\n    save_top_k: int = 2,\n    nsys_profiling: bool = False,\n    nsys_start_step: int = 0,\n    nsys_end_step: Optional[int] = None,\n    nsys_ranks: List[int] = [0],\n    dataset_class: Type[InMemoryProteinDataset] = InMemorySingleValueDataset,\n    config_class: Type[BioBertConfig] = ESM2FineTuneSeqConfig,\n    metric_tracker: Callback | None = None,\n    overlap_grad_reduce: bool = False,  # Default to False to avoid communication issue in gradient synchronization step\n    overlap_param_gather: bool = True,\n    average_in_collective: bool = True,\n    grad_reduce_in_fp32: bool = False,\n    ckpt_async_save: bool = True,\n    label_column: str = \"labels\",\n) -&gt; Tuple[Path, Callback | None, nl.Trainer]:\n    \"\"\"Train an ESM2 model on UR data.\n\n    Args:\n        train_data_path (Path): path to train CSV\n        valid_data_path (Path): path to validation CSV\n        num_nodes (int): Number of nodes to run on\n        devices (int): number of devices\n        min_seq_length (Optional[int]): minimum sequence length\n        max_seq_length (int): maximum sequence length\n        result_dir (Path): directory to store results, logs and checkpoints\n        num_steps (int): number of steps to train the model for\n        limit_val_batches (int): limit the number of validation global batches to this many\n        val_check_interval (int): number of steps to periodically check the validation loss\n        log_every_n_steps (Optional[int]): log every n steps\n        num_dataset_workers (int): number of dataset workers\n        lr (float): learning rate\n        micro_batch_size (int): micro batch size, from this and parallelism settings we infer the global batch size\n        accumulate_grad_batches (int): number of batches to accumulate gradients for\n        experiment_name (str): experiment name, this is the name used for the wandb run, and the sub-directory of the\n            result_dir that stores the logs and checkpoints.\n        resume_if_exists (bool): attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]\n        precision (PrecisionTypes): Precision type for training (e.g., float16, float32)\n        task_type (Literal[\"classification\", \"regression\"]): Fine-tuning task type. Default is regression.\n        encoder_frozen (bool): Freeze the encoder parameters. Default is False.\n        scale_lr_layer (Optional[str]): layer names for which the lr is scaled by lr_multiplier\n        lr_multiplier (float): lr multiplier for parameters in scale_lr_layer\n        mlp_ft_dropout (float): dropout for single value classification / regression mlp\n        mlp_hidden_size (int): dimension of hidden layer in mlp task head\n        mlp_target_size: (int): output dimension of the mlp task head (number of classes in classification tasks)\n        cnn_dropout (float): dropout for token-level classification cnn\n        cnn_hidden_size (int): hidden dimension of cnn head\n        cnn_num_classes (int): number of classes in token-level classification\n        wandb_entity (Optional[str]): The team posting this run (default: your username or your default team)\n        wandb_project (Optional[str]): The name of the project to which this run will belong\n        wandb_offline (bool): Run offline (data can be streamed later to wandb servers).\n        wandb_tags (Optional[List[str]]): Tags associated with this run\n        wandb_group (Optional[str]): A unique string shared by all runs in a given group\n        wandb_id (Optional[str]): Sets the version, mainly used to resume a previous run\n        wandb_anonymous (Optional[bool]): Enables or explicitly disables anonymous logging\n        wandb_log_model (bool): Save checkpoints in wandb dir to upload on W&amp;B servers\n        pipeline_model_parallel_size (int): pipeline model parallel size\n        tensor_model_parallel_size (int): tensor model parallel size\n        create_tensorboard_logger (bool): create the tensorboard logger\n        restore_from_checkpoint_path (Optional[str]): If set, restores the model from the directory passed in. Expects the\n            checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.\n        save_last_checkpoint (bool): whether to save the last checkpoint\n        metric_to_monitor_for_checkpoints (str): metric to monitor for checkpoints\n        save_top_k (int): number of top checkpoints to save\n        nsys_profiling (bool): whether to enable nsys profiling\n        nsys_start_step (int): start step for nsys profiling\n        nsys_end_step (Optional[int]): end step for nsys profiling\n        nsys_ranks (List[int]): ranks for nsys profiling\n        dataset_class (Type[InMemoryProteinDataset]): The dataset class for loading the data from a CSV file\n        config_class (Type[BioBertConfig]): The config class for configuring the model using checkpoint provided\n        metric_tracker: Optional callback to track metrics (used for testing)\n        overlap_grad_reduce (bool): overlap gradient reduction\n        overlap_param_gather (bool): overlap parameter gather\n        average_in_collective (bool): average in collective\n        grad_reduce_in_fp32 (bool): gradient reduction in fp32\n        ckpt_async_save (bool): whether to save ckpt async. Set to False for federated learning\n        label_column (str): name of label column in CSV data file. Defaults to `labels`.\n    \"\"\"\n    # Create the result directory if it does not exist.\n    result_dir.mkdir(parents=True, exist_ok=True)\n\n    # Setup the strategy and trainer\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=micro_batch_size,\n        num_nodes=num_nodes,\n        devices=devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n    )\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        ddp=DistributedDataParallelConfig(\n            check_for_nan_in_grad=True,\n            overlap_grad_reduce=overlap_grad_reduce,\n            overlap_param_gather=overlap_param_gather,\n            average_in_collective=average_in_collective,\n            grad_reduce_in_fp32=grad_reduce_in_fp32,\n            use_distributed_optimizer=True,\n        ),\n        find_unused_parameters=True,\n        gradient_as_bucket_view=True,\n        ckpt_include_optimizer=True,\n        ckpt_async_save=ckpt_async_save,\n        ckpt_parallel_load=True,\n    )\n\n    # for wandb integration\n    # Please refer to https://pytorch-lightning.readthedocs.io/en/0.7.6/api/lightning.pytorch.loggers.html\"\n    wandb_config: Optional[WandbConfig] = (\n        None\n        if wandb_project is None\n        else WandbConfig(\n            offline=wandb_offline,\n            project=wandb_project,\n            entity=wandb_entity,\n            tags=wandb_tags,\n            group=wandb_group,\n            id=wandb_id,\n            anonymous=wandb_anonymous,\n            log_model=wandb_log_model,\n        )\n    )\n\n    callbacks = [\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n        nl_callbacks.PreemptionCallback(),\n    ]\n    if metric_tracker is not None:\n        callbacks.append(metric_tracker)\n    if nsys_profiling:\n        if nsys_end_step is None:\n            nsys_end_step = num_steps\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=nsys_start_step, end_step=nsys_end_step, ranks=nsys_ranks, gen_shape=True\n            )\n        )\n\n    trainer = nl.Trainer(\n        devices=devices,\n        max_steps=num_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        limit_val_batches=limit_val_batches,  # This controls upsampling and downsampling\n        val_check_interval=val_check_interval,\n        log_every_n_steps=log_every_n_steps,\n        num_nodes=num_nodes,\n        callbacks=callbacks,\n        plugins=nl.MegatronMixedPrecision(\n            precision=precision,\n            params_dtype=get_autocast_dtype(precision),\n            pipeline_dtype=get_autocast_dtype(precision),\n            grad_reduce_in_fp32=grad_reduce_in_fp32,\n            autocast_enabled=False,\n        ),\n    )\n\n    tokenizer = get_tokenizer()\n\n    # Initialize the data module.\n    train_dataset = dataset_class.from_csv(train_data_path, task_type=task_type, label_column=label_column)\n    valid_dataset = dataset_class.from_csv(valid_data_path, task_type=task_type, label_column=label_column)\n\n    data_module = ESM2FineTuneDataModule(\n        train_dataset=train_dataset,\n        valid_dataset=valid_dataset,\n        global_batch_size=global_batch_size,\n        micro_batch_size=micro_batch_size,\n        min_seq_length=min_seq_length,\n        max_seq_length=max_seq_length,\n        num_workers=num_dataset_workers,\n        tokenizer=tokenizer,\n    )\n    # Configure the model\n    train_metric = None\n    is_model_parallel = tensor_model_parallel_size * pipeline_model_parallel_size &gt; 1\n    if is_model_parallel:\n        valid_metric = None  # metric logging under model parallelism is not supported yet\n    elif task_type == \"regression\":\n        valid_metric = TorchmetricsConfig(class_path=\"MeanSquaredError\", task=\"regression\", metric_name=\"val_mse\")\n    else:\n        valid_metric = TorchmetricsConfig(\n            class_path=\"Accuracy\",\n            task=\"classification\",\n            kwargs={\n                \"task\": \"multiclass\",\n                \"threshold\": 0.5,\n                \"num_classes\": data_module.train_dataset.label_tokenizer.vocab_size,\n            },\n            metric_name=\"val_acc\",\n        )\n\n    config = config_class(\n        task_type=task_type,\n        encoder_frozen=encoder_frozen,\n        params_dtype=get_autocast_dtype(precision),\n        pipeline_dtype=get_autocast_dtype(precision),\n        autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        initial_ckpt_path=str(restore_from_checkpoint_path),\n        initial_ckpt_skip_keys_with_these_prefixes=[f\"{task_type}_head\"],\n        train_metric=train_metric,\n        valid_metric=valid_metric,\n    )\n    # Mapping of task-dependent config attributes to their new values\n    task_dependent_attr = {\n        \"mlp_ft_dropout\": mlp_ft_dropout,\n        \"mlp_hidden_size\": mlp_hidden_size,\n        \"mlp_target_size\": mlp_target_size,\n        \"cnn_dropout\": cnn_dropout,\n        \"cnn_hidden_size\": cnn_hidden_size,\n        \"cnn_num_classes\": cnn_num_classes,\n    }\n    # Update attributes only if they exist in the config\n    for attr, value in task_dependent_attr.items():\n        if hasattr(config, attr):\n            setattr(config, attr, value)\n\n    optimizer = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=lr,\n            optimizer=\"adam\",  # fused_adam not supported\n            use_distributed_optimizer=True,\n            weight_decay=0.01,\n            adam_beta1=0.9,\n            adam_beta2=0.98,\n        ),\n    )\n    # fiddle is not serializing lambda fn\n    # to bypass serialization of lambda fn scale_lr_condition as part of optimizer configuration\n    if scale_lr_layer:\n        optimizer.scale_lr_cond = lambda name, param: scale_lr_layer in name\n        optimizer.lr_mult = lr_multiplier\n\n    module = biobert_lightning_module(config=config, tokenizer=tokenizer, optimizer=optimizer)\n\n    # Configure our custom Checkpointer\n    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n        save_last=save_last_checkpoint,\n        monitor=metric_to_monitor_for_checkpoints,  # \"val_loss\",\n        save_top_k=save_top_k,\n        every_n_train_steps=val_check_interval,\n        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n        filename=\"checkpoint-{step}-{consumed_samples}\",  # Including step and consumed_samples in the checkpoint filename prevents duplicate filenames and bugs related to this.\n    )\n\n    # Setup the logger and train the model\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=result_dir,\n        name=experiment_name,\n        initialize_tensorboard_logger=create_tensorboard_logger,\n        wandb_config=wandb_config,\n        ckpt_callback=checkpoint_callback,\n    )\n\n    llm.train(\n        model=module,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=resume_if_exists,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n    ckpt_path = Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n    return ckpt_path, metric_tracker, trainer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/infer_esm2/","title":"Infer esm2","text":""},{"location":"API_reference/bionemo/esm2/scripts/infer_esm2/#bionemo.esm2.scripts.infer_esm2.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the cli parser for this tool.</p> Source code in <code>bionemo/esm2/scripts/infer_esm2.py</code> <pre><code>def get_parser():\n    \"\"\"Return the cli parser for this tool.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Infer ESM2.\")\n    parser.add_argument(\n        \"--checkpoint-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the ESM2 pretrained checkpoint\",\n    )\n    parser.add_argument(\n        \"--data-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the CSV file containing sequences and label columns\",\n    )\n    parser.add_argument(\"--results-path\", type=Path, required=True, help=\"Path to the results directory.\")\n\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=get_args(PrecisionTypes),\n        required=False,\n        default=\"bf16-mixed\",\n        help=\"Precision type to use for training.\",\n    )\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-nodes\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of nodes to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--micro-batch-size\",\n        type=int,\n        required=False,\n        default=2,\n        help=\"Micro-batch size. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--pipeline-model-parallel-size\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Pipeline model parallel size. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--tensor-model-parallel-size\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Tensor model parallel size. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--prediction-interval\",\n        type=str,\n        required=False,\n        choices=get_args(IntervalT),\n        default=\"epoch\",\n        help=\"Intervals to write DDP predictions into disk\",\n    )\n    parser.add_argument(\n        \"--include-hiddens\",\n        action=\"store_true\",\n        default=False,\n        help=\"Include hiddens in output of inference\",\n    )\n    parser.add_argument(\n        \"--include-input-ids\",\n        action=\"store_true\",\n        default=False,\n        help=\"Include input_ids in output of inference\",\n    )\n    parser.add_argument(\n        \"--include-embeddings\",\n        action=\"store_true\",\n        default=False,\n        help=\"Include embeddings in output of inference\",\n    )\n    parser.add_argument(\n        \"--include-logits\", action=\"store_true\", default=False, help=\"Include per-token logits in output.\"\n    )\n    config_class_options: Dict[str, Type[BioBertConfig]] = SUPPORTED_CONFIGS\n\n    def config_class_type(desc: str) -&gt; Type[BioBertConfig]:\n        try:\n            return config_class_options[desc]\n        except KeyError:\n            raise argparse.ArgumentTypeError(\n                f\"Do not recognize key {desc}, valid options are: {config_class_options.keys()}\"\n            )\n\n    parser.add_argument(\n        \"--config-class\",\n        type=config_class_type,\n        default=\"ESM2Config\",\n        help=\"Model configs link model classes with losses, and handle model initialization (including from a prior \"\n        \"checkpoint). This is how you can fine-tune a model. First train with one config class that points to one model \"\n        \"class and loss, then implement and provide an alternative config class that points to a variant of that model \"\n        \"and alternative loss. In the future this script should also provide similar support for picking different data \"\n        f\"modules for fine-tuning with different data types. Choices: {config_class_options.keys()}\",\n    )\n    return parser\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/infer_esm2/#bionemo.esm2.scripts.infer_esm2.infer_esm2_entrypoint","title":"<code>infer_esm2_entrypoint()</code>","text":"<p>Entrypoint for running inference on a geneformer checkpoint and data.</p> Source code in <code>bionemo/esm2/scripts/infer_esm2.py</code> <pre><code>def infer_esm2_entrypoint():\n    \"\"\"Entrypoint for running inference on a geneformer checkpoint and data.\"\"\"\n    # 1. get arguments\n    parser = get_parser()\n    args = parser.parse_args()\n    # 2. Call infer with args\n    infer_model(\n        data_path=args.data_path,\n        checkpoint_path=args.checkpoint_path,\n        results_path=args.results_path,\n        include_hiddens=args.include_hiddens,\n        include_embeddings=args.include_embeddings,\n        include_logits=args.include_logits,\n        include_input_ids=args.include_input_ids,\n        micro_batch_size=args.micro_batch_size,\n        precision=args.precision,\n        tensor_model_parallel_size=args.tensor_model_parallel_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        devices=args.num_gpus,\n        num_nodes=args.num_nodes,\n        config_class=args.config_class,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/infer_esm2/#bionemo.esm2.scripts.infer_esm2.infer_model","title":"<code>infer_model(data_path, checkpoint_path, results_path, min_seq_length=1024, include_hiddens=False, include_embeddings=False, include_logits=False, include_input_ids=False, micro_batch_size=64, precision='bf16-mixed', tensor_model_parallel_size=1, pipeline_model_parallel_size=1, devices=1, num_nodes=1, prediction_interval='epoch', config_class=ESM2Config)</code>","text":"<p>Runs inference on a BioNeMo ESM2 model using PyTorch Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the input data.</p> required <code>checkpoint_path</code> <code>Path</code> <p>Path to the model checkpoint.</p> required <code>results_path</code> <code>Path</code> <p>Path to save the inference results.</p> required <code>min_seq_length</code> <code>int</code> <p>minimum sequence length to be padded. This should be at least equal to the length of largest sequence in the dataset</p> <code>1024</code> <code>include_hiddens</code> <code>bool</code> <p>Whether to include hidden states in the output. Defaults to False.</p> <code>False</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the output. Defaults to False.</p> <code>False</code> <code>include_logits</code> <code>(bool, Optional)</code> <p>Whether to include token logits in the output. Defaults to False.</p> <code>False</code> <code>include_input_ids</code> <code>(bool, Optional)</code> <p>Whether to include input_ids in the output. Defaults to False.</p> <code>False</code> <code>micro_batch_size</code> <code>int</code> <p>Micro batch size for inference. Defaults to 64.</p> <code>64</code> <code>precision</code> <code>PrecisionTypes</code> <p>Precision type for inference. Defaults to \"bf16-mixed\".</p> <code>'bf16-mixed'</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>Tensor model parallel size for distributed inference. Defaults to 1.</p> <code>1</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>Pipeline model parallel size for distributed inference. Defaults to 1.</p> <code>1</code> <code>devices</code> <code>int</code> <p>Number of devices to use for inference. Defaults to 1.</p> <code>1</code> <code>num_nodes</code> <code>int</code> <p>Number of nodes to use for distributed inference. Defaults to 1.</p> <code>1</code> <code>prediction_interval</code> <code>IntervalT</code> <p>Intervals to write predict method output into disck for DDP inference. Defaults to epoch.</p> <code>'epoch'</code> <code>config_class</code> <code>Type[BioBertConfig]</code> <p>The config class for configuring the model using checkpoint provided</p> <code>ESM2Config</code> Source code in <code>bionemo/esm2/scripts/infer_esm2.py</code> <pre><code>def infer_model(\n    data_path: Path,\n    checkpoint_path: Path,\n    results_path: Path,\n    min_seq_length: int = 1024,\n    include_hiddens: bool = False,\n    include_embeddings: bool = False,\n    include_logits: bool = False,\n    include_input_ids: bool = False,\n    micro_batch_size: int = 64,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    devices: int = 1,\n    num_nodes: int = 1,\n    prediction_interval: IntervalT = \"epoch\",\n    config_class: Type[BioBertConfig] = ESM2Config,\n) -&gt; None:\n    \"\"\"Runs inference on a BioNeMo ESM2 model using PyTorch Lightning.\n\n    Args:\n        data_path (Path): Path to the input data.\n        checkpoint_path (Path): Path to the model checkpoint.\n        results_path (Path): Path to save the inference results.\n        min_seq_length (int): minimum sequence length to be padded. This should be at least equal to the length of largest sequence in the dataset\n        include_hiddens (bool, optional): Whether to include hidden states in the output. Defaults to False.\n        include_embeddings (bool, optional): Whether to include embeddings in the output. Defaults to False.\n        include_logits (bool, Optional): Whether to include token logits in the output. Defaults to False.\n        include_input_ids (bool, Optional): Whether to include input_ids in the output. Defaults to False.\n        micro_batch_size (int, optional): Micro batch size for inference. Defaults to 64.\n        precision (PrecisionTypes, optional): Precision type for inference. Defaults to \"bf16-mixed\".\n        tensor_model_parallel_size (int, optional): Tensor model parallel size for distributed inference. Defaults to 1.\n        pipeline_model_parallel_size (int, optional): Pipeline model parallel size for distributed inference. Defaults to 1.\n        devices (int, optional): Number of devices to use for inference. Defaults to 1.\n        num_nodes (int, optional): Number of nodes to use for distributed inference. Defaults to 1.\n        prediction_interval (IntervalT, optional): Intervals to write predict method output into disck for DDP inference. Defaults to epoch.\n        config_class (Type[BioBertConfig]): The config class for configuring the model using checkpoint provided\n    \"\"\"\n    # create the directory to save the inference results\n    os.makedirs(results_path, exist_ok=True)\n\n    # Setup the strategy and trainer\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=micro_batch_size,\n        num_nodes=num_nodes,\n        devices=devices,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n    )\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n    )\n\n    prediction_writer = PredictionWriter(output_dir=results_path, write_interval=prediction_interval)\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=devices,\n        strategy=strategy,\n        num_nodes=num_nodes,\n        callbacks=[prediction_writer],\n        plugins=nl.MegatronMixedPrecision(precision=precision),\n    )\n\n    dataset = InMemoryProteinDataset.from_csv(data_path, ignore_labels=True)\n    datamodule = ESM2FineTuneDataModule(\n        predict_dataset=dataset,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        min_seq_length=min_seq_length,\n    )\n\n    config = config_class(\n        params_dtype=get_autocast_dtype(precision),\n        pipeline_dtype=get_autocast_dtype(precision),\n        autocast_dtype=get_autocast_dtype(precision),\n        include_hiddens=include_hiddens,\n        include_embeddings=include_embeddings,\n        include_input_ids=include_input_ids,\n        skip_logits=not include_logits,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        initial_ckpt_path=str(checkpoint_path),\n        initial_ckpt_skip_keys_with_these_prefixes=[],  # load everything from the checkpoint.\n    )\n\n    tokenizer = get_tokenizer()\n    module = biobert_lightning_module(config=config, tokenizer=tokenizer)\n\n    # datamodule is responsible for transforming dataloaders by adding MegatronDataSampler. Alternatively, to\n    # directly use dataloader in predict method, the data sampler should be included in MegatronStrategy\n    trainer.predict(module, datamodule=datamodule)  # return_predictions=False failing due to a lightning bug\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/train_esm2/","title":"Train esm2","text":""},{"location":"API_reference/bionemo/esm2/scripts/train_esm2/#bionemo.esm2.scripts.train_esm2.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the cli parser for this tool.</p> Source code in <code>bionemo/esm2/scripts/train_esm2.py</code> <pre><code>def get_parser():\n    \"\"\"Return the cli parser for this tool.\"\"\"\n    # TODO migrate to hydra config\n    # Parse the arguments and pull them out into local variables for ease of future refactor to a\n    #   config management system.\n    parser = argparse.ArgumentParser(description=\"Pretrain ESM2 with UR data.\")\n    parser.add_argument(\n        \"--train-cluster-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the train cluster data parquet file\",\n    )\n    parser.add_argument(\n        \"--train-database-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the train sequence database file\",\n    )\n    parser.add_argument(\n        \"--valid-cluster-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the valid cluster data parquet file\",\n    )\n    parser.add_argument(\n        \"--valid-database-path\",\n        type=Path,\n        required=True,\n        help=\"Path to the vali sequence database file\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=get_args(PrecisionTypes),\n        required=False,\n        default=\"bf16-mixed\",\n        help=\"Precision type to use for training.\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        required=False,\n        default=4e-4,\n        help=\"Learning rate for training. Default is 4e-4\",\n    )\n    parser.add_argument(\n        \"--scheduler-num-steps\",\n        type=int,\n        required=False,\n        help=\"Number of steps for learning rate scheduler. Will use --num-steps if not given. Default is None.\",\n    )\n    parser.add_argument(\n        \"--create-tensorboard-logger\", action=\"store_true\", default=False, help=\"Create a tensorboard logger.\"\n    )\n    # FIXME (@skothenhill) figure out how checkpointing and resumption should work with the new nemo trainer\n    parser.add_argument(\n        \"--resume-if-exists\", action=\"store_true\", default=False, help=\"Resume training if a checkpoint exists.\"\n    )\n    parser.add_argument(\n        \"--result-dir\", type=Path, required=False, default=Path(\"./results\"), help=\"Path to the result directory.\"\n    )\n    parser.add_argument(\"--experiment-name\", type=str, required=False, default=\"esm2\", help=\"Name of the experiment.\")\n\n    parser.add_argument(\"--wandb-entity\", type=str, default=None, help=\"The team posting this run\")\n    parser.add_argument(\"--wandb-project\", type=str, default=None, help=\"Wandb project name \")\n    parser.add_argument(\"--wandb-tags\", nargs=\"+\", type=str, default=None, help=\"Tags associated with this run\")\n    parser.add_argument(\n        \"--wandb-group\", type=str, default=None, help=\"A unique string shared by all runs in a given group\"\n    )\n    parser.add_argument(\n        \"--wandb-job-type\",\n        type=str,\n        default=None,\n        help=\"A unique string representing a type of run, which is useful when you're grouping runs together into larger experiments using group.\",\n    )\n    parser.add_argument(\n        \"--wandb-id\", type=str, default=None, help=\"Sets the version, mainly used to resume a previous run\"\n    )\n    parser.add_argument(\n        \"--wandb-anonymous\", action=\"store_true\", help=\"Enable or explicitly disable anonymous logging\"\n    )\n    parser.add_argument(\n        \"--wandb-log-model\", action=\"store_true\", help=\"Save checkpoints in wandb dir to upload on W&amp;B servers\"\n    )\n    parser.add_argument(\"--wandb-offline\", action=\"store_true\", help=\"Use wandb in offline mode\")\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-nodes\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of nodes to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-steps\",\n        type=int,\n        required=False,\n        default=500000,\n        help=\"Number of steps to use for training. Default is 500000.\",\n    )\n    parser.add_argument(\n        \"--warmup-steps\",\n        type=int,\n        required=False,\n        default=2000,\n        help=\"Number of warmup steps for WarmupAnnealDecayHold Scheduler. Default is 2000.\",\n    )\n    parser.add_argument(\n        \"--num-dataset-workers\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of workers to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--val-check-interval\",\n        type=int,\n        required=False,\n        default=10000,\n        help=\"Number of steps between validation. Default is 10000.\",\n    )\n    parser.add_argument(\n        \"--log-every-n-steps\",\n        type=int,\n        required=False,\n        help=\"Number of steps between logging. Default is 50.\",\n    )\n    parser.add_argument(\n        \"--min-seq-length\",\n        type=float_or_int_or_none,\n        required=False,\n        default=1024,\n        help=\"Minimum sequence length. Sampled will be padded if less than this value. Set 'None' to unset minimum.\",\n    )\n    parser.add_argument(\n        \"--max-seq-length\",\n        type=int,\n        required=False,\n        default=1024,\n        help=\"Maximum sequence length. Samples will be truncated if exceeds this value.\",\n    )\n    parser.add_argument(\n        \"--limit-val-batches\",\n        type=float_or_int_or_none,\n        required=False,\n        default=2,\n        help=\"Number of global batches used for validation if int. Fraction of validation dataset if float. Default is 2.\",\n    )\n    parser.add_argument(\n        \"--micro-batch-size\",\n        type=int,\n        required=False,\n        default=64,\n        help=\"Micro-batch size. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--pipeline-model-parallel-size\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Pipeline model parallel size. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--tensor-model-parallel-size\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Tensor model parallel size. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--accumulate-grad-batches\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Gradient accumulation steps. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--biobert-spec-option\",\n        type=BiobertSpecOption,\n        choices=[e.value for e in BiobertSpecOption],\n        required=False,\n        default=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec.value,\n        help=\"Biobert spec option to use for the model. Default is 'esm2_bert_layer_with_transformer_engine_spec'.\",\n    )\n    parser.add_argument(\n        \"--nemo1-init-path\",\n        type=Path,\n        required=False,\n        help=\"Path to nemo1 file, if desired to load at init time.\",\n    )\n    parser.add_argument(\n        \"--disable-checkpointing\",\n        action=\"store_false\",\n        default=True,\n        dest=\"create_checkpoint_callback\",\n        help=\"Disable creating a ModelCheckpoint callback.\",\n    )\n    parser.add_argument(\n        \"--save-best-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the best checkpoint based on the metric to monitor.\",\n    )\n    parser.add_argument(\n        \"--no-save-best-checkpoint\",\n        action=\"store_false\",\n        default=True,\n        dest=\"save_best_checkpoint\",\n        help=\"Disable saving the best checkpoint based on the metric to monitor.\",\n    )\n    parser.add_argument(\n        \"--save-last-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the last checkpoint.\",\n    )\n    parser.add_argument(\n        \"--no-save-last-checkpoint\",\n        action=\"store_false\",\n        dest=\"save_last_checkpoint\",\n        default=True,\n        help=\"Disable saving the last checkpoint.\",\n    )\n    parser.add_argument(\n        \"--metric-to-monitor-for-checkpoints\",\n        type=str,\n        required=False,\n        default=\"val_loss\",\n        help=\"The metric to monitor for checkpointing.\",\n    )\n    parser.add_argument(\n        \"--save-top-k\",\n        type=int,\n        required=False,\n        default=2,\n        help=\"Save the top k checkpoints.\",\n    )\n    parser.add_argument(\n        \"--restore-from-checkpoint-path\",\n        type=Path,\n        required=False,\n        default=None,\n        help=\"Path to the checkpoint directory to restore from. Will override `--resume-if-exists` when set.\",\n    )\n    parser.add_argument(\n        \"--nsys-profiling\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling output you must run the whole program with `nsys`. For example: \"\n        \" `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true --capture-range=cudaProfilerApi --capture-range-end=stop  [regular python command here]`\",\n    )\n    # start, end, rank\n    parser.add_argument(\n        \"--nsys-start-step\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Start nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--nsys-end-step\",\n        type=int,\n        required=False,\n        help=\"End nsys profiling after this step.\",\n    )\n    # rank as list of integers\n    parser.add_argument(\n        \"--nsys-ranks\",\n        type=int,\n        nargs=\"+\",\n        required=False,\n        default=[0],\n        help=\"Enable nsys profiling for these ranks.\",\n    )\n\n    # ESM2 specific configuration (default: 650M)\n    parser.add_argument(\n        \"--random-mask-strategy\",\n        type=RandomMaskStrategy,\n        choices=[e.value for e in RandomMaskStrategy],\n        default=RandomMaskStrategy.ALL_TOKENS.value,\n        help=f\"\"\"In ESM2 pretraining, 15%% of all tokens are masked and among which 10%% are replaced with a random token. This class controls the set of random tokens to choose from. Options are: '{\"', '\".join([e.value for e in RandomMaskStrategy])}'. Note that 'all_token' will introduce non-canonical amino acid tokens as effective mask tokens, and the resultant loss will appear lower than that from 'amino_acids_only'. Note that 'all_token' is the method used in hugging face as well as portions of fairseq.\"\"\",\n    )\n    parser.add_argument(\n        \"--num-layers\",\n        type=int,\n        required=False,\n        default=33,\n        help=\"Number of layers in the model. Default is 33.\",\n    )\n    parser.add_argument(\n        \"--hidden-size\",\n        type=int,\n        required=False,\n        default=1280,\n        help=\"Hidden size of the model. Default is 1280.\",\n    )\n    parser.add_argument(\n        \"--num-attention-heads\",\n        type=int,\n        required=False,\n        default=20,\n        help=\"Number of attention heads in the model. Default is 20.\",\n    )\n    parser.add_argument(\n        \"--ffn-hidden-size\",\n        type=int,\n        required=False,\n        default=4 * 1280,\n        help=\"FFN hidden size of the model. Default is 4 * 1280.\",\n    )\n    # DDP config\n    parser.add_argument(\n        \"--no-overlap-grad-reduce\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--no-overlap-param-gather\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--no-average-in-collective\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--grad-reduce-in-fp32\",\n        action=\"store_true\",\n        default=False,\n    )\n    return parser\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/train_esm2/#bionemo.esm2.scripts.train_esm2.main","title":"<code>main(train_cluster_path, train_database_path, valid_cluster_path, valid_database_path, num_nodes, devices, min_seq_length, max_seq_length, result_dir, num_steps, scheduler_num_steps, warmup_steps, limit_val_batches, val_check_interval, log_every_n_steps, num_dataset_workers, biobert_spec_option, lr, micro_batch_size, accumulate_grad_batches, experiment_name, resume_if_exists, precision, wandb_entity=None, wandb_project=None, wandb_offline=False, wandb_tags=None, wandb_group=None, wandb_job_type=None, wandb_id=None, wandb_anonymous=False, wandb_log_model=False, pipeline_model_parallel_size=1, tensor_model_parallel_size=1, create_tensorboard_logger=False, nemo1_init_path=None, create_checkpoint_callback=True, restore_from_checkpoint_path=None, save_best_checkpoint=True, save_last_checkpoint=True, metric_to_monitor_for_checkpoints='val_loss', save_top_k=2, nsys_profiling=False, nsys_start_step=0, nsys_end_step=None, nsys_ranks=[0], random_mask_strategy=RandomMaskStrategy.ALL_TOKENS, num_layers=33, hidden_size=1280, num_attention_heads=20, ffn_hidden_size=1280 * 4, overlap_grad_reduce=True, overlap_param_gather=True, average_in_collective=True, grad_reduce_in_fp32=False)</code>","text":"<p>Train an ESM2 model on UR data.</p> <p>Parameters:</p> Name Type Description Default <code>train_cluster_path</code> <code>Path</code> <p>path to train cluster partquet</p> required <code>train_database_path</code> <code>Path</code> <p>path to train database</p> required <code>valid_cluster_path</code> <code>Path</code> <p>path to validation cluster parquet</p> required <code>valid_database_path</code> <code>Path</code> <p>path to validation database</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes to run on</p> required <code>devices</code> <code>int</code> <p>number of devices</p> required <code>min_seq_length</code> <code>Optional[int]</code> <p>minimum sequence length</p> required <code>max_seq_length</code> <code>int</code> <p>maximum sequence length</p> required <code>result_dir</code> <code>Path</code> <p>directory to store results, logs and checkpoints</p> required <code>num_steps</code> <code>int</code> <p>number of steps to train the model for</p> required <code>warmup_steps</code> <code>int</code> <p>number of steps for warmup phase</p> required <code>limit_val_batches</code> <code>int</code> <p>limit the number of validation global batches to this many</p> required <code>val_check_interval</code> <code>int</code> <p>number of steps to periodically check the validation loss</p> required <code>log_every_n_steps</code> <code>Optional[int]</code> <p>log every n steps</p> required <code>num_dataset_workers</code> <code>int</code> <p>number of dataset workers</p> required <code>biobert_spec_option</code> <code>BiobertSpecOption</code> <p>the biobert spec option (architecture) to use for this run</p> required <code>lr</code> <code>float</code> <p>learning rate</p> required <code>scheduler_num_steps</code> <code>Optional[int]</code> <p>Number of steps in learning rate scheduler. Use num_steps if not provided.</p> required <code>micro_batch_size</code> <code>int</code> <p>micro batch size, from this and parallelism settings we infer the global batch size</p> required <code>accumulate_grad_batches</code> <code>int</code> <p>number of batches to accumulate gradients for</p> required <code>experiment_name</code> <code>str</code> <p>experiment name, this is the name used for the wandb run, and the sub-directory of the result_dir that stores the logs and checkpoints.</p> required <code>resume_if_exists</code> <code>bool</code> <p>attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]</p> required <code>precision</code> <code>PrecisionTypes</code> <p>Precision type for training (e.g., float16, float32)</p> required <code>wandb_entity</code> <code>Optional[str]</code> <p>The team posting this run (default: your username or your default team)</p> <code>None</code> <code>wandb_project</code> <code>Optional[str]</code> <p>The name of the project to which this run will belong</p> <code>None</code> <code>wandb_offline</code> <code>bool</code> <p>Run offline (data can be streamed later to wandb servers).</p> <code>False</code> <code>wandb_tags</code> <code>Optional[List[str]]</code> <p>Tags associated with this run</p> <code>None</code> <code>wandb_group</code> <code>Optional[str]</code> <p>A unique string shared by all runs in a given group</p> <code>None</code> <code>wandb_job_type</code> <code>Optional[str]</code> <p>Type of run, which is useful when you're grouping runs together into larger experiments using group.</p> <code>None</code> <code>wandb_id</code> <code>Optional[str]</code> <p>Sets the version, mainly used to resume a previous run</p> <code>None</code> <code>wandb_anonymous</code> <code>Optional[bool]</code> <p>Enables or explicitly disables anonymous logging</p> <code>False</code> <code>wandb_log_model</code> <code>bool</code> <p>Save checkpoints in wandb dir to upload on W&amp;B servers</p> <code>False</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>pipeline model parallel size</p> <code>1</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>tensor model parallel size</p> <code>1</code> <code>create_tensorboard_logger</code> <code>bool</code> <p>create the tensorboard logger</p> <code>False</code> <code>nemo1_init_path</code> <code>Optional[Path]</code> <p>Nemo 1 initialization path</p> <code>None</code> <code>create_checkpoint_callback</code> <code>bool</code> <p>create a ModelCheckpoint callback and attach it to the pytorch lightning trainer</p> <code>True</code> <code>restore_from_checkpoint_path</code> <code>Optional[str]</code> <p>If set, restores the model from the directory passed in. Expects the checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.</p> <code>None</code> <code>save_best_checkpoint</code> <code>bool</code> <p>whether to save the best checkpoint</p> <code>True</code> <code>save_last_checkpoint</code> <code>bool</code> <p>whether to save the last checkpoint</p> <code>True</code> <code>metric_to_monitor_for_checkpoints</code> <code>str</code> <p>metric to monitor for checkpoints</p> <code>'val_loss'</code> <code>save_top_k</code> <code>int</code> <p>number of top checkpoints to save</p> <code>2</code> <code>nsys_profiling</code> <code>bool</code> <p>whether to enable nsys profiling</p> <code>False</code> <code>nsys_start_step</code> <code>int</code> <p>start step for nsys profiling</p> <code>0</code> <code>nsys_end_step</code> <code>Optional[int]</code> <p>end step for nsys profiling</p> <code>None</code> <code>nsys_ranks</code> <code>List[int]</code> <p>ranks for nsys profiling</p> <code>[0]</code> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>random mask strategy</p> <code>ALL_TOKENS</code> <code>num_layers</code> <code>int</code> <p>number of layers</p> <code>33</code> <code>hidden_size</code> <code>int</code> <p>hidden size</p> <code>1280</code> <code>num_attention_heads</code> <code>int</code> <p>number of attention heads</p> <code>20</code> <code>ffn_hidden_size</code> <code>int</code> <p>feed forward hidden size</p> <code>1280 * 4</code> <code>overlap_grad_reduce</code> <code>bool</code> <p>overlap gradient reduction</p> <code>True</code> <code>overlap_param_gather</code> <code>bool</code> <p>overlap parameter gather</p> <code>True</code> <code>average_in_collective</code> <code>bool</code> <p>average in collective</p> <code>True</code> <code>grad_reduce_in_fp32</code> <code>bool</code> <p>gradient reduction in fp32</p> <code>False</code> Source code in <code>bionemo/esm2/scripts/train_esm2.py</code> <pre><code>def main(\n    train_cluster_path: Path,\n    train_database_path: Path,\n    valid_cluster_path: Path,\n    valid_database_path: Path,\n    num_nodes: int,\n    devices: int,\n    min_seq_length: Optional[int],\n    max_seq_length: int,\n    result_dir: Path,\n    num_steps: int,\n    scheduler_num_steps: Optional[int],\n    warmup_steps: int,\n    limit_val_batches: int,\n    val_check_interval: int,\n    log_every_n_steps: Optional[int],\n    num_dataset_workers: int,\n    biobert_spec_option: BiobertSpecOption,\n    lr: float,\n    micro_batch_size: int,\n    accumulate_grad_batches: int,\n    experiment_name: str,\n    resume_if_exists: bool,\n    precision: PrecisionTypes,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_offline: bool = False,\n    wandb_tags: Optional[List[str]] = None,\n    wandb_group: Optional[str] = None,\n    wandb_job_type: Optional[str] = None,\n    wandb_id: Optional[str] = None,\n    wandb_anonymous: Optional[bool] = False,\n    wandb_log_model: bool = False,\n    pipeline_model_parallel_size: int = 1,\n    tensor_model_parallel_size: int = 1,\n    create_tensorboard_logger: bool = False,\n    nemo1_init_path: Optional[Path] = None,\n    create_checkpoint_callback: bool = True,\n    restore_from_checkpoint_path: Optional[str] = None,\n    save_best_checkpoint: bool = True,\n    save_last_checkpoint: bool = True,\n    metric_to_monitor_for_checkpoints: str = \"val_loss\",\n    save_top_k: int = 2,\n    nsys_profiling: bool = False,\n    nsys_start_step: int = 0,\n    nsys_end_step: Optional[int] = None,\n    nsys_ranks: List[int] = [0],\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n    num_layers: int = 33,\n    hidden_size: int = 1280,\n    num_attention_heads: int = 20,\n    ffn_hidden_size: int = 1280 * 4,\n    overlap_grad_reduce: bool = True,\n    overlap_param_gather: bool = True,\n    average_in_collective: bool = True,\n    grad_reduce_in_fp32: bool = False,\n) -&gt; nl.Trainer:\n    \"\"\"Train an ESM2 model on UR data.\n\n    Args:\n        train_cluster_path (Path): path to train cluster partquet\n        train_database_path (Path): path to train database\n        valid_cluster_path (Path): path to validation cluster parquet\n        valid_database_path (Path): path to validation database\n        num_nodes (int): Number of nodes to run on\n        devices (int): number of devices\n        min_seq_length (Optional[int]): minimum sequence length\n        max_seq_length (int): maximum sequence length\n        result_dir (Path): directory to store results, logs and checkpoints\n        num_steps (int): number of steps to train the model for\n        warmup_steps (int): number of steps for warmup phase\n        limit_val_batches (int): limit the number of validation global batches to this many\n        val_check_interval (int): number of steps to periodically check the validation loss\n        log_every_n_steps (Optional[int]): log every n steps\n        num_dataset_workers (int): number of dataset workers\n        biobert_spec_option (BiobertSpecOption): the biobert spec option (architecture) to use for this run\n        lr (float): learning rate\n        scheduler_num_steps (Optional[int]): Number of steps in learning rate scheduler. Use num_steps if not provided.\n        micro_batch_size (int): micro batch size, from this and parallelism settings we infer the global batch size\n        accumulate_grad_batches (int): number of batches to accumulate gradients for\n        experiment_name (str): experiment name, this is the name used for the wandb run, and the sub-directory of the\n            result_dir that stores the logs and checkpoints.\n        resume_if_exists (bool): attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]\n        precision (PrecisionTypes): Precision type for training (e.g., float16, float32)\n        wandb_entity (Optional[str]): The team posting this run (default: your username or your default team)\n        wandb_project (Optional[str]): The name of the project to which this run will belong\n        wandb_offline (bool): Run offline (data can be streamed later to wandb servers).\n        wandb_tags (Optional[List[str]]): Tags associated with this run\n        wandb_group (Optional[str]): A unique string shared by all runs in a given group\n        wandb_job_type (Optional[str]): Type of run, which is useful when you're grouping runs together into larger experiments using group.\n        wandb_id (Optional[str]): Sets the version, mainly used to resume a previous run\n        wandb_anonymous (Optional[bool]): Enables or explicitly disables anonymous logging\n        wandb_log_model (bool): Save checkpoints in wandb dir to upload on W&amp;B servers\n        pipeline_model_parallel_size (int): pipeline model parallel size\n        tensor_model_parallel_size (int): tensor model parallel size\n        create_tensorboard_logger (bool): create the tensorboard logger\n        nemo1_init_path (Optional[Path]): Nemo 1 initialization path\n        create_checkpoint_callback (bool): create a ModelCheckpoint callback and attach it to the pytorch lightning trainer\n        restore_from_checkpoint_path (Optional[str]): If set, restores the model from the directory passed in. Expects the\n            checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.\n        save_best_checkpoint (bool): whether to save the best checkpoint\n        save_last_checkpoint (bool): whether to save the last checkpoint\n        metric_to_monitor_for_checkpoints (str): metric to monitor for checkpoints\n        save_top_k (int): number of top checkpoints to save\n        nsys_profiling (bool): whether to enable nsys profiling\n        nsys_start_step (int): start step for nsys profiling\n        nsys_end_step (Optional[int]): end step for nsys profiling\n        nsys_ranks (List[int]): ranks for nsys profiling\n        random_mask_strategy (RandomMaskStrategy): random mask strategy\n        num_layers (int): number of layers\n        hidden_size (int): hidden size\n        num_attention_heads (int): number of attention heads\n        ffn_hidden_size (int): feed forward hidden size\n        overlap_grad_reduce (bool): overlap gradient reduction\n        overlap_param_gather (bool): overlap parameter gather\n        average_in_collective (bool): average in collective\n        grad_reduce_in_fp32 (bool): gradient reduction in fp32\n    \"\"\"\n    # Create the result directory if it does not exist.\n    result_dir.mkdir(parents=True, exist_ok=True)\n\n    # Setup the strategy and trainer\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=micro_batch_size,\n        num_nodes=num_nodes,\n        devices=devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n    )\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        pipeline_dtype=get_autocast_dtype(precision),\n        ddp=DistributedDataParallelConfig(\n            check_for_nan_in_grad=True,\n            overlap_grad_reduce=overlap_grad_reduce,\n            overlap_param_gather=overlap_param_gather,\n            average_in_collective=average_in_collective,\n            grad_reduce_in_fp32=grad_reduce_in_fp32,\n            use_distributed_optimizer=True,\n        ),\n        find_unused_parameters=True,\n        gradient_as_bucket_view=True,\n        ckpt_include_optimizer=True,\n        ckpt_async_save=True,\n        ckpt_parallel_load=True,\n    )\n\n    # for wandb integration\n    # Please refer to https://pytorch-lightning.readthedocs.io/en/0.7.6/api/lightning.pytorch.loggers.html\"\n    wandb_config: Optional[WandbConfig] = (\n        None\n        if wandb_project is None\n        else WandbConfig(\n            offline=wandb_offline,\n            project=wandb_project,\n            entity=wandb_entity,\n            tags=wandb_tags,\n            group=wandb_group,\n            job_type=wandb_job_type,\n            id=wandb_id,\n            anonymous=wandb_anonymous,\n            log_model=wandb_log_model,\n        )\n    )\n\n    callbacks = [\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n        nl_callbacks.PreemptionCallback(),\n        TimingCallback(),\n    ]\n    if nsys_profiling:\n        if nsys_end_step is None:\n            nsys_end_step = num_steps\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=nsys_start_step, end_step=nsys_end_step, ranks=nsys_ranks, gen_shape=True\n            )\n        )\n\n    trainer = nl.Trainer(\n        devices=devices,\n        max_steps=num_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        limit_val_batches=limit_val_batches,  # This controls upsampling and downsampling\n        val_check_interval=val_check_interval,\n        log_every_n_steps=log_every_n_steps,\n        num_nodes=num_nodes,\n        callbacks=callbacks,\n        plugins=nl.MegatronMixedPrecision(\n            precision=precision,\n            params_dtype=get_autocast_dtype(precision),\n            pipeline_dtype=get_autocast_dtype(precision),\n            grad_reduce_in_fp32=grad_reduce_in_fp32,\n            autocast_enabled=False,\n        ),\n        enable_checkpointing=create_checkpoint_callback,\n    )\n\n    tokenizer = get_tokenizer()\n\n    # Initialize the data module.\n    data = ESMDataModule(\n        train_cluster_path=train_cluster_path,\n        train_database_path=train_database_path,\n        valid_cluster_path=valid_cluster_path,\n        valid_database_path=valid_database_path,\n        global_batch_size=global_batch_size,\n        micro_batch_size=micro_batch_size,\n        min_seq_length=min_seq_length,\n        max_seq_length=max_seq_length,\n        num_workers=num_dataset_workers,\n        random_mask_strategy=random_mask_strategy,\n        tokenizer=tokenizer,\n    )\n    # Configure the model\n    train_metric = None\n    is_model_parallel = tensor_model_parallel_size * pipeline_model_parallel_size &gt; 1\n    if is_model_parallel:\n        valid_metric = None  # metric logging under model parallelism is not supported yet\n    else:\n        valid_metric = TorchmetricsConfig(\n            class_path=\"text.Perplexity\",\n            task=\"pretraining\",\n            kwargs={\"ignore_index\": MLM_LOSS_IGNORE_INDEX},\n            metric_name=\"val_ppl\",\n        )\n\n    esm2_config = ESM2Config(\n        seq_length=max_seq_length,\n        num_layers=num_layers,\n        hidden_size=hidden_size,\n        num_attention_heads=num_attention_heads,\n        ffn_hidden_size=ffn_hidden_size,\n        params_dtype=get_autocast_dtype(precision),\n        pipeline_dtype=get_autocast_dtype(precision),\n        autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n        # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n        initial_ckpt_path=str(restore_from_checkpoint_path) if restore_from_checkpoint_path is not None else None,\n        variable_seq_lengths=min_seq_length != max_seq_length,\n        train_metric=train_metric,\n        valid_metric=valid_metric,\n    )\n\n    if scheduler_num_steps is None:\n        scheduler_num_steps = num_steps\n\n    model = biobert_lightning_module(\n        esm2_config,\n        tokenizer=tokenizer,\n        optimizer=MegatronOptimizerModule(\n            config=OptimizerConfig(\n                lr=lr,\n                optimizer=\"adam\",\n                use_distributed_optimizer=True,\n                weight_decay=0.01,\n                adam_beta1=0.9,\n                adam_beta2=0.98,\n            ),\n            lr_scheduler=WarmupAnnealDecayHoldScheduler(\n                warmup_steps=warmup_steps,\n                max_steps=scheduler_num_steps,\n                max_lr=lr,\n                min_lr=0.0,\n                anneal_percentage=0.10,\n            ),\n        ),\n    )\n\n    # Configure our custom Checkpointer\n    if create_checkpoint_callback:\n        checkpoint_callback = nl_callbacks.ModelCheckpoint(\n            save_last=save_last_checkpoint,\n            monitor=metric_to_monitor_for_checkpoints,  # \"val_loss\",\n            save_top_k=save_top_k,\n            every_n_train_steps=val_check_interval,\n            always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n            filename=\"{epoch}-{val_loss:.2f}-{step}-{consumed_samples}\",  # Including step and consumed_samples in the checkpoint filename prevents duplicate filenames and bugs related to this.\n        )\n    else:\n        checkpoint_callback = None\n\n    # Setup the logger and train the model\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=result_dir,\n        name=experiment_name,\n        initialize_tensorboard_logger=create_tensorboard_logger,\n        wandb_config=wandb_config,\n        ckpt_callback=checkpoint_callback,\n    )\n\n    llm.train(\n        model=model,\n        data=data,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=resume_if_exists,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n    return trainer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/scripts/train_esm2/#bionemo.esm2.scripts.train_esm2.train_esm2_entrypoint","title":"<code>train_esm2_entrypoint()</code>","text":"<p>Entrypoint for running inference on a geneformer checkpoint and data.</p> Source code in <code>bionemo/esm2/scripts/train_esm2.py</code> <pre><code>def train_esm2_entrypoint():\n    \"\"\"Entrypoint for running inference on a geneformer checkpoint and data.\"\"\"\n    # 1. get arguments\n    parser = get_parser()\n    args = parser.parse_args()\n    # 2. Call pretrain with args\n    main(\n        train_cluster_path=args.train_cluster_path,\n        train_database_path=args.train_database_path,\n        valid_cluster_path=args.valid_cluster_path,\n        valid_database_path=args.valid_database_path,\n        num_nodes=args.num_nodes,\n        devices=args.num_gpus,\n        min_seq_length=args.min_seq_length,\n        max_seq_length=args.max_seq_length,\n        result_dir=args.result_dir,\n        wandb_entity=args.wandb_entity,\n        wandb_project=args.wandb_project,\n        wandb_tags=args.wandb_tags,\n        wandb_group=args.wandb_group,\n        wandb_job_type=args.wandb_job_type,\n        wandb_id=args.wandb_id,\n        wandb_anonymous=args.wandb_anonymous,\n        wandb_log_model=args.wandb_log_model,\n        wandb_offline=args.wandb_offline,\n        num_steps=args.num_steps,\n        warmup_steps=args.warmup_steps,\n        limit_val_batches=args.limit_val_batches,\n        val_check_interval=args.val_check_interval,\n        log_every_n_steps=args.log_every_n_steps,\n        num_dataset_workers=args.num_dataset_workers,\n        biobert_spec_option=args.biobert_spec_option,\n        lr=args.lr,\n        scheduler_num_steps=args.scheduler_num_steps,\n        micro_batch_size=args.micro_batch_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        tensor_model_parallel_size=args.tensor_model_parallel_size,\n        accumulate_grad_batches=args.accumulate_grad_batches,\n        precision=args.precision,\n        experiment_name=args.experiment_name,\n        resume_if_exists=args.resume_if_exists,\n        nemo1_init_path=args.nemo1_init_path,\n        create_checkpoint_callback=args.create_checkpoint_callback,\n        restore_from_checkpoint_path=args.restore_from_checkpoint_path,\n        save_best_checkpoint=args.save_best_checkpoint,\n        save_last_checkpoint=args.save_last_checkpoint,\n        metric_to_monitor_for_checkpoints=args.metric_to_monitor_for_checkpoints,\n        save_top_k=args.save_top_k,\n        nsys_profiling=args.nsys_profiling,\n        nsys_start_step=args.nsys_start_step,\n        nsys_end_step=args.nsys_end_step,\n        nsys_ranks=args.nsys_ranks,\n        random_mask_strategy=args.random_mask_strategy,\n        num_layers=args.num_layers,\n        hidden_size=args.hidden_size,\n        num_attention_heads=args.num_attention_heads,\n        ffn_hidden_size=args.ffn_hidden_size,\n        overlap_grad_reduce=not args.no_overlap_grad_reduce,\n        overlap_param_gather=not args.no_overlap_param_gather,\n        average_in_collective=not args.no_average_in_collective,\n        grad_reduce_in_fp32=args.grad_reduce_in_fp32,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/testing/compare/","title":"Compare","text":""},{"location":"API_reference/bionemo/esm2/testing/compare/#bionemo.esm2.testing.compare.assert_model_equivalence","title":"<code>assert_model_equivalence(ckpt_path, model_tag, precision='fp32', rtol=None, atol=None)</code>","text":"<p>Testing utility to compare the outputs of a NeMo2 checkpoint to the original HuggingFace model weights.</p> <p>Compares the cosine similarity of the logit and hidden state outputs of a NeMo2 model checkpoint to the outputs of the corresponding HuggingFace model.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_path</code> <code>Path | str</code> <p>A path to a NeMo2 checkpoint for an ESM-2 model.</p> required <code>model_tag</code> <code>str</code> <p>The HuggingFace model tag for the model to compare against.</p> required <code>precision</code> <code>PrecisionTypes</code> <p>The precision type to use for the comparison. Defaults to \"fp32\".</p> <code>'fp32'</code> <code>rtol</code> <code>float | None</code> <p>The relative tolerance to use for the comparison. Defaults to None, which chooses the tolerance based on the precision.</p> <code>None</code> <code>atol</code> <code>float | None</code> <p>The absolute tolerance to use for the comparison. Defaults to None, which chooses the tolerance based on the precision.</p> <code>None</code> Source code in <code>bionemo/esm2/testing/compare.py</code> <pre><code>def assert_model_equivalence(\n    ckpt_path: Path | str,\n    model_tag: str,\n    precision: PrecisionTypes = \"fp32\",\n    rtol: float | None = None,\n    atol: float | None = None,\n) -&gt; None:\n    \"\"\"Testing utility to compare the outputs of a NeMo2 checkpoint to the original HuggingFace model weights.\n\n    Compares the cosine similarity of the logit and hidden state outputs of a NeMo2 model checkpoint to the outputs of\n    the corresponding HuggingFace model.\n\n    Args:\n        ckpt_path: A path to a NeMo2 checkpoint for an ESM-2 model.\n        model_tag: The HuggingFace model tag for the model to compare against.\n        precision: The precision type to use for the comparison. Defaults to \"fp32\".\n        rtol: The relative tolerance to use for the comparison. Defaults to None, which chooses the tolerance based on\n            the precision.\n        atol: The absolute tolerance to use for the comparison. Defaults to None, which chooses the tolerance based on\n            the precision.\n    \"\"\"\n    tokenizer = get_tokenizer()\n\n    test_proteins = [\n        \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLA\",\n        \"MKTVRQERLKSI&lt;mask&gt;RILERSKEPVSGAQLAEELS&lt;mask&gt;SRQVIVQDIAYLRSLGYN&lt;mask&gt;VATPRGYVLAGG\",\n    ]\n    tokens = tokenizer(test_proteins, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n    input_ids = tokens[\"input_ids\"]\n    attention_mask = tokens[\"attention_mask\"]\n\n    dtype = get_autocast_dtype(precision)\n    nemo_config = ESM2Config(\n        initial_ckpt_path=str(ckpt_path),\n        include_embeddings=True,\n        include_hiddens=True,\n        params_dtype=dtype,\n        pipeline_dtype=dtype,\n        autocast_dtype=dtype,\n        bf16=dtype is torch.bfloat16,\n        fp16=dtype is torch.float16,\n    )\n\n    nemo_model = nemo_config.configure_model(tokenizer).to(\"cuda\").eval()\n\n    if dtype is torch.float16 or dtype is torch.bfloat16:\n        nemo_model = Float16Module(nemo_config, nemo_model)\n\n    nemo_output = nemo_model(input_ids, attention_mask)\n    nemo_logits = nemo_output[\"token_logits\"].transpose(0, 1).contiguous()[..., : tokenizer.vocab_size]\n    nemo_hidden_state = nemo_output[\"hidden_states\"]\n\n    del nemo_model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    hf_model = AutoModelForMaskedLM.from_pretrained(model_tag, torch_dtype=get_autocast_dtype(precision)).cuda().eval()\n    hf_output_all = hf_model(input_ids, attention_mask, output_hidden_states=True)\n    hf_hidden_state = hf_output_all.hidden_states[-1]\n\n    # Rather than directly comparing the logit or hidden state tensors, we compare their cosine similarity. These\n    # should be essentially 1 if the outputs are equivalent, but is less sensitive to small numerical differences.\n    # We don't care about the padding tokens, so we only compare the non-padding tokens.\n    logit_similarity = torch.nn.functional.cosine_similarity(nemo_logits, hf_output_all.logits, dim=2)\n    logit_similarity = logit_similarity[attention_mask == 1]\n\n    hidden_state_similarity = torch.nn.functional.cosine_similarity(nemo_hidden_state, hf_hidden_state, dim=2)\n    hidden_state_similarity = hidden_state_similarity[attention_mask == 1]\n\n    torch.testing.assert_close(logit_similarity, torch.ones_like(logit_similarity), rtol=rtol, atol=atol)\n    torch.testing.assert_close(hidden_state_similarity, torch.ones_like(hidden_state_similarity), rtol=rtol, atol=atol)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/","title":"Evo2 Data Preparation","text":""},{"location":"API_reference/bionemo/evo2/data/#data-preprocessing","title":"Data Preprocessing","text":"<p>To streamline the process of preparing and building datasets for training Evo2 on DNA sequences, we provide a configurable preprocessing script (<code>preprocess.py</code>) that can preprocess and tokenize a collection of <code>.fasta</code> files and convert them into Megatron-compatible <code>IndexedDataset</code>.</p> <p><pre><code>preprocess_evo2 -c &lt;CONFIG_PATH&gt;\n</code></pre> or if you are running the script outside of the BioNeMo container or you haven't pip-installed <code>bionemo-evo2</code>, then you can run the script directly: <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/data/preprocess.py -c &lt;CONFIG_PATH&gt;\n</code></pre></p> <p>Configuration YAML parameters for the script can be found in <code>utils/config.py</code>: <pre><code>class Evo2PreprocessingConfig(BaseModel):\n    \"\"\"Pydantic model class specifying the configuration schema for a preprocessed IndexedDataset (.bin, .idx).\"\"\"\n    # Collection of FASTA files to preprocess and wrap into a single IndexedDataset.\n    datapaths: list[Path] = []\n    # Output directory for the preprocessed dataset .bin/.idx.\n    output_dir: None | Path = None\n    # Output file prefix for identifying your datasets.\n    output_prefix: None | str = None\n    # Random Sequence-Level Datasplit\n    train_split: float = 0.7\n    valid_split: float = 0.2\n    test_split: float = 0.1\n    # Overwrite existing binaries. Otherwise, skip already preprocessed datasets.\n    overwrite: bool = False\n    # Raw Preprocessing Transforms\n    # For every sequence, include a reverse-complemented copy of that sequence in the dataset. Doubles the size of the dataset.\n    embed_reverse_complement: bool = False\n    # For every sequence, randomly reverse complement the sequence with the specified probability instead of using the original sequence.\n    random_reverse_complement: float = 0.0\n    # For sequences associated with taxonomic lineages specified in `taxonomy_data`, randomly drop out nodes of the lineage with the specified probability. For instance: |d__KINGDOM;p__None;c__CLASS;o__None;f__None;g__None;s__None|\n    random_lineage_dropout: float = 0.0\n    # Transcribe (DNA -&gt; RNA) or Back-Transcribe (RNA -&gt; DNA) the sequence before tokenization.\n    transcribe: None | Literal[\"transcribe\", \"back_transcribe\"] = None\n    # Force upper-case alphabetical characters in the `.fasta` sequences.\n    force_uppercase: bool = False\n    # Data type of the IndexedDataset. When using the byte-level tokenizer, uint8 is more than sufficient with a vocabulary size of 255 for ASCII.\n    indexed_dataset_dtype: str = \"uint8\"\n    # Tokenization Transforms\n    # Append end-of-document token to the end of each sequence.\n    append_eod: bool = False\n    # Enforce the length of the sequence, by padding shorter sequences and raising exceptions when the length is exceeded.\n    enforce_sample_length: None | int = None\n    # Run ftfy on the sequence characters prior to tokenization to fix encoding issues.\n    ftfy: bool = False\n    # Tokenizer\n    tokenizer_type: Literal[\n        \"Byte-Level\",\n        \"HuggingFace\",\n        \"SentencePiece\",\n        \"Regex\",\n        \"Megatron\",\n        \"Tiktoken\",\n    ] = \"Byte-Level\"  # Recommended for DNA / RNA sequences. All other tokenizers have not been tested, and only supported here for experimentation!\n    # For more information on the behavior of the following parameters, refer to NeMo:\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/tokenizer_utils.py\n    vocab_file: None | Path = None\n    vocab_size: None | int = 512\n    merges_file: None | Path = None\n    tokenizer_model_name: None | str = None\n    pretrained_tokenizer_model: None | str = None\n    special_tokens: None | dict[str, str] = {}\n    fast_hf_tokenizer: bool = False\n    # Compute Configuration\n    # NOTE: If preprocessing a large amount of short individual sequences (&lt; 1000 bp), do NOT use\n    # multiprocessing (workers &gt; 1) because sequence-level parallel IPC will dominate the preprocessing time!\n    workers: int = 1\n    # Number of sequences to load into memory at any given time during preprocessing.\n    # Prevents OOM while doing sequence-parallel.\n    preproc_concurrency: int = 100000\n    chunksize: int = 1\n    # Data Filters\n    drop_empty_sequences: bool = False\n    # If `NNN` is detected in the sequence, drop it from the preprocessed dataset.\n    nnn_filter: bool = False\n    # RNG\n    seed: None | int = None\n    # Evo2 Taxonomic Lineage Tags\n    # SeqID Sub-String Indexing: \"ABC\" will have taxonomy data from \"A\".\n    taxonomy_data: dict[str, Evo2TaxonomyLineage] = {}\n    # Periodicity of injecting phylogenetic lineage tags in the sequence prior to tokenization.\n    prompt_spacer_length: int = 131072\n</code></pre></p> <p>Furthermore, the <code>taxonomy_data</code> field contains a map from sequence ID substrings to phylogenetic lineage data of the form: <pre><code>class Evo2TaxonomyLineage(BaseModel):\n    \"\"\"Pydantic model class that defines the source lineage of a DNA sequence.\"\"\"\n    kingdom: None | str = None\n    phylum: None | str = None\n    clazz: None | str = None\n    order: None | str = None\n    family: None | str = None\n    genus: None | str = None\n    species: None | str = None\n</code></pre> which gets converted into a lineage string prior to tokenization as a prefix to the sequence: <pre><code># (Example) Escherichia coli\n|d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales;f__Enterobacteriaceae;g__Escherichia;s__Escherichia coli|ATCGTACGTACATCTCTA...\n</code></pre> In the Evo2 model, this special \"token\" is masked out in the loss function, so the model will learn to not generate tokens of this form.</p>"},{"location":"API_reference/bionemo/evo2/data/#testing","title":"Testing","text":"<p>To test equivalence with the reference implementation we first downloaded source-of-truth preprocessed Megatron <code>IndexedDataset</code> containing promoters data:</p> <pre><code>$ ls -lah\n-rwxr-xr-x  1 bionemo bionemo 1.2M Dec  4 00:56 data_promoters_test_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo  20K Dec  4 00:56 data_promoters_test_text_CharLevelTokenizer_document.idx\n-rwxr-xr-x  1 bionemo bionemo 392M Dec  4 00:56 data_promoters_train_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo 6.6M Dec  4 00:56 data_promoters_train_text_CharLevelTokenizer_document.idx\n-rwxr-xr-x  1 bionemo bionemo 1.2M Dec  4 00:56 data_promoters_valid_text_CharLevelTokenizer_document.bin\n-rwxr-xr-x  1 bionemo bionemo  20K Dec  4 00:56 data_promoters_valid_text_CharLevelTokenizer_document.idx\n</code></pre> <p>Next we acquired the <code>.fasta</code> file that was used to generate this, and configured our scripts to preprocess the sequence data into equivalent Megatron <code>IndexedDataset</code>.</p> <pre><code># mmseqs_promotors_config.yaml\n- datapaths: [\"/workspace/bionemo2/data/mmseqs_results_rep_seq_distinct.fasta\"]\n  output_dir: \"/workspace/bionemo2/data\"\n  output_prefix: promoters_uint8_distinct\n  train_split: 1.0  # We're just going to dump everything into a single file and compare against the union of the 3 splits in the SoT.\n  valid_split: 0.0\n  test_split: 0.0\n  overwrite: True\n  embed_reverse_complement: true\n  random_reverse_complement: 0.0\n  random_lineage_dropout: 0.0\n  include_sequence_id: false\n  transcribe: \"back_transcribe\"\n  force_uppercase: true\n  indexed_dataset_dtype: \"uint8\"\n  tokenizer_type: \"Byte-Level\"\n  vocab_file: null\n  vocab_size: null\n  merges_file: null\n  pretrained_tokenizer_model: null\n  special_tokens: null\n  fast_hf_tokenizer: true\n  append_eod: true\n  enforce_sample_length: null\n  ftfy: false\n  workers: 1\n  preproc_concurrency: 100000\n  chunksize: 25\n  drop_empty_sequences: true\n  nnn_filter: true\n  seed: null  # Not relevant because we are not using random reverse complement or lineage dropout.\n</code></pre> <p>To run the preprocessing script, we ran the following command: <pre><code>$ python preprocess.py -c mmseqs_promotors_config.yaml\n</code></pre></p> <p>To check equivalence of the two preprocessed datasets, we verify that we get the same elements out of our processed dataset as the original, but do not enforce ordering of the data. (<code>bionemo-noodles</code> does not sequentially read the <code>.fasta</code> file.)</p> <pre><code>&gt;&gt;&gt; from megatron.core.datasets.indexed_dataset import IndexedDataset\n&gt;&gt;&gt; ds_train_ref = IndexedDataset(\"./data_promoters_train_text_CharLevelTokenizer_document\")\n&gt;&gt;&gt; ds_val_ref = IndexedDataset(\"./data_promoters_valid_text_CharLevelTokenizer_document\")\n&gt;&gt;&gt; ds_test_ref = IndexedDataset(\"./data_promoters_test_text_CharLevelTokenizer_document\")\n&gt;&gt;&gt; ds_train_ours = IndexedDataset(\"./promoters_uint8_distinct_byte-level_train\")\n&gt;&gt;&gt; len(ds_train_ours) == len(ds_train_ref) + len(ds_test_ref) + len(ds_val_ref)\nTrue\n&gt;&gt;&gt;  # Example of what one of these set elements looks like, it's just a string representation of the token list for an\n&gt;&gt;&gt;  #  element of the training dataset. We can then compare all of these to make sure that the two datasets have the\n&gt;&gt;&gt;  #  same set of samples.\n&gt;&gt;&gt; ','.join([str(t) for t in ds_train_ref[0]])\n'67,84,71,71,65,71,67,67,84,71,65,67,67,65,84,65,65,71,84,65,71,84,71,71,67,84,65,84,65,65,67,71,65,71,71,65,65,71,65,65,71,65,84,71,65,65,71,65,71,65,84,84,65,71,65,71,65,65,65,65,84,71,65,65,84,71,84,84,67,84,84,71,65,65,71,84,65,71,67,67,65,84,84,71,84,84,71,84,65,71,84,84,71,84,84,71,84,71,84,71,84,71,84,65,84,71,84,84,71,65,71,65,84,71,84,84,84,84,71,71,71,71,84,84,84,71,84,84,65,84,65,84,65,71,65,71,65,71,65,71,65,84,71,84,65,71,84,84,84,71,71,84,71,65,65,71,65,71,84,65,71,71,65,84,84,67,84,67,84,84,65,67,84,65,71,84,71,84,71,65,65,71,65,84,84,65,84,84,65,67,84,65,71,71,84,65,65,67,84,65,65,65,84,71,65,71,65,84,84,67,84,65,84,67,65,65,67,84,65,65,71,84,67,65,84,84,65,71,65,71,65,84,84,71,71,65,65,65,84,71,84,84,84,67,84,84,84,84,65,71,71,84,84,84,65,65,84,65,65,65,71,84,84,84,71,84,84,84,71,65,65,84,84,71,65,71,65,65,65,71,65,71,65,71,65,71,71,65,71,65,71,65,67,65,84,84,71,67,84,84,84,71,65,65,71,71,71,65,71,65,71,84,84,84,71,71,71,84,71,71,71,84,71,65,71,71,65,84,84,71,65,65,65,65,84,71,65,65,65,65,65,84,71,65,65,67,84,71,65,65,65,65,65,71,71,84,71,84,84,65,84,65,71,84,71,65,67,67,84,71,84,67,65,65,65,65,65,65,71,67,84,71,84,71,65,65,71,65,65,71,84,71,84,84,65,84,67,67,65,65,71,65,65,65,84,65,84,71,71,65,84,84,71,67,84,65,65,84,67,65,84,65,67,84,65,67,84,71,84,84,67,65,84,84,65,84,71,65,84,84,84,84,65,84,71,84,71,84,67,65,84,71,84,71,84,71,84,71,67,67,84,65,84,67,65,84,67,65,84,84,67,67,84,84,65,84,65,84,84,84,84,65,71,84,84,71,71,67,65,65,65,65,65,65,65,65,65,65,65,71,65,67,84,84,71,71,65,65,71,84,65,84,84,71,65,65,65,65,67,67,65,65,65,84,67,84,71,65,84,67,84,67,65,65,67,67,84,65,71,65,67,65,65,71,84,67,71,65,84,84,65,65,65,71,67,84,65,65,65,67,67,71,65,65,65,65,67,67,71,65,65,84,67,67,67,71,65,67,67,71,71,84,84,65,65,84,84,71,65,65,65,65,67,67,71,65,84,67,67,65,0'\n&gt;&gt;&gt; # Create a set of all of these elements:\n&gt;&gt;&gt; all_ref_data = {','.join([str(t) for t in rec]) for ds in [ds_train_ref, ds_val_ref, ds_test_ref] for rec in ds}\n&gt;&gt;&gt; # Verify that there is no redundancy so we can do set equality safely\n&gt;&gt;&gt; len(all_ref_data) == len(ds_train_ours)\nTrue\n&gt;&gt;&gt; len(all_ref_data)\n343504\n&gt;&gt;&gt; all_our_data = {','.join([str(t) for t in rec]) for ds in [ds_train_ours] for rec in ds}\n&gt;&gt;&gt; len(all_our_data)\n343504\n&gt;&gt;&gt; # Verify set equality to show that we have processed an identical dataset\n&gt;&gt;&gt; #  (ignoring shuffling order and train/test/val splits)\n&gt;&gt;&gt; all_our_data == all_ref_data\nTrue\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/#sequence-splicing-stitching","title":"Sequence Splicing &amp; Stitching","text":"<p>Evo2 has also been trained on spliced DNA and mRNA sequences, where introns are removed leaving only the concatenated exons of the genome. Moreover, \"stitched\" variants of spliced transcripts have been introduced into Evo2's training dataset, which include 1024 bp of sequence from the promoter and 32 bp around each exon.</p> <p>To perform splicing or \"stitched\" splicing on sequences in a FASTA file given an associated gene transfer format (GTF) file, execute the following command: <pre><code>$ splice_evo2 --help\nusage: splice_evo2 [-h] --fasta-path FASTA_PATH --gtf-path GTF_PATH [--output-path OUTPUT_PATH] [--transcript-type {default,stitched}] [--stitched-promoter STITCHED_PROMOTER] [--stitched-intron STITCHED_INTRON] [--stitched-overlap] [--only-longest-transcript] [-v]\n\nExtract spliced transcripts from a FASTA and GTF.\n\noptions:\n  -h, --help            show this help message and exit\n  --fasta-path FASTA_PATH\n                        Path to FASTA file to extract transcripts from.\n  --gtf-path GTF_PATH   Path to gene transfer format (GTF) file associated with the FASTA.\n  --output-path OUTPUT_PATH\n                        Path to output FASTA file.\n  --transcript-type {default,stitched}\n                        Type of transcript to extract from the GTF and FASTA files for splicing. 'Stitched' transcripts include 1024 bp of sequence from the promoter and 32 bp around each exon.\n  --stitched-promoter STITCHED_PROMOTER\n                        Number of bp to include in the promoter region when --transcript-type=stitched is used. Defaults to 1024.\n  --stitched-intron STITCHED_INTRON\n                        Number of bp to include from neighboring introns when --transcript-type=stitched is used. Defaults to 32.\n  --stitched-overlap    Allow overlap of neighboring intron windows when --transcript-type=stitched is used. Defaults to False, i.e. prevents overlap by shortening the intron windows for a contiguous splice.\n  --only-longest-transcript\n                        Only extract the longest transcript per gene.\n  -v, --verbose         Turn on verbose log messages.\n</code></pre></p>"},{"location":"API_reference/bionemo/evo2/data/fasta_dataset/","title":"Fasta dataset","text":""},{"location":"API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset","title":"<code>SimpleFastaDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A simple dataset for Evo2 prediction.</p> <p>Currently, this will not work for pre-training or fine-tuning, as that would require: 1) including \"labels\" in the input and 2) offsetting/rolling either the labels or input_ids to handle the off-by-one token prediction alignment.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>class SimpleFastaDataset(torch.utils.data.Dataset):\n    \"\"\"A simple dataset for Evo2 prediction.\n\n    Currently, this will not work for pre-training or fine-tuning, as that would require:\n    1) including \"labels\" in the input and 2) offsetting/rolling either the labels or\n    input_ids to handle the off-by-one token prediction alignment.\n    \"\"\"\n\n    def __init__(self, fasta_path: Path, tokenizer, prepend_bos: bool = True):\n        \"\"\"Initialize the dataset.\"\"\"\n        super().__init__()\n        self.fasta = NvFaidx(fasta_path)\n        self.seqids = sorted(self.fasta.keys())\n        self.tokenizer = tokenizer\n        self.prepend_bos = prepend_bos  # needed for getting predictions for the requested set of tokens.\n\n    def write_idx_map(self, output_dir: Path):\n        \"\"\"Write the index map to the output directory.\"\"\"\n        with open(output_dir / \"seq_idx_map.json\", \"w\") as f:\n            json.dump({seqid: idx for idx, seqid in enumerate(self.seqids)}, f)\n\n    def __len__(self):\n        \"\"\"Get the length of the dataset.\"\"\"\n        return len(self.seqids)\n\n    def __getitem__(self, idx: int) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Get an item from the dataset.\"\"\"\n        sequence = self.fasta[self.seqids[idx]].sequence().upper()\n        tokenized_seq = self.tokenizer.text_to_ids(sequence)\n        if self.prepend_bos:  # in pretraining we use EOS to start new sequences.\n            tokens: list[int] = [self.tokenizer.eod] + tokenized_seq\n        else:\n            tokens: list[int] = tokenized_seq\n        loss_mask = torch.ones_like(torch.tensor(tokens, dtype=torch.long), dtype=torch.long)\n        if self.prepend_bos:\n            loss_mask[0] = (\n                0  # mask the eos token which we use for causal offsetting. Later in predict we take the output\n            )\n            #  for the first [:-1] tokens which align with the sequence starting after the EOS.\n        return {\n            \"tokens\": torch.tensor(tokens, dtype=torch.long),\n            \"position_ids\": torch.arange(len(tokens), dtype=torch.long),\n            \"seq_idx\": torch.tensor(idx, dtype=torch.long),\n            \"loss_mask\": loss_mask,\n        }\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an item from the dataset.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Get an item from the dataset.\"\"\"\n    sequence = self.fasta[self.seqids[idx]].sequence().upper()\n    tokenized_seq = self.tokenizer.text_to_ids(sequence)\n    if self.prepend_bos:  # in pretraining we use EOS to start new sequences.\n        tokens: list[int] = [self.tokenizer.eod] + tokenized_seq\n    else:\n        tokens: list[int] = tokenized_seq\n    loss_mask = torch.ones_like(torch.tensor(tokens, dtype=torch.long), dtype=torch.long)\n    if self.prepend_bos:\n        loss_mask[0] = (\n            0  # mask the eos token which we use for causal offsetting. Later in predict we take the output\n        )\n        #  for the first [:-1] tokens which align with the sequence starting after the EOS.\n    return {\n        \"tokens\": torch.tensor(tokens, dtype=torch.long),\n        \"position_ids\": torch.arange(len(tokens), dtype=torch.long),\n        \"seq_idx\": torch.tensor(idx, dtype=torch.long),\n        \"loss_mask\": loss_mask,\n    }\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.__init__","title":"<code>__init__(fasta_path, tokenizer, prepend_bos=True)</code>","text":"<p>Initialize the dataset.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def __init__(self, fasta_path: Path, tokenizer, prepend_bos: bool = True):\n    \"\"\"Initialize the dataset.\"\"\"\n    super().__init__()\n    self.fasta = NvFaidx(fasta_path)\n    self.seqids = sorted(self.fasta.keys())\n    self.tokenizer = tokenizer\n    self.prepend_bos = prepend_bos  # needed for getting predictions for the requested set of tokens.\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the length of the dataset.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Get the length of the dataset.\"\"\"\n    return len(self.seqids)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/fasta_dataset/#bionemo.evo2.data.fasta_dataset.SimpleFastaDataset.write_idx_map","title":"<code>write_idx_map(output_dir)</code>","text":"<p>Write the index map to the output directory.</p> Source code in <code>bionemo/evo2/data/fasta_dataset.py</code> <pre><code>def write_idx_map(self, output_dir: Path):\n    \"\"\"Write the index map to the output directory.\"\"\"\n    with open(output_dir / \"seq_idx_map.json\", \"w\") as f:\n        json.dump({seqid: idx for idx, seqid in enumerate(self.seqids)}, f)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/","title":"Preprocess","text":"<p>Module containing data preprocessing and splitting functions for Evo2 in BioNeMo.</p> <p>It can also be utilized as a script to dump pre-processed data to JSON.</p>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor","title":"<code>Evo2Preprocessor</code>","text":"<p>Data preprocessing class for Evo2.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>class Evo2Preprocessor:\n    \"\"\"Data preprocessing class for Evo2.\"\"\"\n\n    BIN = \".bin\"\n    IDX = \".idx\"\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"\n\n    def __init__(self, params: Evo2PreprocessingConfig | None = None):\n        \"\"\"Initialize Evo2Preprocessor.\n\n        Args:\n            params (Evo2PreprocessingConfig | None): Configuration parameters for preprocessing.\n        \"\"\"\n        self.tokenizer: Evo2Tokenizer = Evo2Tokenizer(params)\n\n    @staticmethod\n    @contextmanager\n    def preprocessing_context_manager(seed: Optional[int] = None):\n        \"\"\"Context manager for setting and restoring the random number generator state.\n\n        Args:\n            seed (int | None): Seed for the random number generator. Defaults to None.\n        \"\"\"\n        # Track current state.\n        current_state = random.getstate()\n        try:\n            # Set random seed.\n            random.seed(seed)\n            yield seed\n        finally:\n            # Restore random state.\n            random.setstate(current_state)\n\n    @staticmethod\n    def _get_output_filename(\n        config: Evo2PreprocessingConfig, ext: Optional[str] = None, split: Optional[str] = None, temp: bool = False\n    ) -&gt; Path:\n        \"\"\"Generate the output filename for the preprocessed data.\n\n        Args:\n            config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n            ext (Optional[str]): File extension for the output file. Defaults to None.\n            split (Optional[str]): Data split type (e.g., 'train', 'val', 'test'). Defaults to None.\n            temp (bool): Flag indicating whether the file is temporary. Defaults to False.\n\n        Returns:\n            Path: The constructed output file path.\n        \"\"\"\n        # Get output directory. Defaults to CWD.\n        output_dir = config.output_dir\n        if output_dir is None:\n            output_dir = Path.cwd()\n        # Pickup output file prefix.\n        config_prefix = \"{}_{}\".format(config.output_prefix, config.tokenizer_type.lower().replace(\" \", \"\"))\n        output_filepath = Path(output_dir) / (\n            config_prefix\n            + (f\"_{split}\" if split is not None else \"\")\n            + (ext if ext is not None else \"\")\n            + (\".tmp\" if temp else \"\")\n        )\n        return output_filepath\n\n    @staticmethod\n    def _subsequence_generator(sequence: str, subsequence_length: Optional[int] = None, offset: Optional[int] = None):\n        \"\"\"Generate subsequences from a given sequence.\n\n        Args:\n            sequence (str): The input sequence.\n            subsequence_length (int | None): Length of each subsequence. Defaults to the length of the sequence.\n            offset (int | None): Step size for generating subsequences. Defaults to subsequence_length.\n\n        Yields:\n            str: Subsequences of the input sequence.\n        \"\"\"\n        subsequence_length = subsequence_length if subsequence_length is not None else len(sequence)\n        step_size = offset if offset is not None else subsequence_length\n        for i in range(0, len(sequence), step_size):\n            yield sequence[i : i + subsequence_length]\n\n    @staticmethod\n    def _random_reverse_complement(seq: str, prob: float = 0.0, seed: Optional[int] = None):\n        \"\"\"Randomly reverse complements a DNA sequence based on a given probability.\n\n        Args:\n            seq (str): The DNA sequence to potentially reverse complement.\n            prob (float): The probability of reverse complementing the sequence. Defaults to 0.0.\n            seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            str: The original or reverse complemented DNA sequence based on the probability.\n        \"\"\"\n        with Evo2Preprocessor.preprocessing_context_manager(seed):\n            if random.random() &lt; prob:\n                return complement_sequence(reverse_sequence(seq))\n            else:\n                return seq\n\n    @staticmethod\n    def _reverse_complement_expansion(seq: str):\n        \"\"\"Generate a list containing the original and reverse complemented sequence.\n\n        Args:\n            seq (str): The input DNA sequence.\n\n        Returns:\n            list[str]: List containing the original and reverse complemented sequence.\n        \"\"\"\n        return [seq, complement_sequence(reverse_sequence(seq))]\n\n    @staticmethod\n    def _train_val_test_split(train_weight: float, val_weight: float, test_weight: float, seed: Optional[int] = None):\n        \"\"\"Randomly assign a data point to train, validation, or test split based on provided weights.\n\n        Args:\n            train_weight (float): The weight for the training split.\n            val_weight (float): The weight for the validation split.\n            test_weight (float): The weight for the test split.\n            seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            str: The split assignment ('train', 'val', or 'test').\n\n        Raises:\n            ValueError: If the sum of the weights is zero or negative.\n        \"\"\"\n        with Evo2Preprocessor.preprocessing_context_manager(seed if seed is not None else None):\n            # Generate random number.\n            roll = random.random()\n            # Rectify and normalize split ratios.\n            total_weight = abs(train_weight) + abs(val_weight) + abs(test_weight)\n            if total_weight &lt;= 0:\n                raise ValueError(\"Train-validation-test split proportions cannot be zero.\")\n            train_split = abs(train_weight) / total_weight\n            test_split = abs(test_weight) / total_weight\n            split = \"train\"\n            if roll &gt; train_split:\n                if roll &lt; 1 - test_split:\n                    split = \"val\"\n                else:\n                    split = \"test\"\n            return split\n\n    @staticmethod\n    def _construct_taxonomy_token(\n        lineage: Evo2TaxonomyLineage, dropout: float = 0.0, seed: Optional[int] = None\n    ) -&gt; Optional[str]:\n        \"\"\"Construct a special Taxonomy token for natural language prompting of DNA generation models.\n\n        Args:\n            lineage (Evo2TaxonomyLineage): The taxonomy lineage information.\n            dropout (float): The probability of dropping out segments of the lineage. Defaults to 0.0.\n            seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n        Returns:\n            Optional[str]: The constructed taxonomy token or None if lineage is None.\n        \"\"\"\n        # If dropout &gt; 0, randomly drop out segments of the lineage for training on incomplete lineages.\n        with Evo2Preprocessor.preprocessing_context_manager(seed if seed is not None else None):\n            return (\n                \"|d__{};p__{};c__{};o__{};f__{};g__{};s__{}|\".format(\n                    lineage.domain if random.random() &gt;= dropout else None,\n                    lineage.phylum if random.random() &gt;= dropout else None,\n                    lineage.clazz if random.random() &gt;= dropout else None,\n                    lineage.order if random.random() &gt;= dropout else None,\n                    lineage.family if random.random() &gt;= dropout else None,\n                    lineage.genus if random.random() &gt;= dropout else None,\n                    lineage.species if random.random() &gt;= dropout else None,\n                )\n                if lineage is not None\n                else None\n            )\n\n    def preprocess_data(self, filepath: str, seqid: str, seq: str, seq_idx: int, config: Evo2PreprocessingConfig):\n        \"\"\"Preprocess fasta datapaths.\n\n        Args:\n            filepath (str): Path to the .fasta file.\n            seqid (str): Sequence ID.\n            seq (str): DNA sequence.\n            seq_idx (int): Sequence index.\n            config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n        Returns:\n            tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n        \"\"\"\n        # Timing.\n        start = time.time()\n        # Retrieve taxonomy lineage string if SeqID has associated taxonomy data.\n        # Note: Better implemented as a suffix tree substring dictionary, but convenient\n        # for identifying a large amount of sequences with identical lineages.\n        # Slow for extremely large dictionaries of (SeqID Substr, Taxonomy) pairs.\n        lineage = None\n        for id, tax in config.taxonomy_data.items():\n            # Taxonomy ID is a substring of Seq ID.\n            if id in seqid:\n                lineage = tax\n                break\n\n        # Preprocess data.\n        preproc_data = []\n        with self.preprocessing_context_manager(\n            config.seed + hash(filepath) + seq_idx if config.seed is not None else None\n        ):\n            # Randomly reverse complement the sequence.\n            seq = self._random_reverse_complement(seq, prob=config.random_reverse_complement)\n            seqs_to_parse = self._reverse_complement_expansion(seq) if config.embed_reverse_complement else [seq]\n            for seq in seqs_to_parse:\n                # Sequence Modifiers\n                if config.force_uppercase:\n                    seq = seq.upper()\n                if config.transcribe == \"transcribe\":\n                    seq = transcribe_sequence(seq)\n                elif config.transcribe == \"back_transcribe\":\n                    seq = back_transcribe_sequence(seq)\n                if config.drop_empty_sequences and len(seq) == 0:\n                    continue\n                if config.nnn_filter and \"NNN\" in seq.upper():\n                    continue\n\n                # Construct taxonomy token with random dropout on the lineage categories per sequence.\n                taxonomy_token = self._construct_taxonomy_token(lineage, dropout=config.random_lineage_dropout)\n\n                # Inject taxonomy lineage tokens every prompt_spacer_length tokens in the sequence.\n                # If the taxonomy lineage token is not provided, then just take the original sequence.\n                target_length = (\n                    config.prompt_spacer_length - len(taxonomy_token) if taxonomy_token is not None else None\n                )\n                taxonomy_injected_sequence = [\n                    taxonomy_token + str(subseq) if taxonomy_token is not None else str(subseq)\n                    for subseq in self._subsequence_generator(seq, target_length, target_length)\n                ]\n\n                # Wrap and tokenize.\n                preproc_data_record = {\n                    \"text\": \"\".join(taxonomy_injected_sequence),\n                }\n                preproc_data_record[\"tokens\"] = self.tokenizer.tokenize(\n                    preproc_data_record[\"text\"],\n                    use_ftfy=config.ftfy,\n                    enforce_sample_length=config.enforce_sample_length,\n                    append_eod=config.append_eod,\n                    drop_empty_sequences=config.drop_empty_sequences,\n                )\n                preproc_data.append(preproc_data_record)\n        end = time.time()\n        return preproc_data, end - start\n\n    def preprocess_data_task(self, file_sequence_config):\n        \"\"\"Wrapper function to unpack args for preprocess_data.\n\n        Args:\n            file_sequence_config (tuple): Tuple containing arguments for preprocess_data.\n\n        Returns:\n            tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n        \"\"\"\n        return self.preprocess_data(*file_sequence_config)\n\n    @staticmethod\n    def _yield_sequences_from_files(config: Evo2PreprocessingConfig, semaphore: Semaphore):\n        \"\"\"Iterator over sequences within multiple input documents. Arguments for multiprocessing tasks.\n\n        Utilized to limit the amount of sequences streamed into memory.\n\n        Args:\n            config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n            semaphore (Semaphore): Semaphore to limit the number of sequences in memory.\n\n        Yields:\n            tuple: Arguments for preprocess_data.\n        \"\"\"\n\n        def yielder(fname, semaphore):\n            # Read FASTA.\n            index = NvFaidx(fname)\n            for i, (seqid, sequence) in enumerate(index.items()):\n                semaphore.acquire()\n                # Yield filename and sequence within fasta.\n                yield str(fname), seqid, sequence, i, config\n\n        for fname in config.datapaths:\n            semaphore.acquire()\n            yield from yielder(fname, semaphore)\n\n    def preprocess_generator(self, preproc_config: Evo2PreprocessingConfig):\n        \"\"\"Main function to preprocess data for Evo2.\n\n        Args:\n            preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n        Yields:\n            tuple[dict, float]: Preprocessed sequence data and the time taken for preprocessing.\n        \"\"\"\n        # Track which splits have been assigned\n        split_assignments = {\n            \"train\": preproc_config.train_split &gt; 0,\n            \"val\": preproc_config.valid_split &gt; 0,\n            \"test\": preproc_config.test_split &gt; 0,\n        }\n        splits_needed = {k for k, v in split_assignments.items() if v}\n\n        # Instantiate multiprocessing pool. Use semaphore to limit the amount of sequences to read into memory.\n        semaphore = Semaphore(preproc_config.preproc_concurrency + preproc_config.workers)\n        if preproc_config.workers &gt; 1:\n            pool = mp.Pool(preproc_config.workers)\n            # Ordered imap for downstream seeded splitting.\n            preproc_tasks = pool.imap(\n                self.preprocess_data_task,\n                self._yield_sequences_from_files(preproc_config, semaphore),\n                chunksize=preproc_config.chunksize,\n            )\n        else:\n            preproc_tasks = (\n                self.preprocess_data_task(x) for x in self._yield_sequences_from_files(preproc_config, semaphore)\n            )\n\n        # Preprocess data and split results into train, test, and split.\n        with self.preprocessing_context_manager(preproc_config.seed if preproc_config.seed is not None else None):\n            for result, elapsed_time in preproc_tasks:\n                # Release semaphore for the task associated with the result.\n                semaphore.release()\n                # If we still need to ensure splits are assigned\n                if splits_needed:\n                    # Force assign to a needed split\n                    split = splits_needed.pop()\n                else:\n                    # Regular random assignment\n                    split = self._train_val_test_split(\n                        preproc_config.train_split, preproc_config.valid_split, preproc_config.test_split\n                    )\n                for sequence in result:\n                    sequence[\"split\"] = split\n                    yield sequence, elapsed_time\n\n    def preprocess_offline(self, preproc_config: Evo2PreprocessingConfig):\n        \"\"\"Offline data preprocessing script for Evo2.\n\n        Args:\n            preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n        \"\"\"\n        # Validate if binaries have already been produced for the given config and overwrite is set to False.\n        if any(\n            self._get_output_filename(preproc_config, ext, split).is_file()\n            for ext, split in zip([self.BIN, self.IDX], [self.TRAIN, self.VAL, self.TEST])\n        ):\n            if not preproc_config.overwrite:\n                # Skip this dataset!\n                logging.info(\n                    f\"Skipped overwriting (overwrite: False) existing preprocessed data: {preproc_config.output_prefix}\"\n                )\n                return\n            else:\n                logging.info(\n                    f\"Overwriting (overwrite: True) existing preprocessed data: {preproc_config.output_prefix}\"\n                )\n\n        # Instantiate indexed data builders.\n        dataset_dtype = getattr(np, preproc_config.indexed_dataset_dtype)\n        temp_train_bin = self._get_output_filename(preproc_config, self.BIN, self.TRAIN, temp=True)\n        temp_val_bin = self._get_output_filename(preproc_config, self.BIN, self.VAL, temp=True)\n        temp_test_bin = self._get_output_filename(preproc_config, self.BIN, self.TEST, temp=True)\n        train_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_train_bin), dtype=dataset_dtype)\n        val_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_val_bin), dtype=dataset_dtype)\n        test_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_test_bin), dtype=dataset_dtype)\n        logging.info(f\"Created temporary binary datasets: {temp_train_bin} {temp_val_bin} {temp_test_bin}\")\n\n        # Preprocess data and split results into train, validation, or test.\n        avg_preproc_time = 0.0\n        avg_index_time = 0.0\n        count = 0\n        for sequence, elapsed_time in self.preprocess_generator(preproc_config):\n            index_start_time = time.time()\n            if sequence[\"split\"] == \"train\":\n                train_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n                train_builder.end_document()\n            elif sequence[\"split\"] == \"val\":\n                val_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n                val_builder.end_document()\n            elif sequence[\"split\"] == \"test\":\n                test_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n                test_builder.end_document()\n            index_end_time = time.time()\n            # Update average preprocessing and indexing time.\n            avg_preproc_time = (avg_preproc_time * count + elapsed_time) / (count + 1)\n            avg_index_time = (avg_index_time * count + index_end_time - index_start_time) / (count + 1)\n            count += 1\n\n        # Report timing.\n        logging.info(f\"Average preprocessing time per sequence: {avg_preproc_time}\")\n        logging.info(f\"Average indexing time per sequence: {avg_index_time}\")\n        logging.info(f\"Number of sequences processed: {count}\")\n\n        # Write preprocessed index data to disk. Rename temporary binaries to denote preprocessing completion.\n        train_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TRAIN)))\n        val_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.VAL)))\n        test_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TEST)))\n        os.rename(temp_train_bin, self._get_output_filename(preproc_config, self.BIN, self.TRAIN))\n        os.rename(temp_val_bin, self._get_output_filename(preproc_config, self.BIN, self.VAL))\n        os.rename(temp_test_bin, self._get_output_filename(preproc_config, self.BIN, self.TEST))\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.__init__","title":"<code>__init__(params=None)</code>","text":"<p>Initialize Evo2Preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Evo2PreprocessingConfig | None</code> <p>Configuration parameters for preprocessing.</p> <code>None</code> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def __init__(self, params: Evo2PreprocessingConfig | None = None):\n    \"\"\"Initialize Evo2Preprocessor.\n\n    Args:\n        params (Evo2PreprocessingConfig | None): Configuration parameters for preprocessing.\n    \"\"\"\n    self.tokenizer: Evo2Tokenizer = Evo2Tokenizer(params)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor._construct_taxonomy_token","title":"<code>_construct_taxonomy_token(lineage, dropout=0.0, seed=None)</code>  <code>staticmethod</code>","text":"<p>Construct a special Taxonomy token for natural language prompting of DNA generation models.</p> <p>Parameters:</p> Name Type Description Default <code>lineage</code> <code>Evo2TaxonomyLineage</code> <p>The taxonomy lineage information.</p> required <code>dropout</code> <code>float</code> <p>The probability of dropping out segments of the lineage. Defaults to 0.0.</p> <code>0.0</code> <code>seed</code> <code>Optional[int]</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The constructed taxonomy token or None if lineage is None.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\ndef _construct_taxonomy_token(\n    lineage: Evo2TaxonomyLineage, dropout: float = 0.0, seed: Optional[int] = None\n) -&gt; Optional[str]:\n    \"\"\"Construct a special Taxonomy token for natural language prompting of DNA generation models.\n\n    Args:\n        lineage (Evo2TaxonomyLineage): The taxonomy lineage information.\n        dropout (float): The probability of dropping out segments of the lineage. Defaults to 0.0.\n        seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        Optional[str]: The constructed taxonomy token or None if lineage is None.\n    \"\"\"\n    # If dropout &gt; 0, randomly drop out segments of the lineage for training on incomplete lineages.\n    with Evo2Preprocessor.preprocessing_context_manager(seed if seed is not None else None):\n        return (\n            \"|d__{};p__{};c__{};o__{};f__{};g__{};s__{}|\".format(\n                lineage.domain if random.random() &gt;= dropout else None,\n                lineage.phylum if random.random() &gt;= dropout else None,\n                lineage.clazz if random.random() &gt;= dropout else None,\n                lineage.order if random.random() &gt;= dropout else None,\n                lineage.family if random.random() &gt;= dropout else None,\n                lineage.genus if random.random() &gt;= dropout else None,\n                lineage.species if random.random() &gt;= dropout else None,\n            )\n            if lineage is not None\n            else None\n        )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor._get_output_filename","title":"<code>_get_output_filename(config, ext=None, split=None, temp=False)</code>  <code>staticmethod</code>","text":"<p>Generate the output filename for the preprocessed data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required <code>ext</code> <code>Optional[str]</code> <p>File extension for the output file. Defaults to None.</p> <code>None</code> <code>split</code> <code>Optional[str]</code> <p>Data split type (e.g., 'train', 'val', 'test'). Defaults to None.</p> <code>None</code> <code>temp</code> <code>bool</code> <p>Flag indicating whether the file is temporary. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The constructed output file path.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\ndef _get_output_filename(\n    config: Evo2PreprocessingConfig, ext: Optional[str] = None, split: Optional[str] = None, temp: bool = False\n) -&gt; Path:\n    \"\"\"Generate the output filename for the preprocessed data.\n\n    Args:\n        config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n        ext (Optional[str]): File extension for the output file. Defaults to None.\n        split (Optional[str]): Data split type (e.g., 'train', 'val', 'test'). Defaults to None.\n        temp (bool): Flag indicating whether the file is temporary. Defaults to False.\n\n    Returns:\n        Path: The constructed output file path.\n    \"\"\"\n    # Get output directory. Defaults to CWD.\n    output_dir = config.output_dir\n    if output_dir is None:\n        output_dir = Path.cwd()\n    # Pickup output file prefix.\n    config_prefix = \"{}_{}\".format(config.output_prefix, config.tokenizer_type.lower().replace(\" \", \"\"))\n    output_filepath = Path(output_dir) / (\n        config_prefix\n        + (f\"_{split}\" if split is not None else \"\")\n        + (ext if ext is not None else \"\")\n        + (\".tmp\" if temp else \"\")\n    )\n    return output_filepath\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor._random_reverse_complement","title":"<code>_random_reverse_complement(seq, prob=0.0, seed=None)</code>  <code>staticmethod</code>","text":"<p>Randomly reverse complements a DNA sequence based on a given probability.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>The DNA sequence to potentially reverse complement.</p> required <code>prob</code> <code>float</code> <p>The probability of reverse complementing the sequence. Defaults to 0.0.</p> <code>0.0</code> <code>seed</code> <code>Optional[int]</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The original or reverse complemented DNA sequence based on the probability.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\ndef _random_reverse_complement(seq: str, prob: float = 0.0, seed: Optional[int] = None):\n    \"\"\"Randomly reverse complements a DNA sequence based on a given probability.\n\n    Args:\n        seq (str): The DNA sequence to potentially reverse complement.\n        prob (float): The probability of reverse complementing the sequence. Defaults to 0.0.\n        seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        str: The original or reverse complemented DNA sequence based on the probability.\n    \"\"\"\n    with Evo2Preprocessor.preprocessing_context_manager(seed):\n        if random.random() &lt; prob:\n            return complement_sequence(reverse_sequence(seq))\n        else:\n            return seq\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor._reverse_complement_expansion","title":"<code>_reverse_complement_expansion(seq)</code>  <code>staticmethod</code>","text":"<p>Generate a list containing the original and reverse complemented sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>str</code> <p>The input DNA sequence.</p> required <p>Returns:</p> Type Description <p>list[str]: List containing the original and reverse complemented sequence.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\ndef _reverse_complement_expansion(seq: str):\n    \"\"\"Generate a list containing the original and reverse complemented sequence.\n\n    Args:\n        seq (str): The input DNA sequence.\n\n    Returns:\n        list[str]: List containing the original and reverse complemented sequence.\n    \"\"\"\n    return [seq, complement_sequence(reverse_sequence(seq))]\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor._subsequence_generator","title":"<code>_subsequence_generator(sequence, subsequence_length=None, offset=None)</code>  <code>staticmethod</code>","text":"<p>Generate subsequences from a given sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>str</code> <p>The input sequence.</p> required <code>subsequence_length</code> <code>int | None</code> <p>Length of each subsequence. Defaults to the length of the sequence.</p> <code>None</code> <code>offset</code> <code>int | None</code> <p>Step size for generating subsequences. Defaults to subsequence_length.</p> <code>None</code> <p>Yields:</p> Name Type Description <code>str</code> <p>Subsequences of the input sequence.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\ndef _subsequence_generator(sequence: str, subsequence_length: Optional[int] = None, offset: Optional[int] = None):\n    \"\"\"Generate subsequences from a given sequence.\n\n    Args:\n        sequence (str): The input sequence.\n        subsequence_length (int | None): Length of each subsequence. Defaults to the length of the sequence.\n        offset (int | None): Step size for generating subsequences. Defaults to subsequence_length.\n\n    Yields:\n        str: Subsequences of the input sequence.\n    \"\"\"\n    subsequence_length = subsequence_length if subsequence_length is not None else len(sequence)\n    step_size = offset if offset is not None else subsequence_length\n    for i in range(0, len(sequence), step_size):\n        yield sequence[i : i + subsequence_length]\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor._train_val_test_split","title":"<code>_train_val_test_split(train_weight, val_weight, test_weight, seed=None)</code>  <code>staticmethod</code>","text":"<p>Randomly assign a data point to train, validation, or test split based on provided weights.</p> <p>Parameters:</p> Name Type Description Default <code>train_weight</code> <code>float</code> <p>The weight for the training split.</p> required <code>val_weight</code> <code>float</code> <p>The weight for the validation split.</p> required <code>test_weight</code> <code>float</code> <p>The weight for the test split.</p> required <code>seed</code> <code>Optional[int]</code> <p>The seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The split assignment ('train', 'val', or 'test').</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sum of the weights is zero or negative.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\ndef _train_val_test_split(train_weight: float, val_weight: float, test_weight: float, seed: Optional[int] = None):\n    \"\"\"Randomly assign a data point to train, validation, or test split based on provided weights.\n\n    Args:\n        train_weight (float): The weight for the training split.\n        val_weight (float): The weight for the validation split.\n        test_weight (float): The weight for the test split.\n        seed (Optional[int]): The seed for the random number generator. Defaults to None.\n\n    Returns:\n        str: The split assignment ('train', 'val', or 'test').\n\n    Raises:\n        ValueError: If the sum of the weights is zero or negative.\n    \"\"\"\n    with Evo2Preprocessor.preprocessing_context_manager(seed if seed is not None else None):\n        # Generate random number.\n        roll = random.random()\n        # Rectify and normalize split ratios.\n        total_weight = abs(train_weight) + abs(val_weight) + abs(test_weight)\n        if total_weight &lt;= 0:\n            raise ValueError(\"Train-validation-test split proportions cannot be zero.\")\n        train_split = abs(train_weight) / total_weight\n        test_split = abs(test_weight) / total_weight\n        split = \"train\"\n        if roll &gt; train_split:\n            if roll &lt; 1 - test_split:\n                split = \"val\"\n            else:\n                split = \"test\"\n        return split\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor._yield_sequences_from_files","title":"<code>_yield_sequences_from_files(config, semaphore)</code>  <code>staticmethod</code>","text":"<p>Iterator over sequences within multiple input documents. Arguments for multiprocessing tasks.</p> <p>Utilized to limit the amount of sequences streamed into memory.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required <code>semaphore</code> <code>Semaphore</code> <p>Semaphore to limit the number of sequences in memory.</p> required <p>Yields:</p> Name Type Description <code>tuple</code> <p>Arguments for preprocess_data.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\ndef _yield_sequences_from_files(config: Evo2PreprocessingConfig, semaphore: Semaphore):\n    \"\"\"Iterator over sequences within multiple input documents. Arguments for multiprocessing tasks.\n\n    Utilized to limit the amount of sequences streamed into memory.\n\n    Args:\n        config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n        semaphore (Semaphore): Semaphore to limit the number of sequences in memory.\n\n    Yields:\n        tuple: Arguments for preprocess_data.\n    \"\"\"\n\n    def yielder(fname, semaphore):\n        # Read FASTA.\n        index = NvFaidx(fname)\n        for i, (seqid, sequence) in enumerate(index.items()):\n            semaphore.acquire()\n            # Yield filename and sequence within fasta.\n            yield str(fname), seqid, sequence, i, config\n\n    for fname in config.datapaths:\n        semaphore.acquire()\n        yield from yielder(fname, semaphore)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_data","title":"<code>preprocess_data(filepath, seqid, seq, seq_idx, config)</code>","text":"<p>Preprocess fasta datapaths.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the .fasta file.</p> required <code>seqid</code> <code>str</code> <p>Sequence ID.</p> required <code>seq</code> <code>str</code> <p>DNA sequence.</p> required <code>seq_idx</code> <code>int</code> <p>Sequence index.</p> required <code>config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required <p>Returns:</p> Type Description <p>tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_data(self, filepath: str, seqid: str, seq: str, seq_idx: int, config: Evo2PreprocessingConfig):\n    \"\"\"Preprocess fasta datapaths.\n\n    Args:\n        filepath (str): Path to the .fasta file.\n        seqid (str): Sequence ID.\n        seq (str): DNA sequence.\n        seq_idx (int): Sequence index.\n        config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n    Returns:\n        tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n    \"\"\"\n    # Timing.\n    start = time.time()\n    # Retrieve taxonomy lineage string if SeqID has associated taxonomy data.\n    # Note: Better implemented as a suffix tree substring dictionary, but convenient\n    # for identifying a large amount of sequences with identical lineages.\n    # Slow for extremely large dictionaries of (SeqID Substr, Taxonomy) pairs.\n    lineage = None\n    for id, tax in config.taxonomy_data.items():\n        # Taxonomy ID is a substring of Seq ID.\n        if id in seqid:\n            lineage = tax\n            break\n\n    # Preprocess data.\n    preproc_data = []\n    with self.preprocessing_context_manager(\n        config.seed + hash(filepath) + seq_idx if config.seed is not None else None\n    ):\n        # Randomly reverse complement the sequence.\n        seq = self._random_reverse_complement(seq, prob=config.random_reverse_complement)\n        seqs_to_parse = self._reverse_complement_expansion(seq) if config.embed_reverse_complement else [seq]\n        for seq in seqs_to_parse:\n            # Sequence Modifiers\n            if config.force_uppercase:\n                seq = seq.upper()\n            if config.transcribe == \"transcribe\":\n                seq = transcribe_sequence(seq)\n            elif config.transcribe == \"back_transcribe\":\n                seq = back_transcribe_sequence(seq)\n            if config.drop_empty_sequences and len(seq) == 0:\n                continue\n            if config.nnn_filter and \"NNN\" in seq.upper():\n                continue\n\n            # Construct taxonomy token with random dropout on the lineage categories per sequence.\n            taxonomy_token = self._construct_taxonomy_token(lineage, dropout=config.random_lineage_dropout)\n\n            # Inject taxonomy lineage tokens every prompt_spacer_length tokens in the sequence.\n            # If the taxonomy lineage token is not provided, then just take the original sequence.\n            target_length = (\n                config.prompt_spacer_length - len(taxonomy_token) if taxonomy_token is not None else None\n            )\n            taxonomy_injected_sequence = [\n                taxonomy_token + str(subseq) if taxonomy_token is not None else str(subseq)\n                for subseq in self._subsequence_generator(seq, target_length, target_length)\n            ]\n\n            # Wrap and tokenize.\n            preproc_data_record = {\n                \"text\": \"\".join(taxonomy_injected_sequence),\n            }\n            preproc_data_record[\"tokens\"] = self.tokenizer.tokenize(\n                preproc_data_record[\"text\"],\n                use_ftfy=config.ftfy,\n                enforce_sample_length=config.enforce_sample_length,\n                append_eod=config.append_eod,\n                drop_empty_sequences=config.drop_empty_sequences,\n            )\n            preproc_data.append(preproc_data_record)\n    end = time.time()\n    return preproc_data, end - start\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_data_task","title":"<code>preprocess_data_task(file_sequence_config)</code>","text":"<p>Wrapper function to unpack args for preprocess_data.</p> <p>Parameters:</p> Name Type Description Default <code>file_sequence_config</code> <code>tuple</code> <p>Tuple containing arguments for preprocess_data.</p> required <p>Returns:</p> Type Description <p>tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_data_task(self, file_sequence_config):\n    \"\"\"Wrapper function to unpack args for preprocess_data.\n\n    Args:\n        file_sequence_config (tuple): Tuple containing arguments for preprocess_data.\n\n    Returns:\n        tuple[list[dict], float]: Preprocessed data and the time taken for preprocessing.\n    \"\"\"\n    return self.preprocess_data(*file_sequence_config)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_generator","title":"<code>preprocess_generator(preproc_config)</code>","text":"<p>Main function to preprocess data for Evo2.</p> <p>Parameters:</p> Name Type Description Default <code>preproc_config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required <p>Yields:</p> Type Description <p>tuple[dict, float]: Preprocessed sequence data and the time taken for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_generator(self, preproc_config: Evo2PreprocessingConfig):\n    \"\"\"Main function to preprocess data for Evo2.\n\n    Args:\n        preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n\n    Yields:\n        tuple[dict, float]: Preprocessed sequence data and the time taken for preprocessing.\n    \"\"\"\n    # Track which splits have been assigned\n    split_assignments = {\n        \"train\": preproc_config.train_split &gt; 0,\n        \"val\": preproc_config.valid_split &gt; 0,\n        \"test\": preproc_config.test_split &gt; 0,\n    }\n    splits_needed = {k for k, v in split_assignments.items() if v}\n\n    # Instantiate multiprocessing pool. Use semaphore to limit the amount of sequences to read into memory.\n    semaphore = Semaphore(preproc_config.preproc_concurrency + preproc_config.workers)\n    if preproc_config.workers &gt; 1:\n        pool = mp.Pool(preproc_config.workers)\n        # Ordered imap for downstream seeded splitting.\n        preproc_tasks = pool.imap(\n            self.preprocess_data_task,\n            self._yield_sequences_from_files(preproc_config, semaphore),\n            chunksize=preproc_config.chunksize,\n        )\n    else:\n        preproc_tasks = (\n            self.preprocess_data_task(x) for x in self._yield_sequences_from_files(preproc_config, semaphore)\n        )\n\n    # Preprocess data and split results into train, test, and split.\n    with self.preprocessing_context_manager(preproc_config.seed if preproc_config.seed is not None else None):\n        for result, elapsed_time in preproc_tasks:\n            # Release semaphore for the task associated with the result.\n            semaphore.release()\n            # If we still need to ensure splits are assigned\n            if splits_needed:\n                # Force assign to a needed split\n                split = splits_needed.pop()\n            else:\n                # Regular random assignment\n                split = self._train_val_test_split(\n                    preproc_config.train_split, preproc_config.valid_split, preproc_config.test_split\n                )\n            for sequence in result:\n                sequence[\"split\"] = split\n                yield sequence, elapsed_time\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocess_offline","title":"<code>preprocess_offline(preproc_config)</code>","text":"<p>Offline data preprocessing script for Evo2.</p> <p>Parameters:</p> Name Type Description Default <code>preproc_config</code> <code>Evo2PreprocessingConfig</code> <p>Configuration object containing preprocessing settings.</p> required Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def preprocess_offline(self, preproc_config: Evo2PreprocessingConfig):\n    \"\"\"Offline data preprocessing script for Evo2.\n\n    Args:\n        preproc_config (Evo2PreprocessingConfig): Configuration object containing preprocessing settings.\n    \"\"\"\n    # Validate if binaries have already been produced for the given config and overwrite is set to False.\n    if any(\n        self._get_output_filename(preproc_config, ext, split).is_file()\n        for ext, split in zip([self.BIN, self.IDX], [self.TRAIN, self.VAL, self.TEST])\n    ):\n        if not preproc_config.overwrite:\n            # Skip this dataset!\n            logging.info(\n                f\"Skipped overwriting (overwrite: False) existing preprocessed data: {preproc_config.output_prefix}\"\n            )\n            return\n        else:\n            logging.info(\n                f\"Overwriting (overwrite: True) existing preprocessed data: {preproc_config.output_prefix}\"\n            )\n\n    # Instantiate indexed data builders.\n    dataset_dtype = getattr(np, preproc_config.indexed_dataset_dtype)\n    temp_train_bin = self._get_output_filename(preproc_config, self.BIN, self.TRAIN, temp=True)\n    temp_val_bin = self._get_output_filename(preproc_config, self.BIN, self.VAL, temp=True)\n    temp_test_bin = self._get_output_filename(preproc_config, self.BIN, self.TEST, temp=True)\n    train_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_train_bin), dtype=dataset_dtype)\n    val_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_val_bin), dtype=dataset_dtype)\n    test_builder: IndexedDatasetBuilder = IndexedDatasetBuilder(bin_path=str(temp_test_bin), dtype=dataset_dtype)\n    logging.info(f\"Created temporary binary datasets: {temp_train_bin} {temp_val_bin} {temp_test_bin}\")\n\n    # Preprocess data and split results into train, validation, or test.\n    avg_preproc_time = 0.0\n    avg_index_time = 0.0\n    count = 0\n    for sequence, elapsed_time in self.preprocess_generator(preproc_config):\n        index_start_time = time.time()\n        if sequence[\"split\"] == \"train\":\n            train_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n            train_builder.end_document()\n        elif sequence[\"split\"] == \"val\":\n            val_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n            val_builder.end_document()\n        elif sequence[\"split\"] == \"test\":\n            test_builder.add_item(torch.Tensor(sequence[\"tokens\"]))\n            test_builder.end_document()\n        index_end_time = time.time()\n        # Update average preprocessing and indexing time.\n        avg_preproc_time = (avg_preproc_time * count + elapsed_time) / (count + 1)\n        avg_index_time = (avg_index_time * count + index_end_time - index_start_time) / (count + 1)\n        count += 1\n\n    # Report timing.\n    logging.info(f\"Average preprocessing time per sequence: {avg_preproc_time}\")\n    logging.info(f\"Average indexing time per sequence: {avg_index_time}\")\n    logging.info(f\"Number of sequences processed: {count}\")\n\n    # Write preprocessed index data to disk. Rename temporary binaries to denote preprocessing completion.\n    train_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TRAIN)))\n    val_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.VAL)))\n    test_builder.finalize(idx_path=str(self._get_output_filename(preproc_config, self.IDX, self.TEST)))\n    os.rename(temp_train_bin, self._get_output_filename(preproc_config, self.BIN, self.TRAIN))\n    os.rename(temp_val_bin, self._get_output_filename(preproc_config, self.BIN, self.VAL))\n    os.rename(temp_test_bin, self._get_output_filename(preproc_config, self.BIN, self.TEST))\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.Evo2Preprocessor.preprocessing_context_manager","title":"<code>preprocessing_context_manager(seed=None)</code>  <code>staticmethod</code>","text":"<p>Context manager for setting and restoring the random number generator state.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Seed for the random number generator. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>@staticmethod\n@contextmanager\ndef preprocessing_context_manager(seed: Optional[int] = None):\n    \"\"\"Context manager for setting and restoring the random number generator state.\n\n    Args:\n        seed (int | None): Seed for the random number generator. Defaults to None.\n    \"\"\"\n    # Track current state.\n    current_state = random.getstate()\n    try:\n        # Set random seed.\n        random.seed(seed)\n        yield seed\n    finally:\n        # Restore random state.\n        random.setstate(current_state)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.main","title":"<code>main()</code>","text":"<p>Main function to execute the preprocessing script.</p> <p>This function parses command-line arguments, reads the configuration file, and initiates the preprocessing of data as specified in the configuration.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def main():\n    \"\"\"Main function to execute the preprocessing script.\n\n    This function parses command-line arguments, reads the configuration file,\n    and initiates the preprocessing of data as specified in the configuration.\n    \"\"\"\n    # Parse arguments.\n    args = parse_args()\n    # Read config YAML.\n    with open(args.config, \"r\") as yaml_fs:\n        evo2_preproc_config_batch = yaml.safe_load(yaml_fs)\n    for config in evo2_preproc_config_batch:\n        start = time.time()\n        # Convert into Evo2PreprocessingConfig.\n        evo2_preproc_config = Evo2PreprocessingConfig(**config)\n        if evo2_preproc_config.output_dir is not None:\n            evo2_preproc_config.output_dir.mkdir(parents=True, exist_ok=True)\n        # Instantiate Evo2Preprocessor.\n        evo2_preprocessor = Evo2Preprocessor(evo2_preproc_config)\n        # Preprocess data specified in config.\n        evo2_preprocessor.preprocess_offline(evo2_preproc_config)\n        end = time.time()\n        logging.info(\n            f\"Finished preprocessing {evo2_preproc_config.output_prefix} ({evo2_preproc_config.datapaths}) in {end - start:.3f} seconds with {evo2_preproc_config.workers} workers.\"\n        )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/preprocess/#bionemo.evo2.data.preprocess.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse arguments for preprocessing.</p> Source code in <code>bionemo/evo2/data/preprocess.py</code> <pre><code>def parse_args():\n    \"\"\"Parse arguments for preprocessing.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Preprocess FASTA files for training Evo2.\")\n    parser.add_argument(\"-c\", \"--config\", type=str, required=True, help=\"Path to data preprocessing config JSON.\")\n    return parser.parse_args()\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/tokenizer/","title":"Tokenizer","text":""},{"location":"API_reference/bionemo/evo2/data/tokenizer/#bionemo.evo2.data.tokenizer.Evo2Tokenizer","title":"<code>Evo2Tokenizer</code>","text":"<p>Tokenizer for Evo2.</p> Source code in <code>bionemo/evo2/data/tokenizer.py</code> <pre><code>class Evo2Tokenizer:\n    \"\"\"Tokenizer for Evo2.\"\"\"\n\n    def __init__(self, params: Evo2PreprocessingConfig | None = None):\n        \"\"\"Initialize the Evo2Tokenizer.\"\"\"\n        # Pass all NeMo2/Megatron-compliant parameters associated with config.Evo2PreprocessingConfig.\n        self.params: Evo2PreprocessingConfig = params if params is not None else Evo2PreprocessingConfig()\n        self.tokenizer: TokenizerSpec = get_nmt_tokenizer(\n            library=self.params.tokenizer_type.lower(),\n            vocab_file=str(self.params.vocab_file) if self.params.vocab_file is not None else None,\n            merges_file=str(self.params.merges_file) if self.params.merges_file is not None else None,\n            model_name=self.params.tokenizer_model_name,\n            tokenizer_model=self.params.pretrained_tokenizer_model,\n            special_tokens=self.params.special_tokens,\n            use_fast=self.params.fast_hf_tokenizer,\n        )\n\n    def tokenize(\n        self,\n        text: str | list[str],\n        use_ftfy: bool = False,\n        enforce_sample_length: None | int = None,\n        append_eod: bool = False,\n        drop_empty_sequences: bool = False,\n    ):\n        \"\"\"Tokenize the input text data for Evo2.\"\"\"\n        if isinstance(text, str):\n            text = [text]\n        # Tokenize a document or batch of strings.\n        doc_ids = []\n        for l, t in enumerate(text):\n            if use_ftfy:\n                t = ftfy.fix_text(t)\n            # Tokenize the string.\n            text_ids: list = self.tokenizer.text_to_ids(t)\n            if drop_empty_sequences and len(text_ids) == 0:\n                continue\n            # Append EOD token (EOD ID: 0) if appropriate.\n            eod_length = int(append_eod and l == len(text) - 1)\n            token_length = len(text_ids) + eod_length\n            text_ids += [0] * eod_length\n            if enforce_sample_length is not None:\n                # Pad shorter sequences (Pad ID: 1) and except excessive sequences.\n                if token_length &gt; enforce_sample_length:\n                    raise ValueError(\n                        \"Detected input text with a length greater than the maximum \"\n                        f\"possible sample length of {enforce_sample_length}.)\"\n                    )\n                else:\n                    text_ids += [1] * (enforce_sample_length - token_length)\n            # Append to document.\n            doc_ids.append(text_ids)\n        return doc_ids\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/tokenizer/#bionemo.evo2.data.tokenizer.Evo2Tokenizer.__init__","title":"<code>__init__(params=None)</code>","text":"<p>Initialize the Evo2Tokenizer.</p> Source code in <code>bionemo/evo2/data/tokenizer.py</code> <pre><code>def __init__(self, params: Evo2PreprocessingConfig | None = None):\n    \"\"\"Initialize the Evo2Tokenizer.\"\"\"\n    # Pass all NeMo2/Megatron-compliant parameters associated with config.Evo2PreprocessingConfig.\n    self.params: Evo2PreprocessingConfig = params if params is not None else Evo2PreprocessingConfig()\n    self.tokenizer: TokenizerSpec = get_nmt_tokenizer(\n        library=self.params.tokenizer_type.lower(),\n        vocab_file=str(self.params.vocab_file) if self.params.vocab_file is not None else None,\n        merges_file=str(self.params.merges_file) if self.params.merges_file is not None else None,\n        model_name=self.params.tokenizer_model_name,\n        tokenizer_model=self.params.pretrained_tokenizer_model,\n        special_tokens=self.params.special_tokens,\n        use_fast=self.params.fast_hf_tokenizer,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/tokenizer/#bionemo.evo2.data.tokenizer.Evo2Tokenizer.tokenize","title":"<code>tokenize(text, use_ftfy=False, enforce_sample_length=None, append_eod=False, drop_empty_sequences=False)</code>","text":"<p>Tokenize the input text data for Evo2.</p> Source code in <code>bionemo/evo2/data/tokenizer.py</code> <pre><code>def tokenize(\n    self,\n    text: str | list[str],\n    use_ftfy: bool = False,\n    enforce_sample_length: None | int = None,\n    append_eod: bool = False,\n    drop_empty_sequences: bool = False,\n):\n    \"\"\"Tokenize the input text data for Evo2.\"\"\"\n    if isinstance(text, str):\n        text = [text]\n    # Tokenize a document or batch of strings.\n    doc_ids = []\n    for l, t in enumerate(text):\n        if use_ftfy:\n            t = ftfy.fix_text(t)\n        # Tokenize the string.\n        text_ids: list = self.tokenizer.text_to_ids(t)\n        if drop_empty_sequences and len(text_ids) == 0:\n            continue\n        # Append EOD token (EOD ID: 0) if appropriate.\n        eod_length = int(append_eod and l == len(text) - 1)\n        token_length = len(text_ids) + eod_length\n        text_ids += [0] * eod_length\n        if enforce_sample_length is not None:\n            # Pad shorter sequences (Pad ID: 1) and except excessive sequences.\n            if token_length &gt; enforce_sample_length:\n                raise ValueError(\n                    \"Detected input text with a length greater than the maximum \"\n                    f\"possible sample length of {enforce_sample_length}.)\"\n                )\n            else:\n                text_ids += [1] * (enforce_sample_length - token_length)\n        # Append to document.\n        doc_ids.append(text_ids)\n    return doc_ids\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/","title":"Transcript extraction","text":""},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.extract_default_transcript_sequences","title":"<code>extract_default_transcript_sequences(transcript_info, fasta_records, output_file)</code>","text":"<p>Extracts default transcript sequences from the provided transcript information and writes them to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_info</code> <code>dict</code> <p>Dictionary containing transcript and exon information.</p> required <code>fasta_records</code> <code>NvFaidx</code> <p>Indexed FASTA records.</p> required <code>output_file</code> <code>TextIO</code> <p>File object to write the output sequences.</p> required Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def extract_default_transcript_sequences(transcript_info, fasta_records, output_file):\n    \"\"\"Extracts default transcript sequences from the provided transcript information and writes them to an output file.\n\n    Args:\n        transcript_info (dict): Dictionary containing transcript and exon information.\n        fasta_records (NvFaidx): Indexed FASTA records.\n        output_file (TextIO): File object to write the output sequences.\n    \"\"\"\n    for transcript_id in transcript_info[\"transcripts\"]:\n        gene_id = transcript_info[\"transcript2gene\"][transcript_id]\n        this_exons = sorted(transcript_info[\"transcript2exon\"][transcript_id], key=lambda x: x[-1])\n\n        seqname = None\n        exon_qc_failed = False\n        if len(this_exons) &gt; 1:\n            for i in range(1, len(this_exons)):\n                this_exon = this_exons[i]\n                prev_exon = this_exons[i - 1]\n                this_coords = transcript_info[\"exons\"][this_exon]\n                prev_coords = transcript_info[\"exons\"][prev_exon]\n                if this_coords[\"strand\"] != prev_coords[\"strand\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"+\" and this_coords[\"start\"] &lt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"-\" and this_coords[\"start\"] &gt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"seqname\"] != prev_coords[\"seqname\"]:\n                    exon_qc_failed = True\n\n        if exon_qc_failed:\n            continue\n\n        transcript_seq = \"\"\n        for exon in this_exons:\n            coords = transcript_info[\"exons\"][exon]\n            if seqname is None:\n                seqname = coords[\"seqname\"]\n            exon_seq = str(fasta_records[coords[\"seqname\"]][coords[\"start\"] : coords[\"end\"]])\n            if coords[\"strand\"] == \"-\":\n                exon_seq = reverse_sequence(complement_sequence(exon_seq))\n            transcript_seq += exon_seq\n\n        print(f\"&gt;{seqname}|{gene_id}|{transcript_id}\\n{transcript_seq}\", file=output_file)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.extract_stitched_transcript_sequences","title":"<code>extract_stitched_transcript_sequences(transcript_info, fasta_records, output_file, stitch_token='@', promoter_size=1024, intron_window=32, overlap=False)</code>","text":"<p>Extracts stitched transcript sequences from the provided transcript information and writes them to an output file.</p> <p>The \"stitched\" word refers to the process of combining sequences from different regions of the genome to form a single, continuous transcript sequence. This includes: Promoter Region: A specified number of base pairs (bp) upstream of the transcript start site. Exons: The coding regions of the transcript. Intron Windows: A specified number of bp from the neighboring introns around each exon.</p> <p>The stitch_token is used to denote the boundaries between these regions in the stitched transcript sequences.</p> <p>Parameters:</p> Name Type Description Default <code>transcript_info</code> <code>dict</code> <p>Dictionary containing transcript and exon information.</p> required <code>fasta_records</code> <code>NvFaidx</code> <p>Indexed FASTA records.</p> required <code>output_file</code> <code>TextIO</code> <p>File object to write the output sequences.</p> required <code>stitch_token</code> <code>str</code> <p>Token to use for stitching sequences. Defaults to \"@\".</p> <code>'@'</code> <code>promoter_size</code> <code>int</code> <p>Number of bp to include in the promoter region. Defaults to 1024.</p> <code>1024</code> <code>intron_window</code> <code>int</code> <p>Number of bp to include from neighboring introns. Defaults to 32.</p> <code>32</code> <code>overlap</code> <code>bool</code> <p>Whether to allow overlap of neighboring intron windows. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def extract_stitched_transcript_sequences(\n    transcript_info, fasta_records, output_file, stitch_token=\"@\", promoter_size=1024, intron_window=32, overlap=False\n):\n    \"\"\"Extracts stitched transcript sequences from the provided transcript information and writes them to an output file.\n\n    The \"stitched\" word refers to the process of combining sequences from different regions of the genome to form a single,\n    continuous transcript sequence.\n    This includes:\n    Promoter Region: A specified number of base pairs (bp) upstream of the transcript start site.\n    Exons: The coding regions of the transcript.\n    Intron Windows: A specified number of bp from the neighboring introns around each exon.\n\n    The stitch_token is used to denote the boundaries between\n    these regions in the stitched transcript sequences.\n\n    Args:\n        transcript_info (dict): Dictionary containing transcript and exon information.\n        fasta_records (NvFaidx): Indexed FASTA records.\n        output_file (TextIO): File object to write the output sequences.\n        stitch_token (str, optional): Token to use for stitching sequences. Defaults to \"@\".\n        promoter_size (int, optional): Number of bp to include in the promoter region. Defaults to 1024.\n        intron_window (int, optional): Number of bp to include from neighboring introns. Defaults to 32.\n        overlap (bool, optional): Whether to allow overlap of neighboring intron windows. Defaults to False.\n    \"\"\"\n    for transcript_id in transcript_info[\"transcripts\"]:\n        gene_id = transcript_info[\"transcript2gene\"][transcript_id]\n        this_exons = sorted(transcript_info[\"transcript2exon\"][transcript_id], key=lambda x: x[-1])\n\n        exon_qc_failed = False\n        if len(this_exons) &gt; 1:\n            for i in range(1, len(this_exons)):\n                this_exon = this_exons[i]\n                prev_exon = this_exons[i - 1]\n                this_coords = transcript_info[\"exons\"][this_exon]\n                prev_coords = transcript_info[\"exons\"][prev_exon]\n                if this_coords[\"strand\"] != prev_coords[\"strand\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"+\" and this_coords[\"start\"] &lt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"strand\"] == \"-\" and this_coords[\"start\"] &gt; prev_coords[\"start\"]:\n                    exon_qc_failed = True\n                if this_coords[\"seqname\"] != prev_coords[\"seqname\"]:\n                    exon_qc_failed = True\n\n        if exon_qc_failed:\n            continue\n\n        transcript_seq = \"\"\n        seqname = None\n        for i in range(len(this_exons)):\n            # Previous Exon\n            prev_exon = this_exons[i - 1] if i &gt; 0 else None\n            prev_coords = transcript_info[\"exons\"].get(prev_exon, None)\n            # Current Exon\n            cur_exon = this_exons[i]\n            cur_coords = transcript_info[\"exons\"].get(cur_exon, None)\n            exon_number = cur_exon[-1]\n            if seqname is None:\n                seqname = cur_coords[\"seqname\"]\n            # Next Exon\n            next_exon = this_exons[i + 1] if i &lt; len(this_exons) - 1 else None\n            next_coords = transcript_info[\"exons\"].get(next_exon, None)\n            # Extract the stitched spliced sequence without overlapping intron windows.\n            intron_window_left = (\n                min(intron_window, math.floor(abs(cur_coords[\"start\"] - prev_coords[\"end\"]) / 2))\n                if not overlap and prev_coords is not None\n                else intron_window\n            )\n            intron_window_right = (\n                min(intron_window, math.ceil(abs(next_coords[\"start\"] - cur_coords[\"end\"]) / 2))\n                if not overlap and next_coords is not None\n                else intron_window\n            )\n            if cur_coords[\"strand\"] == \"+\" and exon_number == 1:\n                exon_start = cur_coords[\"start\"] - promoter_size\n                exon_end = cur_coords[\"end\"] + intron_window_right\n            elif cur_coords[\"strand\"] == \"-\" and exon_number == 1:\n                exon_start = cur_coords[\"start\"] - intron_window_left\n                exon_end = cur_coords[\"end\"] + promoter_size\n            else:\n                exon_start = cur_coords[\"start\"] - intron_window_left\n                exon_end = cur_coords[\"end\"] + intron_window_right\n            exon_seq = str(fasta_records[cur_coords[\"seqname\"]][exon_start:exon_end])\n            if cur_coords[\"strand\"] == \"-\":\n                exon_seq = stitch_token + reverse_sequence(complement_sequence(exon_seq))\n            transcript_seq += exon_seq\n\n        if stitch_token and len(stitch_token) &gt; 0:\n            transcript_seq = transcript_seq[len(stitch_token) :]\n\n        print(f\"&gt;{seqname}|{gene_id}|{transcript_id}\\n{transcript_seq}\", file=output_file)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.extract_transcript_exons","title":"<code>extract_transcript_exons(gtf_path, only_longest_transcript)</code>","text":"<p>Extracts transcript exons from a GTF file and optionally keeps only the longest transcript per gene.</p> <p>Parameters:</p> Name Type Description Default <code>gtf_path</code> <code>str</code> <p>Path to the GTF file.</p> required <code>only_longest_transcript</code> <code>bool</code> <p>Whether to keep only the longest transcript per gene.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing transcript and exon information.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def extract_transcript_exons(gtf_path: str, only_longest_transcript: bool):\n    \"\"\"Extracts transcript exons from a GTF file and optionally keeps only the longest transcript per gene.\n\n    Args:\n        gtf_path (str): Path to the GTF file.\n        only_longest_transcript (bool): Whether to keep only the longest transcript per gene.\n\n    Returns:\n        dict: A dictionary containing transcript and exon information.\n    \"\"\"\n    genes = defaultdict(set)\n    gene2transcripts = defaultdict(set)\n    transcripts = {}\n    exons = {}\n    exon2transcript = {}\n    transcript2gene = {}\n    transcript2exon = defaultdict(set)\n    skip_transcripts = set()\n\n    gtf_fields = [\"seqname\", \"source\", \"feature\", \"start\", \"end\", \"score\", \"strand\", \"frame\", \"attribute\"]\n    with open(gtf_path) as infile:\n        for line in infile:\n            # skip header lines\n            if line.startswith(\"#\"):\n                continue\n            line = line.strip().split(\"\\t\")\n            if len(line) &lt; 9:\n                continue\n\n            # parse the attributes into a dictionary\n            line = dict(zip(gtf_fields, line))\n            attribs = parse_gtf_attributes(line[\"attribute\"])\n\n            if line[\"feature\"] == \"gene\":\n                contig, start, end, strand = line[\"seqname\"], line[\"start\"], line[\"end\"], line[\"strand\"]\n                start, end = int(line[\"start\"]) - 1, int(line[\"end\"])\n                gene_id = attribs.get(\"gene_id\", None)\n                if not gene_id:\n                    continue\n                genes[gene_id].add((contig, start, end, strand))\n\n            elif line[\"feature\"] == \"exon\":\n                contig, start, end, strand = line[\"seqname\"], line[\"start\"], line[\"end\"], line[\"strand\"]\n                start, end = int(line[\"start\"]) - 1, int(line[\"end\"])\n                gene_id = attribs.get(\"gene_id\", None)\n                if not gene_id:\n                    continue\n                transcript_id = attribs[\"transcript_id\"]\n                gene2transcripts[gene_id].add(transcript_id)\n\n                # Skip exons that have already been handled and are likely errors\n                if transcript_id in skip_transcripts:\n                    continue\n                exon_number = int(attribs[\"exon_number\"])\n\n                exon_id = (gene_id, transcript_id, exon_number)\n                if exon_id in exons:\n                    del exons[exon_id]\n                    if transcript_id in transcripts:\n                        del transcripts[transcript_id]\n                    if transcript_id in transcript2exon:\n                        del transcript2exon[transcript_id]\n                    skip_transcripts.add(transcript_id)\n                    continue\n\n                exons[exon_id] = {\"seqname\": contig, \"start\": start, \"end\": end, \"strand\": strand}\n                if exon_id in exon2transcript:\n                    raise Exception(\"Exon Already Exists in exon2transcript\")\n                exon2transcript[exon_id] = transcript_id\n                transcript2exon[transcript_id].add(exon_id)\n\n            elif line[\"feature\"] == \"transcript\":\n                contig, start, end, strand = line[\"seqname\"], line[\"start\"], line[\"end\"], line[\"strand\"]\n                start, end = int(line[\"start\"]) - 1, int(line[\"end\"])\n                gene_id = attribs.get(\"gene_id\", None)\n                if not gene_id:\n                    continue\n                gbkey = attribs[\"gbkey\"]\n                transcript_biotype = attribs[\"transcript_biotype\"]\n                transcript_id = attribs[\"transcript_id\"]\n                if transcript_id in skip_transcripts:\n                    continue\n\n                transcripts[transcript_id] = {\n                    \"seqname\": contig,\n                    \"start\": start,\n                    \"end\": end,\n                    \"strand\": strand,\n                    \"gbkey\": gbkey,\n                    \"transcript_biotype\": transcript_biotype,\n                }\n                transcript2gene[transcript_id] = gene_id\n                gene2transcripts[gene_id].add(transcript_id)\n\n    if only_longest_transcript:\n        transcript_lengths = defaultdict(int)\n        for exon in exons:\n            transcript_lengths[exon[1]] += exons[exon][\"end\"] - exons[exon][\"start\"]\n\n        keep_transcripts = {}\n        keep_exons = {}\n        keep_exon2transcript = {}\n        keep_transcript2gene = {}\n        keep_transcript2exon = defaultdict(set)\n        keep_skip_transcripts = set()\n\n        for gene in gene2transcripts:\n            this_transcripts = gene2transcripts[gene]\n            this_transcript_lengths = [(transcript, transcript_lengths[transcript]) for transcript in this_transcripts]\n            longest_transcript = max(this_transcript_lengths, key=lambda x: x[1])[0]\n            keep_transcripts[longest_transcript] = dict(transcripts[longest_transcript])\n            for exon in transcript2exon[longest_transcript]:\n                keep_exons[exon] = dict(exons[exon])\n                keep_exon2transcript[exon] = longest_transcript\n                keep_transcript2exon[longest_transcript].add(exon)\n                keep_transcript2gene[longest_transcript] = gene\n\n        transcripts = keep_transcripts\n        exons = keep_exons\n        exon2transcript = keep_exon2transcript\n        transcript2gene = keep_transcript2gene\n        transcript2exon = keep_transcript2exon\n        skip_transcripts = keep_skip_transcripts\n\n    return {\n        \"transcripts\": transcripts,\n        \"exons\": exons,\n        \"exon2transcript\": exon2transcript,\n        \"transcript2gene\": transcript2gene,\n        \"transcript2exon\": transcript2exon,\n    }\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.main","title":"<code>main()</code>","text":"<p>Entry point for the script. Parses arguments and runs the extraction process.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def main():\n    \"\"\"Entry point for the script. Parses arguments and runs the extraction process.\"\"\"\n    args = parse_args()\n    if args.verbose:\n        logging.info(args)\n    run(args)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.parse_args","title":"<code>parse_args()</code>","text":"<p>Parses command line arguments for the transcript extraction script.</p> <p>Returns:</p> Type Description <p>argparse.Namespace: Parsed command line arguments.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def parse_args():\n    \"\"\"Parses command line arguments for the transcript extraction script.\n\n    Returns:\n        argparse.Namespace: Parsed command line arguments.\n    \"\"\"\n    ap = argparse.ArgumentParser(description=\"Extract spliced transcripts from a FASTA and GTF.\")\n    ap.add_argument(\"--fasta-path\", type=str, required=True, help=\"Path to FASTA file to extract transcripts from.\")\n    ap.add_argument(\n        \"--gtf-path\",\n        type=str,\n        required=True,\n        help=\"Path to gene transfer format (GTF) file associated with the FASTA.\",\n    )\n    ap.add_argument(\"--output-path\", type=str, default=None, help=\"Path to output FASTA file.\")\n    ap.add_argument(\n        \"--transcript-type\",\n        type=str,\n        default=\"default\",\n        choices=[\"default\", \"stitched\"],\n        help=\"Type of transcript to extract from the GTF and FASTA files for splicing. 'Stitched' transcripts include 1024 bp of sequence from the promoter and 32 bp around each exon.\",\n    )\n    ap.add_argument(\n        \"--stitched-promoter\",\n        type=int,\n        default=1024,\n        help=\"Number of bp to include in the promoter region when --transcript-type=stitched is used. Defaults to 1024.\",\n    )\n    ap.add_argument(\n        \"--stitched-intron\",\n        type=int,\n        default=32,\n        help=\"Number of bp to include from neighboring introns when --transcript-type=stitched is used. Defaults to 32.\",\n    )\n    ap.add_argument(\n        \"--stitched-overlap\",\n        action=\"store_true\",\n        help=\"Allow overlap of neighboring intron windows when --transcript-type=stitched is used. Defaults to False, i.e. prevents overlap by shortening the intron windows for a contiguous splice.\",\n    )\n    ap.add_argument(\n        \"--only-longest-transcript\", action=\"store_true\", help=\"Only extract the longest transcript per gene.\"\n    )\n    ap.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Turn on verbose log messages.\")\n    return ap.parse_args()\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.parse_gtf_attributes","title":"<code>parse_gtf_attributes(attributes)</code>","text":"<p>Parses the attributes field of a GTF file line into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>str</code> <p>The attributes field from a GTF file line.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary of attribute key-value pairs.</p> Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def parse_gtf_attributes(attributes: str):\n    \"\"\"Parses the attributes field of a GTF file line into a dictionary.\n\n    Args:\n        attributes (str): The attributes field from a GTF file line.\n\n    Returns:\n        dict: A dictionary of attribute key-value pairs.\n    \"\"\"\n    # Split on all semicolons that are not inside quotes\n    attributes = re.split(r';(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)', attributes)\n    out = {}\n    for a in attributes:\n        if len(a) == 0:\n            continue\n        key = a.split()[0]\n        value = a.split('\"')[1]\n        out[key] = value\n    return out\n</code></pre>"},{"location":"API_reference/bionemo/evo2/data/transcript_extraction/#bionemo.evo2.data.transcript_extraction.run","title":"<code>run(args)</code>","text":"<p>Main function to run the transcript extraction process based on command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Parsed command line arguments.</p> required Source code in <code>bionemo/evo2/data/transcript_extraction.py</code> <pre><code>def run(args):\n    \"\"\"Main function to run the transcript extraction process based on command line arguments.\n\n    Args:\n        args (argparse.Namespace): Parsed command line arguments.\n    \"\"\"\n    with open(args.output_path, \"w\") if args.output_path is not None else sys.stdout as output_file:\n        if args.verbose:\n            logging.info(\"Indexing FASTA file...\")\n\n        fasta_index = NvFaidx(args.fasta_path)\n\n        if args.transcript_type == \"default\":\n            if args.verbose:\n                logging.info(\"Extracting default transcripts...\")\n                if args.only_longest_transcript:\n                    logging.info(\"Only extracting the longest transcript per gene.\")\n                else:\n                    logging.info(\"Extracting all transcripts regardless of length.\")\n\n        elif args.transcript_type == \"stitched\":\n            if args.verbose:\n                logging.info(\"Extracting stitched transcripts...\")\n                if args.only_longest_transcript:\n                    logging.info(\"Only extracting the longest transcript per gene.\")\n                else:\n                    logging.info(\"Extracting all transcripts regardless of length.\")\n\n        transcript_info = extract_transcript_exons(args.gtf_path, args.only_longest_transcript)\n\n        if args.transcript_type == \"default\":\n            extract_default_transcript_sequences(transcript_info, fasta_index, output_file)\n        elif args.transcript_type == \"stitched\":\n            extract_stitched_transcript_sequences(\n                transcript_info,\n                fasta_index,\n                output_file,\n                promoter_size=args.stitched_promoter,\n                intron_window=args.stitched_intron,\n                overlap=args.stitched_overlap,\n            )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/infer/","title":"Infer","text":""},{"location":"API_reference/bionemo/evo2/run/infer/#bionemo.evo2.run.infer.infer","title":"<code>infer(prompt, ckpt_dir, temperature, top_k, top_p, max_new_tokens, tensor_parallel_size, pipeline_model_parallel_size, context_parallel_size, output_file=None, ckpt_format='torch_dist', seed=None)</code>","text":"<p>Inference workflow for Evo2.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to generate text from Evo2.</p> required <code>ckpt_dir</code> <code>str</code> <p>Path to checkpoint directory containing pre-trained Evo2 model.</p> required <code>temperature</code> <code>float</code> <p>Temperature during sampling for generation.</p> required <code>top_k</code> <code>int</code> <p>Top K during sampling for generation.</p> required <code>top_p</code> <code>float</code> <p>Top P during sampling for generation.</p> required <code>max_new_tokens</code> <code>int</code> <p>Maximum number of tokens to generate.</p> required <code>tensor_parallel_size</code> <code>int</code> <p>Order of tensor parallelism.</p> required <code>pipeline_model_parallel_size</code> <code>int</code> <p>Order of pipeline parallelism.</p> required <code>context_parallel_size</code> <code>int</code> <p>Order of context parallelism.</p> required <code>output_file</code> <code>str</code> <p>Output file containing the generated text produced by the Evo2 model.</p> <code>None</code> <code>ckpt_format</code> <code>CheckpointFormats</code> <p>Checkpoint format to use.</p> <code>'torch_dist'</code> <code>seed</code> <code>int</code> <p>Random seed for generation.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>bionemo/evo2/run/infer.py</code> <pre><code>def infer(\n    prompt: str,\n    ckpt_dir: str,\n    temperature: float,\n    top_k: int,\n    top_p: float,\n    max_new_tokens: int,\n    tensor_parallel_size: int,\n    pipeline_model_parallel_size: int,\n    context_parallel_size: int,\n    output_file: Optional[str] = None,\n    ckpt_format: CheckpointFormats = \"torch_dist\",\n    seed: Optional[int] = None,\n):\n    \"\"\"Inference workflow for Evo2.\n\n    Args:\n        prompt (str): Prompt to generate text from Evo2.\n        ckpt_dir (str): Path to checkpoint directory containing pre-trained Evo2 model.\n        temperature (float): Temperature during sampling for generation.\n        top_k (int): Top K during sampling for generation.\n        top_p (float): Top P during sampling for generation.\n        max_new_tokens (int): Maximum number of tokens to generate.\n        tensor_parallel_size (int): Order of tensor parallelism.\n        pipeline_model_parallel_size (int): Order of pipeline parallelism.\n        context_parallel_size (int): Order of context parallelism.\n        output_file (str): Output file containing the generated text produced by the Evo2 model.\n        ckpt_format (CheckpointFormats): Checkpoint format to use.\n        seed (int): Random seed for generation.\n\n    Returns:\n        None\n    \"\"\"\n    model_parallel_size = tensor_parallel_size * pipeline_model_parallel_size * context_parallel_size\n    if model_parallel_size &gt; torch.cuda.device_count():\n        raise ValueError(\n            f\"Requested model parallel size {model_parallel_size} is greater than the \"\n            f\"number of available CUDA devices {torch.cuda.device_count()}\"\n        )\n    # Create PTL trainer.\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=model_parallel_size,\n        strategy=nl.MegatronStrategy(\n            tensor_model_parallel_size=tensor_parallel_size,\n            pipeline_model_parallel_size=pipeline_model_parallel_size,\n            context_parallel_size=context_parallel_size,\n            pipeline_dtype=torch.bfloat16,\n            ckpt_load_optimizer=False,  # Needs to be false for a normal model checkpoint.\n            ckpt_save_optimizer=False,\n            ckpt_async_save=False,\n            save_ckpt_format=ckpt_format,\n            ckpt_load_strictness=\"log_all\",\n        ),\n        log_every_n_steps=1,\n        limit_val_batches=10,\n        num_sanity_val_steps=0,\n        plugins=nl.MegatronMixedPrecision(\n            precision=\"bf16-mixed\",\n            params_dtype=torch.bfloat16,\n        ),\n    )\n\n    # transformers generate method has more options than NeMo/Megatron.\n    results = generate(\n        path=ckpt_dir,\n        prompts=[prompt],\n        trainer=trainer,\n        inference_params=CommonInferenceParams(\n            temperature,\n            top_k,\n            top_p,\n            return_log_probs=False,\n            num_tokens_to_generate=max_new_tokens,\n        ),\n        text_only=True,\n        random_seed=seed if seed is not None else None,\n    )\n\n    if torch.distributed.get_rank() == 0:\n        if output_file is None:\n            logging.info(results)\n        else:\n            with open(output_file, \"w\") as f:\n                f.write(f\"{results}\\n\")\n\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/infer/#bionemo.evo2.run.infer.main","title":"<code>main()</code>","text":"<p>Main function for Evo2 inference.</p> Source code in <code>bionemo/evo2/run/infer.py</code> <pre><code>def main():\n    \"\"\"Main function for Evo2 inference.\"\"\"\n    # Parse args.\n    args = parse_args()\n    infer(\n        prompt=args.prompt,\n        ckpt_dir=args.ckpt_dir,\n        temperature=args.temperature,\n        top_k=args.top_k,\n        top_p=args.top_p,\n        max_new_tokens=args.max_new_tokens,\n        tensor_parallel_size=args.tensor_parallel_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        context_parallel_size=args.context_parallel_size,\n        output_file=args.output_file,\n        ckpt_format=args.ckpt_format,\n        seed=args.seed,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/infer/#bionemo.evo2.run.infer.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse arguments for Evo2 inference.</p> Source code in <code>bionemo/evo2/run/infer.py</code> <pre><code>def parse_args():\n    \"\"\"Parse arguments for Evo2 inference.\"\"\"\n    ap = argparse.ArgumentParser()\n\n    # generation args:\n    default_prompt = (\n        \"|d__Bacteria;\"\n        + \"p__Pseudomonadota;\"\n        + \"c__Gammaproteobacteria;\"\n        + \"o__Enterobacterales;\"\n        + \"f__Enterobacteriaceae;\"\n        + \"g__Escherichia;\"\n        + \"s__Escherichia|\"\n    )\n    ap.add_argument(\n        \"--prompt\",\n        type=str,\n        default=default_prompt,\n        help=\"Prompt to generate text from Evo2. Defaults to a phylogenetic lineage tag for E coli.\",\n    )\n    ap.add_argument(\n        \"--ckpt-dir\", type=str, required=True, help=\"Path to checkpoint directory containing pre-trained Evo2 model.\"\n    )\n    ap.add_argument(\"--temperature\", type=float, default=1.0, help=\"Temperature during sampling for generation.\")\n    ap.add_argument(\"--top-k\", type=int, default=0, help=\"Top K during sampling for generation.\")\n    ap.add_argument(\"--top-p\", type=float, default=0.0, help=\"Top P during sampling for generation.\")\n    ap.add_argument(\"--max-new-tokens\", type=int, default=1024, help=\"Maximum number of tokens to generate.\")\n    ap.add_argument(\"--seed\", type=int, default=None, help=\"Random seed for generation.\")\n    # compute args:\n    ap.add_argument(\"--tensor-parallel-size\", type=int, default=1, help=\"Order of tensor parallelism. Defaults to 1.\")\n    ap.add_argument(\n        \"--pipeline-model-parallel-size\", type=int, default=1, help=\"Order of pipeline parallelism. Defaults to 1.\"\n    )\n    ap.add_argument(\n        \"--context-parallel-size\", type=int, default=1, help=\"Order of context parallelism. Defaults to 1.\"\n    )\n    # output args:\n    ap.add_argument(\n        \"--output-file\",\n        type=str,\n        default=None,\n        help=\"Output file containing the generated text produced by the Evo2 model. If not provided, the output will be logged.\",\n    )\n    # extra:\n    ap.add_argument(\n        \"--ckpt-format\",\n        type=str,\n        choices=[\"torch_dist\", \"zarr\"],\n        default=\"torch_dist\",\n        help=\"Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated.\",\n    )\n\n    return ap.parse_args()\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/","title":"Predict","text":""},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.HyenaPredictor","title":"<code>HyenaPredictor</code>","text":"<p>               Bases: <code>LightningPassthroughPredictionMixin</code>, <code>HyenaModel</code></p> <p>A predictor for the Hyena model. This adds in the predict step and the passthrough method.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>class HyenaPredictor(LightningPassthroughPredictionMixin, HyenaModel):\n    \"\"\"A predictor for the Hyena model. This adds in the predict step and the passthrough method.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        output_log_prob_seqs: bool = False,\n        log_prob_collapse_option: Literal[\"sum\", \"mean\"] = \"mean\",\n        **kwargs,\n    ):\n        \"\"\"Initialize the predictor with our needs around computing log probabilities.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.output_log_prob_seqs = output_log_prob_seqs\n        self.log_prob_collapse_option = log_prob_collapse_option\n\n    def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"Alias for forward_step, also log the pad mask since sequences may not all have the same length.\"\"\"\n        if len(batch) == 0:\n            return\n        forward_out = self.forward_step(batch)\n        if not isinstance(forward_out, Tensor):\n            return forward_out\n        # Reminder: the model's predictions for input i land at output i+1. To get everything to align, we prepend the\n        # EOS token to the input sequences and take the outputs for all but the first token.\n        forward_out_tp_gathered = _gather_along_last_dim(forward_out)\n        # else:\n        #     forward_out_tp_gathered = _collect_into_dim(forward_out, dim=-1)\n        forward_out_gathered = _gather_along_cp_dim(forward_out_tp_gathered)\n        assert self.tokenizer.vocab_size == forward_out_gathered.shape[-1]\n        if self.output_log_prob_seqs:\n            softmax_logprobs = torch.log_softmax(forward_out_gathered, dim=-1)\n            softmax_logprobs = softmax_logprobs[:, :-1]\n            input_ids = batch[\"tokens\"][:, 1:]\n            assert softmax_logprobs.shape[1] == input_ids.shape[1]\n\n            logprobs = torch.gather(\n                softmax_logprobs,  # Gather likelihoods...\n                2,  # along the vocab dimension...\n                input_ids.unsqueeze(-1),  # using the token ids to index.\n            ).squeeze(-1)\n            log_prob_seqs = torch.sum(logprobs * batch[\"loss_mask\"][:, 1:].float(), dim=-1)\n            if self.log_prob_collapse_option == \"mean\":\n                log_prob_seqs = log_prob_seqs / (batch[\"loss_mask\"][:, 1:].float().sum(dim=-1) + 1e-8)\n            return {\"log_probs_seqs\": log_prob_seqs.cpu(), \"seq_idx\": batch[\"seq_idx\"].cpu()}\n        else:\n            # If the user wants to match back to logits, then they will need to do the offsetting logic themselves.\n            return {\n                \"token_logits\": forward_out_gathered.cpu(),\n                \"pad_mask\": batch[\"loss_mask\"].cpu(),\n                \"seq_idx\": batch[\"seq_idx\"].cpu(),\n            }\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.HyenaPredictor.__init__","title":"<code>__init__(*args, output_log_prob_seqs=False, log_prob_collapse_option='mean', **kwargs)</code>","text":"<p>Initialize the predictor with our needs around computing log probabilities.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    output_log_prob_seqs: bool = False,\n    log_prob_collapse_option: Literal[\"sum\", \"mean\"] = \"mean\",\n    **kwargs,\n):\n    \"\"\"Initialize the predictor with our needs around computing log probabilities.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.output_log_prob_seqs = output_log_prob_seqs\n    self.log_prob_collapse_option = log_prob_collapse_option\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.HyenaPredictor.predict_step","title":"<code>predict_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward_step, also log the pad mask since sequences may not all have the same length.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"Alias for forward_step, also log the pad mask since sequences may not all have the same length.\"\"\"\n    if len(batch) == 0:\n        return\n    forward_out = self.forward_step(batch)\n    if not isinstance(forward_out, Tensor):\n        return forward_out\n    # Reminder: the model's predictions for input i land at output i+1. To get everything to align, we prepend the\n    # EOS token to the input sequences and take the outputs for all but the first token.\n    forward_out_tp_gathered = _gather_along_last_dim(forward_out)\n    # else:\n    #     forward_out_tp_gathered = _collect_into_dim(forward_out, dim=-1)\n    forward_out_gathered = _gather_along_cp_dim(forward_out_tp_gathered)\n    assert self.tokenizer.vocab_size == forward_out_gathered.shape[-1]\n    if self.output_log_prob_seqs:\n        softmax_logprobs = torch.log_softmax(forward_out_gathered, dim=-1)\n        softmax_logprobs = softmax_logprobs[:, :-1]\n        input_ids = batch[\"tokens\"][:, 1:]\n        assert softmax_logprobs.shape[1] == input_ids.shape[1]\n\n        logprobs = torch.gather(\n            softmax_logprobs,  # Gather likelihoods...\n            2,  # along the vocab dimension...\n            input_ids.unsqueeze(-1),  # using the token ids to index.\n        ).squeeze(-1)\n        log_prob_seqs = torch.sum(logprobs * batch[\"loss_mask\"][:, 1:].float(), dim=-1)\n        if self.log_prob_collapse_option == \"mean\":\n            log_prob_seqs = log_prob_seqs / (batch[\"loss_mask\"][:, 1:].float().sum(dim=-1) + 1e-8)\n        return {\"log_probs_seqs\": log_prob_seqs.cpu(), \"seq_idx\": batch[\"seq_idx\"].cpu()}\n    else:\n        # If the user wants to match back to logits, then they will need to do the offsetting logic themselves.\n        return {\n            \"token_logits\": forward_out_gathered.cpu(),\n            \"pad_mask\": batch[\"loss_mask\"].cpu(),\n            \"seq_idx\": batch[\"seq_idx\"].cpu(),\n        }\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule","title":"<code>PredictDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Create a dataloader for prediction.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>class PredictDataModule(LightningDataModule):\n    \"\"\"Create a dataloader for prediction.\"\"\"\n\n    def __init__(self, dataset: torch.utils.data.Dataset, batch_size: int = 1):\n        \"\"\"Create a dataloader for prediction.\"\"\"\n        super().__init__()\n        self.dataset = dataset\n        self.batch_size = batch_size\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"Set up the dataloader.\"\"\"\n        pass\n\n    def predict_dataloader(self):\n        \"\"\"Create a dataloader for prediction.\"\"\"\n        # need to use this to communicate that we are in predict mode and safe to not drop last batch\n        return WrappedDataLoader(\n            mode=\"predict\",\n            dataset=self.dataset,\n            batch_size=self.batch_size,\n            num_workers=8,\n            shuffle=False,\n            drop_last=False,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule.__init__","title":"<code>__init__(dataset, batch_size=1)</code>","text":"<p>Create a dataloader for prediction.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def __init__(self, dataset: torch.utils.data.Dataset, batch_size: int = 1):\n    \"\"\"Create a dataloader for prediction.\"\"\"\n    super().__init__()\n    self.dataset = dataset\n    self.batch_size = batch_size\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Create a dataloader for prediction.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def predict_dataloader(self):\n    \"\"\"Create a dataloader for prediction.\"\"\"\n    # need to use this to communicate that we are in predict mode and safe to not drop last batch\n    return WrappedDataLoader(\n        mode=\"predict\",\n        dataset=self.dataset,\n        batch_size=self.batch_size,\n        num_workers=8,\n        shuffle=False,\n        drop_last=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.PredictDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up the dataloader.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"Set up the dataloader.\"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict._gather_along_cp_dim","title":"<code>_gather_along_cp_dim(input_, seq_dim=1)</code>","text":"<p>Gather tensors and concatenate along the last dimension.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def _gather_along_cp_dim(input_, seq_dim: int = 1):\n    \"\"\"Gather tensors and concatenate along the last dimension.\"\"\"\n    world_size = parallel_state.get_context_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    dim_size = list(input_.size())\n    dim_size[0] = dim_size[0] * world_size\n\n    output = torch.empty(dim_size, dtype=input_.dtype, device=torch.cuda.current_device())\n    torch.distributed.all_gather_into_tensor(\n        output, input_.contiguous(), group=parallel_state.get_tensor_model_parallel_group()\n    )\n    tensor_list = output.chunk(world_size, dim=0)\n    output = torch.cat(tensor_list, dim=seq_dim).contiguous()\n\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.hyena_predict_data_step","title":"<code>hyena_predict_data_step(dataloader_iter)</code>","text":"<p>Data step for the Hyena model prediction. Modified from the original gpt data step to include the seq_idx.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def hyena_predict_data_step(dataloader_iter) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Data step for the Hyena model prediction. Modified from the original gpt data step to include the seq_idx.\"\"\"\n    from megatron.core import parallel_state\n\n    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842\n\n    batch = next(dataloader_iter)\n\n    _batch: dict\n    if isinstance(batch, tuple) and len(batch) == 3:\n        _batch = batch[0]\n    else:\n        _batch = batch\n\n    required_device_keys = set()\n    required_host_keys = set()\n\n    required_device_keys.add(\"attention_mask\")\n    if \"cu_seqlens\" in _batch:\n        required_device_keys.add(\"cu_seqlens\")\n        required_host_keys.add(\"cu_seqlens_argmin\")\n        required_host_keys.add(\"max_seqlen\")\n\n    if parallel_state.is_pipeline_first_stage():\n        required_device_keys.update((\"tokens\", \"position_ids\"))\n    if parallel_state.is_pipeline_last_stage():\n        required_device_keys.update((\"labels\", \"loss_mask\", \"seq_idx\"))\n\n    _batch_required_keys = {}\n    for key, val in _batch.items():\n        if key in required_device_keys:\n            _batch_required_keys[key] = val.cuda(non_blocking=True)\n        elif key in required_host_keys:\n            _batch_required_keys[key] = val.cpu()\n        else:\n            _batch_required_keys[key] = None\n\n    # slice batch along sequence dimension for context parallelism\n    output = get_batch_on_this_context_parallel_rank(_batch_required_keys)\n\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.hyena_predict_forward_step","title":"<code>hyena_predict_forward_step(model, batch)</code>","text":"<p>Performs a forward step for the Hyena model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Hyena model</p> required <code>batch</code> <p>Dictionary containing input batch data with keys: - tokens: Input token IDs - position_ids: Position IDs - labels: Labels for loss computation - loss_mask: Mask for loss computation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output from the model forward pass</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def hyena_predict_forward_step(model, batch) -&gt; torch.Tensor:\n    \"\"\"Performs a forward step for the Hyena model.\n\n    Args:\n        model: The Hyena model\n        batch: Dictionary containing input batch data with keys:\n            - tokens: Input token IDs\n            - position_ids: Position IDs\n            - labels: Labels for loss computation\n            - loss_mask: Mask for loss computation\n\n    Returns:\n        torch.Tensor: Output from the model forward pass\n    \"\"\"\n    forward_args = {\n        \"input_ids\": batch[\"tokens\"],\n        \"position_ids\": batch[\"position_ids\"],\n        # \"labels\": batch[\"labels\"],\n        # \"loss_mask\": batch[\"loss_mask\"],\n    }\n\n    forward_args[\"attention_mask\"] = None\n    if \"cu_seqlens\" in batch:\n        forward_args[\"packed_seq_params\"] = get_packed_seq_params(batch)\n    return model(**forward_args)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.main","title":"<code>main()</code>","text":"<p>Entrypoint for Evo2 prediction (single inference step, no new tokens).</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def main():\n    \"\"\"Entrypoint for Evo2 prediction (single inference step, no new tokens).\"\"\"\n    args = parse_args()\n    predict(\n        fasta_path=args.fasta,\n        ckpt_dir=args.ckpt_dir,\n        tensor_parallel_size=args.tensor_parallel_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        context_parallel_size=args.context_parallel_size,\n        output_dir=args.output_dir,\n        model_size=args.model_size,\n        ckpt_format=args.ckpt_format,\n        fp8=args.fp8,\n        full_fp8=args.full_fp8,\n        batch_size=args.batch_size,\n        output_log_prob_seqs=args.output_log_prob_seqs,\n        log_prob_collapse_option=args.log_prob_collapse_option,\n        prepend_bos=args.prepend_bos,\n        no_sequence_parallel=args.no_sequence_parallel,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse arguments for Evo2 inference.</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def parse_args():\n    \"\"\"Parse arguments for Evo2 inference.\"\"\"\n    ap = argparse.ArgumentParser()\n\n    ap.add_argument(\"--fasta\", type=Path, required=True, help=\"Fasta path from which to generate logit predictions.\")\n    ap.add_argument(\"--ckpt-dir\", type=Path, required=True, help=\"NeMo2 checkpoint directory for inference.\")\n    ap.add_argument(\"--prepend-bos\", action=\"store_true\", help=\"Prepend BOS token to sequences. Defaults to False.\")\n    ap.add_argument(\"--tensor-parallel-size\", type=int, default=1, help=\"Order of tensor parallelism. Defaults to 1.\")\n    ap.add_argument(\n        \"--pipeline-model-parallel-size\", type=int, default=1, help=\"Order of pipeline parallelism. Defaults to 1.\"\n    )\n    ap.add_argument(\n        \"--context-parallel-size\", type=int, default=1, help=\"Order of context parallelism. Defaults to 1.\"\n    )\n    ap.add_argument(\n        \"--no-sequence-parallel\",\n        action=\"store_true\",\n        help=\"When using TP, skip sequence parallelism. Otherwise sequence parallelism is used whenever tensor \"\n        \"parallelism is used. sequence parallelism should save a small amount of GPU memory so it's on\"\n        \" by default.\",\n    )\n    ap.add_argument(\"--batch-size\", type=int, default=1, help=\"Batch size for prediction. Defaults to 1.\")\n    ap.add_argument(\n        \"--model-size\",\n        type=str,\n        default=\"7b\",\n        choices=sorted(HYENA_MODEL_OPTIONS.keys()),\n        help=\"Model size to use. Defaults to '7b'.\",\n    )\n    # output args:\n    ap.add_argument(\n        \"--output-dir\",\n        type=Path,\n        default=None,\n        help=\"Output dir that will contain the generated text produced by the Evo2 model. If not provided, the output will be logged.\",\n    )\n    ap.add_argument(\n        \"--full-fp8\",\n        action=\"store_true\",\n        help=\"Use full FP8 precision (faster but less accurate) rather than vortex style which \"\n        \"only applies FP8 to the projection layer of the hyena mixer, when using FP8.\",\n    )\n    ap.add_argument(\"--fp8\", action=\"store_true\", help=\"Use FP8 precision. Defaults to BF16.\")\n    # extra:\n    ap.add_argument(\n        \"--ckpt-format\",\n        type=str,\n        choices=[\"torch_dist\", \"zarr\"],\n        default=\"torch_dist\",\n        help=\"Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated.\",\n    )\n    ap.add_argument(\n        \"--output-log-prob-seqs\", action=\"store_true\", help=\"Output log probability of sequences. Defaults to False.\"\n    )\n    ap.add_argument(\n        \"--log-prob-collapse-option\",\n        choices=[\"sum\", \"mean\"],\n        default=\"mean\",\n        help=\"How to collapse the log probabilities across the sequence dimension.\",\n    )\n    return ap.parse_args()\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/predict/#bionemo.evo2.run.predict.predict","title":"<code>predict(fasta_path, ckpt_dir, output_dir, tensor_parallel_size, pipeline_model_parallel_size, context_parallel_size, model_size='7b', ckpt_format='torch_dist', fp8=False, full_fp8=False, work_dir=None, batch_size=1, output_log_prob_seqs=False, log_prob_collapse_option='mean', prepend_bos=False, no_sequence_parallel=False)</code>","text":"<p>Inference workflow for Evo2.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>bionemo/evo2/run/predict.py</code> <pre><code>def predict(\n    fasta_path: Path,\n    ckpt_dir: str,\n    output_dir: Path,\n    tensor_parallel_size: int,\n    pipeline_model_parallel_size: int,\n    context_parallel_size: int,\n    model_size: str = \"7b\",\n    ckpt_format: CheckpointFormats = \"torch_dist\",\n    fp8: bool = False,\n    full_fp8: bool = False,\n    work_dir: Path | None = None,\n    batch_size: int = 1,\n    output_log_prob_seqs: bool = False,\n    log_prob_collapse_option: Literal[\"sum\", \"mean\"] = \"mean\",\n    prepend_bos: bool = False,\n    no_sequence_parallel: bool = False,\n):\n    \"\"\"Inference workflow for Evo2.\n\n    Returns:\n        None\n    \"\"\"\n    if work_dir is None:\n        work_dir = Path(tempfile.mkdtemp())\n    sequence_parallel = tensor_parallel_size &gt; 1 and not no_sequence_parallel\n    output_dir.mkdir(parents=True, exist_ok=True)  # Make sure the output directory exists, files will be written here.\n    model_parallel_size = tensor_parallel_size * pipeline_model_parallel_size * context_parallel_size\n    if model_parallel_size &gt; torch.cuda.device_count():\n        raise ValueError(\n            f\"Requested model parallel size {model_parallel_size} is greater than the \"\n            f\"number of available CUDA devices {torch.cuda.device_count()}\"\n        )\n    # Create PTL trainer.\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=model_parallel_size,\n        strategy=nl.MegatronStrategy(\n            drop_last_batch=False,\n            tensor_model_parallel_size=tensor_parallel_size,\n            pipeline_model_parallel_size=pipeline_model_parallel_size,\n            context_parallel_size=context_parallel_size,\n            pipeline_dtype=torch.bfloat16,\n            ckpt_load_optimizer=False,  # Needs to be false for a normal model checkpoint.\n            ckpt_save_optimizer=False,\n            ckpt_async_save=False,\n            sequence_parallel=tensor_parallel_size &gt; 1 and sequence_parallel,\n            save_ckpt_format=ckpt_format,\n            ckpt_load_strictness=\"log_all\",\n            data_sampler=nl.MegatronDataSampler(\n                micro_batch_size=batch_size,\n                global_batch_size=batch_size,\n                seq_len=8192,\n                output_log=False,  # this is needed for predict step to work\n            ),\n        ),\n        log_every_n_steps=1,\n        limit_val_batches=10,\n        num_sanity_val_steps=0,\n        callbacks=[\n            PredictionWriter(\n                output_dir=output_dir,\n                write_interval=\"epoch\",\n                batch_dim_key_defaults={\"token_logits\": 0},\n                seq_dim_key_defaults={\"token_logits\": 1},\n            )\n        ],\n        plugins=nl.MegatronMixedPrecision(\n            precision=\"bf16-mixed\",\n            params_dtype=torch.bfloat16,\n            # Only use FP8 in this plugin when using full FP8 precision and FP8.\n            #   Otherwise use vortex_style_fp8 in the model config.\n            fp8=\"hybrid\" if fp8 and full_fp8 else None,\n            fp8_amax_history_len=16 if fp8 and full_fp8 else 1,\n            fp8_amax_compute_algo=\"max\" if fp8 and full_fp8 else \"most_recent\",\n        ),\n    )\n    config = HYENA_MODEL_OPTIONS[model_size](\n        forward_step_fn=hyena_predict_forward_step,\n        data_step_fn=hyena_predict_data_step,  # , attention_backend=AttnBackend.fused,\n        distribute_saved_activations=False if sequence_parallel and tensor_parallel_size &gt; 1 else True,\n        # Only use vortex style FP8 in the model config if using FP8 and not full FP8. This will only apply FP8 to\n        #   the projection layer of the hyena mixer.\n        vortex_style_fp8=fp8 and not full_fp8,\n    )\n    trainer.strategy._setup_optimizers = False\n\n    nemo_logger = NeMoLogger(log_dir=work_dir)\n    nemo_logger.setup(trainer, resume_if_exists=True)\n    resume = nl.AutoResume(\n        resume_if_exists=True,\n        resume_ignore_no_checkpoint=False,\n        resume_past_end=False,\n        restore_config=nl.RestoreConfig(\n            path=str(ckpt_dir),  # NeMo expects a string path.\n            load_model_state=True,\n            load_optim_state=False,\n            load_artifacts=False,\n        ),\n    )\n    tokenizer = get_nmt_tokenizer(\"byte-level\")\n    model = HyenaPredictor(\n        config,\n        tokenizer=tokenizer,\n        output_log_prob_seqs=output_log_prob_seqs,\n        log_prob_collapse_option=log_prob_collapse_option,\n    )\n    resume.setup(trainer, model)  # this pulls weights from the starting checkpoint.\n\n    dataset = SimpleFastaDataset(fasta_path, tokenizer, prepend_bos=prepend_bos)\n    datamodule = PredictDataModule(dataset, batch_size=batch_size)\n    trainer.predict(model, datamodule=datamodule)\n    dataset.write_idx_map(\n        output_dir\n    )  # Finally write out the index map so we can match the predictions to the original sequences.\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/train/","title":"Train","text":""},{"location":"API_reference/bionemo/evo2/run/train/#bionemo.evo2.run.train.main","title":"<code>main()</code>","text":"<p>Parsing args and running evo2 training.</p> Source code in <code>bionemo/evo2/run/train.py</code> <pre><code>def main():\n    \"\"\"Parsing args and running evo2 training.\"\"\"\n    args = parse_args()\n    train(args=args)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/train/#bionemo.evo2.run.train.parse_args","title":"<code>parse_args(args=None)</code>","text":"<p>Parse arguments for Evo2 model training.</p> Source code in <code>bionemo/evo2/run/train.py</code> <pre><code>def parse_args(args: Optional[List[str]] = None) -&gt; argparse.Namespace:\n    \"\"\"Parse arguments for Evo2 model training.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Train a Hyena model using NeMo 2.0.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    data_group = parser.add_mutually_exclusive_group(required=True)\n\n    data_group.add_argument(\n        \"-d\",\n        \"--dataset-config\",\n        type=str,\n        help=\"Path to the blended / weighted training dataset configuration YAML.\",\n    )\n    data_group.add_argument(\n        \"--mock-data\",\n        action=\"store_true\",\n        help=\"Train with Mock data (for testing/debugging), either set this or provide a dataset config.\",\n    )\n\n    parser.add_argument(\n        \"--dataset-dir\",\n        type=str,\n        help=\"Absolute path to the dataset directory. Defaults to using the absolute or relative paths (dataset_prefix) specified in the dataset config YAML.\",\n    )\n\n    parser.add_argument(\"--num-nodes\", type=int, default=1, help=\"Number of nodes to use for training, defaults to 1.\")\n    parser.add_argument(\"--devices\", type=int, default=1, help=\"Number of devices to use for training, defaults to 1.\")\n    parser.add_argument(\"--seq-length\", type=int, default=8192, help=\"Training sequence length\")\n    parser.add_argument(\n        \"--tensor-parallel-size\", type=int, default=1, help=\"Order of tensor parallelism. Defaults to 1.\"\n    )\n    parser.add_argument(\n        \"--pipeline-model-parallel-size\", type=int, default=1, help=\"Order of pipeline parallelism. Defaults to 1.\"\n    )\n    parser.add_argument(\n        \"--context-parallel-size\", type=int, default=1, help=\"Order of context parallelism. Defaults to 1.\"\n    )\n    parser.add_argument(\"--no-wandb\", action=\"store_true\", default=False, help=\"Disable Wandb logging\")\n    parser.add_argument(\"--wandb-project\", type=str, default=\"bionemo_evo2\", help=\"Wandb project name\")\n    parser.add_argument(\"--wandb-run-id\", type=str, default=None, help=\"Wandb run identifier\")\n    parser.add_argument(\n        \"--wandb-group\", type=str, default=None, help=\"A unique string shared by all runs in a given group\"\n    )\n    parser.add_argument(\n        \"--wandb-job-type\",\n        type=str,\n        default=None,\n        help=\"A unique string representing a type of run, which is useful when you're grouping runs together into larger experiments using group.\",\n    )\n    parser.add_argument(\"--wandb-offline\", action=\"store_true\", help=\"Use wandb in offline mode\")\n    parser.add_argument(\n        \"--wandb-anonymous\", action=\"store_true\", help=\"Enable or explicitly disable anonymous logging\"\n    )\n    parser.add_argument(\"--sequence-parallel\", action=\"store_true\", help=\"Set to enable sequence parallelism.\")\n    parser.add_argument(\"--fp8\", action=\"store_true\", help=\"Set to enable FP8\")\n    parser.add_argument(\"--micro-batch-size\", type=int, default=1, help=\"Micro-batch size for data-parallel training.\")\n    parser.add_argument(\n        \"--global-batch-size\",\n        type=int,\n        default=None,\n        help=\"Global batch size for training. If set to None, infer it from the TP, CP, and PP parameters.\",\n    )\n    parser.add_argument(\n        \"--grad-acc-batches\", type=int, default=1, help=\"Number of batches to accumulate gradients over.\"\n    )\n    parser.add_argument(\"--max-steps\", type=int, help=\"Number of training optimizer update steps.\")\n    parser.add_argument(\n        \"--val-check-interval\", type=int, help=\"Number of steps between validation measurements and model checkpoints.\"\n    )\n    parser.add_argument(\"--grad-reduce-in-fp32\", action=\"store_true\", default=False, help=\"Gradient reduce in FP32.\")\n    parser.add_argument(\n        \"--fp8-wgrad\",\n        action=\"store_true\",\n        default=False,\n        help=\"Faster option that is maybe less accurate (TBD) when using fp8.\",\n    )\n    parser.add_argument(\"--use-megatron-comm-overlap-llama3-8k\", action=\"store_true\", default=False)\n    parser.add_argument(\n        \"--tp-comm-overlap-backend\",\n        type=str,\n        choices=[\"nccl\", \"mpi\", \"gloo\"],\n        default=\"nccl\",\n        help=\"TP communication backend to use. Defaults to 'nccl'.\",\n    )\n    parser.add_argument(\"--align-param-gather\", action=\"store_true\", default=False)\n    # parser.add_argument(\"--straggler-detection\", action=\"store_true\", default=False)\n    parser.add_argument(\n        \"--model-size\",\n        type=str,\n        choices=sorted(HYENA_MODEL_OPTIONS.keys()),\n        default=\"7b\",\n        help=\"Model architecture to use, choose between 7b, 40b, or test (a sub-model of 4 layers, less than 1B \"\n        \"parameters). '_arc_1m' models have GLU / FFN dimensions that support 1M context length when trained \"\n        \"with TP&lt;=8.\",\n    )\n    parser.add_argument(\n        \"--add-bias-output\",\n        action=\"store_true\",\n        default=False,\n        help=\"Add bias to the output layer to enable learning a simple prior.\",\n    )\n    parser.add_argument(\n        \"--experiment-dir\",\n        type=str,\n        required=True,\n        help=\"Directory to write model checkpoints and results to.\",\n    )\n    parser.add_argument(\n        \"--limit-val-batches\",\n        type=int,\n        default=20,\n        help=\"Number of validation steps\",\n    )\n    parser.add_argument(\n        \"--log-every-n-steps\",\n        type=int,\n        default=1,\n        required=False,\n        help=\"Number of steps between logging.\",\n    )\n    parser.add_argument(\n        \"--ckpt-dir\",\n        type=str,\n        default=None,\n        help=\"Directory to restore an initial checkpoint from. Use this for supervised fine-tuning.\",\n    )\n    parser.add_argument(\"--wd\", type=float, default=0.01, help=\"Weight decay for optimizer.\")\n    parser.add_argument(\n        \"--restore-optimizer-from-ckpt\",\n        action=\"store_true\",\n        help=\"Restore optimizer state from initial checkpoint. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--no-average-in-collective\",\n        action=\"store_true\",\n        default=False,\n        help=\"Avaerage optimizer state in collective rather than dividing by dp size and summing.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Set random seed for training.\")\n    parser.add_argument(\"--workers\", type=int, default=8, help=\"Number of workers to use for data loading.\")\n    parser.add_argument(\n        \"--gc-interval\",\n        type=int,\n        default=0,\n        help=\"Set to a value &gt; 0 if you want to synchronize garbage collection, will do gc every gc-interval steps.\",\n    )\n    parser.add_argument(\n        \"--enable-preemption\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable preemption hooks. If enabled this will save a checkpoint whenver slurm exits.\",\n    )\n    parser.add_argument(\n        \"--ckpt-async-save\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--ckpt-format\",\n        type=str,\n        choices=[\"torch_dist\", \"zarr\"],\n        default=\"torch_dist\",\n        help=\"Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated. Only use if \"\n        \"resuming training from a zarr checkpoint.\",\n    )\n    parser.add_argument(\n        \"--eod-pad-in-loss-mask\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not predict EOD/Pad tokens (typical default, but not default in original evo2).\",\n    )\n    parser.add_argument(\n        \"--cross-entropy-loss-fusion\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use the faster, but maybe less accurate fused form of cross entropy, \"\n        \"which also has bf16 grads internally.\",\n    )\n    parser.add_argument(\n        \"--no-fp32-residual-connection\",\n        action=\"store_true\",\n        default=False,\n        help=\"If set, turn off fp32 residual connections which may be faster but may impact accuracy.\",\n    )\n    parser.add_argument(\n        \"--debug-ddp-parity-freq\",\n        type=int,\n        default=0,\n        help=\"Set to value &gt; 0 to debug DDP weight parity between ranks.\",\n    )\n    parser.add_argument(\n        \"--hybrid-override-pattern\",\n        type=str,\n        help=\"Override the hybrid override pattern in the config (specifies hyena layer ordering and type).\",\n    )\n    parser.add_argument(\n        \"--num-layers\", type=int, help=\"If set, override the number of layers specified in the requested config.\"\n    )\n    parser.add_argument(\n        \"--tflops-callback\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable tflops calculation callback for Hyena / Evo2. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--log-parameters-and-shapes\",\n        action=\"store_true\",\n        default=False,\n        help=\"Log training parameters shapes and dtypes for debugging.\",\n    )\n    parser.add_argument(\"--lr\", type=float, default=3e-4, help=\"Learning rate.\")\n    parser.add_argument(\"--min-lr\", type=float, default=3e-5, help=\"Min learning rate in cosine annealing.\")\n    parser.add_argument(\"--warmup-steps\", type=int, default=2500, help=\"Number of warmup steps in cosine annealing\")\n    # NSYS profiling/tooling arguments\n    parser.add_argument(\n        \"--nsys-profiling\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling\"\n        \" output you must run the whole program with `nsys`. For example: \"\n        \" `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true \"\n        \"--capture-range=cudaProfilerApi --capture-range-end=stop  [regular python command here]`\",\n    )\n    # start, end, rank\n    parser.add_argument(\n        \"--nsys-start-step\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Start nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--nsys-end-step\",\n        type=int,\n        required=False,\n        help=\"End nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--no-renormalize-loss\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not renormalize the loss weights.\",\n    )\n    # rank as list of integers\n    parser.add_argument(\n        \"--nsys-ranks\",\n        type=int,\n        nargs=\"+\",\n        required=False,\n        default=[0],\n        help=\"Enable nsys profiling for these ranks.\",\n    )\n    parser.add_argument(\n        \"--activation-checkpoint-recompute-num-layers\",\n        type=int,\n        help=\"If set, override the default value set in the config.\",\n    )\n    parser.add_argument(\n        \"--disable-checkpointing\",\n        action=\"store_false\",\n        default=True,\n        dest=\"create_checkpoint_callback\",\n        help=\"Disable creating a ModelCheckpoint callback.\",\n    )\n    parser.add_argument(\n        \"--clip-grad\",\n        type=float,\n        default=1.0,\n        help=\"Grad clip value. Note that when using DDP this may need to be inflated.\",\n    )\n    parser.add_argument(\n        \"--seq-len-interpolation-factor\",\n        type=float,\n        help=\"Adjusts the linear scaling of ROPE (Rotary Position Embedding) for context extension. \"\n        \"Set this factor relative to your base context length e.g., for an original context length of 8192 and \"\n        \"an extended context length of 524288, use 524288/8192 = 64.\",\n    )\n    parser.add_argument(\n        \"--overlap-param-gather\",\n        action=\"store_true\",\n        default=False,\n        help=\"Overlap the parameter gather with the optimizer step. This is currently disabled due to a NeMo bug \"\n        \"when using DDP. Making this an option defaulting to False is a temporary solution until the bug is fixed.\",\n    )\n    parser.add_argument(\n        \"--overlap-grad-reduce\",\n        action=\"store_true\",\n        default=False,\n        help=\"Overlap the gradient reduce with the optimizer step.\",\n    )\n    parser.add_argument(\n        \"--hidden-dropout\",\n        type=float,\n        default=0.0,\n        help=\"Dropout probability for the hyena layers\",\n    )\n    parser.add_argument(\n        \"--attention-dropout\",\n        type=float,\n        default=0.0,\n        help=\"Dropout probability for the attention layers.\",\n    )\n    recompute_group = parser.add_mutually_exclusive_group(required=False)\n    recompute_group.add_argument(\"--no-activation-checkpointing\", action=\"store_true\", default=False)\n    recompute_group.add_argument(\"--selective-activation-checkpointing\", action=\"store_true\", default=False)\n    return parser.parse_args(args=args)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/run/train/#bionemo.evo2.run.train.train","title":"<code>train(args)</code>","text":"<p>Main function to run Evo2 training.</p> Source code in <code>bionemo/evo2/run/train.py</code> <pre><code>def train(args: argparse.Namespace):\n    \"\"\"Main function to run Evo2 training.\"\"\"\n    # Instantiate tokenizer.\n    tokenizer = get_nmt_tokenizer(\n        \"byte-level\",\n    )\n\n    # Infer global batch size.\n    global_batch_size = args.global_batch_size\n    if global_batch_size is None:\n        global_batch_size = infer_global_batch_size(\n            micro_batch_size=args.micro_batch_size,\n            num_nodes=args.num_nodes,\n            devices=args.devices,\n            accumulate_grad_batches=args.grad_acc_batches,\n            tensor_model_parallel_size=args.tensor_parallel_size,\n            pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n            context_model_parallel_size=args.context_parallel_size,\n        )\n    if args.mock_data:\n        data = MockDataModule(\n            seq_length=args.seq_length,\n            micro_batch_size=args.micro_batch_size,\n            global_batch_size=global_batch_size,\n            num_workers=args.workers,\n            tokenizer=tokenizer,\n        )\n    else:\n        blended_dataset_config = parse_dataset_config(\n            dataset_config_path=args.dataset_config, dataset_path=args.dataset_dir\n        )\n        dataset_cls = Evo2DatasetPadEodLossMask if args.eod_pad_in_loss_mask else Evo2Dataset\n        # Instantiate pre-training module.\n        data = PreTrainingDataModule(\n            paths=blended_dataset_config,\n            dataset_cls=dataset_cls,\n            seq_length=args.seq_length,\n            micro_batch_size=args.micro_batch_size,\n            global_batch_size=global_batch_size,\n            seed=args.seed,\n            num_workers=args.workers,\n            tokenizer=tokenizer,\n            eod_mask_loss=args.eod_pad_in_loss_mask,\n        )\n\n    if args.no_activation_checkpointing:\n        activation_checkpointing_args = {\n            \"recompute_granularity\": None,\n            \"recompute_method\": None,\n            \"recompute_num_layers\": None,\n        }\n    elif args.selective_activation_checkpointing:\n        activation_checkpointing_args = {\n            \"recompute_granularity\": \"selective\",\n            \"recompute_method\": None,\n            \"recompute_num_layers\": None,\n        }\n    else:\n        if args.activation_checkpoint_recompute_num_layers is not None:\n            activation_checkpointing_args = {\n                \"recompute_num_layers\": args.activation_checkpoint_recompute_num_layers,\n            }\n        else:\n            activation_checkpointing_args = {}\n\n    # Retrieve model config.\n    config_modifiers_init = {\n        \"tp_comm_overlap\": args.use_megatron_comm_overlap_llama3_8k,\n        \"seq_length\": args.seq_length,\n        \"hidden_dropout\": args.hidden_dropout,\n        \"attention_dropout\": args.attention_dropout,\n        \"to_upper\": \"weighted\" if args.no_renormalize_loss else \"normalized_weighted\",\n        \"distribute_saved_activations\": False if args.sequence_parallel else True,\n        \"cross_entropy_loss_fusion\": args.cross_entropy_loss_fusion,\n        \"fp32_residual_connection\": not args.no_fp32_residual_connection,\n        \"add_bias_output\": args.add_bias_output,\n        **activation_checkpointing_args,\n    }\n    if args.hybrid_override_pattern:\n        config_modifiers_init[\"hybrid_override_pattern\"] = args.hybrid_override_pattern\n    if args.num_layers:\n        config_modifiers_init[\"num_layers\"] = args.num_layers\n\n    if args.model_size not in HYENA_MODEL_OPTIONS:\n        raise ValueError(f\"Invalid model size: {args.model_size}\")\n    evo2_config = HYENA_MODEL_OPTIONS[args.model_size](**config_modifiers_init)\n\n    # Instantiate model.\n    model = llm.HyenaModel(evo2_config, tokenizer=data.tokenizer)\n\n    # Setup callbacks.\n    callbacks = [\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n        TimingCallback(),\n    ]\n    if args.create_checkpoint_callback:\n        checkpoint_callback = ModelCheckpoint(\n            every_n_train_steps=args.val_check_interval,\n            dirpath=args.experiment_dir,\n            save_top_k=5,\n            always_save_context=True,\n            save_optim_on_train_end=True,\n            save_context_on_train_end=True,\n        )\n        callbacks.append(checkpoint_callback)\n\n    if args.enable_preemption:\n        callbacks.append(nl_callbacks.PreemptionCallback())\n    if args.debug_ddp_parity_freq &gt; 0:\n        callbacks.append(nl_callbacks.DdpParityChecker(interval=args.debug_ddp_parity_freq))\n    if args.log_parameters_and_shapes:\n        callbacks.append(nl_callbacks.ParameterDebugger())\n    if args.tflops_callback:\n        # Add callback that logs the tera-FLOPS per second per GPU during training.\n        flop_meas_callback = FLOPsMeasurementCallback(\n            evo2_config,\n            data,\n            \"hyena\",\n        )\n        callbacks.append(flop_meas_callback)\n\n    # TODO(@cye): Add this back when it works with 24.12.\n    # if args.straggler_detection:\n    #     callbacks.append(\n    #         res_module.StragglerDetectionCallback(\n    #             report_time_interval=300,\n    #             calc_relative_gpu_perf=True,\n    #             calc_individual_gpu_perf=True,\n    #             num_gpu_perf_scores_to_print=5,\n    #             gpu_relative_perf_threshold=0.7,\n    #             gpu_individual_perf_threshold=0.7,\n    #             stop_if_detected=True,\n    #             enable_ptl_logging=True,\n    #         )\n    #     )\n    if args.use_megatron_comm_overlap_llama3_8k:\n        # Pick the floating point appropriate config.\n        if args.fp8:\n            tp_comm_overlap_cfg = userbuffers_fp8_h100_h8192_tp4_mbs1_seqlen8192\n        else:\n            tp_comm_overlap_cfg = userbuffers_bf16_h100_h8192_tp4_mbs1_seqlen8192\n        callbacks.append(\n            MegatronCommOverlapCallback(\n                tp_comm_overlap=evo2_config.tp_comm_overlap,\n                tp_comm_overlap_cfg=tp_comm_overlap_cfg,\n                tp_comm_bootstrap_backend=args.tp_comm_overlap_backend,\n                wgrad_deferral_limit=22,  # default from NeMo\n                overlap_param_gather_with_optimizer_step=False,  # Currently disabled due to an issue with checkpointing.\n                align_param_gather=args.align_param_gather,\n            )\n        )\n\n    if args.gc_interval &gt; 0:\n        callbacks.append(\n            nl_callbacks.GarbageCollectionCallback(\n                gc_interval_train=args.gc_interval, gc_interval_val=args.gc_interval\n            )\n        )\n    if args.nsys_profiling:\n        if args.nsys_end_step is None:\n            nsys_end_step = args.max_steps\n        else:\n            nsys_end_step = args.nsys_end_step\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=args.nsys_start_step, end_step=nsys_end_step, ranks=args.nsys_ranks, gen_shape=True\n            )\n        )\n\n    loggers = []\n    nemo_logger_kwargs = {}\n    if (not args.no_wandb) and args.wandb_project:\n        wandb_logger = WandbLogger(\n            name=(\n                f\"evo2-size-{args.model_size}-TP{args.tensor_parallel_size}-\"\n                f\"PP{args.pipeline_model_parallel_size}-CP{args.context_parallel_size}\"\n                f\"-GBS{global_batch_size}-MBS{args.micro_batch_size}-SkipLossRenorm{args.no_renormalize_loss}\"\n                f\"-NOAC{args.no_activation_checkpointing}-SELAC{args.selective_activation_checkpointing}\"\n                f\"-ACRNL{evo2_config.recompute_num_layers}\"\n                f\"-PAT{evo2_config.hybrid_override_pattern}\"\n                f\"-F32R{evo2_config.fp32_residual_connection}\"\n                f\"-FCE{evo2_config.cross_entropy_loss_fusion}\"\n                f\"-AIC{not args.no_average_in_collective}\"\n                f\"-PEOD{args.eod_pad_in_loss_mask}\"\n                f\"-BO{args.add_bias_output}\"\n                f\"-GCLP{args.clip_grad}\"\n                f\"-HDO{args.hidden_dropout}\"\n                f\"-ADO{args.attention_dropout}\"\n                f\"-LR{args.lr}-MINLR{args.min_lr}-WUSTEPS{args.warmup_steps}-WD{args.wd}\"\n                f\"-GRFP32{args.grad_reduce_in_fp32}-FP8WG{args.fp8_wgrad and args.fp8}\"\n                f\"-OGR{args.overlap_grad_reduce}-OPG{args.overlap_param_gather}\"\n                f\"-NODES{args.num_nodes}-FP8{args.fp8}\"\n            ),\n            group=args.wandb_group,\n            job_type=args.wandb_job_type,\n            id=args.wandb_run_id,\n            project=args.wandb_project,\n            save_dir=args.experiment_dir,\n            offline=args.wandb_offline,\n            anonymous=args.wandb_anonymous,\n        )\n        loggers.append(wandb_logger)\n        nemo_logger_kwargs[\"wandb\"] = wandb_logger\n    tb_logger = TensorBoardLogger(\n        save_dir=\"dummy\",  ## NOTE: this gets overwritten by default\n    )\n    nemo_logger_kwargs[\"tensorboard\"] = tb_logger\n    loggers.append(tb_logger)\n\n    nemo_logger = NeMoLogger(log_dir=args.experiment_dir, **nemo_logger_kwargs)\n    ddp: DistributedDataParallelConfig = DistributedDataParallelConfig(\n        check_for_nan_in_grad=True,\n        overlap_grad_reduce=args.overlap_grad_reduce,\n        overlap_param_gather=args.overlap_param_gather,  # Verify that this works using\n        grad_reduce_in_fp32=args.grad_reduce_in_fp32,\n        align_param_gather=args.align_param_gather,\n        average_in_collective=not args.no_average_in_collective,\n    )\n    # Initialize Megatron Strategy and Trainer.\n    strategy = nl.MegatronStrategy(\n        ddp=ddp,\n        tensor_model_parallel_size=args.tensor_parallel_size,\n        pipeline_model_parallel_size=args.pipeline_model_parallel_size,\n        context_parallel_size=args.context_parallel_size,\n        pipeline_dtype=torch.bfloat16,\n        sequence_parallel=args.sequence_parallel,\n        ckpt_load_optimizer=True,\n        ckpt_save_optimizer=True,\n        ckpt_async_save=args.ckpt_async_save,\n        save_ckpt_format=args.ckpt_format,\n        ckpt_load_strictness=\"log_all\",  # or rebasing to https://github.com/NVIDIA/NeMo/pull/11988/files#diff-7667eae242a8ef776bff78cd08e79bc81df4896a450f0a781f6ed317a3dfb7ffR139\n    )\n    trainer = nl.Trainer(\n        devices=args.devices,\n        num_nodes=args.num_nodes,\n        max_steps=args.max_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        logger=loggers,\n        callbacks=callbacks,\n        log_every_n_steps=args.log_every_n_steps,\n        limit_val_batches=args.limit_val_batches,\n        num_sanity_val_steps=0,\n        use_distributed_sampler=False,\n        plugins=nl.MegatronMixedPrecision(\n            precision=\"bf16-mixed\",\n            params_dtype=torch.bfloat16,\n            grad_reduce_in_fp32=args.grad_reduce_in_fp32,\n            fp8=\"hybrid\" if args.fp8 else None,\n            fp8_amax_history_len=16 if args.fp8 else 1,\n            fp8_amax_compute_algo=\"max\" if args.fp8 else \"most_recent\",\n            fp8_wgrad=args.fp8\n            and (\n                args.fp8_wgrad or args.use_megatron_comm_overlap_llama3_8k\n            ),  # faster and less accurate when set to True, and MUST be True if using TP communication overlap\n        ),\n        val_check_interval=args.val_check_interval,\n        enable_checkpointing=args.create_checkpoint_callback,\n    )\n\n    # Logger setup\n    nemo_logger.setup(\n        trainer,\n        resume_if_exists=True,\n    )\n\n    resume = nl.AutoResume(\n        resume_if_exists=True,\n        resume_ignore_no_checkpoint=True,\n        resume_past_end=False,\n        resume_from_directory=args.experiment_dir,\n        restore_config=(\n            RestoreConfig(\n                path=args.ckpt_dir,\n                load_model_state=True,\n                load_optim_state=args.restore_optimizer_from_ckpt,\n            )\n            if args.ckpt_dir\n            else None\n        ),\n    )\n    resume.setup(trainer, model)\n\n    # Optimizer and scheduler setup\n    opt_config = OptimizerConfig(\n        optimizer=\"adam\",\n        lr=args.lr,\n        adam_beta1=0.9,\n        adam_beta2=0.95,\n        weight_decay=args.wd,\n        clip_grad=args.clip_grad,\n        use_distributed_optimizer=True,\n        bf16=True,\n    )\n    sched = CosineAnnealingScheduler(\n        max_steps=trainer.max_steps,\n        warmup_steps=args.warmup_steps,\n        min_lr=args.min_lr,\n    )\n\n    opt = MegatronOptimizerModule(opt_config, sched, no_weight_decay_cond=evo2_config.hyena_no_weight_decay_cond_fn)\n    opt.connect(model)\n\n    # Start training\n    trainer.fit(model, data)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/config/","title":"Config","text":""},{"location":"API_reference/bionemo/evo2/utils/config/#bionemo.evo2.utils.config.Evo2PreprocessingConfig","title":"<code>Evo2PreprocessingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model class specifying the configuration schema for a preprocessed IndexedDataset (.bin, .idx).</p> Source code in <code>bionemo/evo2/utils/config.py</code> <pre><code>class Evo2PreprocessingConfig(BaseModel):\n    \"\"\"Pydantic model class specifying the configuration schema for a preprocessed IndexedDataset (.bin, .idx).\"\"\"\n\n    # Paths\n    datapaths: list[Path] = []\n    output_dir: None | Path = None\n    output_prefix: None | str = None\n    # Random Datasplit\n    train_split: float = 0.7\n    valid_split: float = 0.2\n    test_split: float = 0.1\n    # Overwrite existing binaries. Otherwise, skip already preprocessed datasets.\n    overwrite: bool = False\n    # Raw Preprocessing Transforms\n    embed_reverse_complement: bool = False\n    random_reverse_complement: float = 0.0\n    random_lineage_dropout: float = 0.0\n    transcribe: None | Literal[\"transcribe\", \"back_transcribe\"] = None\n    force_uppercase: bool = False\n    indexed_dataset_dtype: str = \"uint8\"\n    # Tokenization Transforms\n    append_eod: bool = True\n    enforce_sample_length: None | int = None\n    ftfy: bool = False\n    # NeMo Tokenizer Configuration\n    tokenizer_type: Literal[\n        \"Byte-Level\",\n        \"HuggingFace\",\n        \"SentencePiece\",\n        \"Regex\",\n        \"Megatron\",\n        \"Tiktoken\",\n    ] = \"Byte-Level\"\n    vocab_file: None | Path = None\n    vocab_size: None | int = 512\n    merges_file: None | Path = None\n    tokenizer_model_name: None | str = None\n    pretrained_tokenizer_model: None | str = None\n    special_tokens: None | dict[str, str] = {}\n    fast_hf_tokenizer: bool = False\n    # Compute Configuration\n    # NOTE: If preprocessing a large amount of short individual sequences (&lt; 1000 bp), do NOT use\n    # multiprocessing (workers &gt; 1) because sequence-level parallel IPC will dominate the preprocessing time!\n    workers: int = 1\n    preproc_concurrency: int = 100000\n    chunksize: int = 1\n    # Filters\n    drop_empty_sequences: bool = False\n    nnn_filter: bool = False\n    # RNG\n    seed: None | int = None\n    # Evo2 Taxonomic Lineage Tags\n    # SeqID Sub-String Indexing: \"ABC\" will have taxonomy data from \"A\".\n    taxonomy_data: dict[str, Evo2TaxonomyLineage] = {}\n    # Periodicity of injecting phylogenetic lineage tags in the sequence prior to tokenization.\n    prompt_spacer_length: int = 131072\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/config/#bionemo.evo2.utils.config.Evo2TaxonomyLineage","title":"<code>Evo2TaxonomyLineage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model class that defines the source lineage of a DNA sequence.</p> Source code in <code>bionemo/evo2/utils/config.py</code> <pre><code>class Evo2TaxonomyLineage(BaseModel):\n    \"\"\"Pydantic model class that defines the source lineage of a DNA sequence.\"\"\"\n\n    domain: None | str = None\n    phylum: None | str = None\n    clazz: None | str = None\n    order: None | str = None\n    family: None | str = None\n    genus: None | str = None\n    species: None | str = None\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/","title":"Evo2 Checkpoint Conversion Library","text":"<p>This library contains helper scripts for converting checkpoint formats for Evo2.</p>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/#converting-zero-1-pytorch-checkpoints-to-nemo2-checkpoints","title":"Converting ZeRO-1 / PyTorch Checkpoints to NeMo2 Checkpoints","text":"<p>To convert a single PyTorch or ZeRO-1 checkpoints (<code>.pt</code>) into NeMo2 format, run the following command: <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_to_nemo.py --model-path &lt;CKPT_FILE&gt; --output-dir &lt;OUTPUT_DIR&gt; --model-size &lt;MODEL_SIZE&gt; --ckpt-format &lt;CONVERTED_CKPT_FORMAT&gt;\n</code></pre> where <code>--model-size</code> can be set to <code>7b</code> or <code>40b</code> (or their <code>_arc_1m</code> variants with modified GLU dimensions) and <code>--ckpt-format</code> can be set to <code>torch_dist</code> or <code>zarr</code>.</p> <p>The NeMo2 checkpoint should have the following structure for <code>torch_dist</code>: <pre><code>default--val_loss=2.3738-epoch=0-consumed_samples=800.0-last\n\u251c\u2500\u2500 context\n\u2502   \u251c\u2500\u2500 io.json\n\u2502   \u2514\u2500\u2500 model.yaml\n\u2514\u2500\u2500 weights\n    \u251c\u2500\u2500 __*_*.distcp\n    \u251c\u2500\u2500 common.pt\n    \u2514\u2500\u2500 metadata.json\n</code></pre> and the following structure for <code>zarr</code>: <pre><code>interleaved_hyena_7b_fix_shape\n\u251c\u2500\u2500 context\n\u2502   \u251c\u2500\u2500 io.json\n\u2502   \u2514\u2500\u2500 model.yaml\n\u2514\u2500\u2500 weights\n    \u251c\u2500\u2500 common.pt\n    \u251c\u2500\u2500 metadata.json\n    \u2514\u2500\u2500 &lt;MODEL_LAYER_NAME&gt;  # Example: module.decoder.layers.0.mixer.dense\n        \u2514\u2500\u2500 shard_*_*.pt\n</code></pre></p>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/#converting-zero-1-mpn-to-zero-1-mp1","title":"Converting ZeRO-1 MP{N} to ZeRO-1 MP1","text":"<p>To convert sharded (MP&gt;1) ZeRO-1 checkpoints to un-sharded (MP1) checkpoints (or any order of model parallelism) compatible with the <code>convert_to_nemo.py</code> conversion script, you can run the following command: <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py --source_dir &lt;CKPT_DIR&gt; --output_dir &lt;OUTPUT_DIR&gt; --mp_size &lt;TARGET_MODEL_PARALLEL_SIZE&gt;\n</code></pre></p> <p>ZeRO-1 checkpoints should have the following structure: <pre><code>arc_7b_tp8_pretrained_ckpt/global_step199400\n\u2514\u2500\u2500 mp_rank_*_model_states.pt\n</code></pre></p>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/#converting-zero-3-to-zero-1","title":"Converting ZeRO-3 to ZeRO-1","text":"<p>To convert ZeRO-3 checkpoints into ZeRO-1 checkpoints, run the following command: <pre><code>python sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1.py &lt;INPUT_DIR&gt; &lt;OUTPUT_DIR&gt; --overwrite --mp_size &lt;MODEL_PARALLEL_SIZE&gt;\n</code></pre></p> <p>ZeRO-3 checkpoints should have the following structure: <pre><code>arc_40b_zero3_w32_mp8_test_notfinal_ckpt/global_step1\n\u251c\u2500\u2500 bf16_zero_pp_rank_*_mp_rank_*_optim_states.pt\n\u251c\u2500\u2500 configs\n\u2502   \u251c\u2500\u2500 40b_test_chkpt.yml\n\u2502   \u2514\u2500\u2500 opengenome.yml\n\u2514\u2500\u2500 zero_pp_rank_*_mp_rank_*_model_states.pt\n</code></pre></p>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/","title":"Convert checkpoint model parallel evo2","text":"<p>This script converts (potentially sharded) ZeRo1 checkpoint parameters to the desired level of model tensor parallelism for the Evo 2 architecture.</p> <p>It only supports Zero-1 checkpoints and does not convert any optimizer state, only the parameters.</p> Usage <p>python convert_checkpoint_model_parallel_evo2.py         --input-checkpoint-dir /path/to/input/checkpoint/global_step1000         --output-checkpoint-dir /path/to/output/checkpoint_mp2/global_step1000         --output-model-parallelism 2</p>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.check_params","title":"<code>check_params(detected, expected, buffers, param_pattern=DEFAULT_PARAM_PATTERN, verbose=False)</code>","text":"<p>Check that all model parameters are expected.</p> <p>Parameters:</p> Name Type Description Default <code>detected</code> <code>List[str]</code> <p>Detected model parameters names.</p> required <code>expected</code> <code>Set[str]</code> <p>Expected model parameters names.</p> required <code>buffers</code> <code>Set[str]</code> <p>Set of buffer names.</p> required <code>param_pattern</code> <code>str</code> <p>Regex pattern to match parameter names. Defaults to DEFAULT_PARAM_PATTERN.</p> <code>DEFAULT_PARAM_PATTERN</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def check_params(\n    detected: List[str],\n    expected: Union[Set[str], List[str]],\n    buffers: Set[str],\n    param_pattern: str = DEFAULT_PARAM_PATTERN,\n    verbose: bool = False,\n):\n    \"\"\"Check that all model parameters are expected.\n\n    Args:\n        detected (List[str]): Detected model parameters names.\n        expected (Set[str]): Expected model parameters names.\n        buffers (Set[str]): Set of buffer names.\n        param_pattern (str, optional): Regex pattern to match parameter names. Defaults to DEFAULT_PARAM_PATTERN.\n        verbose (bool, optional): Whether to print detailed information. Defaults to False.\n    \"\"\"\n    # Expected model parameters.\n    expected = set(expected) if not isinstance(expected, set) else expected\n    # Detected model parameters.\n    model_param_names = []\n    for k in detected:\n        match = re.search(param_pattern, k)\n        if match is not None:\n            model_param_names.append(match.group(1))\n        else:\n            logging.info(f\"Could not match {k}\")\n    detected_param_set = set(model_param_names)\n    if verbose:\n        logging.info(\"Detected Params:\\n  {detected_params}\".format(detected_params=\"\\n  \".join(detected_param_set)))\n\n    # Log unexpected model parameters.\n    missing_params = expected - detected_param_set\n    extra_params = detected_param_set - expected\n    extra_params = [param for param in extra_params if param not in buffers]\n    extra_params = [param for param in extra_params if not param.endswith(\"._extra_state\")]\n    if len(extra_params) &gt; 0:\n        logging.info(f\"WARNING: detected extra params: {extra_params}\")\n    if len(missing_params) &gt; 0:\n        logging.info(f\"WARNING: missing params: {missing_params}\")\n    if not (extra_params or missing_params):\n        logging.info(\"No missing or extra params detected!\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.concatenate_tensors_across_shards","title":"<code>concatenate_tensors_across_shards(tensor_name, data_shards, partition_dim, hidden_dim=None, verbose=False)</code>","text":"<p>Concatenate tensor shards across multiple shards.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_name</code> <code>str</code> <p>Name of the tensor to concatenate.</p> required <code>data_shards</code> <code>List[OrderedDict[str, Tensor]]</code> <p>List of data shards containing tensors.</p> required <code>partition_dim</code> <code>int</code> <p>Dimension along which to partition the tensor.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension of the tensor. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Concatenated tensor.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def concatenate_tensors_across_shards(\n    tensor_name: str,\n    data_shards: List[OrderedDict[str, torch.Tensor]],\n    partition_dim: int,\n    hidden_dim: Optional[int] = None,\n    verbose: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Concatenate tensor shards across multiple shards.\n\n    Args:\n        tensor_name (str): Name of the tensor to concatenate.\n        data_shards (List[OrderedDict[str, torch.Tensor]]): List of data shards containing tensors.\n        partition_dim (int): Dimension along which to partition the tensor.\n        hidden_dim (int, optional): Hidden dimension of the tensor. Defaults to None.\n        verbose (bool, optional): Whether to print detailed information. Defaults to False.\n\n    Returns:\n        torch.Tensor: Concatenated tensor.\n    \"\"\"\n    # Retrieve tensor shards.\n    tensors = [shard[\"module\"][tensor_name] for shard in data_shards]\n\n    # Check shape of tensors without tensor parallelism, i.e. stored in all shards of the checkpoint.\n    if partition_dim is None:\n        for i, tensor in enumerate(tensors):\n            if not torch.allclose(tensors[0], tensor):\n                logging.info(\n                    f\"WARNING: Synchronized params differ for param {tensor_name}: abs max diff = {(tensors[0] - tensor).abs().max()}.\"\n                )\n                # Get the distribution of tensors[0] and tensor.\n                if verbose:\n                    ref_tensor = tensors[0].flatten().to(torch.float32)\n                    ref_min, ref_max = ref_tensor.min(), ref_tensor.max()\n\n                    q = torch.tensor([0.25, 0.5, 0.75], device=ref_tensor.device)\n                    ref_quantiles = ref_tensor.quantile(q)\n                    logging.info(f\"rank0 tensor: min={ref_min}, max={ref_max} quantiles={ref_quantiles}\")\n\n                    target_tensor = tensor.flatten().to(torch.float32)\n                    target_min, target_max = target_tensor.min(), target_tensor.max()\n                    target_quantiles = target_tensor.quantile(q)\n                    logging.info(f\"rank{i} tensor: min={target_min}, max={target_max} quantiles={target_quantiles}\")\n\n                    logging.info(f\"rank0 tensor distribution:\\n {ref_tensor.histc(100, min=ref_min, max=ref_max)}\")\n                    logging.info(f\"rank{i} distribution:\\n {target_tensor.histc(100, min=ref_min, max=ref_max)}\")\n\n        logging.info(f\"tensor {tensor_name} not partitioned, returning rank0 tensor {tensors[0].shape}\")\n        return tensors[0]\n    # Check for sharding across the hidden dimension.\n    elif partition_dim == hidden_dim:\n        raise ValueError(f\"Detected sharding for {tensor_name} across hidden dimension at index {hidden_dim}.\")\n\n    # Check that the tensors have a consistent hidden dimension.\n    expected_dim = None\n    if hidden_dim is not None:\n        for tensor in tensors:\n            if expected_dim is None:\n                # Store expected hidden dimension for all tensors.\n                expected_dim = tensor.shape[hidden_dim]\n            if not tensor.shape[hidden_dim] == expected_dim:\n                raise ValueError(f\"Tensor {tensor_name} has invalid hidden shape {tensor.shape}.\")\n\n    # Concatenate shards.\n    return torch.cat(tensors, dim=partition_dim)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.convert_model_weights","title":"<code>convert_model_weights(input_data_shards, output_data_shards, model_parameter_names, param_list, verbose=False, exclude_extra=False)</code>","text":"<p>Convert model weights from input model parallelism to output model parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>input_data_shards</code> <code>List[OrderedDict]</code> <p>List of input data shards.</p> required <code>output_data_shards</code> <code>List[OrderedDict]</code> <p>List of output data shards.</p> required <code>model_parameter_names</code> <code>List[str]</code> <p>List of model parameter names.</p> required <code>param_list</code> <code>List[Param]</code> <p>List of parameter information.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> <code>exclude_extra</code> <code>bool</code> <p>Whether to exclude extra states in the conversion. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def convert_model_weights(\n    input_data_shards: List[OrderedDict],\n    output_data_shards: List[OrderedDict],\n    model_parameter_names: List[str],\n    param_list: List[Param],\n    verbose: bool = False,\n    exclude_extra: bool = False,\n):\n    \"\"\"Convert model weights from input model parallelism to output model parallelism.\n\n    Args:\n        input_data_shards (List[OrderedDict]): List of input data shards.\n        output_data_shards (List[OrderedDict]): List of output data shards.\n        model_parameter_names (List[str]): List of model parameter names.\n        param_list (List[Param]): List of parameter information.\n        verbose (bool, optional): Whether to print detailed information. Defaults to False.\n        exclude_extra (bool, optional): Whether to exclude extra states in the conversion. Defaults to False.\n    \"\"\"\n    logging.info(\n        f\"Converting {len(model_parameter_names)} parameters from {len(input_data_shards)} input shards to {len(output_data_shards)} output shards...\"\n    )\n    converted = 0\n    skipped = 0\n    for model_parameter in model_parameter_names:\n        if args.verbose:\n            logging.info(f\"Processing {model_parameter}...\")\n\n        # Ignore FP8 extra state.\n        if model_parameter.endswith(\"._extra_state\"):\n            if \"extra_state\" in model_parameter:\n                logging.info(f\"Ignoring {model_parameter} -&gt; contains extra state.\")\n            skipped += 1\n            continue\n\n        # Get the partition dimension and hidden dimension of each parameter.\n        param_info = None\n        for param in param_list:\n            if \".\".join(model_parameter.split(\".\")[2:]) == param.name:\n                if param_info is None:\n                    param_info = param\n                else:\n                    raise ValueError(\n                        f\"Found more than one matching model parallelism parameter for {model_parameter}: {param_info}, {param}\"\n                    )\n        if param_info is None:\n            raise ValueError(f\"Could not find {model_parameter} among known parameters.\")\n\n        # Concatenate shards.\n        concatenated_tensor = concatenate_tensors_across_shards(\n            model_parameter, input_data_shards, param_info.partition_dim, param_info.hidden_dim, verbose=verbose\n        )\n        # Split into shards.\n        split_tensor_across_shards(\n            output_data_shards,\n            concatenated_tensor,\n            model_parameter,\n            param_info.partition_dim,\n        )\n        converted += 1\n    logging.info(f\"Converted {converted} of {len(model_parameter_names)} parameters (skipped {skipped} params).\")\n    num_params = len(output_data_shards[0][\"module\"])\n    logging.info(f\"Total Params: {num_params}\")\n    if not all(num_params == len(shard[\"module\"]) for shard in output_data_shards):\n        raise ValueError(\"Shards have different number of parameters, which is not permitted in model parallelism.\")\n\n    if not exclude_extra:\n        logging.info(\"Adding extra states from rank0 input shard...\")\n        rank0_model = input_data_shards[0][\"module\"]\n        for k in rank0_model.keys():\n            for i, output_shard in enumerate(output_data_shards):\n                if k not in output_shard[\"module\"]:\n                    if i == 0:\n                        logging.info(f\"Adding {k} to output shards.\")\n                    output_shard[\"module\"][k] = rank0_model[k]\n        new_params = len(output_data_shards[0][\"module\"]) - num_params\n        logging.info(f\"Added {new_params} extra states, total params: {num_params + new_params}\")\n        if not all(num_params + new_params == len(shard[\"module\"]) for shard in output_data_shards):\n            raise ValueError(\"Shards have different number of parameters after adding extra states.\")\n\n    for shard_idx, output_data_shard in enumerate(output_data_shards):\n        output_path = Path(output_data_shard[\"output_dir\"]) / format_output_filename(shard_idx)\n        torch.save(\n            output_data_shard,\n            output_path,\n        )\n        logging.info(f\"Converted checkpoint saved to: {output_path}\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.convert_zero1_model_parallel_checkpoint","title":"<code>convert_zero1_model_parallel_checkpoint(source_dir, output_dir, glob_pattern='mp_rank_*_model_states.pt', model_parallel=8, param_list=EVO2_PARAMS, exclude_extra_params=False, verbose=False)</code>","text":"<p>Convert sharded ZeRo1 checkpoint to desired model parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>str</code> <p>Path to the input checkpoint directory.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output checkpoint directory.</p> required <code>glob_pattern</code> <code>str</code> <p>Filename pattern to glob for ZeRo1 checkpoint shards. Defaults to \"mp_rank_*_model_states.pt\".</p> <code>'mp_rank_*_model_states.pt'</code> <code>model_parallel</code> <code>int</code> <p>Desired output model parallelism. Defaults to 8.</p> <code>8</code> <code>param_list</code> <code>List[Param]</code> <p>List of parameter information. Defaults to EVO2_PARAMS.</p> <code>EVO2_PARAMS</code> <code>exclude_extra_params</code> <code>bool</code> <p>Whether to exclude extra states in the conversion. Defaults to False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed information. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def convert_zero1_model_parallel_checkpoint(\n    source_dir: str,\n    output_dir: str,\n    glob_pattern: str = \"mp_rank_*_model_states.pt\",\n    model_parallel: int = 8,\n    param_list: List[Param] = EVO2_PARAMS,\n    exclude_extra_params: bool = False,\n    verbose: bool = False,\n):\n    \"\"\"Convert sharded ZeRo1 checkpoint to desired model parallelism.\n\n    Args:\n        source_dir (str): Path to the input checkpoint directory.\n        output_dir (str): Path to the output checkpoint directory.\n        glob_pattern (str): Filename pattern to glob for ZeRo1 checkpoint shards. Defaults to \"mp_rank_*_model_states.pt\".\n        model_parallel (int): Desired output model parallelism. Defaults to 8.\n        param_list (List[Param]): List of parameter information. Defaults to EVO2_PARAMS.\n        exclude_extra_params (bool): Whether to exclude extra states in the conversion. Defaults to False.\n        verbose (bool): Whether to print detailed information. Defaults to False.\n    \"\"\"\n    # Argument validation.\n    if not os.path.exists(source_dir):\n        raise ValueError(f\"Input checkpoint dir ({source_dir}) not found.\")\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Converting checkpoint from {source_dir} to {output_dir}\")\n\n    # Identify all checkpoint model path files.\n    parameter_paths = sorted(glob(f\"{source_dir}/{glob_pattern}\"))\n    if len(parameter_paths) == 0:\n        raise ValueError(f\"No parameter files found in {source_dir}\")\n\n    # Load all shards from the ZeRo1 checkpoint.\n    input_data_shards = [torch.load(path, map_location=DEVICE) for path in parameter_paths]\n    buffers = {buf for x in input_data_shards for buf in x.get(\"buffer_names\", [])}\n\n    # Initialize output MP shards.\n    output_data_shards = [\n        {\n            \"module\": OrderedDict(),\n            \"param_shapes\": OrderedDict(),\n            \"dp_world_size\": input_data_shards[0][\"dp_world_size\"],\n            \"output_dir\": output_dir,\n        }\n        for _ in range(model_parallel)\n    ]\n    model_parameter_names = input_data_shards[0][\"module\"].keys()\n\n    # Check no missing or extra params\n    check_params(\n        detected=list(model_parameter_names),\n        expected={param.name for param in param_list},\n        buffers=buffers,\n        verbose=verbose,\n    )\n    # Convert the checkpoint\n    convert_model_weights(\n        input_data_shards,\n        output_data_shards,\n        model_parameter_names,\n        param_list,\n        verbose=verbose,\n        exclude_extra=exclude_extra_params,\n    )\n    logging.info(\"Done!\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.format_output_filename","title":"<code>format_output_filename(shard)</code>","text":"<p>Format the output filename for a given shard index.</p> <p>Parameters:</p> Name Type Description Default <code>shard</code> <code>int</code> <p>Shard index.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted output filename.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def format_output_filename(shard: int) -&gt; str:\n    \"\"\"Format the output filename for a given shard index.\n\n    Args:\n        shard (int): Shard index.\n\n    Returns:\n        str: Formatted output filename.\n    \"\"\"\n    return f\"mp_rank_{str(shard).zfill(2)}_model_states.pt\"\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.get_args","title":"<code>get_args()</code>","text":"<p>Parse command-line arguments.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def get_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Convert checkpoint parameters to desired model parallelism.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--source_dir\",\n        type=str,\n        required=True,\n        help=\"Path to the input checkpoint directory containing ZeRo1 checkpoint shards, i.e. mp_rank_*_model_states.pt.\",\n    )\n    parser.add_argument(\n        \"--glob-pattern\",\n        type=str,\n        default=\"mp_rank_*_model_states.pt\",\n        required=False,\n        help=\"Filename pattern to glob for ZeRo1 checkpoint shards.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        required=True,\n        help=\"Path to the output checkpoint directory to dump the --mp_size converted model checkpoint (ZeRo1).\",\n    )\n    parser.add_argument(\"--mp_size\", type=int, required=True, help=\"Desired output model parallelism to convert to.\")\n    parser.add_argument(\n        \"--exclude-extra\",\n        action=\"store_true\",\n        help=\"Exclude extra states in the conversion. Default to False, i.e. include extra states.\",\n    )\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Print more information about the conversion.\")\n    args = parser.parse_args()\n    return args\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2/#bionemo.evo2.utils.checkpoint.convert_checkpoint_model_parallel_evo2.split_tensor_across_shards","title":"<code>split_tensor_across_shards(data_shards, tensor, tensor_name, partition_dim)</code>","text":"<p>Split a tensor across multiple shards.</p> <p>Parameters:</p> Name Type Description Default <code>data_shards</code> <code>List[OrderedDict]</code> <p>List of data shards to store the split tensors.</p> required <code>tensor</code> <code>Tensor</code> <p>Tensor to split.</p> required <code>tensor_name</code> <code>str</code> <p>Name of the tensor.</p> required <code>partition_dim</code> <code>int</code> <p>Dimension along which to partition the tensor.</p> required Source code in <code>bionemo/evo2/utils/checkpoint/convert_checkpoint_model_parallel_evo2.py</code> <pre><code>def split_tensor_across_shards(\n    data_shards: List[OrderedDict],\n    tensor: torch.Tensor,\n    tensor_name: str,\n    partition_dim: int,\n) -&gt; None:\n    \"\"\"Split a tensor across multiple shards.\n\n    Args:\n        data_shards (List[OrderedDict]): List of data shards to store the split tensors.\n        tensor (torch.Tensor): Tensor to split.\n        tensor_name (str): Name of the tensor.\n        partition_dim (int): Dimension along which to partition the tensor.\n    \"\"\"\n    if partition_dim is None:\n        # No sharding. Synchronize weights across all shards.\n        for data_shard in data_shards:\n            data_shard[\"module\"][tensor_name] = tensor\n            data_shard[\"param_shapes\"][tensor_name] = tensor.shape\n    else:\n        # Split the tensor along the partition dimension across shards.\n        n_shards = len(data_shards)\n        if tensor.shape[partition_dim] % n_shards != 0:\n            raise ValueError(\n                f\"Cannot shard {tensor_name} of dimension {tensor.shape[partition_dim]} across {n_shards} evenly.\"\n            )\n        for chunk, data_shard in zip(\n            torch.chunk(tensor, chunks=n_shards, dim=partition_dim),\n            data_shards,\n        ):\n            data_shard[\"module\"][tensor_name] = chunk.clone()\n            data_shard[\"param_shapes\"][tensor_name] = chunk.shape\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_to_nemo/","title":"Convert to nemo","text":""},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_to_nemo/#bionemo.evo2.utils.checkpoint.convert_to_nemo.main","title":"<code>main()</code>","text":"<p>Convert a PyTorch Evo2 model checkpoint to a NeMo model checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_to_nemo.py</code> <pre><code>def main():\n    \"\"\"Convert a PyTorch Evo2 model checkpoint to a NeMo model checkpoint.\"\"\"\n    args = parse_args()\n\n    evo2_config = HYENA_MODEL_OPTIONS[args.model_size]()\n    if args.model_path.startswith(\"hf://\"):\n        importer = HuggingFaceSavannaHyenaImporter(args.model_path.lstrip(\"hf://\"), model_config=evo2_config)\n    else:\n        importer = PyTorchHyenaImporter(args.model_path, model_config=evo2_config)\n    importer.apply(args.output_dir)\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_to_nemo/#bionemo.evo2.utils.checkpoint.convert_to_nemo.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse command-line arguments.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_to_nemo.py</code> <pre><code>def parse_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        required=True,\n        help=\"Path to the Evo2 un-sharded (MP1) model checkpoint file, or a Hugging Face model name. Any model \"\n        \"from the Savanna Evo2 family is supported such as 'hf://arcinstitute/savanna_evo2_1b_base'.\",\n    )\n    parser.add_argument(\"--output-dir\", type=str, required=True, help=\"Output directory path for the converted model.\")\n    parser.add_argument(\n        \"--model-size\",\n        type=str,\n        choices=sorted(HYENA_MODEL_OPTIONS.keys()),\n        default=\"1b\",\n        help=\"Model architecture to use, choose between 1b, 7b, 40b, or test (a sub-model of 4 layers, \"\n        \"less than 1B parameters). '*_arc_longcontext' models have GLU / FFN dimensions that support 1M \"\n        \"context length when trained with TP&gt;&gt;8.\",\n    )\n    return parser.parse_args()\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1/","title":"Convert zero3 to zero1","text":""},{"location":"API_reference/bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1/#bionemo.evo2.utils.checkpoint.convert_zero3_to_zero1.convert_zero_checkpoint_to_fp32_state_dict","title":"<code>convert_zero_checkpoint_to_fp32_state_dict(checkpoint_dir, output_dir, tag=None, exclude_frozen_parameters=False, mp_size=8, overwrite=False, num_workers=1, ranks_to_process=None)</code>","text":"<p>Converts a DeepSpeed Zero-3 checkpoint to a PyTorch FP32 state_dict.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Path to the desired checkpoint folder.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the PyTorch FP32 state_dict output files.</p> required <code>tag</code> <code>Optional[str]</code> <p>Checkpoint tag used as a unique identifier or sub-directory that contains the checkpoint.</p> <code>None</code> <code>exclude_frozen_parameters</code> <code>bool</code> <p>Whether to exclude frozen parameters.</p> <code>False</code> <code>mp_size</code> <code>int</code> <p>Model parallel size of the source checkpoint.</p> <code>8</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing MP shards.</p> <code>False</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for processing.</p> <code>1</code> <code>ranks_to_process</code> <code>Optional[List[int]]</code> <p>List of ranks to process.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the checkpoint directory does not exist.</p> Source code in <code>bionemo/evo2/utils/checkpoint/convert_zero3_to_zero1.py</code> <pre><code>def convert_zero_checkpoint_to_fp32_state_dict(\n    checkpoint_dir: str,\n    output_dir: str,\n    tag: Optional[str] = None,\n    exclude_frozen_parameters: bool = False,\n    mp_size: int = 8,\n    overwrite: bool = False,\n    num_workers: int = 1,\n    ranks_to_process: Optional[List[int]] = None,\n):\n    \"\"\"Converts a DeepSpeed Zero-3 checkpoint to a PyTorch FP32 state_dict.\n\n    Args:\n        checkpoint_dir (str): Path to the desired checkpoint folder.\n        output_dir (str): Directory to save the PyTorch FP32 state_dict output files.\n        tag (Optional[str]): Checkpoint tag used as a unique identifier or sub-directory that contains the checkpoint.\n        exclude_frozen_parameters (bool): Whether to exclude frozen parameters.\n        mp_size (int): Model parallel size of the source checkpoint.\n        overwrite (bool): Whether to overwrite existing MP shards.\n        num_workers (int): Number of workers to use for processing.\n        ranks_to_process (Optional[List[int]]): List of ranks to process.\n\n    Raises:\n        FileNotFoundError: If the checkpoint directory does not exist.\n    \"\"\"\n    ds_checkpoint_dir = os.path.join(checkpoint_dir, tag) if tag is not None else checkpoint_dir\n\n    if not os.path.isdir(ds_checkpoint_dir):\n        raise FileNotFoundError(f\"Directory '{ds_checkpoint_dir}' doesn't exist\")\n\n    output_dir = os.path.join(output_dir, tag) if tag is not None else output_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n\n    num_workers = min(num_workers, mp_size)\n\n    if ranks_to_process is not None:\n        ranks_to_process = list(ranks_to_process)\n        assert len(ranks_to_process) &lt;= mp_size, f\"Expected {mp_size} ranks to process, got {len(ranks_to_process)}\"\n        assert all(\n            0 &lt;= r &lt; mp_size for r in ranks_to_process\n        ), f\"Expected ranks to be in range [0, {mp_size}), got {ranks_to_process}\"\n    else:\n        ranks_to_process = list(range(mp_size))\n\n    print(f\"Processing ranks: {ranks_to_process}\", flush=True)\n\n    start = time.time()\n    if num_workers &gt; 1:\n        with Pool(num_workers) as p:\n            p.starmap(\n                process_single_rank,\n                [(i, ds_checkpoint_dir, output_dir, overwrite, exclude_frozen_parameters) for i in ranks_to_process],\n            )\n    else:\n        for i in ranks_to_process:\n            process_single_rank(i, ds_checkpoint_dir, output_dir, overwrite, exclude_frozen_parameters)\n\n    total_time = get_elapsed(time.time() - start)\n    print(f\"All done!\\n-&gt; Total time: {total_time}\\n-&gt; All outputs written to {os.path.abspath(output_dir)}\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/params/","title":"Params","text":""},{"location":"API_reference/bionemo/evo2/utils/checkpoint/params/#bionemo.evo2.utils.checkpoint.params.Param","title":"<code>Param</code>  <code>dataclass</code>","text":"<p>A dataclass representing a parameter in a checkpoint.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the parameter in the checkpoint.</p> <code>partition_dim</code> <code>int</code> <p>The dimension index that gets sharded. <code>None</code> for no sharding.</p> <code>hidden_dim</code> <code>int</code> <p>The hidden dimension index. <code>None</code> for no hidden dimension.</p> Source code in <code>bionemo/evo2/utils/checkpoint/params.py</code> <pre><code>@dataclass\nclass Param:\n    \"\"\"A dataclass representing a parameter in a checkpoint.\n\n    Attributes:\n        name (str): The name of the parameter in the checkpoint.\n        partition_dim (int): The dimension index that gets sharded. `None` for no sharding.\n        hidden_dim (int): The hidden dimension index. `None` for no hidden dimension.\n    \"\"\"\n\n    name: str  # Name of the parameter in the checkpoint.\n    partition_dim: int  # The dimension index that gets sharded. `None` for no sharding.\n    hidden_dim: int  # The hidden dimension index. `None` for no hidden dimension.\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/","title":"Zero3 conversion lib","text":"<p>Helper utility for converting ZeRO3 and ZeRO2 checkpoints to PyTorch.</p>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.ZeroModelState","title":"<code>ZeroModelState</code>  <code>dataclass</code>","text":"<p>A dataclass representing the state of a ZeRO model.</p> <p>Attributes:</p> Name Type Description <code>buffers</code> <code>Dict</code> <p>Buffers in the model state.</p> <code>extra_states</code> <code>Dict</code> <p>Extra states in the model state.</p> <code>param_shapes</code> <code>List</code> <p>Shapes of the parameters.</p> <code>shared_params</code> <code>List</code> <p>Shared parameters in the model state.</p> <code>ds_version</code> <code>int</code> <p>Version of the DeepSpeed checkpoint.</p> <code>frozen_param_shapes</code> <code>Dict</code> <p>Shapes of the frozen parameters.</p> <code>frozen_param_fragments</code> <code>Dict</code> <p>Fragments of the frozen parameters.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>@dataclass\nclass ZeroModelState:\n    \"\"\"A dataclass representing the state of a ZeRO model.\n\n    Attributes:\n        buffers (Dict): Buffers in the model state.\n        extra_states (Dict): Extra states in the model state.\n        param_shapes (List): Shapes of the parameters.\n        shared_params (List): Shared parameters in the model state.\n        ds_version (int): Version of the DeepSpeed checkpoint.\n        frozen_param_shapes (Dict): Shapes of the frozen parameters.\n        frozen_param_fragments (Dict): Fragments of the frozen parameters.\n    \"\"\"\n\n    buffers: Dict\n    extra_states: Dict\n    param_shapes: List\n    shared_params: List\n    ds_version: int\n    frozen_param_shapes: Dict\n    frozen_param_fragments: Dict\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib._get_fp32_state_dict_from_zero3_checkpoint","title":"<code>_get_fp32_state_dict_from_zero3_checkpoint(world_size, fp32_flat_groups, zero_model_states, exclude_frozen_parameters)</code>","text":"<p>Returns the fp32 state dictionary reconstructed from a ZeRO3 checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <code>int</code> <p>The world size.</p> required <code>fp32_flat_groups</code> <code>List[Tensor]</code> <p>The list of fp32 flat groups.</p> required <code>zero_model_states</code> <code>List[ZeroModelState]</code> <p>The list of ZeroModelState objects.</p> required <code>exclude_frozen_parameters</code> <code>bool</code> <p>Whether to exclude frozen parameters.</p> required <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>The reconstructed fp32 state dictionary.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def _get_fp32_state_dict_from_zero3_checkpoint(\n    world_size: int,\n    fp32_flat_groups: List[torch.Tensor],\n    zero_model_states: List[ZeroModelState],\n    exclude_frozen_parameters: bool,\n):\n    \"\"\"Returns the fp32 state dictionary reconstructed from a ZeRO3 checkpoint.\n\n    Args:\n        world_size (int): The world size.\n        fp32_flat_groups (List[torch.Tensor]): The list of fp32 flat groups.\n        zero_model_states (List[ZeroModelState]): The list of ZeroModelState objects.\n        exclude_frozen_parameters (bool): Whether to exclude frozen parameters.\n\n    Returns:\n        OrderedDict: The reconstructed fp32 state dictionary.\n    \"\"\"\n    state_dict = OrderedDict()\n\n    # buffers\n    buffers = zero_model_states[0].buffers\n    state_dict.update(buffers)\n    if debug:\n        print_pid(f\"added {len(buffers)} buffers\")\n\n    # extra state (e.g., fp8)\n    extra_states = zero_model_states[0].extra_states\n    state_dict.update(extra_states)\n    if debug:\n        print_pid(f\"added {len(extra_states)} extra_states\")\n\n    if not exclude_frozen_parameters:\n        _zero3_merge_frozen_params(state_dict, world_size, zero_model_states)\n\n    _zero3_merge_trainable_params(state_dict, world_size, fp32_flat_groups, zero_model_states)\n\n    # recover shared parameters\n    for pair in zero_model_states[0].shared_params:\n        if pair[1] in state_dict:\n            state_dict[pair[0]] = state_dict[pair[1]]\n\n    return state_dict\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib._get_fp32_state_dict_from_zero_checkpoint","title":"<code>_get_fp32_state_dict_from_zero_checkpoint(ds_checkpoint_dir, rank, exclude_frozen_parameters=False)</code>","text":"<p>Returns the fp32 state dictionary reconstructed from a ZeRO checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>ds_checkpoint_dir</code> <code>str</code> <p>Path to the DeepSpeed checkpoint folder.</p> required <code>rank</code> <code>int</code> <p>The rank to process.</p> required <code>exclude_frozen_parameters</code> <code>bool</code> <p>Whether to exclude frozen parameters.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>The reconstructed fp32 state dictionary.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def _get_fp32_state_dict_from_zero_checkpoint(\n    ds_checkpoint_dir: str, rank: int, exclude_frozen_parameters: bool = False\n):\n    \"\"\"Returns the fp32 state dictionary reconstructed from a ZeRO checkpoint.\n\n    Args:\n        ds_checkpoint_dir (str): Path to the DeepSpeed checkpoint folder.\n        rank (int): The rank to process.\n        exclude_frozen_parameters (bool): Whether to exclude frozen parameters.\n\n    Returns:\n        OrderedDict: The reconstructed fp32 state dictionary.\n    \"\"\"\n    print_pid(f\"Processing zero checkpoint '{ds_checkpoint_dir}'\")\n\n    # optim_files = get_optim_files(ds_checkpoint_dir)\n    # zero_stage, world_size, fp32_flat_groups = parse_optim_states(optim_files, ds_checkpoint_dir)\n\n    optim_files = get_optim_files_by_rank(ds_checkpoint_dir, rank=rank)\n    optim_files_check = get_checkpoint_files(ds_checkpoint_dir, f\"bf16*_{rank:02d}_optim_states.pt\")\n    assert set(optim_files) == set(optim_files_check), f\"Expected {optim_files_check}, got {optim_files}\"\n    # check ordering as well\n    for f1, f2 in zip(optim_files, optim_files_check):\n        assert os.path.basename(f1) == os.path.basename(\n            f2\n        ), f\"Found mismatching optim files for rank {rank}: {os.path.basename(f1)} != {os.path.basename(f2)}\"\n    print_pid(f\" -&gt; Optim files for rank {rank}: {len(optim_files)}\")\n\n    if debug:\n        print_pid(f\"{optim_files=}\")\n\n    if os.environ.get(\"ZERO3_CONVERSION_DEBUG\", \"0\") == \"1\":\n        breakpoint()\n\n    zero_stage, world_size, fp32_flat_groups = parse_optim_states(optim_files, ds_checkpoint_dir)\n    assert len(optim_files) == world_size, f\"Expected {world_size} optim files, got {len(optim_files)}\"\n    if debug:\n        print_pid(\n            f\" -&gt; rank{rank} stage: {zero_stage} {world_size=} {len(fp32_flat_groups)=} {fp32_flat_groups.shape=}\"\n        )\n\n    model_files = get_model_files_by_rank(ds_checkpoint_dir, rank=rank)\n    model_files_check = get_checkpoint_files(ds_checkpoint_dir, f\"zero_*_mp_rank_{rank:02d}_model_states.pt\")\n    assert set(model_files) == set(model_files_check), f\"Expected {model_files_check}, got {model_files}\"\n\n    for f1, f2 in zip(model_files, model_files_check):\n        assert os.path.basename(f1) == os.path.basename(\n            f2\n        ), f\"Found mismatching optim files for rank {rank}: {os.path.basename(f1)} != {os.path.basename(f2)}\"\n    print_pid(f\" -&gt; Model files for rank {rank}: {len(model_files)}\")\n\n    assert len(optim_files) == len(\n        model_files\n    ), f\"Expected same number of optim and model files: {len(optim_files)} != {len(model_files)}\"\n    assert len(optim_files) &gt; 0, f\"Expected at least one optim file, got {len(optim_files)}\"\n\n    zero_model_states = parse_model_states(model_files)\n    print_pid(f\"Parsing checkpoint created by deepspeed=={zero_model_states[0].ds_version}\")\n\n    return _get_fp32_state_dict_from_zero3_checkpoint(\n        world_size, fp32_flat_groups, zero_model_states, exclude_frozen_parameters\n    )\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib._zero3_merge_frozen_params","title":"<code>_zero3_merge_frozen_params(state_dict, world_size, zero_model_states)</code>","text":"<p>Merges frozen parameters into the state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>The state dictionary to update.</p> required <code>world_size</code> <code>int</code> <p>The world size.</p> required <code>zero_model_states</code> <code>List[ZeroModelState]</code> <p>The list of ZeroModelState objects.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def _zero3_merge_frozen_params(state_dict: Dict[str, Any], world_size: int, zero_model_states: List[ZeroModelState]):\n    \"\"\"Merges frozen parameters into the state dictionary.\n\n    Args:\n        state_dict (Dict[str, Any]): The state dictionary to update.\n        world_size (int): The world size.\n        zero_model_states (List[ZeroModelState]): The list of ZeroModelState objects.\n\n    Returns:\n        None\n    \"\"\"\n    if zero_model_states[0].frozen_param_shapes is None or len(zero_model_states[0].frozen_param_shapes) == 0:\n        return\n\n    if debug:\n        for i in range(world_size):\n            num_elem = sum(s.numel() for s in zero_model_states[i].frozen_param_fragments.values())\n            print_pid(f\"rank {i}: {FROZEN_PARAM_SHAPES}.numel = {num_elem}\")\n\n        frozen_param_shapes = zero_model_states[0].frozen_param_shapes\n        wanted_params = len(frozen_param_shapes)\n        wanted_numel = sum(s.numel() for s in frozen_param_shapes.values())\n        avail_numel = sum([p.numel() for p in zero_model_states[0].frozen_param_fragments.values()]) * world_size\n        print_pid(f\"Frozen params: Have {avail_numel} numels to process.\")\n        print_pid(f\"Frozen params: Need {wanted_numel} numels in {wanted_params} params\")\n\n    total_params = 0\n    total_numel = 0\n    for name, shape in zero_model_states[0].frozen_param_shapes.items():\n        total_params += 1\n        unpartitioned_numel = shape.numel()\n        total_numel += unpartitioned_numel\n\n        param_frags = tuple(model_state.frozen_param_fragments[name] for model_state in zero_model_states)\n        state_dict[name] = torch.cat(param_frags, 0).narrow(0, 0, unpartitioned_numel).view(shape)\n\n        partitioned_numel, partitioned_padding_numel = zero3_partitioned_param_info(unpartitioned_numel, world_size)\n\n        if debug:\n            print_pid(\n                f\"Frozen params: {total_params} {name} full shape: {shape} partition0 numel={partitioned_numel} partitioned_padding_numel={partitioned_padding_numel}\"\n            )\n\n    print_pid(f\"Reconstructed Frozen fp32 state dict with {total_params} params {total_numel} elements\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib._zero3_merge_trainable_params","title":"<code>_zero3_merge_trainable_params(state_dict, world_size, fp32_flat_groups, zero_model_states)</code>","text":"<p>Merges trainable parameters into the state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>The state dictionary to update.</p> required <code>world_size</code> <code>int</code> <p>The world size.</p> required <code>fp32_flat_groups</code> <code>List[Tensor]</code> <p>The list of fp32 flat groups.</p> required <code>zero_model_states</code> <code>List[ZeroModelState]</code> <p>The list of ZeroModelState objects.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def _zero3_merge_trainable_params(\n    state_dict: Dict[str, Any],\n    world_size: int,\n    fp32_flat_groups: List[torch.Tensor],\n    zero_model_states: List[ZeroModelState],\n):\n    \"\"\"Merges trainable parameters into the state dictionary.\n\n    Args:\n        state_dict (Dict[str, Any]): The state dictionary to update.\n        world_size (int): The world size.\n        fp32_flat_groups (List[torch.Tensor]): The list of fp32 flat groups.\n        zero_model_states (List[ZeroModelState]): The list of ZeroModelState objects.\n\n    Returns:\n        None\n    \"\"\"\n    if os.environ.get(\"ZERO3_CONVERSION_DEBUG\", \"0\") == \"1\":\n        breakpoint()\n\n    param_shapes = zero_model_states[0].param_shapes\n    avail_numel = fp32_flat_groups[0].numel() * world_size\n    # Reconstruction protocol: For zero3 we need to zip the partitions together at boundary of each\n    # param, re-consolidating each param, while dealing with padding if any\n\n    # merge list of dicts, preserving order\n    param_shapes = {k: v for d in param_shapes for k, v in d.items()}\n\n    if debug:\n        for i in range(world_size):\n            print_pid(f\"{FP32_FLAT_GROUPS}[{i}].shape={fp32_flat_groups[i].shape}\")\n\n        wanted_params = len(param_shapes)\n        wanted_numel = sum(shape.numel() for shape in param_shapes.values())\n        # not asserting if there is a mismatch due to possible padding\n        avail_numel = fp32_flat_groups[0].numel() * world_size\n        print_pid(f\"Trainable params: Have {avail_numel} numels to process.\")\n        print_pid(f\"Trainable params: Need {wanted_numel} numels in {wanted_params} params.\")\n\n    # params\n    # XXX: for huge models that can't fit into the host's RAM we will have to recode this to support\n    # out-of-core computing solution\n    offset = 0\n    total_numel = 0\n    total_params = 0\n    pid = os.getpid()\n    for name, shape in tqdm(param_shapes.items(), desc=f\"{pid=}: Gathering Sharded Weights\"):\n        unpartitioned_numel = shape.numel()\n        total_numel += unpartitioned_numel\n        total_params += 1\n        # NOTE: partitioned_numel includes padding, padding applies if unpartitioned_numel is not divisible by world_size\n        partitioned_numel, partitioned_padding_numel = zero3_partitioned_param_info(unpartitioned_numel, world_size)\n\n        if debug:\n            print_pid(\n                f\"Trainable params: {total_params} {name} full shape: {shape} partition0 numel={partitioned_numel} partitioned_padding_numel={partitioned_padding_numel}\"\n            )\n\n        # XXX: memory usage doubles here\n        state_dict[name] = (\n            torch.cat(tuple(fp32_flat_groups[i].narrow(0, offset, partitioned_numel) for i in range(world_size)), 0)\n            .narrow(0, 0, unpartitioned_numel)\n            .view(shape)\n        )\n        offset += partitioned_numel\n\n    offset *= world_size\n\n    # Sanity check\n    if offset != avail_numel:\n        raise ValueError(f\"consumed {offset} numels out of {avail_numel} - something is wrong\")\n\n    print_pid(f\"Reconstructed Trainable fp32 state dict with {total_params} params {total_numel} elements\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.atoi","title":"<code>atoi(text)</code>","text":"<p>Converts a string to an integer if it is a digit, otherwise returns the string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be converted.</p> required <p>Returns:</p> Type Description <p>int or str: The converted integer or the original string.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def atoi(text: str):\n    \"\"\"Converts a string to an integer if it is a digit, otherwise returns the string.\n\n    Args:\n        text (str): The text to be converted.\n\n    Returns:\n        int or str: The converted integer or the original string.\n    \"\"\"\n    return int(text) if text.isdigit() else text\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.create_ds_output_path","title":"<code>create_ds_output_path(rank)</code>","text":"<p>Creates the output path for a DeepSpeed checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank to create the output path for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The output path for the DeepSpeed checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def create_ds_output_path(rank: int):\n    \"\"\"Creates the output path for a DeepSpeed checkpoint.\n\n    Args:\n        rank (int): The rank to create the output path for.\n\n    Returns:\n        str: The output path for the DeepSpeed checkpoint.\n    \"\"\"\n    return f\"mp_rank_{rank:02}_model_states.pt\"\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.create_zero3_model_state_path","title":"<code>create_zero3_model_state_path(dp_rank, mp_rank)</code>","text":"<p>Creates the path for a ZeRO3 model state file.</p> <p>Parameters:</p> Name Type Description Default <code>dp_rank</code> <code>int</code> <p>The data parallel rank.</p> required <code>mp_rank</code> <code>int</code> <p>The model parallel rank.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path for the ZeRO3 model state file.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def create_zero3_model_state_path(dp_rank: int, mp_rank: int):\n    \"\"\"Creates the path for a ZeRO3 model state file.\n\n    Args:\n        dp_rank (int): The data parallel rank.\n        mp_rank (int): The model parallel rank.\n\n    Returns:\n        str: The path for the ZeRO3 model state file.\n    \"\"\"\n    return f\"zero_pp_rank_{dp_rank}_mp_rank_{mp_rank:02}_model_states.pt\"\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.create_zero3_optim_state_path","title":"<code>create_zero3_optim_state_path(dp_rank, mp_rank)</code>","text":"<p>Creates the path for a ZeRO3 optimizer state file.</p> <p>Parameters:</p> Name Type Description Default <code>dp_rank</code> <code>int</code> <p>The data parallel rank.</p> required <code>mp_rank</code> <code>int</code> <p>The model parallel rank.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path for the ZeRO3 optimizer state file.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def create_zero3_optim_state_path(dp_rank: int, mp_rank: int):\n    \"\"\"Creates the path for a ZeRO3 optimizer state file.\n\n    Args:\n        dp_rank (int): The data parallel rank.\n        mp_rank (int): The model parallel rank.\n\n    Returns:\n        str: The path for the ZeRO3 optimizer state file.\n    \"\"\"\n    return f\"bf16_zero_pp_rank_{dp_rank}_mp_rank_{mp_rank:02}_optim_states.pt\"\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_checkpoint_files","title":"<code>get_checkpoint_files(checkpoint_dir, glob_pattern)</code>","text":"<p>Retrieves checkpoint files from a directory based on a glob pattern.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for checkpoint files.</p> required <code>glob_pattern</code> <code>str</code> <p>The glob pattern to match files.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A sorted list of checkpoint files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no files matching the glob pattern are found.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_checkpoint_files(checkpoint_dir: str, glob_pattern: str):\n    \"\"\"Retrieves checkpoint files from a directory based on a glob pattern.\n\n    Args:\n        checkpoint_dir (str): The directory to search for checkpoint files.\n        glob_pattern (str): The glob pattern to match files.\n\n    Returns:\n        list: A sorted list of checkpoint files.\n\n    Raises:\n        FileNotFoundError: If no files matching the glob pattern are found.\n    \"\"\"\n    # XXX: need to test that this simple glob rule works for multi-node setup too\n    ckpt_files = sorted(glob.glob(os.path.join(checkpoint_dir, glob_pattern)), key=natural_keys)\n\n    if len(ckpt_files) == 0:\n        raise FileNotFoundError(f\"can't find {glob_pattern} files in directory '{checkpoint_dir}'\")\n\n    return ckpt_files\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_elapsed","title":"<code>get_elapsed(t)</code>","text":"<p>Converts elapsed time in seconds to a formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>float</code> <p>The elapsed time in seconds.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The formatted elapsed time as a string.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_elapsed(t: float):\n    \"\"\"Converts elapsed time in seconds to a formatted string.\n\n    Args:\n        t (float): The elapsed time in seconds.\n\n    Returns:\n        str: The formatted elapsed time as a string.\n    \"\"\"\n    minutes = t // 60\n    seconds = t % 60\n    if minutes &gt; 0:\n        total_time = f\"{minutes:.0f}min{seconds:.0f}s\"\n    else:\n        total_time = f\"{seconds:.1f}s\"\n    return total_time\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_model_files_by_rank","title":"<code>get_model_files_by_rank(checkpoint_dir, rank)</code>","text":"<p>Retrieves model files for a specific rank from a checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for model files.</p> required <code>rank</code> <code>int</code> <p>The rank to search for.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of model files for the specified rank.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_model_files_by_rank(checkpoint_dir: str, rank: int):\n    \"\"\"Retrieves model files for a specific rank from a checkpoint directory.\n\n    Args:\n        checkpoint_dir (str): The directory to search for model files.\n        rank (int): The rank to search for.\n\n    Returns:\n        list: A list of model files for the specified rank.\n    \"\"\"\n    return get_checkpoint_files(checkpoint_dir, f\"*mp_rank_{rank:02}_model_states.pt\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_model_state_file","title":"<code>get_model_state_file(checkpoint_dir, zero_stage)</code>","text":"<p>Retrieves the model state file from a checkpoint directory based on the ZeRO stage.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for the model state file.</p> required <code>zero_stage</code> <code>int</code> <p>The ZeRO stage to search for.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path to the model state file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory or model state file is not found.</p> <code>ValueError</code> <p>If the ZeRO stage is not supported.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_model_state_file(checkpoint_dir: str, zero_stage: int):\n    \"\"\"Retrieves the model state file from a checkpoint directory based on the ZeRO stage.\n\n    Args:\n        checkpoint_dir (str): The directory to search for the model state file.\n        zero_stage (int): The ZeRO stage to search for.\n\n    Returns:\n        str: The path to the model state file.\n\n    Raises:\n        FileNotFoundError: If the directory or model state file is not found.\n        ValueError: If the ZeRO stage is not supported.\n    \"\"\"\n    if not os.path.isdir(checkpoint_dir):\n        raise FileNotFoundError(f\"Directory '{checkpoint_dir}' doesn't exist\")\n\n    # there should be only one file\n    if zero_stage &lt;= 2:\n        file = os.path.join(checkpoint_dir, \"mp_rank_00_model_states.pt\")\n    elif zero_stage == 3:\n        file = os.path.join(checkpoint_dir, \"zero_pp_rank_0_mp_rank_00_model_states.pt\")\n    else:\n        raise ValueError(f\"Unsupported zero stage {zero_stage}. Expected 1, 2, or 3\")\n\n    if not os.path.exists(file):\n        raise FileNotFoundError(f\"can't find model states file at '{file}'\")\n\n    return file\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.get_optim_files_by_rank","title":"<code>get_optim_files_by_rank(checkpoint_dir, rank)</code>","text":"<p>Retrieves optimizer files for a specific rank from a checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory to search for optimizer files.</p> required <code>rank</code> <code>int</code> <p>The rank to search for.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of optimizer files for the specified rank.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def get_optim_files_by_rank(checkpoint_dir: str, rank: int):\n    \"\"\"Retrieves optimizer files for a specific rank from a checkpoint directory.\n\n    Args:\n        checkpoint_dir (str): The directory to search for optimizer files.\n        rank (int): The rank to search for.\n\n    Returns:\n        list: A list of optimizer files for the specified rank.\n    \"\"\"\n    return get_checkpoint_files(checkpoint_dir, f\"*mp_rank_{rank:02}_optim_states.pt\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.natural_keys","title":"<code>natural_keys(text)</code>","text":"<p>Sorts a list in human order.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be sorted.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>The sorted list.</p> Note <p>alist.sort(key=natural_keys) sorts in human order. http://nedbatchelder.com/blog/200712/human_sorting.html (See Toothy's implementation in the comments)</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def natural_keys(text: str):\n    \"\"\"Sorts a list in human order.\n\n    Args:\n        text (str): The text to be sorted.\n\n    Returns:\n        list: The sorted list.\n\n    Note:\n        alist.sort(key=natural_keys) sorts in human order.\n        http://nedbatchelder.com/blog/200712/human_sorting.html\n        (See Toothy's implementation in the comments)\n    \"\"\"\n    return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.parse_model_states","title":"<code>parse_model_states(files)</code>","text":"<p>Parses model state files and returns a list of ZeroModelState objects.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Set[str]</code> <p>A set of file paths to parse.</p> required <p>Returns:</p> Type Description <p>List[ZeroModelState]: A list of parsed ZeroModelState objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a file is not a model state checkpoint.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def parse_model_states(files: Set[str]):\n    \"\"\"Parses model state files and returns a list of ZeroModelState objects.\n\n    Args:\n        files (Set[str]): A set of file paths to parse.\n\n    Returns:\n        List[ZeroModelState]: A list of parsed ZeroModelState objects.\n\n    Raises:\n        ValueError: If a file is not a model state checkpoint.\n    \"\"\"\n    zero_model_states = []\n    for file in files:\n        state_dict = torch.load(file, map_location=device)\n\n        if BUFFER_NAMES not in state_dict:\n            raise ValueError(f\"{file} is not a model state checkpoint\")\n        buffer_names = state_dict[BUFFER_NAMES]\n        if debug:\n            print_pid(\"Found buffers:\", buffer_names)\n\n        # recover just the buffers while restoring them to fp32 if they were saved in fp16\n        buffers = {k: v.float() for k, v in state_dict[\"module\"].items() if k in buffer_names}\n\n        extra_states = {k: v for k, v in state_dict[\"module\"].items() if k.endswith(EXTRA_STATE)}\n\n        # collect parameters that are included in param_shapes\n        param_shapes = state_dict[PARAM_SHAPES]\n        param_names = []\n        for s in param_shapes:\n            for name in s.keys():\n                param_names.append(name)\n\n        # update with frozen parameters\n        frozen_param_shapes = state_dict.get(FROZEN_PARAM_SHAPES, None)\n        if frozen_param_shapes is not None:\n            if debug:\n                print_pid(f\"Found frozen_param_shapes: {frozen_param_shapes}\")\n            param_names += list(frozen_param_shapes.keys())\n\n        # handle shared params\n        shared_params = [[k, v] for k, v in state_dict[\"shared_params\"].items()]\n\n        ds_version = state_dict.get(DS_VERSION, None)\n\n        frozen_param_fragments = state_dict.get(FROZEN_PARAM_FRAGMENTS, None)\n\n        z_model_state = ZeroModelState(\n            buffers=buffers,\n            extra_states=extra_states,\n            param_shapes=param_shapes,\n            shared_params=shared_params,\n            ds_version=ds_version,\n            frozen_param_shapes=frozen_param_shapes,\n            frozen_param_fragments=frozen_param_fragments,\n        )\n        zero_model_states.append(z_model_state)\n\n    return zero_model_states\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.parse_optim_states","title":"<code>parse_optim_states(files, ds_checkpoint_dir)</code>","text":"<p>Parses optimizer state files and returns the ZeRO stage, world size, and fp32 flat groups.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Set[str]</code> <p>A set of file paths to parse.</p> required <code>ds_checkpoint_dir</code> <code>str</code> <p>The directory containing the DeepSpeed checkpoint.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the ZeRO stage, world size, and fp32 flat groups.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a file is not a ZeRO checkpoint or if the number of files does not match the expected world size.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def parse_optim_states(files: Set[str], ds_checkpoint_dir: str):\n    \"\"\"Parses optimizer state files and returns the ZeRO stage, world size, and fp32 flat groups.\n\n    Args:\n        files (Set[str]): A set of file paths to parse.\n        ds_checkpoint_dir (str): The directory containing the DeepSpeed checkpoint.\n\n    Returns:\n        tuple: A tuple containing the ZeRO stage, world size, and fp32 flat groups.\n\n    Raises:\n        ValueError: If a file is not a ZeRO checkpoint or if the number of files does not match the expected world size.\n    \"\"\"\n    total_files = len(files)\n    state_dicts = []\n    for f in files:\n        state_dict = torch.load(f, map_location=device)\n        # immediately discard the potentially huge 2 optimizer states as we only care for fp32 master weights\n        # and also handle the case where it was already removed by another helper script\n        state_dict[\"optimizer_state_dict\"].pop(\"optimizer_state_dict\", None)\n        state_dict[OPTIMIZER_STATE_DICT] = {\n            FP32_FLAT_GROUPS: state_dict[OPTIMIZER_STATE_DICT][FP32_FLAT_GROUPS],\n            ZERO_STAGE: state_dict[OPTIMIZER_STATE_DICT][ZERO_STAGE],\n            PARTITION_COUNT: state_dict[OPTIMIZER_STATE_DICT][PARTITION_COUNT],\n        }\n        state_dicts.append(state_dict)\n\n    if ZERO_STAGE not in state_dicts[0][OPTIMIZER_STATE_DICT]:\n        raise ValueError(f\"{files[0]} is not a zero checkpoint\")\n    zero_stage = state_dicts[0][OPTIMIZER_STATE_DICT][ZERO_STAGE]\n    world_size = state_dicts[0][OPTIMIZER_STATE_DICT][PARTITION_COUNT]\n\n    # For ZeRO-2 each param group can have different partition_count as data parallelism for expert\n    # parameters can be different from data parallelism for non-expert parameters. So we can just\n    # use the max of the partition_count to get the dp world_size.\n\n    if type(world_size) is list:\n        world_size = max(world_size)\n\n    if world_size != total_files:\n        raise ValueError(\n            f\"Expected {world_size} of '*_optim_states.pt' under '{ds_checkpoint_dir}' but found {total_files} files. \"\n            \"Possibly due to an overwrite of an old checkpoint, or a checkpoint didn't get saved by one or more processes.\"\n        )\n\n    # the groups are named differently in each stage\n    if zero_stage &lt;= 2:\n        fp32_groups_key = SINGLE_PARTITION_OF_FP32_GROUPS\n    elif zero_stage == 3:\n        fp32_groups_key = FP32_FLAT_GROUPS\n    else:\n        raise ValueError(f\"unknown zero stage {zero_stage}\")\n\n    if zero_stage &lt;= 2:\n        fp32_flat_groups = [state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key] for i in range(len(state_dicts))]\n    elif zero_stage == 3:\n        # if there is more than one param group, there will be multiple flattened tensors - one\n        # flattened tensor per group - for simplicity merge them into a single tensor\n        #\n        # XXX: could make the script more memory efficient for when there are multiple groups - it\n        # will require matching the sub-lists of param_shapes for each param group flattened tensor\n\n        fp32_flat_groups = [\n            torch.cat(state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key], 0) for i in range(len(state_dicts))\n        ]\n\n    return zero_stage, world_size, fp32_flat_groups\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.print_pid","title":"<code>print_pid(msg)</code>","text":"<p>Prints the process ID along with a message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>The message to be printed.</p> required Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def print_pid(msg: str):\n    \"\"\"Prints the process ID along with a message.\n\n    Args:\n        msg (str): The message to be printed.\n    \"\"\"\n    pid = os.getpid()\n    print(f\"{pid=}:{msg}\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.process_single_rank","title":"<code>process_single_rank(rank, ds_checkpoint_dir, output_dir, overwrite=False, exclude_frozen_parameters=False)</code>","text":"<p>Processes a single rank to gather and save the state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank to process.</p> required <code>ds_checkpoint_dir</code> <code>str</code> <p>Path to the DeepSpeed checkpoint folder.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the output.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files. Default is False.</p> <code>False</code> <code>exclude_frozen_parameters</code> <code>bool</code> <p>Whether to exclude frozen parameters. Default is False.</p> <code>False</code> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def process_single_rank(\n    rank: int,\n    ds_checkpoint_dir: str,\n    output_dir: str,\n    overwrite: bool = False,\n    exclude_frozen_parameters: bool = False,\n):\n    \"\"\"Processes a single rank to gather and save the state dictionary.\n\n    Args:\n        rank (int): The rank to process.\n        ds_checkpoint_dir (str): Path to the DeepSpeed checkpoint folder.\n        output_dir (str): Directory to save the output.\n        overwrite (bool): Whether to overwrite existing files. Default is False.\n        exclude_frozen_parameters (bool): Whether to exclude frozen parameters. Default is False.\n    \"\"\"\n    print_pid(f\"Gathering rank {rank} state_dict...\")\n\n    start = time.time()\n    output_path = os.path.join(output_dir, create_ds_output_path(rank))\n    if os.path.exists(output_path) and not overwrite:\n        print_pid(f\"Output path {output_path} exists, skipping\")\n        return\n\n    print_pid(f\" -&gt; Gathering data parallel partitions for mp rank {rank}...\")\n\n    if os.environ.get(\"ZERO3_CONVERSION_DEBUG\", \"0\") == \"1\":\n        breakpoint()\n\n    state_dict = _get_fp32_state_dict_from_zero_checkpoint(\n        ds_checkpoint_dir=ds_checkpoint_dir, rank=rank, exclude_frozen_parameters=exclude_frozen_parameters\n    )\n    print_pid(f\" -&gt; Done processing rank {rank} state_dict, gathered {len(state_dict)} params\")\n\n    checkpoint = {\n        \"module\": state_dict,\n        \"param_shapes\": OrderedDict(),\n        \"dp_world_size\": 1,\n    }\n\n    for param, value in state_dict.items():\n        if isinstance(value, torch.Tensor):\n            checkpoint[\"param_shapes\"][param] = value.shape\n\n    print_pid(f\" -&gt; Saving mp rank {rank} checkpoint to {output_path}\")\n    torch.save(checkpoint, f\"{output_path}\")\n\n    total_time = get_elapsed(time.time() - start)\n    print_pid(f\" -&gt; rank {rank} took {total_time}\")\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.profile_memory_decorator","title":"<code>profile_memory_decorator(func)</code>","text":"<p>A decorator to profile memory usage of a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Iterable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>wrapper</code> <p>The decorated function with memory profiling.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def profile_memory_decorator(func: Iterable):\n    \"\"\"A decorator to profile memory usage of a function.\n\n    Args:\n        func (Iterable): The function to be decorated.\n\n    Returns:\n        wrapper: The decorated function with memory profiling.\n    \"\"\"\n\n    def profile_memory():\n        pid = os.getpid()\n        process = psutil.Process(pid)\n        memory_info = process.memory_info()\n        print_pid(f\"{pid}: RSS = {memory_info.rss / 1024 ** 2:.2f} MB\")\n\n    def wrapper(*args, **kwargs):\n        profile_memory()\n        func(*args, **kwargs)\n        profile_memory()\n\n    return wrapper\n</code></pre>"},{"location":"API_reference/bionemo/evo2/utils/checkpoint/zero3_conversion_lib/#bionemo.evo2.utils.checkpoint.zero3_conversion_lib.zero3_partitioned_param_info","title":"<code>zero3_partitioned_param_info(unpartitioned_numel, world_size)</code>","text":"<p>Returns the partitioned and padding number of elements for a parameter.</p> <p>Parameters:</p> Name Type Description Default <code>unpartitioned_numel</code> <code>int</code> <p>The number of elements in the unpartitioned parameter.</p> required <code>world_size</code> <code>int</code> <p>The world size.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the partitioned number of elements and the padding number of elements.</p> Source code in <code>bionemo/evo2/utils/checkpoint/zero3_conversion_lib.py</code> <pre><code>def zero3_partitioned_param_info(unpartitioned_numel: int, world_size: int):\n    \"\"\"Returns the partitioned and padding number of elements for a parameter.\n\n    Args:\n        unpartitioned_numel (int): The number of elements in the unpartitioned parameter.\n        world_size (int): The world size.\n\n    Returns:\n        tuple: A tuple containing the partitioned number of elements and the padding number of elements.\n    \"\"\"\n    remainder = unpartitioned_numel % world_size\n    padding_numel = (world_size - remainder) if remainder else 0\n    partitioned_numel = math.ceil(unpartitioned_numel / world_size)\n    return partitioned_numel, padding_numel\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/","title":"Lightning basic","text":"<p>This is intended to be a minimal self-container NeMo2 example.</p>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule","title":"<code>BionemoLightningModule</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>IOMixin</code>, <code>LightningPassthroughPredictionMixin</code></p> <p>A very basic lightning module for testing the megatron strategy and the megatron-nemo2-bionemo contract.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class BionemoLightningModule(pl.LightningModule, io.IOMixin, LightningPassthroughPredictionMixin):\n    \"\"\"A very basic lightning module for testing the megatron strategy and the megatron-nemo2-bionemo contract.\"\"\"\n\n    def __init__(self, config: MegatronBioNeMoTrainableModelConfig):\n        \"\"\"Initializes the model.\n\n        Args:\n            config: a Config object necessary to construct the actual nn.Module (the thing that has the parameters).\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.optim = MegatronOptimizerModule(\n            config=OptimizerConfig(\n                lr=1e-4,\n                optimizer=\"adam\",\n                use_distributed_optimizer=True,\n                bf16=config.bf16,\n                fp16=config.fp16,\n                params_dtype=config.params_dtype,\n            ),\n        )\n        # Bind the configure_optimizers method to the model\n        self.optim.connect(self)\n\n    def forward(self, batch: Dict, batch_idx: int) -&gt; Any:\n        \"\"\"This forward will be called by the megatron scheduler and it will be wrapped.\n\n        !!! note\n\n            The `training_step` defines the training loop and is independent of the `forward` method here.\n\n        Args:\n            batch: A dictionary of data.\n            batch_idx: The index of the batch.\n\n        Returns:\n            The output of the model.\n        \"\"\"\n        x = batch[\"data\"]\n        return self.module(x)\n\n    def training_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"The training step is where the loss is calculated and the backpropagation is done.\n\n        Background:\n        - NeMo's Strategy overrides this method.\n        - The strategies' training step will call the forward method of the model.\n        - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model.\n        - That wrapped forward step is then executed inside the Mcore scheduler, which calls the `_forward_step` method from the\n            MegatronParallel class.\n        - Which then calls the training_step function here.\n\n        In this particular use case, we simply call the forward method of this class, the lightning module.\n\n        Args:\n            batch: A dictionary of data. requires `batch_idx` as default None.\n            batch_idx: The index of the batch.\n        \"\"\"\n        # Forward pass\n        predictions = self(batch, batch_idx)\n\n        # Calculate loss using the training loss reduction function\n        loss_reduction = self.training_loss_reduction()\n        loss_reduction.setup(batch)\n        loss = loss_reduction(predictions)\n\n        # Log the training loss\n        self.log(\"train_loss\", loss[1][\"avg\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return predictions\n\n    def validation_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"Alias for forward step at validation.\"\"\"\n        predictions = self(batch, batch_idx)\n\n        # Calculate loss using the validation loss reduction function\n        loss_reduction = self.validation_loss_reduction()\n        loss_reduction.setup(batch)\n        loss = loss_reduction(predictions)\n        # Log the validation loss\n        self.log(\n            \"val_loss\",\n            loss[1][\"avg\"],\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n\n        return predictions\n\n    def predict_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"Alias for forward step at prediction.\"\"\"\n        return self(batch, batch_idx)\n\n    def training_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def validation_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def test_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def configure_model(self) -&gt; None:\n        \"\"\"This configures the model. It is called lazily by the megatron strategy.\"\"\"\n        self.module = self.config.configure_model()\n\n    def loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n        \"\"\"Get the loss reduction class the user has specified in their config.\"\"\"\n        return self.config.get_loss_reduction_class()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MegatronBioNeMoTrainableModelConfig</code> <p>a Config object necessary to construct the actual nn.Module (the thing that has the parameters).</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: MegatronBioNeMoTrainableModelConfig):\n    \"\"\"Initializes the model.\n\n    Args:\n        config: a Config object necessary to construct the actual nn.Module (the thing that has the parameters).\n    \"\"\"\n    super().__init__()\n    self.config = config\n    self.optim = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=1e-4,\n            optimizer=\"adam\",\n            use_distributed_optimizer=True,\n            bf16=config.bf16,\n            fp16=config.fp16,\n            params_dtype=config.params_dtype,\n        ),\n    )\n    # Bind the configure_optimizers method to the model\n    self.optim.connect(self)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.configure_model","title":"<code>configure_model()</code>","text":"<p>This configures the model. It is called lazily by the megatron strategy.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"This configures the model. It is called lazily by the megatron strategy.\"\"\"\n    self.module = self.config.configure_model()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.forward","title":"<code>forward(batch, batch_idx)</code>","text":"<p>This forward will be called by the megatron scheduler and it will be wrapped.</p> <p>Note</p> <p>The <code>training_step</code> defines the training loop and is independent of the <code>forward</code> method here.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict</code> <p>A dictionary of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The output of the model.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: Dict, batch_idx: int) -&gt; Any:\n    \"\"\"This forward will be called by the megatron scheduler and it will be wrapped.\n\n    !!! note\n\n        The `training_step` defines the training loop and is independent of the `forward` method here.\n\n    Args:\n        batch: A dictionary of data.\n        batch_idx: The index of the batch.\n\n    Returns:\n        The output of the model.\n    \"\"\"\n    x = batch[\"data\"]\n    return self.module(x)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.loss_reduction_class","title":"<code>loss_reduction_class()</code>","text":"<p>Get the loss reduction class the user has specified in their config.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n    \"\"\"Get the loss reduction class the user has specified in their config.\"\"\"\n    return self.config.get_loss_reduction_class()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.predict_step","title":"<code>predict_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward step at prediction.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def predict_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"Alias for forward step at prediction.\"\"\"\n    return self(batch, batch_idx)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.test_loss_reduction","title":"<code>test_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def test_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.training_loss_reduction","title":"<code>training_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def training_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.training_step","title":"<code>training_step(batch, batch_idx=None)</code>","text":"<p>The training step is where the loss is calculated and the backpropagation is done.</p> <p>Background: - NeMo's Strategy overrides this method. - The strategies' training step will call the forward method of the model. - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model. - That wrapped forward step is then executed inside the Mcore scheduler, which calls the <code>_forward_step</code> method from the     MegatronParallel class. - Which then calls the training_step function here.</p> <p>In this particular use case, we simply call the forward method of this class, the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>A dictionary of data. requires <code>batch_idx</code> as default None.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The index of the batch.</p> <code>None</code> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def training_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"The training step is where the loss is calculated and the backpropagation is done.\n\n    Background:\n    - NeMo's Strategy overrides this method.\n    - The strategies' training step will call the forward method of the model.\n    - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model.\n    - That wrapped forward step is then executed inside the Mcore scheduler, which calls the `_forward_step` method from the\n        MegatronParallel class.\n    - Which then calls the training_step function here.\n\n    In this particular use case, we simply call the forward method of this class, the lightning module.\n\n    Args:\n        batch: A dictionary of data. requires `batch_idx` as default None.\n        batch_idx: The index of the batch.\n    \"\"\"\n    # Forward pass\n    predictions = self(batch, batch_idx)\n\n    # Calculate loss using the training loss reduction function\n    loss_reduction = self.training_loss_reduction()\n    loss_reduction.setup(batch)\n    loss = loss_reduction(predictions)\n\n    # Log the training loss\n    self.log(\"train_loss\", loss[1][\"avg\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n    return predictions\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.validation_loss_reduction","title":"<code>validation_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def validation_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward step at validation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def validation_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"Alias for forward step at validation.\"\"\"\n    predictions = self(batch, batch_idx)\n\n    # Calculate loss using the validation loss reduction function\n    loss_reduction = self.validation_loss_reduction()\n    loss_reduction.setup(batch)\n    loss = loss_reduction(predictions)\n    # Log the validation loss\n    self.log(\n        \"val_loss\",\n        loss[1][\"avg\"],\n        on_step=False,\n        on_epoch=True,\n        prog_bar=True,\n        logger=True,\n    )\n\n    return predictions\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction","title":"<code>ClassifierLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ClassifierLossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: Tensor) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        digits = batch[\"label\"]\n        digit_logits = forward_out\n        loss = nn.functional.cross_entropy(digit_logits, digits)\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Tensor</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: Tensor) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    digits = batch[\"label\"]\n    digit_logits = forward_out\n    loss = nn.functional.cross_entropy(digit_logits, digits)\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneBothConfig","title":"<code>ExampleFineTuneBothConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleFineTuneBothModel', 'MSEPlusClassifierLossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleFineTuneBothConfig(\n    ExampleGenericConfig[\"ExampleFineTuneBothModel\", \"MSEPlusClassifierLossReduction\"], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleFineTuneBothModel] = ExampleFineTuneBothModel\n    loss_cls: Type[MSEPlusClassifierLossReduction] = MSEPlusClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneBothModel","title":"<code>ExampleFineTuneBothModel</code>","text":"<p>               Bases: <code>ExampleModel</code></p> <p>Example of taking the example model and adding an output task.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneBothModel(ExampleModel):\n    \"\"\"Example of taking the example model and adding an output task.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig):\n        super().__init__(config)\n        # 10 output digits, and use the latent output layer (z) for making predictions\n        self.digit_classifier = nn.Linear(self.linear2.out_features, 10)\n\n    def forward(self, x: Tensor) -&gt; ExampleFineTuneOutput:\n        parent_out: ExampleModelOutput = super().forward(x)\n        digit_logits = self.digit_classifier(parent_out[\"z\"])\n        return {\n            \"x_hat\": parent_out[\"x_hat\"],\n            \"z\": parent_out[\"z\"],\n            \"digit_logits\": digit_logits,\n        }\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneConfig","title":"<code>ExampleFineTuneConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleFineTuneConfig', 'ClassifierLossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleFineTuneConfig(\n    ExampleGenericConfig[\"ExampleFineTuneConfig\", \"ClassifierLossReduction\"], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleFineTuneModel] = ExampleFineTuneModel\n    loss_cls: Type[ClassifierLossReduction] = ClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneModel","title":"<code>ExampleFineTuneModel</code>","text":"<p>               Bases: <code>ExampleModelTrunk</code></p> <p>Example of taking the example model and replacing output task.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneModel(ExampleModelTrunk):\n    \"\"\"Example of taking the example model and replacing output task.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig):\n        super().__init__(config)\n        # 10 output digits, and use the latent output layer (z) for making predictions\n        self.digit_classifier = nn.Linear(self.linear2.out_features, 10)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        z: Tensor = super().forward(x)\n        digit_logits = self.digit_classifier(z)  # to demonstrate flexibility, in this case we return a tensor\n        return digit_logits\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneOutput","title":"<code>ExampleFineTuneOutput</code>","text":"<p>               Bases: <code>ExampleModelOutput</code></p> <p>Output for the fine-tuned example model implementation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneOutput(ExampleModelOutput):\n    \"\"\"Output for the fine-tuned example model implementation.\"\"\"\n\n    digit_logits: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig","title":"<code>ExampleGenericConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[ExampleModelT, MegatronLossType]</code>, <code>MegatronBioNeMoTrainableModelConfig[ExampleModelT, MegatronLossType]</code></p> <p>ExampleGenericConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleGenericConfig(\n    Generic[ExampleModelT, MegatronLossType], MegatronBioNeMoTrainableModelConfig[ExampleModelT, MegatronLossType]\n):\n    \"\"\"ExampleGenericConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    loss_cls: Type[MegatronLossType] = MSELossReduction  # type: ignore  # this will get overriden by children\n    hidden_size: int = 64  # Needs to be set to avoid zero division error in megatron :(\n    num_attention_heads: int = 1  # Needs to be set to avoid zero division error in megatron :(\n    num_layers: int = 1  # Needs to be set to avoid zero division error in megatron :(\n    # IMPORTANT: Since we're adding/overriding the loss_cls, and that's not how we generally track this, we need to\n    #   add this into the list of config settings that we do not draw from the loaded checkpoint when restoring.\n    override_parent_fields: List[str] = field(default_factory=lambda: OVERRIDE_BIONEMO_CONFIG_DEFAULTS + [\"loss_cls\"])\n\n    def configure_model(self) -&gt; ExampleModelT:\n        \"\"\"Uses model_cls and loss_cls to configure the model.\n\n        Note: Must pass self into Model since model requires having a config object.\n\n        Returns:\n            The model object.\n        \"\"\"\n        # 1. first load any settings that may exist in the checkpoint related to the model.\n        if self.initial_ckpt_path:\n            self.load_settings_from_checkpoint(self.initial_ckpt_path)\n        # 2. then initialize the model\n        model = self.model_cls(self)\n        # 3. Load weights from the checkpoint into the model\n        if self.initial_ckpt_path:\n            self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n        return model\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:\n        \"\"\"Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.\"\"\"\n        return self.loss_cls\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig.configure_model","title":"<code>configure_model()</code>","text":"<p>Uses model_cls and loss_cls to configure the model.</p> <p>Note: Must pass self into Model since model requires having a config object.</p> <p>Returns:</p> Type Description <code>ExampleModelT</code> <p>The model object.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def configure_model(self) -&gt; ExampleModelT:\n    \"\"\"Uses model_cls and loss_cls to configure the model.\n\n    Note: Must pass self into Model since model requires having a config object.\n\n    Returns:\n        The model object.\n    \"\"\"\n    # 1. first load any settings that may exist in the checkpoint related to the model.\n    if self.initial_ckpt_path:\n        self.load_settings_from_checkpoint(self.initial_ckpt_path)\n    # 2. then initialize the model\n    model = self.model_cls(self)\n    # 3. Load weights from the checkpoint into the model\n    if self.initial_ckpt_path:\n        self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n    return model\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:\n    \"\"\"Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.\"\"\"\n    return self.loss_cls\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel","title":"<code>ExampleModel</code>","text":"<p>               Bases: <code>ExampleModelTrunk</code></p> <p>An example model.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModel(ExampleModelTrunk):\n    \"\"\"An example model.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig) -&gt; None:\n        \"\"\"Constructor of the model.\n\n        Args:\n            config: The config object is responsible for telling the strategy what model to create.\n        \"\"\"\n        super().__init__(config)\n        self.linear3 = nn.Linear(3, 64)\n        self.relu2 = nn.ReLU()\n        self.linear4 = nn.Linear(64, 28 * 28)\n\n    def forward(self, x: Tensor) -&gt; ExampleModelOutput:\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x: The input data.\n\n        Returns:\n            x_hat: The result of the last linear layer of the network.\n        \"\"\"\n        z: Tensor = super().forward(x)\n        x_hat = self.linear3(z)\n        x_hat = self.relu2(x_hat)\n        x_hat = self.linear4(x_hat)\n        return {\"x_hat\": x_hat, \"z\": z}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor of the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelParallelConfig</code> <p>The config object is responsible for telling the strategy what model to create.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: ModelParallelConfig) -&gt; None:\n    \"\"\"Constructor of the model.\n\n    Args:\n        config: The config object is responsible for telling the strategy what model to create.\n    \"\"\"\n    super().__init__(config)\n    self.linear3 = nn.Linear(3, 64)\n    self.relu2 = nn.ReLU()\n    self.linear4 = nn.Linear(64, 28 * 28)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>x_hat</code> <code>ExampleModelOutput</code> <p>The result of the last linear layer of the network.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, x: Tensor) -&gt; ExampleModelOutput:\n    \"\"\"Forward pass of the model.\n\n    Args:\n        x: The input data.\n\n    Returns:\n        x_hat: The result of the last linear layer of the network.\n    \"\"\"\n    z: Tensor = super().forward(x)\n    x_hat = self.linear3(z)\n    x_hat = self.relu2(x_hat)\n    x_hat = self.linear4(x_hat)\n    return {\"x_hat\": x_hat, \"z\": z}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelOutput","title":"<code>ExampleModelOutput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output for the example model implementation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModelOutput(TypedDict):\n    \"\"\"Output for the example model implementation.\"\"\"\n\n    x_hat: Tensor\n    z: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk","title":"<code>ExampleModelTrunk</code>","text":"<p>               Bases: <code>MegatronModule</code></p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModelTrunk(MegatronModule):\n    def __init__(self, config: ModelParallelConfig) -&gt; None:\n        \"\"\"Constructor of the model.\n\n        Args:\n            config: The config object is responsible for telling the strategy what model to create.\n        \"\"\"\n        super().__init__(config)\n        # FIXME add an assertion that the user is not trying to do tensor parallelism since this doesn't use\n        #  parallelizable megatron linear layers.\n        self.model_type: ModelType = ModelType.encoder_or_decoder\n        self.linear1 = nn.Linear(28 * 28, 64)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(64, 3)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # we could return a dictionary of strings to tensors here, but let's demonstrate this is not necessary\n        x = x.view(x.size(0), -1)\n        z = self.linear1(x)\n        z = self.relu(z)\n        z = self.linear2(z)\n        return z\n\n    def set_input_tensor(self, input_tensor: Optional[Tensor]) -&gt; None:\n        \"\"\"This _would_ be needed for model parallel and other kinds of more complicated forward passes in megatron.\"\"\"\n        pass\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor of the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelParallelConfig</code> <p>The config object is responsible for telling the strategy what model to create.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: ModelParallelConfig) -&gt; None:\n    \"\"\"Constructor of the model.\n\n    Args:\n        config: The config object is responsible for telling the strategy what model to create.\n    \"\"\"\n    super().__init__(config)\n    # FIXME add an assertion that the user is not trying to do tensor parallelism since this doesn't use\n    #  parallelizable megatron linear layers.\n    self.model_type: ModelType = ModelType.encoder_or_decoder\n    self.linear1 = nn.Linear(28 * 28, 64)\n    self.relu = nn.ReLU()\n    self.linear2 = nn.Linear(64, 3)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk.set_input_tensor","title":"<code>set_input_tensor(input_tensor)</code>","text":"<p>This would be needed for model parallel and other kinds of more complicated forward passes in megatron.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def set_input_tensor(self, input_tensor: Optional[Tensor]) -&gt; None:\n    \"\"\"This _would_ be needed for model parallel and other kinds of more complicated forward passes in megatron.\"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTCustomDataset","title":"<code>MNISTCustomDataset</code>","text":"<p>               Bases: <code>MNIST</code></p> <p>A Wrapper for the MNIST Dataset.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MNISTCustomDataset(MNIST):\n    \"\"\"A Wrapper for the MNIST Dataset.\"\"\"\n\n    def __getitem__(self, idx: int) -&gt; MnistItem:\n        \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict.\n\n        This is instead of a Tuple or tensor.\n\n        Args:\n            idx: The index we want to grab, an int.\n\n        Returns:\n            A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n        \"\"\"\n        data, label = super().__getitem__(idx)\n\n        return {\n            \"data\": data,\n            \"label\": label,\n            \"idx\": idx,\n        }\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTCustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Wraps the getitem method of the MNIST dataset such that we return a Dict.</p> <p>This is instead of a Tuple or tensor.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index we want to grab, an int.</p> required <p>Returns:</p> Type Description <code>MnistItem</code> <p>A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; MnistItem:\n    \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict.\n\n    This is instead of a Tuple or tensor.\n\n    Args:\n        idx: The index we want to grab, an int.\n\n    Returns:\n        A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n    \"\"\"\n    data, label = super().__getitem__(idx)\n\n    return {\n        \"data\": data,\n        \"label\": label,\n        \"idx\": idx,\n    }\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule","title":"<code>MNISTDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A Megatron Compatible Data Module for MNIST.</p> <p>Attributes: data_dir: data directory micro_batch_size: batch_size global_batch_size: global batch size max_len: maximal sequence length for megatron sampler rampup_batch_size: ramp up batch size num_workers: number of workers data_sampler: data_sampler set to be a megatron one</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MNISTDataModule(pl.LightningDataModule):\n    \"\"\"A Megatron Compatible Data Module for MNIST.\n\n    Attributes:\n    data_dir: data directory\n    micro_batch_size: batch_size\n    global_batch_size: global batch size\n    max_len: maximal sequence length for megatron sampler\n    rampup_batch_size: ramp up batch size\n    num_workers: number of workers\n    data_sampler: data_sampler set to be a megatron one\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str | os.PathLike = str(BIONEMO_CACHE_DIR),\n        batch_size: int = 32,\n        num_workers: int = 0,\n        global_batch_size: int | None = None,\n        output_log: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize class.\n\n        Args:\n            data_dir: data directory\n            batch_size: batch_size\n            global_batch_size: global batch size\n            num_workers: number of workers\n            output_log: whether to output logs\n\n        \"\"\"\n        super().__init__()\n        self.data_dir = data_dir\n        self.micro_batch_size = batch_size\n        self.global_batch_size = global_batch_size or batch_size\n        self.max_len = 1048\n        self.rampup_batch_size = None\n        self.num_workers = num_workers\n        #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n        # Wraps the datasampler with the MegatronDataSampler. The MegatronDataSampler is a wrapper that allows the sampler\n        # to be used with megatron. It sets up the capability to utilize micro-batching and gradient accumulation. It is also\n        # the place where the global batch size is constructed.\n        self.data_sampler = MegatronDataSampler(\n            seq_len=self.max_len,\n            micro_batch_size=self.micro_batch_size,\n            global_batch_size=self.global_batch_size,\n            rampup_batch_size=self.rampup_batch_size,\n            output_log=output_log,\n        )\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Sets up the datasets.\n\n        Args:\n            stage: can be one of train / test / predict.\n        \"\"\"\n        self.mnist_test = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(\n                MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=False)\n            ),\n            seed=43,\n            shuffle=False,\n        )\n        mnist_full = MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n        mnist_train, mnist_val = torch.utils.data.random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n        self.mnist_train = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(mnist_train), seed=44, shuffle=True\n        )\n\n        self.mnist_val = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(mnist_val),\n            seed=45,\n            shuffle=False,\n        )\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the training dataloader.\"\"\"\n        return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the validation dataloader.\"\"\"\n        return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the prediction dataloader.\"\"\"\n        return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.__init__","title":"<code>__init__(data_dir=str(BIONEMO_CACHE_DIR), batch_size=32, num_workers=0, global_batch_size=None, output_log=True)</code>","text":"<p>Initialize class.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str | PathLike</code> <p>data directory</p> <code>str(BIONEMO_CACHE_DIR)</code> <code>batch_size</code> <code>int</code> <p>batch_size</p> <code>32</code> <code>global_batch_size</code> <code>int | None</code> <p>global batch size</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>number of workers</p> <code>0</code> <code>output_log</code> <code>bool</code> <p>whether to output logs</p> <code>True</code> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(\n    self,\n    data_dir: str | os.PathLike = str(BIONEMO_CACHE_DIR),\n    batch_size: int = 32,\n    num_workers: int = 0,\n    global_batch_size: int | None = None,\n    output_log: bool = True,\n) -&gt; None:\n    \"\"\"Initialize class.\n\n    Args:\n        data_dir: data directory\n        batch_size: batch_size\n        global_batch_size: global batch size\n        num_workers: number of workers\n        output_log: whether to output logs\n\n    \"\"\"\n    super().__init__()\n    self.data_dir = data_dir\n    self.micro_batch_size = batch_size\n    self.global_batch_size = global_batch_size or batch_size\n    self.max_len = 1048\n    self.rampup_batch_size = None\n    self.num_workers = num_workers\n    #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n    # Wraps the datasampler with the MegatronDataSampler. The MegatronDataSampler is a wrapper that allows the sampler\n    # to be used with megatron. It sets up the capability to utilize micro-batching and gradient accumulation. It is also\n    # the place where the global batch size is constructed.\n    self.data_sampler = MegatronDataSampler(\n        seq_len=self.max_len,\n        micro_batch_size=self.micro_batch_size,\n        global_batch_size=self.global_batch_size,\n        rampup_batch_size=self.rampup_batch_size,\n        output_log=output_log,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns the prediction dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the prediction dataloader.\"\"\"\n    return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Sets up the datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>can be one of train / test / predict.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Sets up the datasets.\n\n    Args:\n        stage: can be one of train / test / predict.\n    \"\"\"\n    self.mnist_test = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(\n            MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=False)\n        ),\n        seed=43,\n        shuffle=False,\n    )\n    mnist_full = MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n    mnist_train, mnist_val = torch.utils.data.random_split(\n        mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n    )\n    self.mnist_train = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(mnist_train), seed=44, shuffle=True\n    )\n\n    self.mnist_val = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(mnist_val),\n        seed=45,\n        shuffle=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the training dataloader.\"\"\"\n    return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the validation dataloader.\"\"\"\n    return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction","title":"<code>MSELossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MSELossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: Dict[str, Tensor]) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        x = batch[\"data\"]\n        x_hat = forward_out[\"x_hat\"]\n        xview = x.view(x.size(0), -1).to(x_hat.dtype)\n        loss = nn.functional.mse_loss(x_hat, xview)\n\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: Dict[str, Tensor]) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    x = batch[\"data\"]\n    x_hat = forward_out[\"x_hat\"]\n    xview = x.view(x.size(0), -1).to(x_hat.dtype)\n    loss = nn.functional.mse_loss(x_hat, xview)\n\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction","title":"<code>MSEPlusClassifierLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MSEPlusClassifierLossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: ExampleFineTuneOutput) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        x = batch[\"data\"]\n        digits = batch[\"label\"]\n        x_hat = forward_out[\"x_hat\"]\n        digit_logits = forward_out[\"digit_logits\"]\n        xview = x.view(x.size(0), -1).to(x_hat.dtype)\n        mse_loss = nn.functional.mse_loss(x_hat, xview)\n        classifier_loss = nn.functional.cross_entropy(digit_logits, digits)\n        loss = classifier_loss + mse_loss\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>ExampleFineTuneOutput</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: ExampleFineTuneOutput) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    x = batch[\"data\"]\n    digits = batch[\"label\"]\n    x_hat = forward_out[\"x_hat\"]\n    digit_logits = forward_out[\"digit_logits\"]\n    xview = x.view(x.size(0), -1).to(x_hat.dtype)\n    mse_loss = nn.functional.mse_loss(x_hat, xview)\n    classifier_loss = nn.functional.cross_entropy(digit_logits, digits)\n    loss = classifier_loss + mse_loss\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MnistItem","title":"<code>MnistItem</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Training input for the MNIST dataset.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MnistItem(TypedDict):\n    \"\"\"Training input for the MNIST dataset.\"\"\"\n\n    data: Tensor\n    label: Tensor\n    idx: int\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.PretrainConfig","title":"<code>PretrainConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleModel', 'MSELossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>PretrainConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass PretrainConfig(ExampleGenericConfig[\"ExampleModel\", \"MSELossReduction\"], iom.IOMixinWithGettersSetters):\n    \"\"\"PretrainConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleModel] = ExampleModel\n    loss_cls: Type[MSELossReduction] = MSELossReduction\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.SameSizeLossDict","title":"<code>SameSizeLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class SameSizeLossDict(TypedDict):\n    \"\"\"This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.\"\"\"\n\n    avg: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/example_model/training_scripts/finetune_mnist/","title":"Finetune mnist","text":""},{"location":"API_reference/bionemo/example_model/training_scripts/finetune_mnist/#bionemo.example_model.training_scripts.finetune_mnist.run_finetune","title":"<code>run_finetune(checkpoint_dir, name, directory_name)</code>","text":"<p>Run the finetuning step.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory with the previous model</p> required <code>name</code> <code>str</code> <p>The experiment name.</p> required <code>directory_name</code> <code>str</code> <p>The directory to write the output</p> required <p>Returns:     str: the path of the trained model.</p> Source code in <code>bionemo/example_model/training_scripts/finetune_mnist.py</code> <pre><code>def run_finetune(checkpoint_dir: str, name: str, directory_name: str):\n    \"\"\"Run the finetuning step.\n\n    Args:\n        checkpoint_dir: The directory with the previous model\n        name: The experiment name.\n        directory_name: The directory to write the output\n    Returns:\n        str: the path of the trained model.\n    \"\"\"\n    save_dir = Path(directory_name) / \"classifier\"\n    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n        save_last=True,\n        save_on_train_epoch_end=True,\n        monitor=\"val_loss\",\n        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n    )\n\n    nemo_logger2 = NeMoLogger(\n        log_dir=str(save_dir),\n        name=name,\n        tensorboard=TensorBoardLogger(save_dir=save_dir, name=name),\n        ckpt=checkpoint_callback,\n        extra_loggers=[CSVLogger(save_dir / \"logs\", name=name)],\n    )\n\n    lightning_module2 = BionemoLightningModule(\n        config=ExampleFineTuneConfig(\n            initial_ckpt_path=checkpoint_dir,\n            initial_ckpt_skip_keys_with_these_prefixes={\"digit_classifier\"},\n        )\n    )\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        limit_val_batches=5,\n        val_check_interval=5,\n        max_steps=100,\n        max_epochs=10,\n        num_nodes=1,\n        log_every_n_steps=5,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n    llm.train(\n        model=lightning_module2,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger2,\n        resume=resume.AutoResume(\n            resume_if_exists=True,\n            resume_ignore_no_checkpoint=True,\n        ),\n    )\n    finetune_dir = Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n    return finetune_dir\n</code></pre>"},{"location":"API_reference/bionemo/example_model/training_scripts/predict_mnist/","title":"Predict mnist","text":""},{"location":"API_reference/bionemo/example_model/training_scripts/predict_mnist/#bionemo.example_model.training_scripts.predict_mnist.run_predict","title":"<code>run_predict(finetune_dir, test_length)</code>","text":"<p>Run the prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>finetune_dir</code> <code>str</code> <p>The directory with the previous step</p> required <code>test_length</code> <code>int</code> <p>The length of the test step.</p> required <p>Returns:</p> Name Type Description <code>tensor</code> <p>the outputs of the model.</p> Source code in <code>bionemo/example_model/training_scripts/predict_mnist.py</code> <pre><code>def run_predict(finetune_dir: str, test_length: int):\n    \"\"\"Run the prediction step.\n\n    Args:\n        finetune_dir: The directory with the previous step\n        test_length: The length of the test step.\n\n    Returns:\n        tensor: the outputs of the model.\n    \"\"\"\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    test_run_trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        num_nodes=1,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n\n    lightning_module3 = BionemoLightningModule(config=ExampleFineTuneConfig(initial_ckpt_path=finetune_dir))\n    new_data_module = MNISTDataModule(data_dir=str(BIONEMO_CACHE_DIR), batch_size=test_length, output_log=False)\n\n    results = test_run_trainer.predict(lightning_module3, datamodule=new_data_module)\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/example_model/training_scripts/pretrain_mnist/","title":"Pretrain mnist","text":""},{"location":"API_reference/bionemo/example_model/training_scripts/pretrain_mnist/#bionemo.example_model.training_scripts.pretrain_mnist.run_pretrain","title":"<code>run_pretrain(name, directory_name)</code>","text":"<p>Run the pretraining step.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The experiment name.</p> required <code>directory_name</code> <code>str</code> <p>The directory to write the output</p> required <p>Returns:     str: the path of the trained model.</p> Source code in <code>bionemo/example_model/training_scripts/pretrain_mnist.py</code> <pre><code>def run_pretrain(name: str, directory_name: str):\n    \"\"\"Run the pretraining step.\n\n    Args:\n        name: The experiment name.\n        directory_name: The directory to write the output\n    Returns:\n        str: the path of the trained model.\n    \"\"\"\n    # Setup the logger train the model\n    save_dir = Path(directory_name) / \"pretrain\"\n\n    nemo_logger = NeMoLogger(\n        log_dir=str(save_dir),\n        name=name,\n        tensorboard=TensorBoardLogger(save_dir=save_dir, name=name),\n        ckpt=checkpoint_callback,\n        extra_loggers=[CSVLogger(save_dir / \"logs\", name=name)],\n    )\n\n    # Set up the training module\n    lightning_module = BionemoLightningModule(config=PretrainConfig())\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        limit_val_batches=5,\n        val_check_interval=5,\n        max_steps=100,\n        max_epochs=10,\n        num_nodes=1,\n        log_every_n_steps=5,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n\n    # This trains the model\n    llm.train(\n        model=lightning_module,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n    return Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n</code></pre>"},{"location":"API_reference/bionemo/fw/dependency_graph/","title":"Dependency graph","text":""},{"location":"API_reference/bionemo/fw/dependency_graph/#bionemo.fw.dependency_graph.build_dependency_graph","title":"<code>build_dependency_graph(base_dir, directories)</code>","text":"<p>Build a dependency graph for all sub-packages.</p> Source code in <code>bionemo/fw/dependency_graph.py</code> <pre><code>def build_dependency_graph(base_dir, directories):\n    \"\"\"Build a dependency graph for all sub-packages.\"\"\"\n    pyproject_files = []\n    for directory in directories:\n        pyproject_files.append(base_dir / directory / \"pyproject.toml\")\n    dependency_graph = defaultdict(dict)\n\n    for pyproject_file in pyproject_files:\n        package_name, dependencies = parse_dependencies(pyproject_file)\n        if package_name:\n            dependency_graph[package_name] = dependencies\n\n    return dependency_graph\n</code></pre>"},{"location":"API_reference/bionemo/fw/dependency_graph/#bionemo.fw.dependency_graph.find_bionemo_subpackages","title":"<code>find_bionemo_subpackages(base_dir, directories)</code>","text":"<p>Find all unique <code>bionemo.&lt;name&gt;</code> imports in Python files within a directory.</p> Source code in <code>bionemo/fw/dependency_graph.py</code> <pre><code>def find_bionemo_subpackages(base_dir, directories):\n    \"\"\"Find all unique `bionemo.&lt;name&gt;` imports in Python files within a directory.\"\"\"\n    bionemo_import_pattern = re.compile(\n        r\"^\\s*(?:from|import)\\s+bionemo\\.([a-zA-Z_][a-zA-Z0-9_]*)(?:\\s+|\\.|$)\", re.MULTILINE\n    )\n    found_imports = {}\n    for dir_name in directories:\n        directory = base_dir / dir_name / \"src\"\n        subpackages = set()\n\n        for file_path in Path(directory).rglob(\"*.py\"):\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n                    matches = bionemo_import_pattern.findall(content)\n                    subpackages.update(matches)\n            except Exception as e:\n                print(f\"Error reading file {file_path}: {e}\")\n        full_subpackage_names = {f\"bionemo-{subpackage}\" for subpackage in subpackages}\n        if dir_name in full_subpackage_names:\n            full_subpackage_names.remove(dir_name)\n        found_imports[dir_name] = full_subpackage_names\n    return found_imports\n</code></pre>"},{"location":"API_reference/bionemo/fw/dependency_graph/#bionemo.fw.dependency_graph.parse_dependencies","title":"<code>parse_dependencies(pyproject_path)</code>","text":"<p>Parse dependencies from a pyproject.toml file.</p> Source code in <code>bionemo/fw/dependency_graph.py</code> <pre><code>def parse_dependencies(pyproject_path):\n    \"\"\"Parse dependencies from a pyproject.toml file.\"\"\"\n    with open(pyproject_path, \"r\") as f:\n        pyproject_data = toml.load(f)\n    dependencies = {}\n    package_name = None\n\n    # Extract package name\n    try:\n        package_name = pyproject_data[\"project\"][\"name\"]\n    except KeyError:\n        print(f\"Warning: Could not find package name in {pyproject_path}\")\n\n    # Extract dependencies\n    try:\n        deps = pyproject_data[\"project\"][\"dependencies\"]\n        if isinstance(deps, dict):  # If dependencies are a dictionary\n            for dep, version in deps.items():\n                if dep.startswith(\"bionemo-\"):\n                    dependencies[dep] = version  # Keep dependency with its version\n\n        elif isinstance(deps, list):  # If dependencies are a list\n            for dep in deps:\n                if dep.startswith(\"bionemo-\"):\n                    dependencies[dep] = \"unpinned\"\n    except KeyError:\n        print(f\"Warning: Could not find dependencies in {pyproject_path}\")\n\n    if \"tool\" in pyproject_data and \"maturin\" in pyproject_data[\"tool\"]:\n        dep = pyproject_data[\"tool\"][\"maturin\"][\"module-name\"]\n        if dep.startswith(\"bionemo.\"):\n            dependencies[dep.replace(\".\", \"-\")] = \"unpinned\"\n\n    return package_name, dependencies\n</code></pre>"},{"location":"API_reference/bionemo/fw/dependency_graph/#bionemo.fw.dependency_graph.parse_tach_toml","title":"<code>parse_tach_toml(toml_path)</code>","text":"<p>Parse dependencies from a tach.toml file.</p> Source code in <code>bionemo/fw/dependency_graph.py</code> <pre><code>def parse_tach_toml(toml_path):\n    \"\"\"Parse dependencies from a tach.toml file.\"\"\"\n    tach_toml_dependencies = {}\n    with open(toml_path, \"r\") as f:\n        toml_data = toml.load(f)\n        for module in toml_data[\"modules\"]:\n            tach_toml_dependencies[(module[\"path\"].replace(\".\", \"-\"))] = [\n                item.replace(\".\", \"-\") for item in module[\"depends_on\"]\n            ]\n    return tach_toml_dependencies\n</code></pre>"},{"location":"API_reference/bionemo/fw/dependency_graph/#bionemo.fw.dependency_graph.resolve_dependencies","title":"<code>resolve_dependencies(subpackage, toml_imports, resolved=None, seen=None)</code>","text":"<p>Recursively resolve all dependencies, including transitive ones.</p> Source code in <code>bionemo/fw/dependency_graph.py</code> <pre><code>def resolve_dependencies(subpackage, toml_imports, resolved=None, seen=None):\n    \"\"\"Recursively resolve all dependencies, including transitive ones.\"\"\"\n    if resolved is None:\n        resolved = set()\n    if seen is None:\n        seen = set()\n\n    if subpackage in seen:\n        return resolved  # Avoid circular dependencies\n    seen.add(subpackage)\n\n    for dep in toml_imports.get(subpackage, []):\n        resolved.add(dep)\n        if dep in toml_imports:  # Resolve further if it's a subpackage\n            resolve_dependencies(dep, toml_imports, resolved, seen)\n\n    return resolved\n</code></pre>"},{"location":"API_reference/bionemo/fw/dependency_graph/#bionemo.fw.dependency_graph.visualize_dependency_graph","title":"<code>visualize_dependency_graph(dependency_graph, filename)</code>","text":"<p>Visualize the dependency graph using NetworkX.</p> Source code in <code>bionemo/fw/dependency_graph.py</code> <pre><code>def visualize_dependency_graph(dependency_graph, filename):\n    \"\"\"Visualize the dependency graph using NetworkX.\"\"\"\n    G = nx.DiGraph()\n    edge_labels = {}\n\n    # Track all packages explicitly\n    all_packages = set(dependency_graph.keys())\n\n    for package, dependencies in dependency_graph.items():\n        if isinstance(dependencies, dict):\n            for dep, version in dependencies.items():\n                G.add_edge(dep, package)  # Add edge from package to dependency\n                edge_labels[(dep, package)] = version  # Label the edge with the version\n                all_packages.add(dep)\n        else:\n            for dep in dependencies:\n                G.add_edge(dep, package)  # Add edge from package to dependency\n                all_packages.add(dep)\n\n    # Ensure isolated nodes (without edges) are included in the graph\n    for package in all_packages:\n        if package not in G:\n            G.add_node(package)\n    # Use a circular layout, ensuring packages are evenly distributed\n    pos = nx.circular_layout(G)\n\n    plt.figure(figsize=(14, 10))\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        node_size=3000,\n        node_color=\"lightblue\",\n        font_size=10,\n        font_weight=\"bold\",\n        arrowsize=20,\n        edge_color=\"gray\",\n    )\n\n    # Draw edge labels for the dependency versions\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, font_color=\"red\")\n    plt.title(\"Dependency Graph\", fontsize=16)\n    plt.savefig(filename)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/","title":"Api","text":""},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.FineTuneSeqLenBioBertConfig","title":"<code>FineTuneSeqLenBioBertConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>BioBert fine-tuning sequence length model configuration.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>@dataclass\nclass FineTuneSeqLenBioBertConfig(\n    BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction],\n    iom.IOMixinWithGettersSetters,\n):\n    \"\"\"BioBert fine-tuning sequence length model configuration.\"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[MegatronBioBertFineTuneSeqLengthModel] = MegatronBioBertFineTuneSeqLengthModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n        \"\"\"Loss function type.\"\"\"\n        return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.FineTuneSeqLenBioBertConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Loss function type.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n    \"\"\"Loss function type.\"\"\"\n    return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.GeneformerConfig","title":"<code>GeneformerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[GeneformerModel, MegatronLossType]</code>, <code>IOMixinWithGettersSetters</code></p> <p>A geneformer config.</p> <p>The geneformer config overrides the parent config, and adds a leaf-level iomixin, please do not inherit from this directly, as your parameters will likely be reset to this method's parameters silently.</p> Source code in <code>bionemo/geneformer/api.py</code> <pre><code>@dataclass\nclass GeneformerConfig(BioBertConfig[GeneformerModel, MegatronLossType], iom.IOMixinWithGettersSetters):\n    \"\"\"A geneformer config.\n\n    The geneformer config overrides the parent config, and adds a leaf-level iomixin, please do not inherit from this\n    directly, as your parameters will likely be reset to this method's parameters silently.\n    \"\"\"\n\n    num_layers: int = 6\n    hidden_size: int = 256\n    ffn_hidden_size: int = 512\n    num_attention_heads: int = 4\n    seq_length: int = 2048\n    fp32_residual_connection: bool = False\n    # Dropout\n    attention_dropout: float = 0.1  # NeMo1 hard-coded, differs from publication of ReLU\n    hidden_dropout: float = 0.02\n    init_method_std: float = 0.02\n    apply_query_key_layer_scaling: bool = False\n    make_vocab_size_divisible_by: int = 128\n    fp16_lm_cross_entropy: bool = False\n    layernorm_zero_centered_gamma: bool = False\n    layernorm_epsilon: float = 1.0e-12\n    activation_func: Callable = F.gelu  # NeMo1 hard-coded, differes from publication of ReLU\n    qk_layernorm: bool = False\n    apply_residual_connection_post_layernorm: bool = False  # False is new default, True was BERT pub.\n    share_embeddings_and_output_weights: bool = True\n    # FUSION SETTINGS\n    parallel_output: bool = True\n    bias_dropout_fusion: bool = True\n    bias_activation_fusion: bool = True\n    masked_softmax_fusion: bool = True\n    persist_layer_norm: bool = True\n    get_attention_mask_from_fusion: bool = True\n\n    position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\"\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n    qk_layernorm: bool = False\n\n    enable_autocast: bool = False\n    model_cls: Type[GeneformerModel] = GeneformerModel\n    loss_reduction_class: Type[MegatronLossType] = BERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/preprocess/","title":"Preprocess","text":""},{"location":"API_reference/bionemo/geneformer/data/preprocess/#bionemo.geneformer.data.preprocess.ResourcePreprocessor","title":"<code>ResourcePreprocessor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface defining a ResourcePreprocessor. Implementors promise to provide both a complete RemoteResource and a freeform preprocess method. This interface can be used to generically define a workflow from a config file.</p> <pre><code>remote -&gt; prepare -&gt; prepared data.\n</code></pre> Source code in <code>bionemo/geneformer/data/preprocess.py</code> <pre><code>@dataclass\nclass ResourcePreprocessor(ABC):\n    \"\"\"Interface defining a ResourcePreprocessor. Implementors promise to provide both a complete RemoteResource and a freeform\n    preprocess method. This interface can be used to generically define a workflow from a config file.\n\n        remote -&gt; prepare -&gt; prepared data.\n    \"\"\"  # noqa: D205\n\n    root_directory: Optional[str] = field(default_factory=RemoteResource.get_env_tmpdir)\n    dest_directory: str = \"data\"\n\n    def get_checksums(self) -&gt; List[str]:  # noqa: D102\n        return [resource.checksum for resource in self.get_remote_resources()]\n\n    def get_urls(self) -&gt; List[str]:  # noqa: D102\n        return [resource.url for resource in self.get_remote_resources()]\n\n    @abstractmethod\n    def get_remote_resources(self) -&gt; List[RemoteResource]:\n        \"\"\"Gets the remote resources associated with this preparor.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def prepare(self) -&gt; List:\n        \"\"\"Returns a list of prepared filenames.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/preprocess/#bionemo.geneformer.data.preprocess.ResourcePreprocessor.get_remote_resources","title":"<code>get_remote_resources()</code>  <code>abstractmethod</code>","text":"<p>Gets the remote resources associated with this preparor.</p> Source code in <code>bionemo/geneformer/data/preprocess.py</code> <pre><code>@abstractmethod\ndef get_remote_resources(self) -&gt; List[RemoteResource]:\n    \"\"\"Gets the remote resources associated with this preparor.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/preprocess/#bionemo.geneformer.data.preprocess.ResourcePreprocessor.prepare","title":"<code>prepare()</code>  <code>abstractmethod</code>","text":"<p>Returns a list of prepared filenames.</p> Source code in <code>bionemo/geneformer/data/preprocess.py</code> <pre><code>@abstractmethod\ndef prepare(self) -&gt; List:\n    \"\"\"Returns a list of prepared filenames.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/datamodule/#bionemo.geneformer.data.singlecell.datamodule.SingleCellDataModule","title":"<code>SingleCellDataModule</code>","text":"<p>               Bases: <code>MegatronDataModule</code></p> <p>LightningDataModule wrapper of <code>SingleCellDataset</code></p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Union[str, PosixPath]</code> <p>Path to preprocessed single-cell data files</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Maps gene names to ids and vice-versa</p> required <code>collator</code> <p>Used to batch samples</p> required <code>process_item</code> <p>Function defining how each item should be processed</p> required <code>num_workers</code> <code>int</code> <p>Number of workers to use</p> <code>10</code> <code>num_mask_per_sample</code> <code>int</code> <p>Number of masked versions of a single sample to be returned by each worker</p> required <code>train_batch_size</code> <code>int</code> <p>Batch size for training</p> required <code>val_batch_size</code> <code>int</code> <p>Batch size for validation</p> required <code>include_unrecognized_vocab_in_dataset</code> <code>bool</code> <p>If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Config</code> <p>Configuration object</p> <code>data_path</code> <code>Union[str, PosixPath]</code> <p>Path to preprocessed single-cell data files</p> <code>median_dict</code> <code>dict</code> <p>Dictionary containing median values</p> <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object</p> <code>setup_called</code> <code>bool</code> <p>Flag indicating if the setup method has been called</p> <code>dataset</code> <code>SingleCellDataset</code> <p>Single-cell dataset object</p> Source code in <code>bionemo/geneformer/data/singlecell/datamodule.py</code> <pre><code>class SingleCellDataModule(MegatronDataModule):\n    \"\"\"LightningDataModule wrapper of `SingleCellDataset`\n\n    Args:\n        data_path (Union[str, PosixPath]): Path to preprocessed single-cell data files\n        tokenizer (Tokenizer): Maps gene names to ids and vice-versa\n        collator: Used to batch samples\n        process_item: Function defining how each item should be processed\n        num_workers (int): Number of workers to use\n        num_mask_per_sample (int): Number of masked versions of a single sample to be returned by each worker\n        train_batch_size (int): Batch size for training\n        val_batch_size (int): Batch size for validation\n        include_unrecognized_vocab_in_dataset (bool, optional): If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.\n\n    Attributes:\n        cfg (Config): Configuration object\n        data_path (Union[str, PosixPath]): Path to preprocessed single-cell data files\n        median_dict (dict): Dictionary containing median values\n        tokenizer (Tokenizer): Tokenizer object\n        setup_called (bool): Flag indicating if the setup method has been called\n        dataset (SingleCellDataset): Single-cell dataset object\n\n    \"\"\"  # noqa: D415\n\n    # Nothing says we cant pass in the dataset...\n    def __init__(  # noqa: D107\n        self,\n        tokenizer: Tokenizer,\n        median_dict: dict[str, float],\n        train_dataset_path: str | Path | None = None,\n        val_dataset_path: str | Path | None = None,\n        test_dataset_path: str | Path | None = None,\n        predict_dataset_path: str | Path | None = None,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,  # 80% mask token\n        random_token_prob: float = 0.1,  # 10% random token, remaining 1-(mask+random) will be identity.\n        seq_length: int = 2048,\n        micro_batch_size: int = 4,\n        global_batch_size: int = 8,\n        rampup_batch_size: Optional[List[int]] = None,\n        seed: int = 42,\n        num_workers: int = 10,  # TODO can this be automatically set?\n        persistent_workers: bool = True,\n        pin_memory: bool = True,\n        include_unrecognized_vocab_in_dataset: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        if predict_dataset_path is None:\n            assert (\n                train_dataset_path is not None and val_dataset_path is not None and test_dataset_path is not None\n            ), \"Provide either predict_dataset_path or (train_dataset_path, val_dataset_path, and test_dataset_path)\"\n        elif train_dataset_path is None:\n            assert (\n                val_dataset_path is None and test_dataset_path is None\n            ), \"Provide either predict_dataset_path or (train_dataset_path, val_dataset_path, and test_dataset_path)\"\n            assert (\n                predict_dataset_path is not None\n            ), \"Provide either predict_dataset_path or (train_dataset_path, val_dataset_path, and test_dataset_path)\"\n        self.data_path_predict = predict_dataset_path\n        self.data_path_train = train_dataset_path\n        self.data_path_val = val_dataset_path\n        self.data_path_test = test_dataset_path\n        self.tokenizer = tokenizer\n        self.median_dict = median_dict\n        self.max_len = seq_length\n        self.mask_prob = mask_prob\n        self.mask_token_prob = mask_token_prob\n        self.random_token_prob = random_token_prob\n        self.seed = seed\n        self.num_workers = num_workers\n        self.persistent_workers = persistent_workers\n        self.pin_memory = pin_memory\n\n        rng = np.random.default_rng(seed)\n        if self.data_path_train is not None:\n            assert self.data_path_val is not None and self.data_path_test is not None\n            self._train_dataset_ori = SingleCellDataset(\n                self.data_path_train,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n                include_unrecognized_vocab_in_dataset=include_unrecognized_vocab_in_dataset,\n            )\n            self._val_dataset_ori = SingleCellDataset(\n                self.data_path_val,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n                include_unrecognized_vocab_in_dataset=include_unrecognized_vocab_in_dataset,\n            )\n            self._test_dataset_ori = SingleCellDataset(\n                self.data_path_test,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n                include_unrecognized_vocab_in_dataset=include_unrecognized_vocab_in_dataset,\n            )\n            self._predict_dataset_ori = None\n        else:\n            assert self.data_path_predict is not None\n            self._predict_dataset_ori = SingleCellDataset(\n                self.data_path_predict,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n                include_unrecognized_vocab_in_dataset=include_unrecognized_vocab_in_dataset,\n            )\n            self._train_dataset_ori = None\n            self._val_dataset_ori = None\n            self._test_dataset_ori = None\n\n        # This is needed here, or you need to specify it in the megatron adapter thing TODO name?\n        #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n        if self.data_path_predict is not None:\n            n_predict = len(self._predict_dataset_ori)\n            self.data_sampler = MegatronDataSampler(\n                seq_len=self.max_len,\n                micro_batch_size=min(micro_batch_size, n_predict),\n                global_batch_size=min(global_batch_size, n_predict),\n                rampup_batch_size=rampup_batch_size,\n                output_log=False,  # this is needed for predict step to work\n            )\n        else:\n            self.data_sampler = MegatronDataSampler(\n                seq_len=self.max_len,\n                micro_batch_size=micro_batch_size,\n                global_batch_size=global_batch_size,\n                rampup_batch_size=rampup_batch_size,\n            )\n\n    def setup(self, stage: str = \"\") -&gt; None:  # noqa: D102\n        assert getattr(self, \"trainer\", None) is not None, \"Please only call setup after trainer is attached.\"\n\n        if self._train_dataset_ori is not None:\n            assert self._val_dataset_ori is not None and self._test_dataset_ori is not None\n            # Trainer API\n            max_train_steps = self.trainer.max_steps\n            if self.trainer.max_epochs &gt; 1:\n                logging.warning(\n                    \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n                )\n            assert max_train_steps &gt; 0, \"Please specify trainer.max_steps\"\n\n            num_train_samples = int(max_train_steps * self.data_sampler.global_batch_size)\n\n            # This happens exactly once during setup.\n            self._train_ds = MultiEpochDatasetResampler(\n                self._train_dataset_ori,\n                num_samples=num_train_samples,\n                shuffle=True,\n                seed=self.seed,\n            )\n            if self.trainer.limit_val_batches == 0:  # disable validation\n                logging.info(\"Skip creating validation dataset because trainer.limit_val_batches=0.\")\n            else:\n                num_val_samples = infer_num_samples(\n                    limit_batches=self.trainer.limit_val_batches,\n                    num_samples_in_dataset=len(self._val_dataset_ori),\n                    global_batch_size=self.data_sampler.global_batch_size,\n                    stage=\"val\",\n                )\n                self._validation_ds = MultiEpochDatasetResampler(\n                    self._val_dataset_ori,\n                    num_samples=num_val_samples,\n                    shuffle=False,\n                    seed=self.seed,\n                )\n            if self.trainer.limit_test_batches == 0:  # disable testing\n                logging.info(\"Skip creating test dataset because trainer.limit_test_batches=0.\")\n\n            else:\n                num_test_samples = infer_num_samples(\n                    limit_batches=self.trainer.limit_test_batches,\n                    num_samples_in_dataset=len(self._test_dataset_ori),\n                    global_batch_size=self.data_sampler.global_batch_size,\n                    stage=\"test\",\n                )\n                self._test_ds = MultiEpochDatasetResampler(\n                    self._test_dataset_ori,\n                    num_samples=num_test_samples,\n                    shuffle=False,\n                    seed=self.seed,\n                )\n        else:\n            assert self._predict_dataset_ori is not None\n            self._predict_ds = MultiEpochDatasetResampler(\n                self._predict_dataset_ori,\n                shuffle=False,\n                seed=self.seed,\n            )\n\n    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._train_ds, mode=\"train\")\n\n    def val_dataloader(self) -&gt; EVAL_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._validation_ds, mode=\"validation\")\n\n    def test_dataloader(self) -&gt; EVAL_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._test_ds, mode=\"test\")\n\n    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._predict_ds, mode=\"predict\", drop_last=False)\n\n    def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n        \"\"\"Create dataloader for train, validation, and test stages.\n\n        Args:\n            dataset: The dataset to create the dataloader for.\n            mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n            **kwargs: Additional arguments to pass to the dataloader.\n        \"\"\"\n        self.update_init_global_step()\n        return WrappedDataLoader(\n            mode=mode,\n            dataset=dataset,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            persistent_workers=self.persistent_workers,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=self.tokenizer.token_to_id(GeneTokenizer.pad_token),\n                min_length=self.max_len,\n                max_length=self.max_len,\n            ),\n            **kwargs,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/datamodule/#bionemo.geneformer.data.singlecell.datamodule.SingleCellDataModule._create_dataloader","title":"<code>_create_dataloader(dataset, mode, **kwargs)</code>","text":"<p>Create dataloader for train, validation, and test stages.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>The dataset to create the dataloader for.</p> required <code>mode</code> <code>Mode</code> <p>Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).</p> required <code>**kwargs</code> <p>Additional arguments to pass to the dataloader.</p> <code>{}</code> Source code in <code>bionemo/geneformer/data/singlecell/datamodule.py</code> <pre><code>def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n    \"\"\"Create dataloader for train, validation, and test stages.\n\n    Args:\n        dataset: The dataset to create the dataloader for.\n        mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n        **kwargs: Additional arguments to pass to the dataloader.\n    \"\"\"\n    self.update_init_global_step()\n    return WrappedDataLoader(\n        mode=mode,\n        dataset=dataset,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        persistent_workers=self.persistent_workers,\n        collate_fn=functools.partial(\n            collate.bert_padding_collate_fn,\n            padding_value=self.tokenizer.token_to_id(GeneTokenizer.pad_token),\n            min_length=self.max_len,\n            max_length=self.max_len,\n        ),\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/","title":"Dataset","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset.SingleCellDataset","title":"<code>SingleCellDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for single-cell pre-training. These can be generated using the sc_memmap.py script. Future updates will contain more comprehensive workflows for generating a Sparse Memmap from scRNA-seq.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path where the single cell files are stored in SingleCell Memmap format. It should contain the following files: - <code>metadata.json</code>: Path containing the number of rows int he dataset. - Gene expression matrix stored in CSR format as <code>numpy.memmap</code>:     - <code>data.npy</code>: Non-zero gene expression values.     - <code>col_ptr.npy</code>: Indices of the corresponding genes for each entry in data.npy.     - <code>row_ptr.npy</code>: Column index pointers for each cell sample.</p> required <code>tokenizer</code> <code>Any</code> <p>The tokenizer to use for tokenizing the input data.</p> required <code>median_dict</code> <code>dict</code> <p>A dictionary containing median values for each gene. Defaults to None.</p> <code>None</code> <code>max_len</code> <code>int</code> <p>The maximum length of the input sequence. Defaults to 1024.</p> <code>1024</code> <code>include_unrecognized_vocab_in_dataset</code> <code>bool</code> <p>If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Path where the single cell files are stored in SCDL memmap format.</p> <code>max_len</code> <code>int</code> <p>The maximum length of the input sequence.</p> <code>metadata</code> <code>dict</code> <p>Metadata loaded from <code>metadata.json</code>.</p> <code>gene_medians</code> <code>dict</code> <p>A dictionary containing median values for each gene. If None, a median of '1' is assumed for all genes.</p> <code>num_train</code> <code>int</code> <p>The number of samples in the training split.</p> <code>num_val</code> <code>int</code> <p>The number of samples in the validation split.</p> <code>num_test</code> <code>int</code> <p>The number of samples in the test split.</p> <code>index_offset</code> <code>int</code> <p>The offset to apply to the indices.</p> <code>length</code> <code>int</code> <p>The total number of samples in the dataset.</p> <code>gene_data</code> <code>memmap</code> <p>Gene expression values stored in CSR format.</p> <code>gene_data_indices</code> <code>memmap</code> <p>Gene indices associated with gene values.</p> <code>gene_data_ptr</code> <code>memmap</code> <p>Column indices for each sample.</p> <code>tokenizer</code> <p>The tokenizer used for tokenizing the input data.</p> <code>dataset_ccum</code> <code>ndarray</code> <p>Cumulative sum of row counts to map row indices to dataset id.</p> <code>dataset_map</code> <code>dict</code> <p>Mapping of dataset id to dataset name.</p> <p>Methods:</p> Name Description <code>__len__</code> <p>Returns the length of the dataset.</p> <code>__getitem__</code> <p>Returns the item at the given index.</p> See Also <p>bionemo/data/singlecell/sc_memmap.py - creates the artifacts required for instantiating a singlecell dataset from hdf5 files.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>class SingleCellDataset(Dataset):\n    \"\"\"A dataset class for single-cell pre-training. These can be generated using the sc_memmap.py script. Future\n    updates will contain more comprehensive workflows for generating a Sparse Memmap from scRNA-seq.\n\n    Args:\n        data_path (str): Path where the single cell files are stored in SingleCell Memmap format. It should contain the following files:\n            - `metadata.json`: Path containing the number of rows int he dataset.\n            - Gene expression matrix stored in CSR format as `numpy.memmap`:\n                - `data.npy`: Non-zero gene expression values.\n                - `col_ptr.npy`: Indices of the corresponding genes for each entry in data.npy.\n                - `row_ptr.npy`: Column index pointers for each cell sample.\n        tokenizer: The tokenizer to use for tokenizing the input data.\n        median_dict (dict, optional): A dictionary containing median values for each gene. Defaults to None.\n        max_len (int, optional): The maximum length of the input sequence. Defaults to 1024.\n        include_unrecognized_vocab_in_dataset (bool, optional): If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.\n\n    Attributes:\n        data_path (str): Path where the single cell files are stored in SCDL memmap format.\n        max_len (int): The maximum length of the input sequence.\n        metadata (dict): Metadata loaded from `metadata.json`.\n        gene_medians (dict): A dictionary containing median values for each gene. If None, a median of '1' is assumed for all genes.\n        num_train (int): The number of samples in the training split.\n        num_val (int): The number of samples in the validation split.\n        num_test (int): The number of samples in the test split.\n        index_offset (int): The offset to apply to the indices.\n        length (int): The total number of samples in the dataset.\n        gene_data (numpy.memmap): Gene expression values stored in CSR format.\n        gene_data_indices (numpy.memmap): Gene indices associated with gene values.\n        gene_data_ptr (numpy.memmap): Column indices for each sample.\n        tokenizer: The tokenizer used for tokenizing the input data.\n        dataset_ccum (numpy.ndarray): Cumulative sum of row counts to map row indices to dataset id.\n        dataset_map (dict): Mapping of dataset id to dataset name.\n\n    Methods:\n        __len__(): Returns the length of the dataset.\n        __getitem__(idx): Returns the item at the given index.\n\n    See Also:\n        bionemo/data/singlecell/sc_memmap.py - creates the artifacts required for instantiating a singlecell dataset from hdf5 files.\n    \"\"\"  # noqa: D205\n\n    def __init__(  # noqa: D107\n        self,\n        data_path: str | Path,\n        tokenizer: Any,\n        median_dict: Optional[dict] = None,\n        max_len: int = 1024,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,\n        random_token_prob: float = 0.1,\n        prepend_cls_token: bool = True,\n        eos_token: int | None = None,\n        include_unrecognized_vocab_in_dataset: bool = False,\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        super().__init__()\n\n        self.data_path = data_path\n        self.max_len = max_len\n        self.random_token_prob = random_token_prob\n        self.mask_token_prob = mask_token_prob\n        self.mask_prob = mask_prob\n        self.prepend_cls_token = prepend_cls_token\n        self._seed = seed\n        self.eos_token = eos_token\n\n        self.scdl = SingleCellMemMapDataset(str(data_path))\n        self.length = len(self.scdl)\n        # - median dict\n        self.gene_medians = median_dict\n        self.tokenizer = tokenizer\n        self.include_unrecognized_vocab_in_dataset = include_unrecognized_vocab_in_dataset\n\n    def __len__(self):  # noqa: D105\n        return self.length\n\n    def __getitem__(self, index: EpochIndex) -&gt; types.BertSample:\n        \"\"\"Performs a lookup and the required transformation for the model.\"\"\"\n        rng = np.random.default_rng([self._seed, index.epoch, index.idx])\n        values, feature_ids = self.scdl.get_row(index.idx, return_features=True, feature_vars=[\"feature_id\"])\n        assert (\n            len(feature_ids) == 1\n        )  # we expect feature_ids to be a list containing one np.array with the row's feature ids\n        gene_data, col_idxs = np.array(values[0]), np.array(values[1])\n        if len(gene_data) == 0:\n            raise ValueError(\n                \"SingleCellMemap data provided is invalid; the gene expression data parsed for the specified index is empty.\"\n            )\n        return process_item(\n            gene_data,\n            col_idxs,\n            feature_ids[0],\n            self.tokenizer,\n            gene_median=self.gene_medians,\n            rng=rng,\n            max_len=self.max_len,\n            mask_token_prob=self.mask_token_prob,\n            mask_prob=self.mask_prob,\n            random_token_prob=self.random_token_prob,\n            prepend_cls_token=self.prepend_cls_token,\n            eos_token=self.eos_token,\n            include_unrecognized_vocab_in_dataset=self.include_unrecognized_vocab_in_dataset,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset.SingleCellDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Performs a lookup and the required transformation for the model.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>def __getitem__(self, index: EpochIndex) -&gt; types.BertSample:\n    \"\"\"Performs a lookup and the required transformation for the model.\"\"\"\n    rng = np.random.default_rng([self._seed, index.epoch, index.idx])\n    values, feature_ids = self.scdl.get_row(index.idx, return_features=True, feature_vars=[\"feature_id\"])\n    assert (\n        len(feature_ids) == 1\n    )  # we expect feature_ids to be a list containing one np.array with the row's feature ids\n    gene_data, col_idxs = np.array(values[0]), np.array(values[1])\n    if len(gene_data) == 0:\n        raise ValueError(\n            \"SingleCellMemap data provided is invalid; the gene expression data parsed for the specified index is empty.\"\n        )\n    return process_item(\n        gene_data,\n        col_idxs,\n        feature_ids[0],\n        self.tokenizer,\n        gene_median=self.gene_medians,\n        rng=rng,\n        max_len=self.max_len,\n        mask_token_prob=self.mask_token_prob,\n        mask_prob=self.mask_prob,\n        random_token_prob=self.random_token_prob,\n        prepend_cls_token=self.prepend_cls_token,\n        eos_token=self.eos_token,\n        include_unrecognized_vocab_in_dataset=self.include_unrecognized_vocab_in_dataset,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset._gather_medians","title":"<code>_gather_medians(gene_names, gene_data, normalize, vocab, gene_median, include_unrecognized_vocab_in_dataset=False)</code>","text":"<p>Filter out genes that are not in the provided tokenizer vocab, and tokenize the gene names.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>def _gather_medians(\n    gene_names: np.ndarray,\n    gene_data: np.ndarray,\n    normalize: bool,\n    vocab: dict[str, int],\n    gene_median: dict[str, float],\n    include_unrecognized_vocab_in_dataset: bool = False,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Filter out genes that are not in the provided tokenizer vocab, and tokenize the gene names.\"\"\"\n    genes, tokens, medians = [], [], []\n    for tok, gene in zip(gene_names, gene_data):\n        if tok in vocab:\n            tokens.append(vocab[tok])\n            genes.append(gene)\n            if normalize:\n                med = gene_median[tok]  # If not in the dictionary we default to no normalization (1)\n                medians.append(med)\n        elif include_unrecognized_vocab_in_dataset:\n            raise ValueError(f\"Provided gene identifier, {str(tok)}, is not in the tokenizer vocab.\")\n    return np.asarray(genes), np.asarray(tokens), np.asarray(medians)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset.process_item","title":"<code>process_item(gene_data, gene_idxs, feature_ids, tokenizer, gene_median, rng, max_len=1024, mask_prob=0.15, mask_token_prob=0.8, random_token_prob=0.1, target_sum=10000, normalize=True, prepend_cls_token=True, eos_token=None, include_unrecognized_vocab_in_dataset=False)</code>","text":"<p>Process a single item in the dataset.</p> <p>Optionally performs median normalization and rank ordering. The tokenizers CLS token is added to the beginning of every sample. Converts gene names to ensemble ids before tokenizing. Expects gene_medians to contain ensembl ids as keys.</p> <p>Parameters:</p> Name Type Description Default <code>gene_data</code> <code>list</code> <p>List of gene data, these are expression counts.</p> required <code>gene_idxs</code> <code>list</code> <p>List of gene indices, these are keys in 'metadata['feature_ids']' and corresponding the CSR entry.</p> required <code>feature_ids</code> <code>list</code> <p>Feature ids for the full dataset.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object.</p> required <code>gene_median</code> <code>optional(dict</code> <p>Dictionary of gene medians. Defaults to None. Expects ensembl IDs to be keys.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator to ensure deterministic results.</p> required <code>max_len</code> <code>int</code> <p>Maximum length of the item. Defaults to 1024. Applies padding to any sequence shorter than max_len and truncates any sequence longer than max_len.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>Probability of masking a token. Defaults to 0.15.</p> <code>0.15</code> <code>target_sum</code> <code>int</code> <p>Target sum for normalization. Defaults to 10000.</p> <code>10000</code> <code>normalize</code> <code>bool</code> <p>Flag to normalize the gene data. Defaults to True. When set, this re-orders the gene tokens by their median expression value.</p> <code>True</code> <code>probabilistic_dirichlet_sampling</code> <code>bool</code> <p>Flag to enable probabilistic dirichlet sampling. Defaults to False.</p> required <code>dirichlet_alpha</code> <code>float</code> <p>Alpha value for dirichlet sampling if set by <code>probabilistic_dirichlet_sampling</code>. Defaults to 0.5.</p> required <code>same_length</code> <code>bool</code> <p>when true, sample the same length of genes as you originally had before the dirichlet sampler.</p> required <code>recompute_globals</code> <code>bool</code> <p>when true, global arrays are always recomputed. this is only useful for testing.</p> required <code>include_unrecognized_vocab_in_dataset</code> <code>bool</code> <p>If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>BertSample</code> <p>Processed item dictionary.</p> this method is very important and very useful. To generalize thiswwe should add an abstraction for <p>Datasets that have some kind of functor transformation.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>def process_item(  # noqa: D417\n    gene_data: np.ndarray,\n    gene_idxs: np.ndarray,\n    feature_ids: np.ndarray,\n    tokenizer: GeneTokenizer,\n    gene_median: dict,\n    rng: np.random.Generator,\n    max_len: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    random_token_prob: float = 0.1,\n    target_sum: int = 10000,\n    normalize: bool = True,\n    prepend_cls_token: bool = True,\n    eos_token: None | int = None,\n    include_unrecognized_vocab_in_dataset: bool = False,\n) -&gt; types.BertSample:\n    \"\"\"Process a single item in the dataset.\n\n    Optionally performs median normalization and rank ordering. The tokenizers CLS token is added to the beginning\n    of every sample. Converts gene names to ensemble ids before tokenizing. Expects gene_medians to contain ensembl ids as keys.\n\n    Args:\n        gene_data (list): List of gene data, these are expression counts.\n        gene_idxs (list): List of gene indices, these are keys in 'metadata['feature_ids']' and corresponding the CSR entry.\n        feature_ids (list): Feature ids for the full dataset.\n        tokenizer (Tokenizer): Tokenizer object.\n        gene_median (optional(dict)): Dictionary of gene medians. Defaults to None. Expects ensembl IDs to be keys.\n        rng: Random number generator to ensure deterministic results.\n        max_len (int): Maximum length of the item. Defaults to 1024. Applies padding to any sequence shorter than max_len and truncates any sequence longer than max_len.\n        mask_prob (float): Probability of masking a token. Defaults to 0.15.\n        target_sum (int): Target sum for normalization. Defaults to 10000.\n        normalize (bool): Flag to normalize the gene data. Defaults to True.\n            When set, this re-orders the gene tokens by their median expression value.\n        probabilistic_dirichlet_sampling (bool): Flag to enable probabilistic dirichlet sampling. Defaults to False.\n        dirichlet_alpha (float): Alpha value for dirichlet sampling if set by `probabilistic_dirichlet_sampling`. Defaults to 0.5.\n        same_length (bool): when true, sample the same length of genes as you originally had before the dirichlet sampler.\n        recompute_globals (bool): when true, global arrays are always recomputed. this is only useful for testing.\n        include_unrecognized_vocab_in_dataset (bool, optional): If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.\n\n    Returns:\n        dict: Processed item dictionary.\n\n    NOTE: this method is very important and very useful. To generalize thiswwe should add an abstraction for\n        Datasets that have some kind of functor transformation.\n    \"\"\"\n    if max_len &lt; 1:\n        raise ValueError(f\"max_len must be greater than 1, {max_len=}\")\n\n    if gene_median is None:\n        raise ValueError(\"gene_median must be provided for this tokenizer\")\n\n    if prepend_cls_token:\n        max_len = max_len - 1  # - minus 1 for [CLS] token\n    if eos_token is not None:\n        max_len = max_len - 1  # - minus 1 for [EOS] token\n\n    gene_names = feature_ids[gene_idxs]\n\n    gene_expression_cell, token_ids, gene_expression_medians = _gather_medians(\n        gene_names,\n        gene_data,\n        normalize,\n        tokenizer.vocab,\n        gene_median,\n        include_unrecognized_vocab_in_dataset=include_unrecognized_vocab_in_dataset,\n    )\n\n    if normalize:\n        # re-order according to expression median normalized rank. descending order.\n\n        gene_expression_cell = gene_expression_cell / gene_expression_cell.sum() * target_sum\n        gene_expression_cell = gene_expression_cell / gene_expression_medians.astype(float)\n        idxs = np.argsort(\n            -gene_expression_cell\n        )  # sort in descending order so that the 0th position is the highest value.\n        gene_expression_cell = gene_expression_cell[idxs]\n        token_ids = token_ids[idxs]\n\n    # - select max_len subset, set sample to false so it doesnt permute the already rank ordered expression values.\n    token_ids = sample_or_truncate(token_ids, max_len, sample=False)\n    with torch.no_grad(), torch.device(\"cpu\"):\n        masked_tokens, labels, loss_mask = masking.apply_bert_pretraining_mask(\n            tokenized_sequence=torch.from_numpy(token_ids),\n            random_seed=int(random_utils.get_seed_from_rng(rng)),\n            mask_config=masking.BertMaskConfig(\n                tokenizer=tokenizer,\n                random_tokens=range(len(tokenizer.special_tokens), len(tokenizer.vocab)),\n                mask_prob=mask_prob,\n                mask_token_prob=mask_token_prob,\n                random_token_prob=random_token_prob,\n            ),\n        )\n        cls_token = tokenizer.token_to_id(tokenizer.cls_token) if prepend_cls_token else None\n        if cls_token is not None or eos_token is not None:\n            masked_tokens, labels, loss_mask = masking.add_cls_and_eos_tokens(\n                sequence=masked_tokens,\n                labels=labels,\n                loss_mask=loss_mask,\n                cls_token=cls_token,\n                eos_token=eos_token,\n            )\n\n        # NeMo megatron assumes this return structure.\n        return {\n            \"text\": masked_tokens,\n            \"types\": torch.zeros_like(masked_tokens, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(masked_tokens, dtype=torch.int64),\n            \"labels\": labels,\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(masked_tokens, dtype=torch.int64),\n        }\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/","title":"Preprocess","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess","title":"<code>GeneformerPreprocess</code>","text":"Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>class GeneformerPreprocess:  # noqa: D101\n    def __init__(self, download_directory: Path, medians_file_path: Path, tokenizer_vocab_path: Path):\n        \"\"\"Downloads HGNC symbols\n\n        preproc_dir (str): Directory to store the reference preproc in\n        tokenizer_vocab_path (str): Filepath to store the tokenizer vocab\n        dataset_conf (OmegaConf): has 'train', 'val', 'test' keys containing\n            the names of preprocessed train/val/test files to use for training.\n        \"\"\"  # noqa: D415\n        self.download_directory = download_directory\n        self.medians_file_path = medians_file_path\n        self.tokenizer_vocab_path = tokenizer_vocab_path\n        self._validate_tokenizer_args(\n            self.tokenizer_vocab_path,\n        )\n\n    def build_and_save_tokenizer(self, median_dict, gene_to_ens, vocab_output_name):\n        \"\"\"Builds the GeneTokenizer using the median dictionary\n        then serializes and saves the dictionary to disk.\n        \"\"\"  # noqa: D205\n        tokenizer = GeneTokenizer.from_medians_and_genes_dicts(median_dict, gene_to_ens)\n        tokenizer.save_vocab(vocab_output_name)\n        return tokenizer\n\n    def _validate_tokenizer_args(self, vocab_output_name):\n        vocab_exists = os.path.exists(vocab_output_name)\n        if vocab_exists:\n            logging.warning(f\"Tokenizer vocab file: {vocab_output_name} already exists. Overwriting...\")\n\n    def preprocess(self) -&gt; dict[Literal[\"tokenizer\", \"median_dict\"], Any]:\n        \"\"\"Preprocesses for the Geneformer model\"\"\"  # noqa: D415\n        gene_name_dict_fn, gene_median_dict_fn = GeneformerResourcePreprocessor(\n            dest_directory=self.download_directory,\n        ).prepare()\n\n        # Load artifacts\n        with open(gene_name_dict_fn, \"rb\") as fd:\n            gene_ens = pickle.load(fd)\n\n        with open(gene_median_dict_fn, \"rb\") as fd:\n            median_dict = pickle.load(fd)\n\n        # Save converted artifacts to JSON to prevent pickle issues.\n        medians_dir = os.path.dirname(self.medians_file_path)\n        if not os.path.exists(medians_dir):\n            os.makedirs(medians_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n        with open(self.medians_file_path, \"w\") as fp:\n            json.dump(median_dict, fp)\n\n        if self.tokenizer_vocab_path is not None:\n            tokenizer = self.build_and_save_tokenizer(\n                median_dict,\n                gene_ens,\n                self.tokenizer_vocab_path,\n            )\n        else:\n            tokenizer = None\n\n        return {\"tokenizer\": tokenizer, \"median_dict\": median_dict}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess.__init__","title":"<code>__init__(download_directory, medians_file_path, tokenizer_vocab_path)</code>","text":"<p>Downloads HGNC symbols</p> <p>preproc_dir (str): Directory to store the reference preproc in tokenizer_vocab_path (str): Filepath to store the tokenizer vocab dataset_conf (OmegaConf): has 'train', 'val', 'test' keys containing     the names of preprocessed train/val/test files to use for training.</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def __init__(self, download_directory: Path, medians_file_path: Path, tokenizer_vocab_path: Path):\n    \"\"\"Downloads HGNC symbols\n\n    preproc_dir (str): Directory to store the reference preproc in\n    tokenizer_vocab_path (str): Filepath to store the tokenizer vocab\n    dataset_conf (OmegaConf): has 'train', 'val', 'test' keys containing\n        the names of preprocessed train/val/test files to use for training.\n    \"\"\"  # noqa: D415\n    self.download_directory = download_directory\n    self.medians_file_path = medians_file_path\n    self.tokenizer_vocab_path = tokenizer_vocab_path\n    self._validate_tokenizer_args(\n        self.tokenizer_vocab_path,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess.build_and_save_tokenizer","title":"<code>build_and_save_tokenizer(median_dict, gene_to_ens, vocab_output_name)</code>","text":"<p>Builds the GeneTokenizer using the median dictionary then serializes and saves the dictionary to disk.</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def build_and_save_tokenizer(self, median_dict, gene_to_ens, vocab_output_name):\n    \"\"\"Builds the GeneTokenizer using the median dictionary\n    then serializes and saves the dictionary to disk.\n    \"\"\"  # noqa: D205\n    tokenizer = GeneTokenizer.from_medians_and_genes_dicts(median_dict, gene_to_ens)\n    tokenizer.save_vocab(vocab_output_name)\n    return tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess.preprocess","title":"<code>preprocess()</code>","text":"<p>Preprocesses for the Geneformer model</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def preprocess(self) -&gt; dict[Literal[\"tokenizer\", \"median_dict\"], Any]:\n    \"\"\"Preprocesses for the Geneformer model\"\"\"  # noqa: D415\n    gene_name_dict_fn, gene_median_dict_fn = GeneformerResourcePreprocessor(\n        dest_directory=self.download_directory,\n    ).prepare()\n\n    # Load artifacts\n    with open(gene_name_dict_fn, \"rb\") as fd:\n        gene_ens = pickle.load(fd)\n\n    with open(gene_median_dict_fn, \"rb\") as fd:\n        median_dict = pickle.load(fd)\n\n    # Save converted artifacts to JSON to prevent pickle issues.\n    medians_dir = os.path.dirname(self.medians_file_path)\n    if not os.path.exists(medians_dir):\n        os.makedirs(medians_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n    with open(self.medians_file_path, \"w\") as fp:\n        json.dump(median_dict, fp)\n\n    if self.tokenizer_vocab_path is not None:\n        tokenizer = self.build_and_save_tokenizer(\n            median_dict,\n            gene_ens,\n            self.tokenizer_vocab_path,\n        )\n    else:\n        tokenizer = None\n\n    return {\"tokenizer\": tokenizer, \"median_dict\": median_dict}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerResourcePreprocessor","title":"<code>GeneformerResourcePreprocessor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ResourcePreprocessor</code></p> <p>ResourcePreprocessor for the Geneformer model. Downloads the gene_name_id_dict.pkl and gene_median_dictionary.pkl files.</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>@dataclass\nclass GeneformerResourcePreprocessor(ResourcePreprocessor):\n    \"\"\"ResourcePreprocessor for the Geneformer model. Downloads the gene_name_id_dict.pkl and gene_median_dictionary.pkl files.\"\"\"\n\n    dest_directory: str = \"geneformer\"\n\n    def get_remote_resources(self) -&gt; List[RemoteResource]:  # noqa: D102\n        url_fn = {\n            \"https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\": \"gene_name_id_dict.pkl\",\n            \"https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\": \"gene_median_dictionary.pkl\",\n        }\n\n        resources = []\n        for url, filename in url_fn.items():\n            resource = RemoteResource(\n                dest_directory=self.dest_directory,\n                dest_filename=filename,\n                root_directory=self.root_directory,\n                checksum=None,\n                url=url,\n            )\n            resources.append(resource)\n        return resources\n\n    def prepare_resource(self, resource: RemoteResource) -&gt; str:\n        \"\"\"Logs and downloads the passed resource.\n\n        resource: RemoteResource - Resource to be prepared.\n\n        Returns - the absolute destination path for the downloaded resource\n        \"\"\"\n        return resource.download_resource()\n\n    def prepare(self):  # noqa: D102\n        return [self.prepare_resource(resource) for resource in self.get_remote_resources()]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerResourcePreprocessor.prepare_resource","title":"<code>prepare_resource(resource)</code>","text":"<p>Logs and downloads the passed resource.</p> <p>resource: RemoteResource - Resource to be prepared.</p> <p>Returns - the absolute destination path for the downloaded resource</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def prepare_resource(self, resource: RemoteResource) -&gt; str:\n    \"\"\"Logs and downloads the passed resource.\n\n    resource: RemoteResource - Resource to be prepared.\n\n    Returns - the absolute destination path for the downloaded resource\n    \"\"\"\n    return resource.download_resource()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/utils/#bionemo.geneformer.data.singlecell.utils.sample_or_truncate","title":"<code>sample_or_truncate(gene_ids, max_length, sample=True)</code>","text":"<p>Truncate and pad samples.</p> <p>Parameters:</p> Name Type Description Default <code>gene_ids</code> <code>ndarray</code> <p>Array of gene IDs.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the samples.</p> required <code>sample</code> <code>bool</code> <p>Whether to sample or truncate the samples. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.array: Tuple containing the truncated or padded gene IDs.</p> Source code in <code>bionemo/geneformer/data/singlecell/utils.py</code> <pre><code>def sample_or_truncate(\n    gene_ids: np.ndarray,\n    max_length: int,\n    sample: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Truncate and pad samples.\n\n    Args:\n        gene_ids (np.ndarray): Array of gene IDs.\n        max_length (int): Maximum length of the samples.\n        sample (bool, optional): Whether to sample or truncate the samples. Defaults to True.\n\n    Returns:\n        np.array: Tuple containing the truncated or padded gene IDs.\n    \"\"\"\n    if len(gene_ids) &lt;= max_length:\n        return gene_ids\n\n    if sample:\n        indices = np.random.permutation(len(gene_ids))[:max_length]\n        return gene_ids[indices]\n    else:\n        return gene_ids[:max_length]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/","title":"Finetune token regressor","text":""},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.FineTuneSeqLenBioBertConfig","title":"<code>FineTuneSeqLenBioBertConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>BioBert fine-tuning sequence length model configuration.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>@dataclass\nclass FineTuneSeqLenBioBertConfig(\n    BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction],\n    iom.IOMixinWithGettersSetters,\n):\n    \"\"\"BioBert fine-tuning sequence length model configuration.\"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[MegatronBioBertFineTuneSeqLengthModel] = MegatronBioBertFineTuneSeqLengthModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n        \"\"\"Loss function type.\"\"\"\n        return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.FineTuneSeqLenBioBertConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Loss function type.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n    \"\"\"Loss function type.\"\"\"\n    return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor","title":"<code>LoRAForGeneFormerTokenRegressor</code>","text":"<p>               Bases: <code>LoRA</code></p> <p>LoRA for Genformer Token Regression.</p> <p>There are a few tricky things here to get everything to work right:</p> <ol> <li>Freezing logic for the transformer has to be updated in order to not freeze the new head layers.</li> <li>The LoRA adapter logic has to be updated to pull the input/output sizes of the layers to be adapted from the    modules that are passed (the previous method was compatible with nn and TE, but not megatrons tensor_parallel    modules that are currently used by geneformer). This method contains a suggested refactor to make these methods    a little more general and extensible with structural pattern matching as well. We should push this    requirement onto NeMo, since we shouldn't duplicate the adapter method.</li> <li>There's a ton of assumptions in NeMo about which module is being called and that it inherits specific mixins.    This could break the if it is updated from a megatron module to a torch module or something else. Functional    calls are generally favored for this reason and some have been made here to avoid updating inheritance throughout    the code base.</li> </ol> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class LoRAForGeneFormerTokenRegressor(LoRA):\n    \"\"\"LoRA for Genformer Token Regression.\n\n    There are a few tricky things here to get everything to work right:\n\n    1. Freezing logic for the transformer has to be updated in order to not freeze the new head layers.\n    2. The LoRA adapter logic has to be updated to pull the input/output sizes of the layers to be adapted from the\n       modules that are passed (the previous method was compatible with nn and TE, but not megatrons tensor_parallel\n       modules that are currently used by geneformer). This method contains a suggested refactor to make these methods\n       a little more general and extensible with structural pattern matching as well. We should push this\n       requirement onto NeMo, since we shouldn't duplicate the adapter method.\n    3. There's a ton of assumptions in NeMo about which module is being called and that it inherits specific mixins.\n       This could break the if it is updated from a megatron module to a torch module or something else. Functional\n       calls are generally favored for this reason and some have been made here to avoid updating inheritance throughout\n       the code base.\n    \"\"\"\n\n    def input_size_getter(self, m: nn.Module) -&gt; int:\n        \"\"\"Gets the input size of the supplied model.\"\"\"\n        match m:\n            case object(input_size=n):\n                return n\n            case object(in_features=n):\n                return n\n            case _:\n                raise ValueError(f\"Module {m} does not have a supported input size calculation.\")\n\n    def output_size_getter(self, m: nn.Module) -&gt; int:\n        \"\"\"Gets the output size of the supplied model.\"\"\"\n        match m:\n            case object(output_size=n):\n                return n\n            case object(out_features=n):\n                return n\n            case _:\n                raise ValueError(f\"Module {m} does not have a supported output size calculation.\")\n\n    def __call__(self, model: nn.Module) -&gt; nn.Module:\n        \"\"\"Inference.\"\"\"\n        fn.walk(model, self.selective_freeze)\n        fn.walk(model, self.transform)\n        return model\n\n    def selective_freeze(self, m: nn.Module, name: str | None = None, prefix: str | None = None) -&gt; nn.Module:\n        \"\"\"Freezes either 'encoder' or 'embedding' parameters of the input model (`m`) iff name is one of these.\"\"\"\n        if name in [\"encoder\", \"embedding\"]:\n            FNMixin.freeze(m)\n        return m\n\n    def transform(self, m: nn.Module, name: str | None = None, prefix: str | None = None) -&gt; nn.Module | LoRALinear:\n        \"\"\"Transforms the input model if the name is in the target modules.\"\"\"\n        tp_size = parallel_state.get_tensor_model_parallel_world_size()\n        if name in self.target_modules:\n            # m.in_features and m.out_features are divided by tp_size already,\n            # but in_features and out_features passed to ParallelLinearAdapter are not.\n            if prefix is not None and \"regression_head\" in prefix:\n                return m\n            if name in [\"linear_qkv\", \"linear_fc1\"]:\n                # Column Parallel Linear\n                input_is_parallel = False\n                in_features = self.input_size_getter(\n                    m\n                )  # TODO(@georgea) note that this could break depending on the impl of `m`\n                out_features = self.output_size_getter(m) * tp_size\n                # LoRA is applied after layernorm, so layernorm output must be returned\n                m.return_layernorm_output = True\n                # perf optimization for LoRA + SP\n                if m.config.sequence_parallel and not m.ub_overlap_ag:\n                    m.return_layernorm_output_gathered = True\n            else:  # name in ['linear_proj', 'linear_fc2']\n                # Row Parallel Linear\n                input_is_parallel = True\n                in_features = (\n                    self.input_size_getter(m) * tp_size\n                )  # TODO(@georgea) note this could break depending on the impl of `m`\n                out_features = self.output_size_getter(m)\n\n            adapter = ParallelLinearAdapter(\n                in_features,\n                out_features,\n                self.dim,\n                activation=\"identity\",\n                norm_position=None,\n                norm_type=None,\n                column_init_method=self.lora_A_init_method,\n                row_init_method=self.lora_B_init_method,\n                gather_output=False,\n                input_is_parallel=input_is_parallel,\n                dropout=self.dropout,\n                dropout_position=self.dropout_position,\n                model_parallel_config=getattr(m, \"config\", None),\n                alpha=self.alpha,\n            )\n            return LoRALinear(m, adapter)\n        return m\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.__call__","title":"<code>__call__(model)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def __call__(self, model: nn.Module) -&gt; nn.Module:\n    \"\"\"Inference.\"\"\"\n    fn.walk(model, self.selective_freeze)\n    fn.walk(model, self.transform)\n    return model\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.input_size_getter","title":"<code>input_size_getter(m)</code>","text":"<p>Gets the input size of the supplied model.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def input_size_getter(self, m: nn.Module) -&gt; int:\n    \"\"\"Gets the input size of the supplied model.\"\"\"\n    match m:\n        case object(input_size=n):\n            return n\n        case object(in_features=n):\n            return n\n        case _:\n            raise ValueError(f\"Module {m} does not have a supported input size calculation.\")\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.output_size_getter","title":"<code>output_size_getter(m)</code>","text":"<p>Gets the output size of the supplied model.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def output_size_getter(self, m: nn.Module) -&gt; int:\n    \"\"\"Gets the output size of the supplied model.\"\"\"\n    match m:\n        case object(output_size=n):\n            return n\n        case object(out_features=n):\n            return n\n        case _:\n            raise ValueError(f\"Module {m} does not have a supported output size calculation.\")\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.selective_freeze","title":"<code>selective_freeze(m, name=None, prefix=None)</code>","text":"<p>Freezes either 'encoder' or 'embedding' parameters of the input model (<code>m</code>) iff name is one of these.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def selective_freeze(self, m: nn.Module, name: str | None = None, prefix: str | None = None) -&gt; nn.Module:\n    \"\"\"Freezes either 'encoder' or 'embedding' parameters of the input model (`m`) iff name is one of these.\"\"\"\n    if name in [\"encoder\", \"embedding\"]:\n        FNMixin.freeze(m)\n    return m\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.transform","title":"<code>transform(m, name=None, prefix=None)</code>","text":"<p>Transforms the input model if the name is in the target modules.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def transform(self, m: nn.Module, name: str | None = None, prefix: str | None = None) -&gt; nn.Module | LoRALinear:\n    \"\"\"Transforms the input model if the name is in the target modules.\"\"\"\n    tp_size = parallel_state.get_tensor_model_parallel_world_size()\n    if name in self.target_modules:\n        # m.in_features and m.out_features are divided by tp_size already,\n        # but in_features and out_features passed to ParallelLinearAdapter are not.\n        if prefix is not None and \"regression_head\" in prefix:\n            return m\n        if name in [\"linear_qkv\", \"linear_fc1\"]:\n            # Column Parallel Linear\n            input_is_parallel = False\n            in_features = self.input_size_getter(\n                m\n            )  # TODO(@georgea) note that this could break depending on the impl of `m`\n            out_features = self.output_size_getter(m) * tp_size\n            # LoRA is applied after layernorm, so layernorm output must be returned\n            m.return_layernorm_output = True\n            # perf optimization for LoRA + SP\n            if m.config.sequence_parallel and not m.ub_overlap_ag:\n                m.return_layernorm_output_gathered = True\n        else:  # name in ['linear_proj', 'linear_fc2']\n            # Row Parallel Linear\n            input_is_parallel = True\n            in_features = (\n                self.input_size_getter(m) * tp_size\n            )  # TODO(@georgea) note this could break depending on the impl of `m`\n            out_features = self.output_size_getter(m)\n\n        adapter = ParallelLinearAdapter(\n            in_features,\n            out_features,\n            self.dim,\n            activation=\"identity\",\n            norm_position=None,\n            norm_type=None,\n            column_init_method=self.lora_A_init_method,\n            row_init_method=self.lora_B_init_method,\n            gather_output=False,\n            input_is_parallel=input_is_parallel,\n            dropout=self.dropout,\n            dropout_position=self.dropout_position,\n            model_parallel_config=getattr(m, \"config\", None),\n            alpha=self.alpha,\n        )\n        return LoRALinear(m, adapter)\n    return m\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronBioBertFineTuneSeqLengthModel","title":"<code>MegatronBioBertFineTuneSeqLengthModel</code>","text":"<p>               Bases: <code>MegatronBioBertModel</code></p> <p>Megatron model for biobert finetuning with sequence length.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class MegatronBioBertFineTuneSeqLengthModel(MegatronBioBertModel):\n    \"\"\"Megatron model for biobert finetuning with sequence length.\"\"\"\n\n    def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n        self.include_hiddens_finetuning = (\n            include_hiddens  # this include_hiddens is for the final output of fine-tuning\n        )\n        # If post_process is True that means that we are at the last megatron parallelism stage and we can\n        #   apply the head.\n        if post_process:\n            # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n            self.regression_head = MegatronRegressionMLPHead(config)\n\n    def forward(self, *args, **kwargs) -&gt; MegatronFineTuneOutput | BioBertOutput | Tensor:\n        \"\"\"Inference.\"\"\"\n        output: MegatronFineTuneOutput | BioBertOutput | Tensor = super().forward(*args, **kwargs)\n        # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n        if not self.post_process:\n            return output  # we are not at the last pipeline stage so just return what the parent has\n        # Double check that the output from the parent has everything we need to do prediction in this head.\n        if not isinstance(output, dict) or (\"hidden_states\" not in output):\n            raise ValueError(\n                f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n                \"Make sure include_hiddens=True in the call to super().__init__\"\n            )\n        # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n        hidden_states: Tensor = output[\"hidden_states\"][:, 0]  # [b s h] =&gt; [b h], use [CLS] (first) token for reg\n        # Predict our 1d regression target\n        regression_output = self.regression_head(hidden_states)\n        if not self.include_hiddens_finetuning:\n            del output[\"hidden_states\"]\n        output[\"regression_output\"] = regression_output\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronBioBertFineTuneSeqLengthModel.__init__","title":"<code>__init__(config, *args, include_hiddens=False, post_process=True, **kwargs)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n    self.include_hiddens_finetuning = (\n        include_hiddens  # this include_hiddens is for the final output of fine-tuning\n    )\n    # If post_process is True that means that we are at the last megatron parallelism stage and we can\n    #   apply the head.\n    if post_process:\n        # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n        self.regression_head = MegatronRegressionMLPHead(config)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronBioBertFineTuneSeqLengthModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; MegatronFineTuneOutput | BioBertOutput | Tensor:\n    \"\"\"Inference.\"\"\"\n    output: MegatronFineTuneOutput | BioBertOutput | Tensor = super().forward(*args, **kwargs)\n    # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n    if not self.post_process:\n        return output  # we are not at the last pipeline stage so just return what the parent has\n    # Double check that the output from the parent has everything we need to do prediction in this head.\n    if not isinstance(output, dict) or (\"hidden_states\" not in output):\n        raise ValueError(\n            f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n            \"Make sure include_hiddens=True in the call to super().__init__\"\n        )\n    # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n    hidden_states: Tensor = output[\"hidden_states\"][:, 0]  # [b s h] =&gt; [b h], use [CLS] (first) token for reg\n    # Predict our 1d regression target\n    regression_output = self.regression_head(hidden_states)\n    if not self.include_hiddens_finetuning:\n        del output[\"hidden_states\"]\n    output[\"regression_output\"] = regression_output\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronFineTuneOutput","title":"<code>MegatronFineTuneOutput</code>","text":"<p>               Bases: <code>BioBertOutput</code></p> <p>Inference output type for MegatronBioBertFineTuneSeqLengthModel.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class MegatronFineTuneOutput(BioBertOutput):\n    \"\"\"Inference output type for MegatronBioBertFineTuneSeqLengthModel.\"\"\"\n\n    regression_output: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronRegressionMLPHead","title":"<code>MegatronRegressionMLPHead</code>","text":"<p>               Bases: <code>MegatronModule</code></p> <p>A megatron MLP head.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class MegatronRegressionMLPHead(MegatronModule):\n    \"\"\"A megatron MLP head.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config)\n        # FC layer over just the [CLS] token embedding\n        # TODO use bias/activation fusion if requested\n        self.linear_fc1 = nn.Linear(in_features=config.hidden_size, out_features=config.ffn_hidden_size)\n        self.activation_function = config.activation_func\n        self.linear_fc2 = nn.Linear(in_features=config.ffn_hidden_size, out_features=1)\n\n    def forward(self, hidden_states: Tensor) -&gt; Tensor:\n        \"\"\"Inference.\"\"\"\n        return self.linear_fc2(self.activation_function(self.linear_fc1(hidden_states)))\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronRegressionMLPHead.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def __init__(self, config: TransformerConfig):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config)\n    # FC layer over just the [CLS] token embedding\n    # TODO use bias/activation fusion if requested\n    self.linear_fc1 = nn.Linear(in_features=config.hidden_size, out_features=config.ffn_hidden_size)\n    self.activation_function = config.activation_func\n    self.linear_fc2 = nn.Linear(in_features=config.ffn_hidden_size, out_features=1)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronRegressionMLPHead.forward","title":"<code>forward(hidden_states)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def forward(self, hidden_states: Tensor) -&gt; Tensor:\n    \"\"\"Inference.\"\"\"\n    return self.linear_fc2(self.activation_function(self.linear_fc1(hidden_states)))\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.SequenceLengthRMSEPlusBERTMLMLossWithReduction","title":"<code>SequenceLengthRMSEPlusBERTMLMLossWithReduction</code>","text":"<p>               Bases: <code>BERTMLMLossWithReduction</code></p> <p>Loss function.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class SequenceLengthRMSEPlusBERTMLMLossWithReduction(BERTMLMLossWithReduction):\n    \"\"\"Loss function.\"\"\"\n\n    def forward(\n        self,\n        batch: SeqLenRmsepBatch,\n        forward_out: Dict[str, Tensor],\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n        \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently.\n\n        In the future this will be extended to handle other loss types like sequence loss if it is present in the\n        forward_out and batch.\n\n        Args:\n            batch: The batch of data. Each tensor should be of shape [batch_size, *, *],\n                and match the corresponding dimension for that particular key in the batch output.\n                For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n            forward_out: The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n        Taken from:\n        https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976\n        \"\"\"\n        if \"labels\" not in batch:\n            raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n        unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])\n        regression_output = forward_out[\"regression_output\"]\n        n_tokens = batch[\"attention_mask\"].sum(dim=-1, keepdim=True).to(dtype=regression_output.dtype)\n        assert len(n_tokens.shape) == 2\n        assert n_tokens.shape[-1] == 1\n        rmse_loss = torch.nn.functional.mse_loss(regression_output, n_tokens)\n\n        # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            # reduce the loss across the micro batch\n            loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n        else:\n            # reduce the loss across the micro batch.\n            # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n            #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n            #  other necessary keys to the batch. Thanks!\n            loss_for_microbatch = masked_token_loss_context_parallel(\n                unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n            )\n\n        # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n        #  reducing the loss across the data parallel group.\n        if self.validation_step and not self.val_drop_last:\n            num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n            if loss_for_microbatch.isnan():\n                # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n                #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n                #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n                if batch[\"loss_mask\"].count_nonzero() != 0:\n                    raise ValueError(\"Got NaN loss with non-empty input\")\n                loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n            else:\n                loss_sum_for_microbatch = num_valid_tokens_in_microbatch * loss_for_microbatch\n\n            # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n            loss_sum_and_microbatch_size_all_gpu = torch.cat(\n                [\n                    loss_sum_for_microbatch.clone().detach().view(1),\n                    torch.tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n                ]\n            )\n            torch.distributed.all_reduce(\n                loss_sum_and_microbatch_size_all_gpu, group=parallel_state.get_data_parallel_group()\n            )\n            return loss_for_microbatch * cp_size, {\n                \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n            }\n        loss_for_microbatch = loss_for_microbatch + rmse_loss  # add in the RMSE loss after reducing the logit loss\n        # average the losses across the data parallel group, but also return the unreduced loss\n        reduced_loss: Tensor = average_losses_across_data_parallel_group([loss_for_microbatch])\n        return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.SequenceLengthRMSEPlusBERTMLMLossWithReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Computes loss of <code>labels</code> in the batch vs <code>token_logits</code> in the forward output currently.</p> <p>In the future this will be extended to handle other loss types like sequence loss if it is present in the forward_out and batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SeqLenRmsepBatch</code> <p>The batch of data. Each tensor should be of shape [batch_size, , ], and match the corresponding dimension for that particular key in the batch output. For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>The forward output from the model. Each tensor should be of shape [batch_size, , ]</p> required <p>Taken from: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def forward(\n    self,\n    batch: SeqLenRmsepBatch,\n    forward_out: Dict[str, Tensor],\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n    \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently.\n\n    In the future this will be extended to handle other loss types like sequence loss if it is present in the\n    forward_out and batch.\n\n    Args:\n        batch: The batch of data. Each tensor should be of shape [batch_size, *, *],\n            and match the corresponding dimension for that particular key in the batch output.\n            For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n        forward_out: The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n    Taken from:\n    https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976\n    \"\"\"\n    if \"labels\" not in batch:\n        raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n    unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])\n    regression_output = forward_out[\"regression_output\"]\n    n_tokens = batch[\"attention_mask\"].sum(dim=-1, keepdim=True).to(dtype=regression_output.dtype)\n    assert len(n_tokens.shape) == 2\n    assert n_tokens.shape[-1] == 1\n    rmse_loss = torch.nn.functional.mse_loss(regression_output, n_tokens)\n\n    # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        # reduce the loss across the micro batch\n        loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n    else:\n        # reduce the loss across the micro batch.\n        # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n        #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n        #  other necessary keys to the batch. Thanks!\n        loss_for_microbatch = masked_token_loss_context_parallel(\n            unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n        )\n\n    # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n    #  reducing the loss across the data parallel group.\n    if self.validation_step and not self.val_drop_last:\n        num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n        if loss_for_microbatch.isnan():\n            # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n            #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n            #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n            if batch[\"loss_mask\"].count_nonzero() != 0:\n                raise ValueError(\"Got NaN loss with non-empty input\")\n            loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n        else:\n            loss_sum_for_microbatch = num_valid_tokens_in_microbatch * loss_for_microbatch\n\n        # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n        loss_sum_and_microbatch_size_all_gpu = torch.cat(\n            [\n                loss_sum_for_microbatch.clone().detach().view(1),\n                torch.tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n            ]\n        )\n        torch.distributed.all_reduce(\n            loss_sum_and_microbatch_size_all_gpu, group=parallel_state.get_data_parallel_group()\n        )\n        return loss_for_microbatch * cp_size, {\n            \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n        }\n    loss_for_microbatch = loss_for_microbatch + rmse_loss  # add in the RMSE loss after reducing the logit loss\n    # average the losses across the data parallel group, but also return the unreduced loss\n    reduced_loss: Tensor = average_losses_across_data_parallel_group([loss_for_microbatch])\n    return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/","title":"Config models","text":""},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.ExposedFineTuneSeqLenBioBertConfig","title":"<code>ExposedFineTuneSeqLenBioBertConfig</code>","text":"<p>               Bases: <code>ExposedModelConfig[FineTuneSeqLenBioBertConfig]</code></p> <p>Config for models that fine-tune a BioBERT model from a pre-trained checkpoint.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>class ExposedFineTuneSeqLenBioBertConfig(ExposedModelConfig[FineTuneSeqLenBioBertConfig]):\n    \"\"\"Config for models that fine-tune a BioBERT model from a pre-trained checkpoint.\n\n    Parameters:\n        initial_ckpt_path - path to a directory containing checkpoint files for initializing the model. This is only\n            required on the first execution of the model, any restored checkpoints should skip this step.\n        initial_ckpt_skip_keys_with_these_prefixes - skip any layer that contains this key during restoration. Useful\n            for ignoring extra additional layers used for finetuning. Layers with these keys are then randomly initialized.\n    \"\"\"\n\n    # Custom parameters for FineTuning\n    initial_ckpt_path: Optional[str] = None\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    def model_class(self) -&gt; Type[FineTuneSeqLenBioBertConfig]:\n        \"\"\"Binds the class to FineTuneSeqLenBioBertConfig.\"\"\"\n        return FineTuneSeqLenBioBertConfig\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.ExposedFineTuneSeqLenBioBertConfig.model_class","title":"<code>model_class()</code>","text":"<p>Binds the class to FineTuneSeqLenBioBertConfig.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>def model_class(self) -&gt; Type[FineTuneSeqLenBioBertConfig]:\n    \"\"\"Binds the class to FineTuneSeqLenBioBertConfig.\"\"\"\n    return FineTuneSeqLenBioBertConfig\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.ExposedGeneformerPretrainConfig","title":"<code>ExposedGeneformerPretrainConfig</code>","text":"<p>               Bases: <code>ExposedModelConfig[GeneformerConfig]</code></p> <p>Exposes custom parameters for pretraining and binds the class to GeneformerConfig.</p> <p>Attributes:</p> Name Type Description <code>initial_ckpt_path</code> <code>str</code> <p>Path to a directory containing checkpoint files for initializing the model. This is only</p> <code>initial_ckpt_skip_keys_with_these_prefixes</code> <code>List[str]</code> <p>Skip any layer that contains this key during restoration. Useful for finetuning, set the names of the task heads so checkpoint restoration does not errorniously try to restore these.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>class ExposedGeneformerPretrainConfig(ExposedModelConfig[GeneformerConfig]):\n    \"\"\"Exposes custom parameters for pretraining and binds the class to GeneformerConfig.\n\n    Attributes:\n        initial_ckpt_path (str): Path to a directory containing checkpoint files for initializing the model. This is only\n        initial_ckpt_skip_keys_with_these_prefixes (List[str]): Skip any layer that contains this key during restoration. Useful for finetuning, set the names of the task heads so checkpoint restoration does not errorniously try to restore these.\n    \"\"\"\n\n    # Custom parameters for FineTuning\n    initial_ckpt_path: Optional[str] = None\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n\n    def model_class(self) -&gt; Type[GeneformerConfig]:  # noqa: D102\n        return GeneformerConfig\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerDataArtifacts","title":"<code>GeneformerDataArtifacts</code>  <code>dataclass</code>","text":"<p>Data artifacts produced by the geneformer preprocess.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>@dataclass\nclass GeneformerDataArtifacts:\n    \"\"\"Data artifacts produced by the geneformer preprocess.\"\"\"\n\n    tokenizer: Tokenizer\n    median_dict: dict\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig","title":"<code>GeneformerPretrainingDataConfig</code>","text":"<p>               Bases: <code>DataConfig[SingleCellDataModule]</code></p> <p>Configuration class for Geneformer pretraining data.</p> <p>Expects train/test/val to be prior split by directory and processed by <code>sub-packages/bionemo-scdl/src/bionemo/scdl/scripts/convert_h5ad_to_scdl.py</code>.</p> <p>Attributes:</p> Name Type Description <code>data_dir</code> <code>str</code> <p>Directory where the data is stored.</p> <code>result_dir</code> <code>str | Path</code> <p>Directory where the results will be stored. Defaults to \"./results\".</p> <code>micro_batch_size</code> <code>int</code> <p>Size of the micro-batch. Defaults to 8.</p> <code>seq_length</code> <code>int</code> <p>Sequence length for the data. Defaults to 2048.</p> <code>num_dataset_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> Properties <p>train_data_path (str): Path to the training data. val_data_path (str): Path to the validation data. test_data_path (str): Path to the test data.</p> <p>Methods:</p> Name Description <code>geneformer_preprocess</code> <p>Preprocesses the data using a legacy preprocessor from BioNeMo 1 and returns the necessary artifacts.</p> <code>construct_data_module</code> <p>int) -&gt; SingleCellDataModule: Constructs and returns a SingleCellDataModule using the preprocessed data artifacts.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>class GeneformerPretrainingDataConfig(DataConfig[SingleCellDataModule]):\n    \"\"\"Configuration class for Geneformer pretraining data.\n\n    Expects train/test/val to be prior split by directory and processed by `sub-packages/bionemo-scdl/src/bionemo/scdl/scripts/convert_h5ad_to_scdl.py`.\n\n    Attributes:\n        data_dir (str): Directory where the data is stored.\n        result_dir (str | pathlib.Path): Directory where the results will be stored. Defaults to \"./results\".\n        micro_batch_size (int): Size of the micro-batch. Defaults to 8.\n        seq_length (int): Sequence length for the data. Defaults to 2048.\n        num_dataset_workers (int): Number of workers for data loading. Defaults to 0.\n\n    Properties:\n        train_data_path (str): Path to the training data.\n        val_data_path (str): Path to the validation data.\n        test_data_path (str): Path to the test data.\n\n    Methods:\n        geneformer_preprocess() -&gt; GeneformerDataArtifacts:\n            Preprocesses the data using a legacy preprocessor from BioNeMo 1 and returns the necessary artifacts.\n        construct_data_module(global_batch_size: int) -&gt; SingleCellDataModule:\n            Constructs and returns a SingleCellDataModule using the preprocessed data artifacts.\n    \"\"\"\n\n    # Shadow two attributes from the parent for visibility.\n    data_dir: str\n    result_dir: str | pathlib.Path = \"./results\"\n    micro_batch_size: int = 8\n\n    seq_length: int = 2048\n    num_dataset_workers: int = 0\n\n    @field_serializer(\"result_dir\")\n    def serialize_paths(self, value: pathlib.Path) -&gt; str:  # noqa: D102\n        return serialize_path_or_str(value)\n\n    @field_validator(\"result_dir\")\n    def deserialize_paths(cls, value: str) -&gt; pathlib.Path:  # noqa: D102\n        return deserialize_str_to_path(value)\n\n    @property\n    def train_data_path(self) -&gt; str:  # noqa: D102\n        return self.data_dir + \"/train\"\n\n    @property\n    def val_data_path(self) -&gt; str:  # noqa: D102\n        return self.data_dir + \"/val\"\n\n    @property\n    def test_data_path(self) -&gt; str:  # noqa: D102\n        return self.data_dir + \"/test\"\n\n    def geneformer_preprocess(self) -&gt; GeneformerDataArtifacts:\n        \"\"\"Geneformer datamodule expects certain artifacts to be present in the data directory.\n\n        This method uses a legacy 'preprocessor' from BioNeMo 1 to acquire the associated artifacts.\n        \"\"\"\n        preprocessor = GeneformerPreprocess(\n            download_directory=pathlib.Path(self.train_data_path),\n            medians_file_path=pathlib.Path(self.train_data_path + \"/medians.json\"),\n            tokenizer_vocab_path=pathlib.Path(self.train_data_path + \"/geneformer.vocab\"),\n        )\n        result = preprocessor.preprocess()\n        if \"tokenizer\" in result and \"median_dict\" in result:\n            logging.info(\"*************** Preprocessing Finished ************\")\n            return GeneformerDataArtifacts(tokenizer=result[\"tokenizer\"], median_dict=result[\"median_dict\"])\n        else:\n            logging.error(\"Preprocessing failed.\")\n            raise ValueError(\"Preprocessing failed to create tokenizer and/or median dictionary.\")\n\n    def construct_data_module(self, global_batch_size: int) -&gt; SingleCellDataModule:\n        \"\"\"Downloads the requisite data artifacts and instantiates the DataModule.\"\"\"\n        geneformer_data_artifacts: GeneformerDataArtifacts = self.geneformer_preprocess()\n        data = SingleCellDataModule(\n            seq_length=self.seq_length,\n            tokenizer=geneformer_data_artifacts.tokenizer,\n            train_dataset_path=self.train_data_path,\n            val_dataset_path=self.val_data_path,\n            test_dataset_path=self.test_data_path,\n            random_token_prob=0.02,\n            median_dict=geneformer_data_artifacts.median_dict,\n            micro_batch_size=self.micro_batch_size,\n            global_batch_size=global_batch_size,\n            persistent_workers=self.num_dataset_workers &gt; 0,\n            pin_memory=False,\n            num_workers=self.num_dataset_workers,\n        )\n        return data\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig.construct_data_module","title":"<code>construct_data_module(global_batch_size)</code>","text":"<p>Downloads the requisite data artifacts and instantiates the DataModule.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>def construct_data_module(self, global_batch_size: int) -&gt; SingleCellDataModule:\n    \"\"\"Downloads the requisite data artifacts and instantiates the DataModule.\"\"\"\n    geneformer_data_artifacts: GeneformerDataArtifacts = self.geneformer_preprocess()\n    data = SingleCellDataModule(\n        seq_length=self.seq_length,\n        tokenizer=geneformer_data_artifacts.tokenizer,\n        train_dataset_path=self.train_data_path,\n        val_dataset_path=self.val_data_path,\n        test_dataset_path=self.test_data_path,\n        random_token_prob=0.02,\n        median_dict=geneformer_data_artifacts.median_dict,\n        micro_batch_size=self.micro_batch_size,\n        global_batch_size=global_batch_size,\n        persistent_workers=self.num_dataset_workers &gt; 0,\n        pin_memory=False,\n        num_workers=self.num_dataset_workers,\n    )\n    return data\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig.geneformer_preprocess","title":"<code>geneformer_preprocess()</code>","text":"<p>Geneformer datamodule expects certain artifacts to be present in the data directory.</p> <p>This method uses a legacy 'preprocessor' from BioNeMo 1 to acquire the associated artifacts.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>def geneformer_preprocess(self) -&gt; GeneformerDataArtifacts:\n    \"\"\"Geneformer datamodule expects certain artifacts to be present in the data directory.\n\n    This method uses a legacy 'preprocessor' from BioNeMo 1 to acquire the associated artifacts.\n    \"\"\"\n    preprocessor = GeneformerPreprocess(\n        download_directory=pathlib.Path(self.train_data_path),\n        medians_file_path=pathlib.Path(self.train_data_path + \"/medians.json\"),\n        tokenizer_vocab_path=pathlib.Path(self.train_data_path + \"/geneformer.vocab\"),\n    )\n    result = preprocessor.preprocess()\n    if \"tokenizer\" in result and \"median_dict\" in result:\n        logging.info(\"*************** Preprocessing Finished ************\")\n        return GeneformerDataArtifacts(tokenizer=result[\"tokenizer\"], median_dict=result[\"median_dict\"])\n    else:\n        logging.error(\"Preprocessing failed.\")\n        raise ValueError(\"Preprocessing failed to create tokenizer and/or median dictionary.\")\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/main/","title":"Main","text":""},{"location":"API_reference/bionemo/geneformer/run/recipes/","title":"Recipes","text":""},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.GeneformerRecipes","title":"<code>GeneformerRecipes</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pre-baked recipes for Geneformer.</p> <p>THIS PYDANTIC MODEL IS NOT MEANT FOR SERIALIZATION. Only used to facilitate argparse. Each recipe should take <code>args</code> as the only argument. We use partials so we can provide this information at runtime. Add new recipes to this model.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>class GeneformerRecipes(BaseModel):\n    \"\"\"Pre-baked recipes for Geneformer.\n\n    THIS PYDANTIC MODEL IS NOT MEANT FOR SERIALIZATION. Only used to facilitate argparse. Each recipe should take `args`\n    as the only argument. We use partials so we can provide this information at runtime. Add new recipes to this model.\n    \"\"\"\n\n    # Use partials so we can still parameterize the recipes from the CLI (e.g. data paths.)\n    geneformer_10m_finetune_recipe: Callable[\n        [argparse.Namespace], MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig]\n    ] = partial(geneformer_10m_finetune_recipe)\n    geneformer_10m_pretrain_recipe: Callable[\n        [argparse.Namespace], MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]\n    ] = partial(geneformer_10m_pretrain_recipe)\n    geneformer_106m_pretrain_recipe: Callable[\n        [argparse.Namespace], MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]\n    ] = partial(geneformer_106m_pretrain_recipe)\n    geneformer_tiny_test_recipe: Callable[\n        [argparse.Namespace], MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]\n    ] = partial(pretrain_tiny_test_recipe)\n    finetune_test_recipe: Callable[\n        [argparse.Namespace], MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig]\n    ] = partial(finetune_test_recipe)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.default_adam_optimizer_with_cosine_annealing_recipe","title":"<code>default_adam_optimizer_with_cosine_annealing_recipe()</code>","text":"<p>Default optimizer scheduler config for Geneformer. See OptimizerSchedulerConfig for defaults.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def default_adam_optimizer_with_cosine_annealing_recipe() -&gt; OptimizerSchedulerConfig:\n    \"\"\"Default optimizer scheduler config for Geneformer. See OptimizerSchedulerConfig for defaults.\"\"\"\n    return OptimizerSchedulerConfig()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.default_trainer_config_recipe","title":"<code>default_trainer_config_recipe()</code>","text":"<p>Default trainer config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def default_trainer_config_recipe() -&gt; TrainingConfig:\n    \"\"\"Default trainer config for Geneformer.\"\"\"\n    return TrainingConfig(max_steps=55000, limit_val_batches=2, val_check_interval=100)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.experiment_config_recipe","title":"<code>experiment_config_recipe()</code>","text":"<p>Default experiment config for Geneformer. Used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def experiment_config_recipe() -&gt; ExperimentConfig:\n    \"\"\"Default experiment config for Geneformer. Used in testing.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=\"./results\",\n        experiment_name=\"default_experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"reduced_train_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.finetune_test_recipe","title":"<code>finetune_test_recipe(args)</code>","text":"<p>Recipe for finetuning a regression head on the masked tokens.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def finetune_test_recipe(args) -&gt; MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for finetuning a regression head on the masked tokens.\"\"\"\n    data_path = args.data_path\n    result_dir = args.result_dir\n\n    parallel_config = ParallelConfig(\n        tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=2\n    )\n    training_config = TrainingConfig(\n        max_steps=10, limit_val_batches=2, val_check_interval=2, precision=\"bf16-mixed\", accelerator=\"gpu\"\n    )\n    data_config = GeneformerPretrainingDataConfig(\n        seq_length=128,\n        micro_batch_size=2,\n        num_dataset_workers=0,\n        data_dir=data_path,\n    )\n    experiment_config = ExperimentConfig(\n        save_every_n_steps=training_config.val_check_interval,\n        result_dir=result_dir,\n        experiment_name=\"test-experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"reduced_train_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n\n    optim_config = OptimizerSchedulerConfig(lr_scheduler=\"cosine\")\n    geneformer_config = geneformer_10m_finetune_config(\n        seq_length=data_config.seq_length, initial_ckpt_path=args.initial_ckpt_path\n    )\n\n    return MainConfig(\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=geneformer_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_experiment_config","title":"<code>geneformer_106m_experiment_config(result_dir)</code>","text":"<p>Experiment config for Geneformer 106m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for Geneformer 106m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=result_dir,\n        experiment_name=\"geneformer-106m\",\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_model_config","title":"<code>geneformer_106m_model_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer 106m model config settings.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_model_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedGeneformerPretrainConfig:\n    \"\"\"Geneformer 106m model config settings.\"\"\"\n    geneformer_config = ExposedGeneformerPretrainConfig(\n        num_layers=12,\n        hidden_size=768,\n        ffn_hidden_size=3072,\n        num_attention_heads=12,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_parallel_config","title":"<code>geneformer_106m_parallel_config()</code>","text":"<p>Base parallel config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Base parallel config for Geneformer.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        num_devices=8,\n        num_nodes=1,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_pretrain_recipe","title":"<code>geneformer_106m_pretrain_recipe(args)</code>","text":"<p>Recipe for pretraining the 106m model. Uses 8 GPUs for data parallelism.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_pretrain_recipe(\n    args,\n) -&gt; MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for pretraining the 106m model. Uses 8 GPUs for data parallelism.\"\"\"\n    data_config: GeneformerPretrainingDataConfig = geneformer_data_recipe(data_dir=args.data_path)\n    parallel_config = geneformer_106m_parallel_config()\n    training_config = geneformer_base_training_config()\n    bionemo_model_config = geneformer_106m_model_config(initial_ckpt_path=args.initial_ckpt_path)\n    optim_config = geneformer_base_optimizer_scheduler_config()\n    experiment_config = geneformer_106m_experiment_config(result_dir=args.result_dir)\n    wandb_config = geneformer_106m_wandb_config()\n    main_config = MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_wandb_config","title":"<code>geneformer_106m_wandb_config()</code>","text":"<p>Wandb config for Geneformer 106m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for Geneformer 106m.\"\"\"\n    wandb_config = WandbConfig(\n        entity=\"geneformer-106m_pretraining\",\n        project=\"geneformer-106m_pretraining\",\n        group=\"geneformer-106m\",\n        tags=[\"geneformer-106m\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n    return wandb_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_experiment_config","title":"<code>geneformer_10m_experiment_config(result_dir)</code>","text":"<p>Experiment config for Geneformer 10m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for Geneformer 10m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=result_dir,\n        experiment_name=\"geneformer-10m\",\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_finetune_config","title":"<code>geneformer_10m_finetune_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer 10m finetuning config settings.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_finetune_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedFineTuneSeqLenBioBertConfig:\n    \"\"\"Geneformer 10m finetuning config settings.\"\"\"\n    geneformer_config = ExposedFineTuneSeqLenBioBertConfig(\n        num_layers=6,\n        hidden_size=256,\n        ffn_hidden_size=512,\n        num_attention_heads=4,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_finetune_recipe","title":"<code>geneformer_10m_finetune_recipe(args)</code>","text":"<p>Recipe for finetuning the 10m model on a token regression head. Used as an example and for testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_finetune_recipe(\n    args,\n) -&gt; MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for finetuning the 10m model on a token regression head. Used as an example and for testing.\"\"\"\n    data_config: GeneformerPretrainingDataConfig = geneformer_data_recipe(data_dir=args.data_path)\n    parallel_config = simple_parallel_recipe()\n    training_config = default_trainer_config_recipe()\n    bionemo_model_config = geneformer_finetuning_regression_head_recipe(initial_ckpt_path=args.initial_ckpt_path)\n    optim_config = default_adam_optimizer_with_cosine_annealing_recipe()\n    experiment_config = experiment_config_recipe()\n    wandb_config = WandbConfig(\n        project=\"bionemo2-demo\",\n        entity=\"nvidia\",\n        offline=True,\n        tags=[],\n        group=\"dev\",\n        id=\"dev\",\n        log_model=False,\n        anonymous=True,\n    )\n    main_config = MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_model_config","title":"<code>geneformer_10m_model_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer 10m model config settings.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_model_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedGeneformerPretrainConfig:\n    \"\"\"Geneformer 10m model config settings.\"\"\"\n    geneformer_config = ExposedGeneformerPretrainConfig(\n        num_layers=6,\n        hidden_size=256,\n        ffn_hidden_size=512,\n        num_attention_heads=4,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_pretrain_recipe","title":"<code>geneformer_10m_pretrain_recipe(args)</code>","text":"<p>Recipe for pretraining the 10m model.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_pretrain_recipe(\n    args,\n) -&gt; MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for pretraining the 10m model.\"\"\"\n    data_config: GeneformerPretrainingDataConfig = geneformer_data_recipe(data_dir=args.data_path)\n    parallel_config = simple_parallel_recipe()\n    training_config = geneformer_base_training_config()\n    bionemo_model_config = geneformer_10m_model_config(initial_ckpt_path=args.initial_ckpt_path)\n    optim_config = geneformer_base_optimizer_scheduler_config()\n    experiment_config = geneformer_10m_experiment_config(result_dir=args.result_dir)\n    wandb_config = geneformer_10m_wandb_config()\n    main_config = MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_wandb_config","title":"<code>geneformer_10m_wandb_config()</code>","text":"<p>Wandb config for Geneformer 10m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for Geneformer 10m.\"\"\"\n    wandb_config = WandbConfig(\n        entity=\"geneformer-10m_pretraining\",\n        project=\"geneformer-10m_pretraining\",\n        group=\"geneformer-10m\",\n        tags=[\"geneformer-10m\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n    return wandb_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_base_optimizer_scheduler_config","title":"<code>geneformer_base_optimizer_scheduler_config()</code>","text":"<p>Base optimizer scheduler config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_base_optimizer_scheduler_config() -&gt; OptimizerSchedulerConfig:\n    \"\"\"Base optimizer scheduler config for Geneformer.\"\"\"\n    return OptimizerSchedulerConfig(lr=1e-3, lr_scheduler=\"cosine\")  # Matches bionemo1\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_base_parallel_config","title":"<code>geneformer_base_parallel_config()</code>","text":"<p>Base parallel config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_base_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Base parallel config for Geneformer.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        num_devices=1,\n        num_nodes=1,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_base_training_config","title":"<code>geneformer_base_training_config()</code>","text":"<p>Base training config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_base_training_config() -&gt; TrainingConfig:\n    \"\"\"Base training config for Geneformer.\"\"\"\n    return TrainingConfig(\n        max_steps=400000, limit_val_batches=8, val_check_interval=100, precision=\"bf16-mixed\"\n    )  # matches bionemo1\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_data_recipe","title":"<code>geneformer_data_recipe(data_dir)</code>","text":"<p>Recipe that produces the base geneformer small data configuration.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_data_recipe(data_dir) -&gt; GeneformerPretrainingDataConfig:\n    \"\"\"Recipe that produces the base geneformer small data configuration.\"\"\"\n    return GeneformerPretrainingDataConfig(data_dir=data_dir)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_finetuning_regression_head_recipe","title":"<code>geneformer_finetuning_regression_head_recipe(precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, initial_ckpt_skip_keys_with_these_prefixes=None)</code>","text":"<p>Recipe for finetuning a regression head on the masked tokens.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_finetuning_regression_head_recipe(\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    initial_ckpt_skip_keys_with_these_prefixes: Optional[List[str]] = None,\n) -&gt; ExposedFineTuneSeqLenBioBertConfig:\n    \"\"\"Recipe for finetuning a regression head on the masked tokens.\"\"\"\n    partial_finetuning_config = partial(\n        ExposedFineTuneSeqLenBioBertConfig,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n        biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n    )\n    if initial_ckpt_skip_keys_with_these_prefixes:\n        finetuning_config = partial_finetuning_config(\n            initial_ckpt_skip_keys_with_these_prefixes=initial_ckpt_skip_keys_with_these_prefixes\n        )\n    else:\n        # Use the sensible default when None is passed\n        finetuning_config = partial_finetuning_config()\n    return finetuning_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_tiny_config","title":"<code>geneformer_tiny_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer tiny model config settings, used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_tiny_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedGeneformerPretrainConfig:\n    \"\"\"Geneformer tiny model config settings, used in testing.\"\"\"\n    geneformer_config = ExposedGeneformerPretrainConfig(\n        num_layers=2,\n        hidden_size=32,\n        ffn_hidden_size=4 * 32,\n        num_attention_heads=2,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.pretrain_tiny_test_recipe","title":"<code>pretrain_tiny_test_recipe(args)</code>","text":"<p>Recipe for pretraining a tiny model. Used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def pretrain_tiny_test_recipe(args) -&gt; MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for pretraining a tiny model. Used in testing.\"\"\"\n    data_path = args.data_path\n    result_dir = args.result_dir\n\n    parallel_config = ParallelConfig(\n        tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=2\n    )\n    training_config = TrainingConfig(\n        max_steps=10, limit_val_batches=2, val_check_interval=2, precision=\"bf16-mixed\", accelerator=\"gpu\"\n    )\n    data_config = GeneformerPretrainingDataConfig(\n        seq_length=128,\n        micro_batch_size=2,\n        num_dataset_workers=0,\n        data_dir=data_path,\n    )\n    experiment_config = ExperimentConfig(\n        save_every_n_steps=training_config.val_check_interval,\n        result_dir=result_dir,\n        experiment_name=\"test-experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"reduced_train_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n\n    optim_config = OptimizerSchedulerConfig(lr_scheduler=\"cosine\")\n    geneformer_config = geneformer_tiny_config(\n        seq_length=data_config.seq_length, initial_ckpt_path=args.initial_ckpt_path\n    )\n\n    return MainConfig(\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=geneformer_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.simple_parallel_recipe","title":"<code>simple_parallel_recipe(tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=1)</code>","text":"<p>Simple parallel config for Geneformer, only used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def simple_parallel_recipe(\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    num_devices: int = 1,\n    accumulate_grad_batches: int = 1,\n) -&gt; ParallelConfig:\n    \"\"\"Simple parallel config for Geneformer, only used in testing.\"\"\"\n    assert (\n        num_devices &gt;= tensor_model_parallel_size * pipeline_model_parallel_size\n    ), \"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\"\n    return ParallelConfig(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        accumulate_grad_batches=accumulate_grad_batches,\n        num_devices=num_devices,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/","title":"Index","text":""},{"location":"API_reference/bionemo/geneformer/scripts/#geneformer-scripts-directory","title":"Geneformer Scripts Directory","text":"<p>This is a collection for one-off scripts that can be ran through the command line. See the <code>[project.scripts]</code> section of the pyproject.toml file for how these are generated.</p>"},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/","title":"Infer geneformer","text":""},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/#bionemo.geneformer.scripts.infer_geneformer.geneformer_infer_entrypoint","title":"<code>geneformer_infer_entrypoint()</code>","text":"<p>Entrypoint for running inference on a geneformer checkpoint and data.</p> Source code in <code>bionemo/geneformer/scripts/infer_geneformer.py</code> <pre><code>def geneformer_infer_entrypoint():\n    \"\"\"Entrypoint for running inference on a geneformer checkpoint and data.\"\"\"\n    # 1. get arguments\n    parser = get_parser()\n    args = parser.parse_args()\n    # 2. Call infer with args\n    infer_model(\n        data_path=args.data_dir,\n        checkpoint_path=args.checkpoint_path,\n        results_path=args.results_path,\n        include_hiddens=args.include_hiddens,\n        micro_batch_size=args.micro_batch_size,\n        include_embeddings=not args.no_embeddings,\n        include_logits=args.include_logits,\n        include_input_ids=args.include_input_ids,\n        seq_length=args.seq_length,\n        precision=args.precision,\n        devices=args.num_gpus,\n        num_nodes=args.num_nodes,\n        num_dataset_workers=args.num_dataset_workers,\n        config_class=args.config_class,\n        include_unrecognized_vocab_in_dataset=args.include_unrecognized_vocab_in_dataset,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/#bionemo.geneformer.scripts.infer_geneformer.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the cli parser for this tool.</p> Source code in <code>bionemo/geneformer/scripts/infer_geneformer.py</code> <pre><code>def get_parser():\n    \"\"\"Return the cli parser for this tool.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Infer processed single cell data in SCDL memmap format with Geneformer from a checkpoint.\"\n    )\n    parser.add_argument(\n        \"--data-dir\",\n        type=Path,\n        required=True,\n        help=\"Path to the data directory, for example this might be \"\n        \"/workspace/bionemo2/data/cellxgene_2023-12-15_small/processed_train\",\n    )\n    parser.add_argument(\n        \"--checkpoint-path\",\n        type=Path,\n        required=False,\n        default=None,\n        help=\"Path to the checkpoint directory to restore from.\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=get_args(PrecisionTypes),\n        required=False,\n        default=\"bf16-mixed\",\n        help=\"Precision type to use for training.\",\n    )\n    parser.add_argument(\"--include-hiddens\", action=\"store_true\", default=False, help=\"Include hiddens in output.\")\n    parser.add_argument(\"--no-embeddings\", action=\"store_true\", default=False, help=\"Do not output embeddings.\")\n    parser.add_argument(\n        \"--include-logits\", action=\"store_true\", default=False, help=\"Include per-token logits in output.\"\n    )\n    parser.add_argument(\n        \"--include-input-ids\",\n        action=\"store_true\",\n        default=False,\n        help=\"Include input_ids in output of inference\",\n    )\n    parser.add_argument(\"--results-path\", type=Path, required=True, help=\"Path to the results directory.\")\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-nodes\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of nodes to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--prediction-interval\",\n        type=str,\n        required=False,\n        choices=get_args(IntervalT),\n        default=\"epoch\",\n        help=\"Intervals to write DDP predictions into disk\",\n    )\n    parser.add_argument(\n        \"--num-dataset-workers\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Number of steps to use for training. Default is 0.\",\n    )\n    parser.add_argument(\n        \"--seq-length\",\n        type=int,\n        required=False,\n        default=2048,\n        help=\"Sequence length of cell. Default is 2048.\",\n    )\n    parser.add_argument(\n        \"--micro-batch-size\",\n        type=int,\n        required=False,\n        default=32,\n        help=\"Micro-batch size. Global batch size is inferred from this.\",\n    )\n\n    parser.add_argument(\n        \"--include-unrecognized-vocab-in-dataset\",\n        action=\"store_true\",\n        help=\"If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.\",\n    )\n\n    # TODO consider whether nemo.run or some other method can simplify this config class lookup.\n    config_class_options: Dict[str, Type[BioBertConfig]] = {\n        \"GeneformerConfig\": GeneformerConfig,\n        \"FineTuneSeqLenBioBertConfig\": FineTuneSeqLenBioBertConfig,\n    }\n\n    def config_class_type(desc: str) -&gt; Type[BioBertConfig]:\n        try:\n            return config_class_options[desc]\n        except KeyError:\n            raise argparse.ArgumentTypeError(\n                f\"Do not recognize key {desc}, valid options are: {config_class_options.keys()}\"\n            )\n\n    parser.add_argument(\n        \"--config-class\",\n        type=config_class_type,\n        default=\"GeneformerConfig\",\n        help=\"Model configs link model classes with losses, and handle model initialization (including from a prior \"\n        \"checkpoint). This is how you can fine-tune a model. First train with one config class that points to one model \"\n        \"class and loss, then implement and provide an alternative config class that points to a variant of that model \"\n        \"and alternative loss. In the future this script should also provide similar support for picking different data \"\n        f\"modules for fine-tuning with different data types. Choices: {config_class_options.keys()}\",\n    )\n    return parser\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/#bionemo.geneformer.scripts.infer_geneformer.infer_model","title":"<code>infer_model(data_path, checkpoint_path, results_path, include_hiddens=False, include_embeddings=False, include_logits=False, include_input_ids=False, seq_length=2048, micro_batch_size=64, precision='bf16-mixed', tensor_model_parallel_size=1, pipeline_model_parallel_size=1, devices=1, num_nodes=1, num_dataset_workers=0, prediction_interval='epoch', config_class=GeneformerConfig, include_unrecognized_vocab_in_dataset=False)</code>","text":"<p>Inference function (requires DDP and only training data that fits in memory).</p> Source code in <code>bionemo/geneformer/scripts/infer_geneformer.py</code> <pre><code>def infer_model(\n    data_path: Path,\n    checkpoint_path: Path,\n    results_path: Path,\n    include_hiddens: bool = False,\n    include_embeddings: bool = False,\n    include_logits: bool = False,\n    include_input_ids: bool = False,\n    seq_length: int = 2048,\n    micro_batch_size: int = 64,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    devices: int = 1,\n    num_nodes: int = 1,\n    num_dataset_workers: int = 0,\n    prediction_interval: IntervalT = \"epoch\",\n    config_class: Type[BioBertConfig] = GeneformerConfig,\n    include_unrecognized_vocab_in_dataset: bool = False,\n) -&gt; None:\n    \"\"\"Inference function (requires DDP and only training data that fits in memory).\"\"\"\n    # create the directory to save the inference results\n    os.makedirs(results_path, exist_ok=True)\n\n    # This is just used to get the tokenizer :(\n    train_data_path: Path = (\n        load(\"single_cell/testdata-20241203\") / \"cellxgene_2023-12-15_small_processed_scdl\" / \"train\"\n    )\n\n    # Setup the strategy and trainer\n    pipeline_model_parallel_size = 1\n    tensor_model_parallel_size = 1\n    accumulate_grad_batches = 1\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=micro_batch_size,\n        num_nodes=num_nodes,\n        devices=devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n    )\n\n    preprocessor = GeneformerPreprocess(\n        download_directory=train_data_path,\n        medians_file_path=train_data_path / \"medians.json\",\n        tokenizer_vocab_path=train_data_path / \"geneformer.vocab\",\n    )\n    match preprocessor.preprocess():\n        case {\"tokenizer\": tokenizer, \"median_dict\": median_dict}:\n            logging.info(\"*************** Preprocessing Finished ************\")\n        case _:\n            logging.error(\"Preprocessing failed.\")\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n        progress_interval=1,\n    )\n\n    prediction_writer = PredictionWriter(output_dir=results_path, write_interval=prediction_interval)\n\n    trainer = nl.Trainer(\n        devices=devices,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        num_nodes=num_nodes,\n        callbacks=[prediction_writer],\n        plugins=nl.MegatronMixedPrecision(precision=precision),\n    )\n    # Configure the data module and model\n    datamodule = SingleCellDataModule(\n        seq_length=seq_length,\n        tokenizer=tokenizer,\n        train_dataset_path=None,\n        val_dataset_path=None,\n        test_dataset_path=None,\n        predict_dataset_path=data_path,\n        mask_prob=0,\n        mask_token_prob=0,\n        random_token_prob=0,  # changed to represent the incorrect setting we originally used.\n        median_dict=median_dict,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        # persistent workers is supported when num_dataset_workers &gt; 0\n        persistent_workers=num_dataset_workers &gt; 0,\n        pin_memory=False,\n        num_workers=num_dataset_workers,\n        include_unrecognized_vocab_in_dataset=include_unrecognized_vocab_in_dataset,\n    )\n    config = config_class(\n        seq_length=seq_length,\n        params_dtype=get_autocast_dtype(precision),\n        pipeline_dtype=get_autocast_dtype(precision),\n        autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n        # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n        initial_ckpt_path=str(checkpoint_path) if checkpoint_path is not None else None,\n        include_embeddings=include_embeddings,\n        include_hiddens=include_hiddens,\n        include_input_ids=include_input_ids,\n        skip_logits=not include_logits,\n        initial_ckpt_skip_keys_with_these_prefixes=[],  # load everything from the checkpoint.\n    )\n    # The lightning class owns a copy of the actual model, and a loss function, both of which are configured\n    #  and lazily returned by the `config` object defined above.\n    module = biobert_lightning_module(config=config, tokenizer=tokenizer)\n\n    trainer.predict(module, datamodule=datamodule)  # return_predictions=False failing due to a lightning bug\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/train_geneformer/","title":"Train geneformer","text":""},{"location":"API_reference/bionemo/geneformer/scripts/train_geneformer/#bionemo.geneformer.scripts.train_geneformer.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the cli parser for this tool.</p> Source code in <code>bionemo/geneformer/scripts/train_geneformer.py</code> <pre><code>def get_parser():\n    \"\"\"Return the cli parser for this tool.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Pretrain Geneformer with single cell data.\")\n    parser.add_argument(\n        \"--data-dir\",\n        type=Path,\n        required=True,\n        help=\"Path to the data base directory, for example this might be \"\n        \"/workspace/bionemo2/data/cellxgene_2023-12-15_small\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=get_args(PrecisionTypes),\n        required=False,\n        default=\"bf16-mixed\",\n        help=\"Precision type to use for training.\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        required=False,\n        default=1e-4,\n        help=\"Learning rate for training. Default is 1e-4. With bigger global batches try 1e-3\",\n    )\n    parser.add_argument(\n        \"--create-tensorboard-logger\", action=\"store_true\", default=False, help=\"Create a tensorboard logger.\"\n    )\n    # FIXME (@skothenhill) figure out how checkpointing and resumption should work with the new nemo trainer\n    parser.add_argument(\n        \"--resume-if-exists\", action=\"store_true\", default=False, help=\"Resume training if a checkpoint exists.\"\n    )\n    parser.add_argument(\n        \"--result-dir\", type=Path, required=False, default=Path(\"./results\"), help=\"Path to the result directory.\"\n    )\n    parser.add_argument(\n        \"--experiment-name\", type=str, required=False, default=\"geneformer\", help=\"Name of the experiment.\"\n    )\n    parser.add_argument(\"--wandb-entity\", type=str, default=None, help=\"The team posting this run\")\n    parser.add_argument(\"--wandb-project\", type=str, default=None, help=\"Wandb project name \")\n    parser.add_argument(\"--wandb-tags\", nargs=\"+\", type=str, default=[], help=\"Tags associated with this run\")\n    parser.add_argument(\n        \"--wandb-group\", type=str, default=None, help=\"A unique string shared by all runs in a given group\"\n    )\n    parser.add_argument(\n        \"--wandb-job-type\",\n        type=str,\n        default=None,\n        help=\"A unique string representing a type of run, which is useful when you're grouping runs together into larger experiments using group.\",\n    )\n    parser.add_argument(\n        \"--wandb-id\", type=str, default=None, help=\"Sets the version, mainly used to resume a previous run\"\n    )\n    parser.add_argument(\n        \"--wandb-anonymous\", action=\"store_true\", help=\"Enable or explicitly disable anonymous logging\"\n    )\n    parser.add_argument(\n        \"--wandb-log-model\", action=\"store_true\", help=\"Save checkpoints in wandb dir to upload on W&amp;B servers\"\n    )\n    parser.add_argument(\"--wandb-offline\", action=\"store_true\", help=\"Use wandb in offline mode\")\n    parser.add_argument(\n        \"--cosine-rampup-frac\",\n        type=float,\n        required=False,\n        default=0.01,\n        help=\"Fraction of steps in which to ramp up the learning rate. Default is 0.01.\",\n    )\n    parser.add_argument(\n        \"--include-unrecognized-vocab-in-dataset\",\n        action=\"store_true\",\n        help=\"If set to true, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded.\",\n    )\n    parser.add_argument(\n        \"--cosine-hold-frac\",\n        type=float,\n        required=False,\n        default=0.05,\n        help=\"Fraction of final steps in which to hold the minimum LR. Default is 0.05.\",\n    )\n\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-nodes\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of nodes to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-steps\",\n        type=int,\n        required=False,\n        default=10000,\n        help=\"Number of steps to use for training. Default is 10000.\",\n    )\n    parser.add_argument(\n        \"--num-dataset-workers\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Number of steps to use for training. Default is 0.\",\n    )\n    parser.add_argument(\n        \"--val-check-interval\",\n        type=int,\n        required=False,\n        default=10000,\n        help=\"Number of steps to use for training. Default is 10000.\",\n    )\n    parser.add_argument(\n        \"--log-every-n-steps\",\n        type=int,\n        required=False,\n        default=50,\n        help=\"Number of steps between logging. Default is 50.\",\n    )\n    parser.add_argument(\n        \"--seq-length\",\n        type=int,\n        required=False,\n        default=2048,\n        help=\"Sequence length of cell. Default is 2048.\",\n    )\n    parser.add_argument(\n        \"--limit-val-batches\",\n        type=float_or_int_or_none,\n        required=False,\n        default=2,\n        help=\"Number of global batches used for validation if int. Fraction of validation dataset if float. Default is 2.\",\n    )\n    parser.add_argument(\n        \"--micro-batch-size\",\n        type=int,\n        required=False,\n        default=64,\n        help=\"Micro-batch size. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--accumulate-grad-batches\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Gradient accumulation steps. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--biobert-spec-option\",\n        type=BiobertSpecOption,\n        choices=[e.value for e in BiobertSpecOption],\n        required=False,\n        default=BiobertSpecOption.bert_layer_with_transformer_engine_spec.value,\n        help=\"Biobert spec option to use for the model. Default is 'bert_layer_with_transformer_engine_spec'.\",\n    )\n    parser.add_argument(\n        \"--nemo1-init-path\",\n        type=Path,\n        required=False,\n        help=\"Path to nemo1 file, if desired to load at init time.\",\n    )\n    parser.add_argument(\n        \"--disable-checkpointing\",\n        action=\"store_false\",\n        default=True,\n        dest=\"create_checkpoint_callback\",\n        help=\"Disable creating a ModelCheckpoint callback.\",\n    )\n    parser.add_argument(\n        \"--save-best-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the best checkpoint based on the metric to monitor.\",\n    )\n    parser.add_argument(\n        \"--save-last-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the last checkpoint.\",\n    )\n    parser.add_argument(\n        \"--metric-to-monitor-for-checkpoints\",\n        type=str,\n        required=False,\n        default=\"val_loss\",\n        help=\"The metric to monitor for checkpointing.\",\n    )\n    parser.add_argument(\n        \"--save-top-k\",\n        type=int,\n        required=False,\n        default=2,\n        help=\"Save the top k checkpoints.\",\n    )\n    parser.add_argument(\n        \"--restore-from-checkpoint-path\",\n        type=Path,\n        required=False,\n        default=None,\n        help=\"Path to the checkpoint directory to restore from. Will override `--resume-if-exists` when set.\",\n    )\n    parser.add_argument(\"--num-layers\", type=int, default=6, help=\"Number of layers in geneformer. Default to 6.\")\n    parser.add_argument(\"--hidden-size\", type=int, default=256, help=\"Hidden size in geneformer. Default to 256.\")\n    parser.add_argument(\n        \"--ffn-hidden-size\", type=int, default=512, help=\"Feedforward hidden size in geneformer. Default to 512.\"\n    )\n    parser.add_argument(\n        \"--num-attention-heads\", type=int, default=4, help=\"Number of attention heads in geneformer. Default to 4.\"\n    )\n    # TODO consider whether nemo.run or some other method can simplify this config class lookup.\n    config_class_options: Dict[str, Type[BioBertConfig]] = {\n        \"GeneformerConfig\": GeneformerConfig,\n        \"FineTuneSeqLenBioBertConfig\": FineTuneSeqLenBioBertConfig,\n    }\n\n    def config_class_type(desc: str) -&gt; Type[BioBertConfig]:\n        try:\n            return config_class_options[desc]\n        except KeyError:\n            raise argparse.ArgumentTypeError(\n                f\"Do not recognize key {desc}, valid options are: {config_class_options.keys()}\"\n            )\n\n    parser.add_argument(\n        \"--training-model-config-class\",\n        type=config_class_type,\n        default=\"GeneformerConfig\",\n        help=\"Model configs link model classes with losses, and handle model initialization (including from a prior \"\n        \"checkpoint). This is how you can fine-tune a model. First train with one config class that points to one model \"\n        \"class and loss, then implement and provide an alternative config class that points to a variant of that model \"\n        \"and alternative loss. In the future this script should also provide similar support for picking different data \"\n        f\"modules for fine-tuning with different data types. Choices: {config_class_options.keys()}\",\n    )\n\n    parser.add_argument(\n        \"--nsys-profiling\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling output you must run the whole program with `nsys`. For example: \"\n        \" `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true --capture-range=cudaProfilerApi --capture-range-end=stop  [regular python command here]`\",\n    )\n    # start, end, rank\n    parser.add_argument(\n        \"--nsys-start-step\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Start nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--nsys-end-step\",\n        type=int,\n        required=False,\n        help=\"End nsys profiling after this step.\",\n    )\n    # rank as list of integers\n    parser.add_argument(\n        \"--nsys-ranks\",\n        type=int,\n        nargs=\"+\",\n        required=False,\n        default=[0],\n        help=\"Enable nsys profiling for these ranks.\",\n    )\n\n    parser.add_argument(\n        \"--gc-interval\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Run garbage collection on the cluster every --gc-interval steps, 0 to disable (default). Keeping gc interval\"\n        \" in sync this way on large cluster runs is important for training performance.\",\n    )\n\n    parser.add_argument(\n        \"--aligned-megatron-ddp\",\n        action=\"store_true\",\n        default=False,\n        help=\"By default param overlap/etc is disabled in megatron, this enables all of those settings. This is probably \"\n        \"good for cluster performance.\",\n    )\n    parser.add_argument(\n        \"--recompilation-check\",\n        action=\"store_true\",\n        default=False,\n        help=\"Activate this and make sure a small training loop runs, this tells you that your settings are not \"\n        \"triggering regular recompilations which can be very expensive for fused gpu kernels.\",\n    )\n\n    return parser\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/train_geneformer/#bionemo.geneformer.scripts.train_geneformer.main","title":"<code>main(data_dir, num_nodes, devices, seq_length, result_dir, num_steps, limit_val_batches, val_check_interval, num_dataset_workers, biobert_spec_option, lr, micro_batch_size, accumulate_grad_batches, cosine_rampup_frac, cosine_hold_frac, experiment_name, resume_if_exists, precision, wandb_entity=None, wandb_project=None, wandb_offline=False, wandb_tags=None, wandb_group=None, wandb_job_type=None, wandb_id=None, wandb_anonymous=False, wandb_log_model=False, create_tensorboard_logger=False, nemo1_init_path=None, create_checkpoint_callback=True, restore_from_checkpoint_path=None, num_layers=6, hidden_size=256, ffn_hidden_size=512, num_attention_heads=4, save_last_checkpoint=True, metric_to_monitor_for_checkpoints='val_loss', save_top_k=2, nsys_profiling=False, nsys_start_step=0, nsys_end_step=None, nsys_ranks=[0], config_class=GeneformerConfig, log_every_n_steps=50, gc_interval=0, aligned_megatron_ddp=False, recompilation_check=False, include_unrecognized_vocab_in_dataset=False)</code>","text":"<p>Train a Geneformer model on single cell data.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Base directory for the data.</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes to run on</p> required <code>devices</code> <code>int</code> <p>number of devices</p> required <code>seq_length</code> <code>int</code> <p>sequence length</p> required <code>result_dir</code> <code>Path</code> <p>directory to store results, logs and checkpoints</p> required <code>num_steps</code> <code>int</code> <p>number of steps to train the model for</p> required <code>limit_val_batches</code> <code>int</code> <p>limit the number of validation global batches to this many</p> required <code>val_check_interval</code> <code>int</code> <p>number of steps to periodically check the validation loss and save</p> required <code>num_dataset_workers</code> <code>int</code> <p>num dataset workers</p> required <code>biobert_spec_option</code> <code>BiobertSpecOption</code> <p>the biobert spec option (architecture) to use for this run</p> required <code>lr</code> <code>float</code> <p>learning rate</p> required <code>micro_batch_size</code> <code>int</code> <p>micro batch size, from this and parallelism settings we infer the global batch size</p> required <code>cosine_rampup_frac</code> <code>float</code> <p>fraction of steps at the beginning of the run to ramp up the learning rate</p> required <code>cosine_hold_frac</code> <code>float</code> <p>fraction of steps to hold the minimum learning rate at the end of the run</p> required <code>experiment_name</code> <code>str</code> <p>experiment name, this is the name used for the wandb run, and the sub-directory of the result_dir that stores the logs and checkpoints.</p> required <code>accumulate_grad_batches</code> <code>int</code> <p>if requested, gradients are only updated every <code>accumulate_grad_batches</code> steps.</p> required <code>config_class</code> <code>Type[BioBertConfig]</code> <p>which model config do you want to train?</p> <code>GeneformerConfig</code> <code>metric_to_monitor_for_checkpoints</code> <code>str</code> <p>which metric do you want to monitor for checkpoints?</p> <code>'val_loss'</code> <code>nemo1_init_path</code> <code>str</code> <p>if you have a nemo1 checkpoint you want to initialize the model weights from, you can provide that. Note that settings are not pulled from the model.</p> <code>None</code> <code>precision</code> <code>str</code> <p>desired training precision</p> required <code>save_last_checkpoint</code> <code>bool</code> <p>if you want the last checkpoint saved</p> <code>True</code> <code>save_top_k</code> <code>int</code> <p>if you want the top k checkpoints all saved.</p> <code>2</code> <code>resume_if_exists</code> <code>bool</code> <p>attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]</p> required <code>wandb_entity</code> <code>str</code> <p>The team posting this run (default: your username or your default team)</p> <code>None</code> <code>wandb_project</code> <code>str</code> <p>The name of the project to which this run will belong.</p> <code>None</code> <code>wandb_tags</code> <code>List[str]</code> <p>Tags associated with this run.</p> <code>None</code> <code>wandb_group</code> <code>str</code> <p>A unique string shared by all runs in a given group</p> <code>None</code> <code>wandb_job_type</code> <code>Optional[str]</code> <p>Type of run, which is useful when you're grouping runs together into larger experiments using group.</p> <code>None</code> <code>wandb_offline</code> <code>bool</code> <p>Run offline (data can be streamed later to wandb servers).</p> <code>False</code> <code>wandb_id</code> <code>str</code> <p>Sets the version, mainly used to resume a previous run.</p> <code>None</code> <code>wandb_anonymous</code> <code>bool</code> <p>Enables or explicitly disables anonymous logging.</p> <code>False</code> <code>wandb_log_model</code> <code>bool</code> <p>Save checkpoints in wandb dir to upload on W&amp;B servers.</p> <code>False</code> <code>create_tensorboard_logger</code> <code>bool</code> <p>create the tensorboard logger</p> <code>False</code> <code>create_checkpoint_callback</code> <code>bool</code> <p>create a ModelCheckpoint callback and attach it to the pytorch lightning trainer</p> <code>True</code> <code>restore_from_checkpoint_path</code> <code>path</code> <p>If set, restores the model from the directory passed in. Expects the checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.</p> <code>None</code> <code>num_layers</code> <code>int</code> <p>Number of layers in geneformer. Default to 6.</p> <code>6</code> <code>hidden_size</code> <code>int</code> <p>Hidden size in geneformer. Default to 256.</p> <code>256</code> <code>ffn_hidden_size</code> <code>int</code> <p>Feedforward hidden size in geneformer. Default to 512.</p> <code>512</code> <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads in geneformer. Default to 4.</p> <code>4</code> <code>log_every_n_steps</code> <code>int</code> <p>log at this interval.</p> <code>50</code> <code>nsys_profiling</code> <code>bool</code> <p>Whether to enable the nsys profiling callback hooks. You still need to execute the function with nsys on the command line, but this enables more useful outputs in your nsys profiles, as well as control over which step ranges are captured.</p> <code>False</code> <code>nsys_start_step</code> <code>int</code> <p>Step to start profiling.</p> <code>0</code> <code>nsys_ranks</code> <code>list[int]</code> <p>GPU/node ranks to profile. Defaults to [0] (only main gpu.)</p> <code>[0]</code> <code>nsys_end_step</code> <code>int</code> <p>Step to stop profiling.</p> <code>None</code> <code>gc_interval</code> <code>int</code> <p>if a value &gt; 0 is provided, this will turn off automatic garbage collection and only run at this requested interval of train/val steps. This will likely slow down single GPU runs.</p> <code>0</code> <code>aligned_megatron_ddp</code> <code>bool</code> <p>if activated, this will activate a number of communication optimizations that are good for clusters. This will likely slow down single node runs though.</p> <code>False</code> <code>recompilation_check</code> <code>bool</code> <p>enable a recompilation check (only do on a small run) to verify that fused gpu kernels are not being regularly recompiled, which is very expensive, with a particular model/settings.</p> <code>False</code> <code>include_unrecognized_vocab_in_dataset</code> <code>bool</code> <p>If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded..</p> <code>False</code> Source code in <code>bionemo/geneformer/scripts/train_geneformer.py</code> <pre><code>def main(\n    data_dir: Path,\n    num_nodes: int,\n    devices: int,\n    seq_length: int,\n    result_dir: Path,\n    num_steps: int,\n    limit_val_batches: int,\n    val_check_interval: int,\n    num_dataset_workers: int,\n    biobert_spec_option: BiobertSpecOption,\n    lr: float,\n    micro_batch_size: int,\n    accumulate_grad_batches: int,\n    cosine_rampup_frac: float,\n    cosine_hold_frac: float,\n    experiment_name: str,\n    resume_if_exists: bool,\n    precision: PrecisionTypes,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_offline: bool = False,\n    wandb_tags: List[str] | None = None,\n    wandb_group: Optional[str] = None,\n    wandb_job_type: Optional[str] = None,\n    wandb_id: Optional[str] = None,\n    wandb_anonymous: bool = False,\n    wandb_log_model: bool = False,\n    create_tensorboard_logger: bool = False,\n    nemo1_init_path: Path | None = None,\n    create_checkpoint_callback: bool = True,\n    restore_from_checkpoint_path: Path | None = None,\n    num_layers: int = 6,\n    hidden_size: int = 256,\n    ffn_hidden_size: int = 512,\n    num_attention_heads: int = 4,\n    save_last_checkpoint: bool = True,\n    metric_to_monitor_for_checkpoints: str = \"val_loss\",\n    save_top_k: int = 2,\n    nsys_profiling: bool = False,\n    nsys_start_step: int = 0,\n    nsys_end_step: Optional[int] = None,\n    nsys_ranks: List[int] = [0],\n    config_class: Type[BioBertConfig] = GeneformerConfig,\n    log_every_n_steps: int = 50,\n    gc_interval: int = 0,\n    aligned_megatron_ddp: bool = False,\n    recompilation_check: bool = False,\n    include_unrecognized_vocab_in_dataset: bool = False,\n    # TODO add datamodule class, and ability to change data step to get full support for pretraining workflows\n) -&gt; None:\n    \"\"\"Train a Geneformer model on single cell data.\n\n    Args:\n        data_dir (Path): Base directory for the data.\n        num_nodes (int): Number of nodes to run on\n        devices (int): number of devices\n        seq_length (int): sequence length\n        result_dir (Path): directory to store results, logs and checkpoints\n        num_steps (int): number of steps to train the model for\n        limit_val_batches (int): limit the number of validation global batches to this many\n        val_check_interval (int): number of steps to periodically check the validation loss and save\n        num_dataset_workers (int): num dataset workers\n        biobert_spec_option (BiobertSpecOption): the biobert spec option (architecture) to use for this run\n        lr (float): learning rate\n        micro_batch_size (int): micro batch size, from this and parallelism settings we infer the global batch size\n        cosine_rampup_frac (float): fraction of steps at the beginning of the run to ramp up the learning rate\n        cosine_hold_frac (float): fraction of steps to hold the minimum learning rate at the end of the run\n        experiment_name (str): experiment name, this is the name used for the wandb run, and the sub-directory of the\n            result_dir that stores the logs and checkpoints.\n        accumulate_grad_batches (int): if requested, gradients are only updated every `accumulate_grad_batches` steps.\n        config_class (Type[BioBertConfig]): which model config do you want to train?\n        metric_to_monitor_for_checkpoints (str): which metric do you want to monitor for checkpoints?\n        nemo1_init_path (str): if you have a nemo1 checkpoint you want to initialize the model weights from, you can\n            provide that. Note that settings are not pulled from the model.\n        precision (str): desired training precision\n        save_last_checkpoint (bool): if you want the last checkpoint saved\n        save_top_k (int): if you want the top k checkpoints all saved.\n        resume_if_exists (bool): attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]\n        wandb_entity (str): The team posting this run (default: your username or your default team)\n        wandb_project (str): The name of the project to which this run will belong.\n        wandb_tags (List[str]): Tags associated with this run.\n        wandb_group (str): A unique string shared by all runs in a given group\n        wandb_job_type (Optional[str]): Type of run, which is useful when you're grouping runs together into larger experiments using group.\n        wandb_offline (bool): Run offline (data can be streamed later to wandb servers).\n        wandb_id (str): Sets the version, mainly used to resume a previous run.\n        wandb_anonymous (bool): Enables or explicitly disables anonymous logging.\n        wandb_log_model (bool): Save checkpoints in wandb dir to upload on W&amp;B servers.\n        create_tensorboard_logger (bool): create the tensorboard logger\n        create_checkpoint_callback (bool): create a ModelCheckpoint callback and attach it to the pytorch lightning trainer\n        restore_from_checkpoint_path (path): If set, restores the model from the directory passed in. Expects the\n            checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.\n        num_layers (int): Number of layers in geneformer. Default to 6.\n        hidden_size (int): Hidden size in geneformer. Default to 256.\n        ffn_hidden_size (int): Feedforward hidden size in geneformer. Default to 512.\n        num_attention_heads (int): Number of attention heads in geneformer. Default to 4.\n        log_every_n_steps (int): log at this interval.\n        nsys_profiling (bool): Whether to enable the nsys profiling callback hooks. You still need to execute the\n            function with nsys on the command line, but this enables more useful outputs in your nsys profiles, as\n            well as control over which step ranges are captured.\n        nsys_start_step (int): Step to start profiling.\n        nsys_ranks (list[int]): GPU/node ranks to profile. Defaults to [0] (only main gpu.)\n        nsys_end_step (int): Step to stop profiling.\n        gc_interval (int): if a value &gt; 0 is provided, this will turn off automatic garbage collection and only run\n            at this requested interval of train/val steps. This will likely slow down single GPU runs.\n        aligned_megatron_ddp (bool): if activated, this will activate a number of communication optimizations that are\n            good for clusters. This will likely slow down single node runs though.\n        recompilation_check (bool): enable a recompilation check (only do on a small run) to verify that fused gpu\n            kernels are not being regularly recompiled, which is very expensive, with a particular model/settings.\n        include_unrecognized_vocab_in_dataset (bool): If set to True, a hard-check is performed to verify all gene identifers are in the user supplied tokenizer vocab. Defaults to False which means any gene identifier not in the user supplied tokenizer vocab will be excluded..\n    \"\"\"\n    # Create the result directory if it does not exist.\n    if wandb_tags is None:\n        wandb_tags = []\n    result_dir.mkdir(parents=True, exist_ok=True)\n    val_check_interval = min(val_check_interval, num_steps)  # Training will fail if val_check_interval &gt; num_steps\n\n    # Setup train/test/val data paths\n    train_data_path = data_dir / \"train\"\n    val_data_path = data_dir / \"val\"\n    test_data_path = data_dir / \"test\"\n\n    # Setup the strategy and trainer\n    pipeline_model_parallel_size = 1\n    tensor_model_parallel_size = 1\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=micro_batch_size,\n        num_nodes=num_nodes,\n        devices=devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n    )\n    if aligned_megatron_ddp:\n        ddp: str | DistributedDataParallelConfig = DistributedDataParallelConfig(\n            check_for_nan_in_grad=True,\n            grad_reduce_in_fp32=False,\n            overlap_grad_reduce=True,\n            overlap_param_gather=True,\n            average_in_collective=True,\n            use_distributed_optimizer=True,  # this should inherit from the optimizer config, but just in case...\n        )\n    else:\n        ddp = \"megatron\"  # this will launch DistributedDataParallelConfig(check_for_nan_in_grad=True).\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        ddp=ddp,\n        progress_interval=log_every_n_steps,\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n        gradient_as_bucket_view=True,\n        ckpt_async_save=True,\n        ckpt_parallel_load=True,\n    )\n\n    # for wandb integration\n    # Please refer to https://pytorch-lightning.readthedocs.io/en/0.7.6/api/lightning.pytorch.loggers.html\"\n    wandb_options: Optional[WandbConfig] = (\n        None\n        if wandb_project is None\n        else WandbConfig(\n            offline=wandb_offline,\n            project=wandb_project,\n            entity=wandb_entity,\n            tags=wandb_tags,\n            group=wandb_group,\n            job_type=wandb_job_type,\n            id=wandb_id,\n            anonymous=wandb_anonymous,\n            log_model=wandb_log_model,\n        )\n    )\n    callbacks = [\n        # Skip perplexity and disable forward output in the loss for speed\n        RichModelSummary(max_depth=4),\n        TimingCallback(),\n        LearningRateMonitor(),\n    ]\n\n    if gc_interval &gt; 0:\n        callbacks.append(\n            nl_callbacks.GarbageCollectionCallback(gc_interval_train=gc_interval, gc_interval_val=gc_interval)\n        )\n\n    if nsys_profiling:\n        if nsys_end_step is None:\n            nsys_end_step = num_steps\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=nsys_start_step, end_step=nsys_end_step, ranks=nsys_ranks, gen_shape=True\n            )\n        )\n\n    trainer = nl.Trainer(\n        devices=devices,\n        max_steps=num_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        limit_val_batches=limit_val_batches,  # This controls upsampling and downsampling\n        val_check_interval=val_check_interval,  # TODO(@jstjohn) Checkpoint saving is currently broken, fix and change this.\n        log_every_n_steps=log_every_n_steps,\n        num_nodes=num_nodes,\n        callbacks=callbacks,\n        use_distributed_sampler=False,\n        plugins=nl.MegatronMixedPrecision(precision=precision),\n        enable_checkpointing=create_checkpoint_callback,\n    )\n\n    preprocessor = GeneformerPreprocess(\n        download_directory=train_data_path,\n        medians_file_path=train_data_path / \"medians.json\",\n        tokenizer_vocab_path=train_data_path / \"geneformer.vocab\",\n    )\n    match preprocessor.preprocess():\n        case {\"tokenizer\": tokenizer, \"median_dict\": median_dict}:\n            logging.info(\"*************** Preprocessing Finished ************\")\n        case _:\n            logging.error(\"Preprocessing failed.\")\n\n    # Configure the data module and model\n    data = SingleCellDataModule(\n        seq_length=seq_length,\n        tokenizer=tokenizer,\n        train_dataset_path=str(train_data_path),\n        val_dataset_path=str(val_data_path),\n        test_dataset_path=str(test_data_path),\n        random_token_prob=0.02,  # changed to represent the incorrect setting we originally used.\n        median_dict=median_dict,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        # persistent workers is supported when num_dataset_workers &gt; 0\n        persistent_workers=num_dataset_workers &gt; 0,\n        pin_memory=False,\n        num_workers=num_dataset_workers,\n        include_unrecognized_vocab_in_dataset=include_unrecognized_vocab_in_dataset,\n    )\n    geneformer_config = config_class(\n        num_layers=num_layers,\n        hidden_size=hidden_size,\n        ffn_hidden_size=ffn_hidden_size,\n        num_attention_heads=num_attention_heads,\n        seq_length=seq_length,\n        bias_dropout_fusion=True,  # TODO fix the recompilation issue, but for now it's faster even with recompilations\n        bias_activation_fusion=True,  # TODO same note as above. Set these to False to see recompilation go away\n        defer_embedding_wgrad_compute=pipeline_model_parallel_size &gt; 1,\n        params_dtype=get_autocast_dtype(precision),\n        pipeline_dtype=get_autocast_dtype(precision),\n        autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n        # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n        initial_ckpt_path=str(restore_from_checkpoint_path) if restore_from_checkpoint_path is not None else None,\n    )\n\n    # The lightning class owns a copy of the actual model, and a loss function, both of which are configured\n    #  and lazily returned by the `geneformer_config` object defined above.\n    model = biobert_lightning_module(\n        geneformer_config,\n        tokenizer=tokenizer,\n        optimizer=MegatronOptimizerModule(\n            config=OptimizerConfig(\n                lr=lr,\n                # TODO(@jstjohn) try decoupled_lr\n                optimizer=\"adam\",\n                use_distributed_optimizer=True,\n                # Pass through fp16/bf16 settings to avoid errors around model having bf16 enabled but optimizer not.\n                fp16=geneformer_config.fp16,\n                bf16=geneformer_config.bf16,\n            ),\n            lr_scheduler=CosineAnnealingScheduler(\n                max_steps=num_steps,\n                # minimum learning rate is 1/100th of the initial learning rate, so eg lr=1e-3 -&gt; min_lr=1e-5\n                min_lr=lr / 100,\n                warmup_steps=int(math.ceil(num_steps * cosine_rampup_frac)),\n                interval=\"step\",\n                monitor=\"val_loss\",\n                constant_steps=int(math.ceil(num_steps * cosine_hold_frac)),\n            ),\n        ),\n    )\n    # Configure our custom Checkpointer\n    if create_checkpoint_callback:\n        checkpoint_callback = nl_callbacks.ModelCheckpoint(\n            save_last=save_last_checkpoint,\n            monitor=metric_to_monitor_for_checkpoints,\n            save_top_k=save_top_k,\n            every_n_train_steps=val_check_interval,\n            always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n            filename=\"{epoch}-{val_loss:.2f}-{step}-{consumed_samples}\",  # Including step and consumed_samples in the checkpoint filename prevents duplicate filenames and bugs related to this.\n        )\n    else:\n        checkpoint_callback = None\n\n    # Setup the logger and train the model\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=result_dir,\n        name=experiment_name,\n        initialize_tensorboard_logger=create_tensorboard_logger,\n        wandb_config=wandb_options,\n        ckpt_callback=checkpoint_callback,\n    )\n    if recompilation_check:\n        \"\"\"This is _very_ useful for debugging slow forward passes. Check that your fused kernels are not\n        getting recompiled. Once verified, turn this off again.\n        \"\"\"\n        torch._dynamo.config.error_on_recompile = True\n    llm.train(\n        model=model,\n        data=data,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            # TODO: uncomment this once nemo2 supports our fine-tuning workflow\n            #  for now this happens inside of our config file in the configure_model step.\n            # path=restore_from_checkpoint_path,\n            resume_if_exists=resume_if_exists,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/","title":"Gene tokenizer","text":""},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer","title":"<code>GeneTokenizer</code>","text":"<p>               Bases: <code>Label2IDTokenizer</code>, <code>IOMixin</code></p> <p>Initializes the GeneTokenizer object.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>class GeneTokenizer(Label2IDTokenizer, io.IOMixin):\n    \"\"\"Initializes the GeneTokenizer object.\"\"\"\n\n    cls_token: str = \"[CLS]\"\n    mask_token: str = \"[MASK]\"\n    pad_token: str = \"[PAD]\"\n    sep_token: str = \"[SEP]\"\n    ukw_token: str = \"[UKW]\"\n    special_tokens: Tuple[str, str, str, str, str] = (cls_token, mask_token, pad_token, sep_token, ukw_token)\n\n    def __init__(self, vocab: Dict[str, int], gene_to_ens: Dict[str, str]):  # noqa: D107\n        # Sets up vocab/decode_vocab dictionaries, parent class is sateful.\n        super().__init__()\n        assert set(self.special_tokens).issubset(\n            set(vocab.keys())\n        ), f\"Vocab must contain all of {self.special_tokens}, missing {set(self.special_tokens) - set(vocab.keys())}\"\n        self.gene_to_ens = deepcopy(gene_to_ens)\n        self.ens_to_gene = {v: k for k, v in self.gene_to_ens.items()}\n        self.vocab = deepcopy(vocab)\n        self.decode_vocab = {v: k for k, v in self.vocab.items()}\n\n    @classmethod\n    def from_medians_and_genes_dicts(cls, median_dict: Dict[str, float], gene_to_ens: Dict[str, str]) -&gt; T:\n        \"\"\"Creates a tokenizer from a median dictionary.\"\"\"\n        tokens = list(cls.special_tokens) + list(median_dict.keys())\n        vocab = cls._build_vocab(tokens)\n        return cls(vocab, gene_to_ens)\n\n    @staticmethod\n    def _build_vocab(strings: Union[List[str], str]) -&gt; Dict[str, int]:\n        \"\"\"We override the parent because complete strings are tokens. Otherwise, has the same behavior.\"\"\"\n        vocab: Dict[str, int] = {}\n        if isinstance(strings, str):\n            strings = [strings]\n\n        for token in strings:\n            if token not in vocab:\n                vocab[token] = len(vocab)\n        return vocab\n\n    def token_to_id(self, token: str) -&gt; int:\n        \"\"\"Converts a token to its corresponding ID.\n\n        Args:\n            token: The token to be converted.\n\n        Returns:\n            The ID corresponding to the token.\n        \"\"\"\n        return self.vocab.get(token)\n\n    @property\n    def pad_id(self) -&gt; int:  # noqa: D102\n        return self.token_to_id(self.pad_token)\n\n    @property\n    def mask_token_id(self) -&gt; int:  # noqa: D102\n        return self.token_to_id(self.mask_token)\n\n    @property\n    def all_special_ids(self) -&gt; list[int]:  # noqa: D102\n        return [self.token_to_id(tok) for tok in self.special_tokens]\n\n    @property\n    def class_id(self) -&gt; int:  # noqa: D102\n        return self.token_to_id(self.cls_token)\n\n    def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:  # noqa: D102\n        return super().tokens_to_ids(tokens)\n\n    def save_vocab(self, vocab_file: str) -&gt; None:\n        \"\"\"Saves the vocabulary as a newline delimieted vocabulary file, each line represents an int -&gt; token mapping. line number is assumed to be the integer.\"\"\"\n        vocab_dir = os.path.dirname(vocab_file)\n        if not os.path.exists(vocab_dir):\n            os.makedirs(vocab_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n\n        to_serialize = {}\n        to_serialize[\"vocab\"] = self.vocab\n        to_serialize[\"gene_to_ens\"] = self.gene_to_ens\n\n        with open(vocab_file, \"w\") as f:\n            json.dump(to_serialize, f)\n\n    @classmethod\n    def from_vocab_file(cls, vocab_file: str) -&gt; None:\n        \"\"\"This method adds a layer on the constructor in the case we are working from a filename instead of a dictionary.\"\"\"\n        if not os.path.exists(vocab_file):\n            raise FileNotFoundError(f\"Vocab file {vocab_file} not found, run preprocessing to create it.\")\n\n        with open(vocab_file) as f:\n            to_deserialize = json.load(f)\n            vocab = to_deserialize[\"vocab\"]\n            gene_to_ens = to_deserialize[\"gene_to_ens\"]\n\n        tokenizer = GeneTokenizer(vocab, gene_to_ens)\n        return tokenizer\n\n    def gene_tok_to_ens(self, gene: str) -&gt; str:\n        \"\"\"Converts a gene token to its corresponding Ensembl ID.\n\n        Args:\n            gene (str): The gene token to be converted.\n\n        Returns:\n            str: The Ensembl ID corresponding to the gene token.\n        \"\"\"\n        return self.gene_to_ens[gene]\n\n    def ens_tok_to_gene(self, ens: str) -&gt; str:\n        \"\"\"Converts an Ensembl token to a gene name.\n\n        Args:\n            ens (str): The Ensembl token to be converted.\n\n        Returns:\n            str: The corresponding gene name.\n        \"\"\"\n        return self.ens_to_gene[ens]\n\n    def genes_to_enss(self, genes: List[str]) -&gt; List[str]:\n        \"\"\"Converts a list of gene names to Ensembl IDs.\n\n        Args:\n            genes (List[str]): A list of gene names.\n\n        Returns:\n            List[str]: A list of corresponding Ensembl IDs.\n\n        Raises:\n            ValueError: If a gene name is not found in the gene_to_ens dictionary.\n        \"\"\"\n        ens_ids = []\n        for gene in genes:\n            if gene in self.gene_to_ens:\n                ens_ids.append(self.gene_to_ens[gene])\n            else:\n                raise ValueError(f\"{gene} not found\")\n        return ens_ids\n\n    def enss_to_genes(self, ensemble_ids: List[str]) -&gt; List[str]:\n        \"\"\"Converts a list of ensemble IDs to gene names.\n\n        Args:\n            ensemble_ids (List[str]): A list of ensemble IDs.\n\n        Returns:\n            List[str]: A list of gene names corresponding to the ensemble IDs.\n\n        Raises:\n            ValueError: If an ensemble ID is not found in the mapping.\n        \"\"\"\n        genes = []\n        for ens_id in ensemble_ids:\n            if ens_id in self.ens_to_gene:\n                genes.append(self.ens_to_gene[ens_id])\n            else:\n                raise ValueError(f\"{ens_id} not found\")\n        return genes\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer._build_vocab","title":"<code>_build_vocab(strings)</code>  <code>staticmethod</code>","text":"<p>We override the parent because complete strings are tokens. Otherwise, has the same behavior.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>@staticmethod\ndef _build_vocab(strings: Union[List[str], str]) -&gt; Dict[str, int]:\n    \"\"\"We override the parent because complete strings are tokens. Otherwise, has the same behavior.\"\"\"\n    vocab: Dict[str, int] = {}\n    if isinstance(strings, str):\n        strings = [strings]\n\n    for token in strings:\n        if token not in vocab:\n            vocab[token] = len(vocab)\n    return vocab\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.ens_tok_to_gene","title":"<code>ens_tok_to_gene(ens)</code>","text":"<p>Converts an Ensembl token to a gene name.</p> <p>Parameters:</p> Name Type Description Default <code>ens</code> <code>str</code> <p>The Ensembl token to be converted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The corresponding gene name.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def ens_tok_to_gene(self, ens: str) -&gt; str:\n    \"\"\"Converts an Ensembl token to a gene name.\n\n    Args:\n        ens (str): The Ensembl token to be converted.\n\n    Returns:\n        str: The corresponding gene name.\n    \"\"\"\n    return self.ens_to_gene[ens]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.enss_to_genes","title":"<code>enss_to_genes(ensemble_ids)</code>","text":"<p>Converts a list of ensemble IDs to gene names.</p> <p>Parameters:</p> Name Type Description Default <code>ensemble_ids</code> <code>List[str]</code> <p>A list of ensemble IDs.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of gene names corresponding to the ensemble IDs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an ensemble ID is not found in the mapping.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def enss_to_genes(self, ensemble_ids: List[str]) -&gt; List[str]:\n    \"\"\"Converts a list of ensemble IDs to gene names.\n\n    Args:\n        ensemble_ids (List[str]): A list of ensemble IDs.\n\n    Returns:\n        List[str]: A list of gene names corresponding to the ensemble IDs.\n\n    Raises:\n        ValueError: If an ensemble ID is not found in the mapping.\n    \"\"\"\n    genes = []\n    for ens_id in ensemble_ids:\n        if ens_id in self.ens_to_gene:\n            genes.append(self.ens_to_gene[ens_id])\n        else:\n            raise ValueError(f\"{ens_id} not found\")\n    return genes\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.from_medians_and_genes_dicts","title":"<code>from_medians_and_genes_dicts(median_dict, gene_to_ens)</code>  <code>classmethod</code>","text":"<p>Creates a tokenizer from a median dictionary.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>@classmethod\ndef from_medians_and_genes_dicts(cls, median_dict: Dict[str, float], gene_to_ens: Dict[str, str]) -&gt; T:\n    \"\"\"Creates a tokenizer from a median dictionary.\"\"\"\n    tokens = list(cls.special_tokens) + list(median_dict.keys())\n    vocab = cls._build_vocab(tokens)\n    return cls(vocab, gene_to_ens)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.from_vocab_file","title":"<code>from_vocab_file(vocab_file)</code>  <code>classmethod</code>","text":"<p>This method adds a layer on the constructor in the case we are working from a filename instead of a dictionary.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>@classmethod\ndef from_vocab_file(cls, vocab_file: str) -&gt; None:\n    \"\"\"This method adds a layer on the constructor in the case we are working from a filename instead of a dictionary.\"\"\"\n    if not os.path.exists(vocab_file):\n        raise FileNotFoundError(f\"Vocab file {vocab_file} not found, run preprocessing to create it.\")\n\n    with open(vocab_file) as f:\n        to_deserialize = json.load(f)\n        vocab = to_deserialize[\"vocab\"]\n        gene_to_ens = to_deserialize[\"gene_to_ens\"]\n\n    tokenizer = GeneTokenizer(vocab, gene_to_ens)\n    return tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.gene_tok_to_ens","title":"<code>gene_tok_to_ens(gene)</code>","text":"<p>Converts a gene token to its corresponding Ensembl ID.</p> <p>Parameters:</p> Name Type Description Default <code>gene</code> <code>str</code> <p>The gene token to be converted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Ensembl ID corresponding to the gene token.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def gene_tok_to_ens(self, gene: str) -&gt; str:\n    \"\"\"Converts a gene token to its corresponding Ensembl ID.\n\n    Args:\n        gene (str): The gene token to be converted.\n\n    Returns:\n        str: The Ensembl ID corresponding to the gene token.\n    \"\"\"\n    return self.gene_to_ens[gene]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.genes_to_enss","title":"<code>genes_to_enss(genes)</code>","text":"<p>Converts a list of gene names to Ensembl IDs.</p> <p>Parameters:</p> Name Type Description Default <code>genes</code> <code>List[str]</code> <p>A list of gene names.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of corresponding Ensembl IDs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a gene name is not found in the gene_to_ens dictionary.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def genes_to_enss(self, genes: List[str]) -&gt; List[str]:\n    \"\"\"Converts a list of gene names to Ensembl IDs.\n\n    Args:\n        genes (List[str]): A list of gene names.\n\n    Returns:\n        List[str]: A list of corresponding Ensembl IDs.\n\n    Raises:\n        ValueError: If a gene name is not found in the gene_to_ens dictionary.\n    \"\"\"\n    ens_ids = []\n    for gene in genes:\n        if gene in self.gene_to_ens:\n            ens_ids.append(self.gene_to_ens[gene])\n        else:\n            raise ValueError(f\"{gene} not found\")\n    return ens_ids\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.save_vocab","title":"<code>save_vocab(vocab_file)</code>","text":"<p>Saves the vocabulary as a newline delimieted vocabulary file, each line represents an int -&gt; token mapping. line number is assumed to be the integer.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def save_vocab(self, vocab_file: str) -&gt; None:\n    \"\"\"Saves the vocabulary as a newline delimieted vocabulary file, each line represents an int -&gt; token mapping. line number is assumed to be the integer.\"\"\"\n    vocab_dir = os.path.dirname(vocab_file)\n    if not os.path.exists(vocab_dir):\n        os.makedirs(vocab_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n\n    to_serialize = {}\n    to_serialize[\"vocab\"] = self.vocab\n    to_serialize[\"gene_to_ens\"] = self.gene_to_ens\n\n    with open(vocab_file, \"w\") as f:\n        json.dump(to_serialize, f)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.token_to_id","title":"<code>token_to_id(token)</code>","text":"<p>Converts a token to its corresponding ID.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token to be converted.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The ID corresponding to the token.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def token_to_id(self, token: str) -&gt; int:\n    \"\"\"Converts a token to its corresponding ID.\n\n    Args:\n        token: The token to be converted.\n\n    Returns:\n        The ID corresponding to the token.\n    \"\"\"\n    return self.vocab.get(token)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/","title":"Atom featurizers","text":""},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AromaticityFeaturizer","title":"<code>AromaticityFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom based on its aromaticity.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class AromaticityFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom based on its aromaticity.\"\"\"\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 1\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes features of atoms of all of select atoms.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor of representing if atoms are aromatic as integers.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor([int(mol.GetAtomWithIdx(a).GetIsAromatic()) for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AromaticityFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AromaticityFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes features of atoms of all of select atoms.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor of representing if atoms are aromatic as integers.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes features of atoms of all of select atoms.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor of representing if atoms are aromatic as integers.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor([int(mol.GetAtomWithIdx(a).GetIsAromatic()) for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicNumberFeaturizer","title":"<code>AtomicNumberFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its atomic number.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class AtomicNumberFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its atomic number.\"\"\"\n\n    def __init__(self, dim_atomic_num: Optional[int] = None) -&gt; None:\n        \"\"\"Initializes AtomicNumberFeaturizer class.\"\"\"\n        DIM_ATOMIC_NUM = 118\n        self.dim_atomic_num = dim_atomic_num if dim_atomic_num else DIM_ATOMIC_NUM\n\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return self.dim_atomic_num\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes features of atoms of all of select atoms.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor of integers representing atomic numbers of atoms.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor([mol.GetAtomWithIdx(a).GetAtomicNum() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicNumberFeaturizer.__init__","title":"<code>__init__(dim_atomic_num=None)</code>","text":"<p>Initializes AtomicNumberFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self, dim_atomic_num: Optional[int] = None) -&gt; None:\n    \"\"\"Initializes AtomicNumberFeaturizer class.\"\"\"\n    DIM_ATOMIC_NUM = 118\n    self.dim_atomic_num = dim_atomic_num if dim_atomic_num else DIM_ATOMIC_NUM\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicNumberFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes features of atoms of all of select atoms.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor of integers representing atomic numbers of atoms.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes features of atoms of all of select atoms.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor of integers representing atomic numbers of atoms.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor([mol.GetAtomWithIdx(a).GetAtomicNum() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicNumberFeaturizer.n_dim","title":"<code>n_dim()</code>","text":"<p>Returns dimensionality of the computed features.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def n_dim(self) -&gt; int:\n    \"\"\"Returns dimensionality of the computed features.\"\"\"\n    return self.dim_atomic_num\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicRadiusFeaturizer","title":"<code>AtomicRadiusFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its bond, covalent, and vdW radii.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class AtomicRadiusFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its bond, covalent, and vdW radii.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes AtomicRadiusFeaturizer class.\"\"\"\n        self.pt = Chem.GetPeriodicTable()\n        self._min_val = torch.Tensor(\n            [\n                0.0,  # Bond radius\n                0.28,  # Covalent radius\n                1.2,  # van der Waals radius\n            ]\n        )\n\n        self._max_val = torch.Tensor(\n            [\n                2.4,  # Bond radius\n                2.6,  # Covalent radius\n                3.0,  # van der Waals radius\n            ]\n        )\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 3\n\n    @property\n    def min_val(self) -&gt; torch.tensor:\n        \"\"\"Returns minimum values for features: bond, covalent, and vdW radius.\"\"\"\n        return self._min_val\n\n    @property\n    def max_val(self) -&gt; torch.tensor:\n        \"\"\"Returns maximum values for features: bond, covalent, and vdW radius.\"\"\"\n        return self._max_val\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.Tensor:\n        \"\"\"Computes bond radius, covalent radius, and van der Waals radius without normalization.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.Tensor of different atomic radii. Each atom is featurizer by bond radius, covalent radius, and van der Waals radius.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n\n        feats = []\n        for aidx in _atom_indices:\n            atomic_num = mol.GetAtomWithIdx(aidx).GetAtomicNum()\n            feats.append([self.pt.GetRb0(atomic_num), self.pt.GetRcovalent(atomic_num), self.pt.GetRvdw(atomic_num)])\n\n        return torch.Tensor(feats)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicRadiusFeaturizer.max_val","title":"<code>max_val</code>  <code>property</code>","text":"<p>Returns maximum values for features: bond, covalent, and vdW radius.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicRadiusFeaturizer.min_val","title":"<code>min_val</code>  <code>property</code>","text":"<p>Returns minimum values for features: bond, covalent, and vdW radius.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicRadiusFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicRadiusFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes AtomicRadiusFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes AtomicRadiusFeaturizer class.\"\"\"\n    self.pt = Chem.GetPeriodicTable()\n    self._min_val = torch.Tensor(\n        [\n            0.0,  # Bond radius\n            0.28,  # Covalent radius\n            1.2,  # van der Waals radius\n        ]\n    )\n\n    self._max_val = torch.Tensor(\n        [\n            2.4,  # Bond radius\n            2.6,  # Covalent radius\n            3.0,  # van der Waals radius\n        ]\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.AtomicRadiusFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes bond radius, covalent radius, and van der Waals radius without normalization.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch.Tensor of different atomic radii. Each atom is featurizer by bond radius, covalent radius, and van der Waals radius.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.Tensor:\n    \"\"\"Computes bond radius, covalent radius, and van der Waals radius without normalization.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.Tensor of different atomic radii. Each atom is featurizer by bond radius, covalent radius, and van der Waals radius.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n\n    feats = []\n    for aidx in _atom_indices:\n        atomic_num = mol.GetAtomWithIdx(aidx).GetAtomicNum()\n        feats.append([self.pt.GetRb0(atomic_num), self.pt.GetRcovalent(atomic_num), self.pt.GetRvdw(atomic_num)])\n\n    return torch.Tensor(feats)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ChiralTypeFeaturizer","title":"<code>ChiralTypeFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its chirality type.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class ChiralTypeFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its chirality type.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes ChiralTypeFeaturizer class.\"\"\"\n        self.dim_chiral_types = len(ChiralType.values)\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return self.dim_chiral_types\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes features of atoms of all of select atoms.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor representing chirality type of atoms as integers.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor([int(mol.GetAtomWithIdx(a).GetChiralTag()) for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ChiralTypeFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ChiralTypeFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes ChiralTypeFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes ChiralTypeFeaturizer class.\"\"\"\n    self.dim_chiral_types = len(ChiralType.values)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ChiralTypeFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes features of atoms of all of select atoms.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor representing chirality type of atoms as integers.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes features of atoms of all of select atoms.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor representing chirality type of atoms as integers.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor([int(mol.GetAtomWithIdx(a).GetChiralTag()) for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.CrippenFeaturizer","title":"<code>CrippenFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by Crippen logP and molar refractivity.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class CrippenFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by Crippen logP and molar refractivity.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes CrippenFeaturizer class.\"\"\"\n        self._min_val = torch.Tensor(\n            [\n                -2.996,  # logP\n                0.0,  # MR\n            ]\n        )\n\n        self._max_val = torch.Tensor(\n            [\n                0.8857,  # logP\n                6.0,  # MR\n            ]\n        )\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 2\n\n    @property\n    def min_val(self) -&gt; torch.tensor:\n        \"\"\"Returns minimum values for features: logP and molar refractivity.\"\"\"\n        return self._min_val\n\n    @property\n    def max_val(self) -&gt; torch.tensor:\n        \"\"\"Returns maximum values for features: logP and molar refractivity.\"\"\"\n        return self._max_val\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.Tensor:\n        \"\"\"Compute atomic contributions to Crippen logP and molar refractivity.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.Tensor featurizing atoms by its atomic contribution to logP and molar refractivity.\n        \"\"\"\n        logp_mr_list = torch.Tensor(rdMolDescriptors._CalcCrippenContribs(mol))\n        logp_mr_list = torch.clamp(logp_mr_list, min=self.min_val, max=self.max_val)\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return logp_mr_list[_atom_indices, :]\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.CrippenFeaturizer.max_val","title":"<code>max_val</code>  <code>property</code>","text":"<p>Returns maximum values for features: logP and molar refractivity.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.CrippenFeaturizer.min_val","title":"<code>min_val</code>  <code>property</code>","text":"<p>Returns minimum values for features: logP and molar refractivity.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.CrippenFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.CrippenFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes CrippenFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes CrippenFeaturizer class.\"\"\"\n    self._min_val = torch.Tensor(\n        [\n            -2.996,  # logP\n            0.0,  # MR\n        ]\n    )\n\n    self._max_val = torch.Tensor(\n        [\n            0.8857,  # logP\n            6.0,  # MR\n        ]\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.CrippenFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Compute atomic contributions to Crippen logP and molar refractivity.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch.Tensor featurizing atoms by its atomic contribution to logP and molar refractivity.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.Tensor:\n    \"\"\"Compute atomic contributions to Crippen logP and molar refractivity.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.Tensor featurizing atoms by its atomic contribution to logP and molar refractivity.\n    \"\"\"\n    logp_mr_list = torch.Tensor(rdMolDescriptors._CalcCrippenContribs(mol))\n    logp_mr_list = torch.clamp(logp_mr_list, min=self.min_val, max=self.max_val)\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return logp_mr_list[_atom_indices, :]\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.DegreeFeaturizer","title":"<code>DegreeFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its degree (excluding hydrogens) of connectivity.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class DegreeFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its degree (excluding hydrogens) of connectivity.\"\"\"\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 6\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes features of atoms of all of select atoms.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor of integers representing degree of connectivity of atoms.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor([mol.GetAtomWithIdx(a).GetDegree() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.DegreeFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.DegreeFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes features of atoms of all of select atoms.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor of integers representing degree of connectivity of atoms.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes features of atoms of all of select atoms.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor of integers representing degree of connectivity of atoms.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor([mol.GetAtomWithIdx(a).GetDegree() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ElectronicPropertyFeaturizer","title":"<code>ElectronicPropertyFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its electronic properties.</p> <p>This class computes electronic properties like electronegativity, ionization energy, and electron affinity.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class ElectronicPropertyFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its electronic properties.\n\n    This class computes electronic properties like electronegativity, ionization energy, and electron affinity.\n    \"\"\"\n\n    def __init__(self, data_file=None) -&gt; None:\n        \"\"\"Initializes PeriodicTableFeaturizer class.\n\n        Args:\n            data_file: Path to the data file.\n        \"\"\"\n        if data_file is None:\n            # Use default\n            root_path = Path(__file__).resolve().parent\n            data_file = root_path / \"data\" / \"electronic_data.csv\"\n        self.data_df = pd.read_csv(data_file).set_index(\"AtomicNumber\")\n\n        self.pauling_en_dict = self.data_df[\"Electronegativity\"].to_dict()\n        self.ie_dict = self.data_df[\"IonizationEnergy\"].to_dict()\n        self.ea_dict = self.data_df[\"ElectronAffinity\"].to_dict()\n\n        self._min_val = torch.Tensor(\n            [\n                self.data_df[\"Electronegativity\"].min(),\n                self.data_df[\"IonizationEnergy\"].min(),\n                self.data_df[\"ElectronAffinity\"].min(),\n            ]\n        )\n\n        self._max_val = torch.Tensor(\n            [\n                self.data_df[\"Electronegativity\"].max(),\n                self.data_df[\"IonizationEnergy\"].max(),\n                self.data_df[\"ElectronAffinity\"].max(),\n            ]\n        )\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 3\n\n    @property\n    def min_val(self) -&gt; torch.Tensor:\n        \"\"\"Returns minimum values for features: electronegativity, ionization energy, electron affinity.\"\"\"\n        return self._min_val\n\n    @property\n    def max_val(self) -&gt; torch.Tensor:\n        \"\"\"Returns maximum values for features: electronegativity, ionization energy, electron affinity.\"\"\"\n        return self._max_val\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.Tensor:\n        \"\"\"Returns electronic features of the atom.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.Tensor consisting of Pauling scale electronegativity, ionization energy, and electron affinity for each atom.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n\n        feats = []\n        for aidx in _atom_indices:\n            atomic_num = mol.GetAtomWithIdx(aidx).GetAtomicNum()\n            feats.append([self.pauling_en_dict[atomic_num], self.ie_dict[atomic_num], self.ea_dict[atomic_num]])\n        return torch.Tensor(feats)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ElectronicPropertyFeaturizer.max_val","title":"<code>max_val</code>  <code>property</code>","text":"<p>Returns maximum values for features: electronegativity, ionization energy, electron affinity.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ElectronicPropertyFeaturizer.min_val","title":"<code>min_val</code>  <code>property</code>","text":"<p>Returns minimum values for features: electronegativity, ionization energy, electron affinity.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ElectronicPropertyFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ElectronicPropertyFeaturizer.__init__","title":"<code>__init__(data_file=None)</code>","text":"<p>Initializes PeriodicTableFeaturizer class.</p> <p>Parameters:</p> Name Type Description Default <code>data_file</code> <p>Path to the data file.</p> <code>None</code> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self, data_file=None) -&gt; None:\n    \"\"\"Initializes PeriodicTableFeaturizer class.\n\n    Args:\n        data_file: Path to the data file.\n    \"\"\"\n    if data_file is None:\n        # Use default\n        root_path = Path(__file__).resolve().parent\n        data_file = root_path / \"data\" / \"electronic_data.csv\"\n    self.data_df = pd.read_csv(data_file).set_index(\"AtomicNumber\")\n\n    self.pauling_en_dict = self.data_df[\"Electronegativity\"].to_dict()\n    self.ie_dict = self.data_df[\"IonizationEnergy\"].to_dict()\n    self.ea_dict = self.data_df[\"ElectronAffinity\"].to_dict()\n\n    self._min_val = torch.Tensor(\n        [\n            self.data_df[\"Electronegativity\"].min(),\n            self.data_df[\"IonizationEnergy\"].min(),\n            self.data_df[\"ElectronAffinity\"].min(),\n        ]\n    )\n\n    self._max_val = torch.Tensor(\n        [\n            self.data_df[\"Electronegativity\"].max(),\n            self.data_df[\"IonizationEnergy\"].max(),\n            self.data_df[\"ElectronAffinity\"].max(),\n        ]\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ElectronicPropertyFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Returns electronic features of the atom.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch.Tensor consisting of Pauling scale electronegativity, ionization energy, and electron affinity for each atom.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.Tensor:\n    \"\"\"Returns electronic features of the atom.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.Tensor consisting of Pauling scale electronegativity, ionization energy, and electron affinity for each atom.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n\n    feats = []\n    for aidx in _atom_indices:\n        atomic_num = mol.GetAtomWithIdx(aidx).GetAtomicNum()\n        feats.append([self.pauling_en_dict[atomic_num], self.ie_dict[atomic_num], self.ea_dict[atomic_num]])\n    return torch.Tensor(feats)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.HybridizationFeaturizer","title":"<code>HybridizationFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its hybridization type.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class HybridizationFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its hybridization type.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes HybridizationFeaturizer class.\"\"\"\n        self.dim_hybridization_types = len(HybridizationType.values)\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return self.dim_hybridization_types\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes features of atoms of all of select atoms.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor representing hybridization type of atoms as integers.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor([int(mol.GetAtomWithIdx(a).GetHybridization()) for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.HybridizationFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.HybridizationFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes HybridizationFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes HybridizationFeaturizer class.\"\"\"\n    self.dim_hybridization_types = len(HybridizationType.values)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.HybridizationFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes features of atoms of all of select atoms.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor representing hybridization type of atoms as integers.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes features of atoms of all of select atoms.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor representing hybridization type of atoms as integers.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor([int(mol.GetAtomWithIdx(a).GetHybridization()) for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.PeriodicTableFeaturizer","title":"<code>PeriodicTableFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its position (period and group) in the periodic table.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class PeriodicTableFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its position (period and group) in the periodic table.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes PeriodicTableFeaturizer class.\"\"\"\n        self.pt = Chem.GetPeriodicTable()\n        # The number of elements per period in the periodic table\n        self.period_limits = [2, 10, 18, 36, 54, 86, 118]\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 25\n\n    def get_period(self, atom: Chem.Atom) -&gt; int:\n        \"\"\"Returns periodic table period of atom.\"\"\"\n        atomic_number = atom.GetAtomicNum()\n\n        # Determine the period based on atomic number.\n        for period, limit in enumerate(self.period_limits, start=1):\n            if atomic_number &lt;= limit:\n                return period\n        return None\n\n    def get_group(self, atom: Chem.Atom) -&gt; int:\n        \"\"\"Returns periodic table group of atom.\"\"\"\n        group = self.pt.GetNOuterElecs(atom.GetAtomicNum())\n        return group\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes periodic table position of atoms of all or select atoms specific in `atom_indices`.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor of representing positions of atoms in periodic table. First index represents period and second index represents group.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor(\n            [(self.get_period(mol.GetAtomWithIdx(a)), self.get_group(mol.GetAtomWithIdx(a))) for a in _atom_indices],\n            dtype=torch.int,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.PeriodicTableFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.PeriodicTableFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes PeriodicTableFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes PeriodicTableFeaturizer class.\"\"\"\n    self.pt = Chem.GetPeriodicTable()\n    # The number of elements per period in the periodic table\n    self.period_limits = [2, 10, 18, 36, 54, 86, 118]\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.PeriodicTableFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes periodic table position of atoms of all or select atoms specific in <code>atom_indices</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor of representing positions of atoms in periodic table. First index represents period and second index represents group.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes periodic table position of atoms of all or select atoms specific in `atom_indices`.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor of representing positions of atoms in periodic table. First index represents period and second index represents group.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor(\n        [(self.get_period(mol.GetAtomWithIdx(a)), self.get_group(mol.GetAtomWithIdx(a))) for a in _atom_indices],\n        dtype=torch.int,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.PeriodicTableFeaturizer.get_group","title":"<code>get_group(atom)</code>","text":"<p>Returns periodic table group of atom.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_group(self, atom: Chem.Atom) -&gt; int:\n    \"\"\"Returns periodic table group of atom.\"\"\"\n    group = self.pt.GetNOuterElecs(atom.GetAtomicNum())\n    return group\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.PeriodicTableFeaturizer.get_period","title":"<code>get_period(atom)</code>","text":"<p>Returns periodic table period of atom.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_period(self, atom: Chem.Atom) -&gt; int:\n    \"\"\"Returns periodic table period of atom.\"\"\"\n    atomic_number = atom.GetAtomicNum()\n\n    # Determine the period based on atomic number.\n    for period, limit in enumerate(self.period_limits, start=1):\n        if atomic_number &lt;= limit:\n            return period\n    return None\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ScaffoldFeaturizer","title":"<code>ScaffoldFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom based on whether it is present in Bemis-Murcko scaffold.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class ScaffoldFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom based on whether it is present in Bemis-Murcko scaffold.\"\"\"\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 1\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Returns position of the atoms with respect to Bemis-Murcko scaffold.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor indicating if atoms are present in the Bemis-Murcko scaffold of the molecule.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n\n        scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n        scaffold_atom_idx = set(mol.GetSubstructMatch(scaffold))\n\n        feats = [int(aidx in scaffold_atom_idx) for aidx in _atom_indices]\n        return torch.tensor(feats, dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ScaffoldFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.ScaffoldFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Returns position of the atoms with respect to Bemis-Murcko scaffold.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor indicating if atoms are present in the Bemis-Murcko scaffold of the molecule.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Returns position of the atoms with respect to Bemis-Murcko scaffold.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor indicating if atoms are present in the Bemis-Murcko scaffold of the molecule.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n\n    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n    scaffold_atom_idx = set(mol.GetSubstructMatch(scaffold))\n\n    feats = [int(aidx in scaffold_atom_idx) for aidx in _atom_indices]\n    return torch.tensor(feats, dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.SmartsFeaturizer","title":"<code>SmartsFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by hydrogen donor/acceptor and acidity/basicity.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class SmartsFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by hydrogen donor/acceptor and acidity/basicity.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes SmartsFeaturizer class.\"\"\"\n        self.hydrogen_donor = Chem.MolFromSmarts(\"[$([N;!H0;v3,v4&amp;+1]),$([O,S;H1;+0]),n&amp;H1&amp;+0]\")\n        self.hydrogen_acceptor = Chem.MolFromSmarts(\n            \"[$([O,S;H1;v2;!$(*-*=[O,N,P,S])]),$([O,S;H0;v2]),$([O,S;-]),$([N;v3;!$(N-*=[O,N,P,S])]),\"\n            \"n&amp;H0&amp;+0,$([o,s;+0;!$([o,s]:n);!$([o,s]:c:n)])]\"\n        )\n        self.acidic = Chem.MolFromSmarts(\"[$([C,S](=[O,S,P])-[O;H1,-1])]\")\n        self.basic = Chem.MolFromSmarts(\n            \"[#7;+,$([N;H2&amp;+0][$([C,a]);!$([C,a](=O))]),$([N;H1&amp;+0]([$([C,a]);!$([C,a](=O))])[$([C,a]);\"\n            \"!$([C,a](=O))]),$([N;H0&amp;+0]([C;!$(C(=O))])([C;!$(C(=O))])[C;!$(C(=O))])]\"\n        )\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 4\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes matches by prefixed SMARTS patterns.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            An torch.tensor indicating if atoms are hydrogen bond donors, hydrogen bond acceptors, acidic, or basic.\n        \"\"\"\n        hydrogen_donor_match = sum(mol.GetSubstructMatches(self.hydrogen_donor), ())\n        hydrogen_acceptor_match = sum(mol.GetSubstructMatches(self.hydrogen_acceptor), ())\n        acidic_match = sum(mol.GetSubstructMatches(self.acidic), ())\n        basic_match = sum(mol.GetSubstructMatches(self.basic), ())\n\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        feats = [\n            [\n                aidx in hydrogen_donor_match,\n                aidx in hydrogen_acceptor_match,\n                aidx in acidic_match,\n                aidx in basic_match,\n            ]\n            for aidx in _atom_indices\n        ]\n\n        return torch.tensor(feats, dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.SmartsFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.SmartsFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes SmartsFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes SmartsFeaturizer class.\"\"\"\n    self.hydrogen_donor = Chem.MolFromSmarts(\"[$([N;!H0;v3,v4&amp;+1]),$([O,S;H1;+0]),n&amp;H1&amp;+0]\")\n    self.hydrogen_acceptor = Chem.MolFromSmarts(\n        \"[$([O,S;H1;v2;!$(*-*=[O,N,P,S])]),$([O,S;H0;v2]),$([O,S;-]),$([N;v3;!$(N-*=[O,N,P,S])]),\"\n        \"n&amp;H0&amp;+0,$([o,s;+0;!$([o,s]:n);!$([o,s]:c:n)])]\"\n    )\n    self.acidic = Chem.MolFromSmarts(\"[$([C,S](=[O,S,P])-[O;H1,-1])]\")\n    self.basic = Chem.MolFromSmarts(\n        \"[#7;+,$([N;H2&amp;+0][$([C,a]);!$([C,a](=O))]),$([N;H1&amp;+0]([$([C,a]);!$([C,a](=O))])[$([C,a]);\"\n        \"!$([C,a](=O))]),$([N;H0&amp;+0]([C;!$(C(=O))])([C;!$(C(=O))])[C;!$(C(=O))])]\"\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.SmartsFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes matches by prefixed SMARTS patterns.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>An torch.tensor indicating if atoms are hydrogen bond donors, hydrogen bond acceptors, acidic, or basic.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes matches by prefixed SMARTS patterns.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        An torch.tensor indicating if atoms are hydrogen bond donors, hydrogen bond acceptors, acidic, or basic.\n    \"\"\"\n    hydrogen_donor_match = sum(mol.GetSubstructMatches(self.hydrogen_donor), ())\n    hydrogen_acceptor_match = sum(mol.GetSubstructMatches(self.hydrogen_acceptor), ())\n    acidic_match = sum(mol.GetSubstructMatches(self.acidic), ())\n    basic_match = sum(mol.GetSubstructMatches(self.basic), ())\n\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    feats = [\n        [\n            aidx in hydrogen_donor_match,\n            aidx in hydrogen_acceptor_match,\n            aidx in acidic_match,\n            aidx in basic_match,\n        ]\n        for aidx in _atom_indices\n    ]\n\n    return torch.tensor(feats, dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.TotalDegreeFeaturizer","title":"<code>TotalDegreeFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by its total degree (including hydrogens) of connectivity.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class TotalDegreeFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by its total degree (including hydrogens) of connectivity.\"\"\"\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return 6\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes features of atoms of all of select atoms.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor of integers representing total connectivity (including hydrogens) of atoms.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor([mol.GetAtomWithIdx(a).GetTotalDegree() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.TotalDegreeFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.TotalDegreeFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes features of atoms of all of select atoms.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor of integers representing total connectivity (including hydrogens) of atoms.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes features of atoms of all of select atoms.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor of integers representing total connectivity (including hydrogens) of atoms.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor([mol.GetAtomWithIdx(a).GetTotalDegree() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.TotalNumHFeaturizer","title":"<code>TotalNumHFeaturizer</code>","text":"<p>               Bases: <code>BaseAtomFeaturizer</code></p> <p>Class for featurizing atom by total number of hydrogens.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>class TotalNumHFeaturizer(BaseAtomFeaturizer):\n    \"\"\"Class for featurizing atom by total number of hydrogens.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes TotalNumHFeaturizer class.\"\"\"\n        self.dim_total_num_hydrogen = 5  # 4 + 1 (no hydrogens)\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return self.dim_total_num_hydrogen\n\n    def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n        \"\"\"Computes features of atoms of all of select atoms.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n        Returns:\n            A torch.tensor of integers representing total number of hydrogens on atoms.\n        \"\"\"\n        _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n        return torch.tensor([mol.GetAtomWithIdx(a).GetTotalNumHs() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.TotalNumHFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.TotalNumHFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes TotalNumHFeaturizer class.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes TotalNumHFeaturizer class.\"\"\"\n    self.dim_total_num_hydrogen = 5  # 4 + 1 (no hydrogens)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/atom_featurizers/#bionemo.geometric.atom_featurizers.TotalNumHFeaturizer.get_atom_features","title":"<code>get_atom_features(mol, atom_indices=None)</code>","text":"<p>Computes features of atoms of all of select atoms.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>atom_indices</code> <code>Optional[Iterable]</code> <p>Indices of atoms for feature computation. By default, features for all atoms is computed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>A torch.tensor of integers representing total number of hydrogens on atoms.</p> Source code in <code>bionemo/geometric/atom_featurizers.py</code> <pre><code>def get_atom_features(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; torch.tensor:\n    \"\"\"Computes features of atoms of all of select atoms.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        atom_indices: Indices of atoms for feature computation. By default, features for all atoms is computed.\n\n    Returns:\n        A torch.tensor of integers representing total number of hydrogens on atoms.\n    \"\"\"\n    _atom_indices = atom_indices if atom_indices else range(mol.GetNumAtoms())\n    return torch.tensor([mol.GetAtomWithIdx(a).GetTotalNumHs() for a in _atom_indices], dtype=torch.int)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/","title":"Base featurizer","text":""},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseAtomFeaturizer","title":"<code>BaseAtomFeaturizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base featurizer class for all atom featurization classes.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>class BaseAtomFeaturizer(ABC):\n    \"\"\"Abstract base featurizer class for all atom featurization classes.\"\"\"\n\n    @abstractproperty\n    def n_dim(self) -&gt; int:\n        \"\"\"Number of dimensions of computed feature.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_atom_features(self):\n        \"\"\"Computes atom features.\"\"\"\n        ...\n\n    def __call__(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; list[int]:\n        \"\"\"Returns computed atom features.\"\"\"\n        return self.get_atom_features(mol, atom_indices)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseAtomFeaturizer.__call__","title":"<code>__call__(mol, atom_indices=None)</code>","text":"<p>Returns computed atom features.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>def __call__(self, mol: Mol, atom_indices: Optional[Iterable] = None) -&gt; list[int]:\n    \"\"\"Returns computed atom features.\"\"\"\n    return self.get_atom_features(mol, atom_indices)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseAtomFeaturizer.get_atom_features","title":"<code>get_atom_features()</code>  <code>abstractmethod</code>","text":"<p>Computes atom features.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>@abstractmethod\ndef get_atom_features(self):\n    \"\"\"Computes atom features.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseAtomFeaturizer.n_dim","title":"<code>n_dim()</code>","text":"<p>Number of dimensions of computed feature.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>@abstractproperty\ndef n_dim(self) -&gt; int:\n    \"\"\"Number of dimensions of computed feature.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseBondFeaturizer","title":"<code>BaseBondFeaturizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base featurizer class for all bond featurization classes.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>class BaseBondFeaturizer(ABC):\n    \"\"\"Abstract base featurizer class for all bond featurization classes.\"\"\"\n\n    @abstractproperty\n    def n_dim(self) -&gt; int:\n        \"\"\"Number of dimensions of computed feature.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_bond_features(self):\n        \"\"\"Computes bond features.\"\"\"\n        ...\n\n    def __call__(self, mol: Mol, bond_indices: Optional[Iterable] = None) -&gt; list[int]:\n        \"\"\"Returns computed bond features.\"\"\"\n        return self.get_bond_features(mol, bond_indices)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseBondFeaturizer.__call__","title":"<code>__call__(mol, bond_indices=None)</code>","text":"<p>Returns computed bond features.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>def __call__(self, mol: Mol, bond_indices: Optional[Iterable] = None) -&gt; list[int]:\n    \"\"\"Returns computed bond features.\"\"\"\n    return self.get_bond_features(mol, bond_indices)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseBondFeaturizer.get_bond_features","title":"<code>get_bond_features()</code>  <code>abstractmethod</code>","text":"<p>Computes bond features.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>@abstractmethod\ndef get_bond_features(self):\n    \"\"\"Computes bond features.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseBondFeaturizer.n_dim","title":"<code>n_dim()</code>","text":"<p>Number of dimensions of computed feature.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>@abstractproperty\ndef n_dim(self) -&gt; int:\n    \"\"\"Number of dimensions of computed feature.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseMoleculeFeaturizer","title":"<code>BaseMoleculeFeaturizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base featurizer class for molecule featurization classes.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>class BaseMoleculeFeaturizer(ABC):\n    \"\"\"Abstract base featurizer class for molecule featurization classes.\"\"\"\n\n    @abstractproperty\n    def n_dim(self) -&gt; int:\n        \"\"\"Number of dimensions of computed feature.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_molecule_features(self, mol: Mol) -&gt; torch.Tensor:\n        \"\"\"Computes molecule features.\"\"\"\n        ...\n\n    def __call__(self, mol: Mol) -&gt; torch.Tensor:\n        \"\"\"Returns computed molecule features.\"\"\"\n        return self.get_molecule_features(mol)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseMoleculeFeaturizer.__call__","title":"<code>__call__(mol)</code>","text":"<p>Returns computed molecule features.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>def __call__(self, mol: Mol) -&gt; torch.Tensor:\n    \"\"\"Returns computed molecule features.\"\"\"\n    return self.get_molecule_features(mol)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseMoleculeFeaturizer.get_molecule_features","title":"<code>get_molecule_features(mol)</code>  <code>abstractmethod</code>","text":"<p>Computes molecule features.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>@abstractmethod\ndef get_molecule_features(self, mol: Mol) -&gt; torch.Tensor:\n    \"\"\"Computes molecule features.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.BaseMoleculeFeaturizer.n_dim","title":"<code>n_dim()</code>","text":"<p>Number of dimensions of computed feature.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>@abstractproperty\ndef n_dim(self) -&gt; int:\n    \"\"\"Number of dimensions of computed feature.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.get_boolean_atomic_prop","title":"<code>get_boolean_atomic_prop(atom, prop_list=None)</code>","text":"<p>Retrieves boolean atomic properties for a given atom.</p> <p>This function fetches boolean properties of an atom. If a specific list of properties is provided, it retrieves those properties. Otherwise, it fetches all available boolean properties for the atom.</p> <p>Parameters:</p> Name Type Description Default <code>atom</code> <code>Atom</code> <p>The atom object to retrieve properties from.</p> required <code>prop_list</code> <code>list</code> <p>A list of specific property names to retrieve. If None, all available properties will be fetched. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[bool]</code> <p>A list of boolean values corresponding to the requested properties.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>def get_boolean_atomic_prop(atom: Atom, prop_list=None) -&gt; list[bool]:\n    \"\"\"Retrieves boolean atomic properties for a given atom.\n\n    This function fetches boolean properties of an atom. If a specific list of\n    properties is provided, it retrieves those properties. Otherwise, it fetches\n    all available boolean properties for the atom.\n\n    Args:\n        atom: The atom object to retrieve properties from.\n        prop_list (list, optional): A list of specific property names to retrieve.\n            If None, all available properties will be fetched. Defaults to None.\n\n    Returns:\n        list: A list of boolean values corresponding to the requested properties.\n    \"\"\"\n    _prop_list = prop_list if prop_list else atom.GetPropNames()\n\n    return [atom.GetBoolProp(prop) for prop in _prop_list]\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.get_double_atomic_prop","title":"<code>get_double_atomic_prop(atom, prop_list=None)</code>","text":"<p>Retrieves double atomic properties for a given atom.</p> <p>This function fetches double properties of an atom. If a specific list of properties is provided, it retrieves those properties. Otherwise, it fetches all available double properties for the atom.</p> <p>Parameters:</p> Name Type Description Default <code>atom</code> <p>The atom object to retrieve properties from.</p> required <code>prop_list</code> <code>list</code> <p>A list of specific property names to retrieve. If None, all available properties will be fetched. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[float]</code> <p>A list of float values corresponding to the requested properties.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>def get_double_atomic_prop(atom, prop_list=None) -&gt; list[float]:\n    \"\"\"Retrieves double atomic properties for a given atom.\n\n    This function fetches double properties of an atom. If a specific list of\n    properties is provided, it retrieves those properties. Otherwise, it fetches\n    all available double properties for the atom.\n\n    Args:\n        atom: The atom object to retrieve properties from.\n        prop_list (list, optional): A list of specific property names to retrieve.\n            If None, all available properties will be fetched. Defaults to None.\n\n    Returns:\n        list: A list of float values corresponding to the requested properties.\n    \"\"\"\n    if prop_list is not None:\n        _prop_list = prop_list\n    else:\n        _prop_list = atom.GetPropNames()\n\n    return [atom.GetDoubleProp(prop) for prop in _prop_list]\n</code></pre>"},{"location":"API_reference/bionemo/geometric/base_featurizer/#bionemo.geometric.base_featurizer.one_hot_enc","title":"<code>one_hot_enc(val, num_class)</code>","text":"<p>Performs one-hot encoding on an integer value.</p> <p>This function creates a one-hot encoded representation of the input value as a list of boolean values. The resulting list has a length equal to <code>num_class</code>, where only the element at index <code>val</code> is set to True.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>int</code> <p>An integer representing the value to be one-hot encoded. Must be in the range [0, num_class - 1].</p> required <code>num_class</code> <code>int</code> <p>An integer representing the total number of classes or possible classes.</p> required <p>Returns:</p> Type Description <code>list[bool]</code> <p>One-hot encoding of <code>val</code>.</p> Source code in <code>bionemo/geometric/base_featurizer.py</code> <pre><code>def one_hot_enc(val: int, num_class: int) -&gt; list[bool]:\n    \"\"\"Performs one-hot encoding on an integer value.\n\n    This function creates a one-hot encoded representation of the input value\n    as a list of boolean values. The resulting list has a length equal to\n    `num_class`, where only the element at index `val` is set to True.\n\n    Args:\n        val (int): An integer representing the value to be one-hot encoded.\n            Must be in the range [0, num_class - 1].\n        num_class (int): An integer representing the total number of classes or\n            possible classes.\n\n    Returns:\n        One-hot encoding of `val`.\n    \"\"\"\n    one_hot = [False] * num_class\n    one_hot[val] = True\n    return one_hot\n</code></pre>"},{"location":"API_reference/bionemo/geometric/bond_featurizers/","title":"Bond featurizers","text":""},{"location":"API_reference/bionemo/geometric/bond_featurizers/#bionemo.geometric.bond_featurizers.RingFeaturizer","title":"<code>RingFeaturizer</code>","text":"<p>               Bases: <code>BaseBondFeaturizer</code></p> <p>Class for featurizing bond its ring membership.</p> Source code in <code>bionemo/geometric/bond_featurizers.py</code> <pre><code>class RingFeaturizer(BaseBondFeaturizer):\n    \"\"\"Class for featurizing bond its ring membership.\"\"\"\n\n    def __init__(self, n_ring_sizes=7) -&gt; None:\n        \"\"\"Initializes RingFeaturizer class.\"\"\"\n        self.n_ring_sizes = n_ring_sizes  # ring size 3 - 8 and UNK\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return self.n_ring_sizes\n\n    def get_bond_features(self, mol: Mol, bond_indices: Optional[Iterable]) -&gt; list[tuple[int]]:\n        \"\"\"Computes ring sizes a bonds of the molecule are present in.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n            bond_indices: Indices of bonds for feature computation. By default, features for all bonds is computed.\n\n        Returns:\n            An list of tuples indicating the size of ring(s) the bonds are present in.\n        \"\"\"\n        _bond_indices = bond_indices if bond_indices else range(mol.GetNumBonds())\n\n        ri = mol.GetRingInfo()\n        return [ri.BondRingSizes(bidx) for bidx in _bond_indices]\n</code></pre>"},{"location":"API_reference/bionemo/geometric/bond_featurizers/#bionemo.geometric.bond_featurizers.RingFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/bond_featurizers/#bionemo.geometric.bond_featurizers.RingFeaturizer.__init__","title":"<code>__init__(n_ring_sizes=7)</code>","text":"<p>Initializes RingFeaturizer class.</p> Source code in <code>bionemo/geometric/bond_featurizers.py</code> <pre><code>def __init__(self, n_ring_sizes=7) -&gt; None:\n    \"\"\"Initializes RingFeaturizer class.\"\"\"\n    self.n_ring_sizes = n_ring_sizes  # ring size 3 - 8 and UNK\n</code></pre>"},{"location":"API_reference/bionemo/geometric/bond_featurizers/#bionemo.geometric.bond_featurizers.RingFeaturizer.get_bond_features","title":"<code>get_bond_features(mol, bond_indices)</code>","text":"<p>Computes ring sizes a bonds of the molecule are present in.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <code>bond_indices</code> <code>Optional[Iterable]</code> <p>Indices of bonds for feature computation. By default, features for all bonds is computed.</p> required <p>Returns:</p> Type Description <code>list[tuple[int]]</code> <p>An list of tuples indicating the size of ring(s) the bonds are present in.</p> Source code in <code>bionemo/geometric/bond_featurizers.py</code> <pre><code>def get_bond_features(self, mol: Mol, bond_indices: Optional[Iterable]) -&gt; list[tuple[int]]:\n    \"\"\"Computes ring sizes a bonds of the molecule are present in.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n        bond_indices: Indices of bonds for feature computation. By default, features for all bonds is computed.\n\n    Returns:\n        An list of tuples indicating the size of ring(s) the bonds are present in.\n    \"\"\"\n    _bond_indices = bond_indices if bond_indices else range(mol.GetNumBonds())\n\n    ri = mol.GetRingInfo()\n    return [ri.BondRingSizes(bidx) for bidx in _bond_indices]\n</code></pre>"},{"location":"API_reference/bionemo/geometric/molecule_featurizers/","title":"Molecule featurizers","text":""},{"location":"API_reference/bionemo/geometric/molecule_featurizers/#bionemo.geometric.molecule_featurizers.RDkit2DDescriptorFeaturizer","title":"<code>RDkit2DDescriptorFeaturizer</code>","text":"<p>               Bases: <code>BaseMoleculeFeaturizer</code></p> <p>Class for featurizing molecule by computed RDkit descriptors.</p> <p>Typical usage example: rdf = RDkit2DDescriptorFeaturizer() rdf(Chem.MolFromSmiles(\"CCO\"))</p> Source code in <code>bionemo/geometric/molecule_featurizers.py</code> <pre><code>class RDkit2DDescriptorFeaturizer(BaseMoleculeFeaturizer):\n    \"\"\"Class for featurizing molecule by computed RDkit descriptors.\n\n    Typical usage example:\n    rdf = RDkit2DDescriptorFeaturizer()\n    rdf(Chem.MolFromSmiles(\"CCO\"))\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes RDkit2DDescriptorFeaturizer class.\"\"\"\n        self.n_rdkit_descriptors = len(Descriptors.descList)\n\n    @property\n    def n_dim(self) -&gt; int:\n        \"\"\"Returns dimensionality of the computed features.\"\"\"\n        return self.n_rdkit_descriptors\n\n    def get_molecule_features(self, mol: Mol) -&gt; torch.Tensor:\n        \"\"\"Returns features of the molecule.\n\n        Args:\n            mol: An RDkit Chem.Mol object\n\n        Returns:\n        A torch.tensor representing RDkit-computed 2D descriptors of the molecule.\n        \"\"\"\n        return torch.Tensor([f(mol) for desc_name, f in Descriptors.descList])\n</code></pre>"},{"location":"API_reference/bionemo/geometric/molecule_featurizers/#bionemo.geometric.molecule_featurizers.RDkit2DDescriptorFeaturizer.n_dim","title":"<code>n_dim</code>  <code>property</code>","text":"<p>Returns dimensionality of the computed features.</p>"},{"location":"API_reference/bionemo/geometric/molecule_featurizers/#bionemo.geometric.molecule_featurizers.RDkit2DDescriptorFeaturizer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes RDkit2DDescriptorFeaturizer class.</p> Source code in <code>bionemo/geometric/molecule_featurizers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes RDkit2DDescriptorFeaturizer class.\"\"\"\n    self.n_rdkit_descriptors = len(Descriptors.descList)\n</code></pre>"},{"location":"API_reference/bionemo/geometric/molecule_featurizers/#bionemo.geometric.molecule_featurizers.RDkit2DDescriptorFeaturizer.get_molecule_features","title":"<code>get_molecule_features(mol)</code>","text":"<p>Returns features of the molecule.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Mol</code> <p>An RDkit Chem.Mol object</p> required <p>Returns: A torch.tensor representing RDkit-computed 2D descriptors of the molecule.</p> Source code in <code>bionemo/geometric/molecule_featurizers.py</code> <pre><code>def get_molecule_features(self, mol: Mol) -&gt; torch.Tensor:\n    \"\"\"Returns features of the molecule.\n\n    Args:\n        mol: An RDkit Chem.Mol object\n\n    Returns:\n    A torch.tensor representing RDkit-computed 2D descriptors of the molecule.\n    \"\"\"\n    return torch.Tensor([f(mol) for desc_name, f in Descriptors.descList])\n</code></pre>"},{"location":"API_reference/bionemo/llm/api/","title":"Api","text":""},{"location":"API_reference/bionemo/llm/api/#bionemo.llm.api.BionemoMegatronModel","title":"<code>BionemoMegatronModel</code>","text":"<p>               Bases: <code>MegatronModule</code>, <code>Generic[DataT]</code>, <code>ABC</code></p> <p>Models that use Megatron must be a MegatronModule type.</p> <p>The only major difference is the explicit <code>forward</code> pass method signature that makes this class compatible with bionemo-core's <code>Model</code> structural type.</p> Source code in <code>bionemo/llm/api.py</code> <pre><code>class BionemoMegatronModel(MegatronModule, Generic[DataT], ABC):\n    \"\"\"Models that use Megatron must be a MegatronModule type.\n\n    The only major difference is the explicit `forward` pass method signature that makes this class compatible\n    with bionemo-core's `Model` structural type.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -&gt; DataT:  # noqa: D102\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/","title":"Lightning","text":""},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.DataStep","title":"<code>DataStep = Callable[[Iterator[DataT]], DataT]</code>  <code>module-attribute</code>","text":"<p>Batches together an iterator of individual examples.</p> <p>Necessary for compatability with Megatron. This function type is similiar to the collate function of PyTorch.</p> <p>A <code>DataStep</code> function takes an iterator over individual examples. Each example may be a tensor, sequence of tensors, or a set of named tensors (provided as a <code>dict</code> mapping <code>str</code> names to each <code>Tensor</code>). Each iteration must yield the same type.</p> <p>The output of this function will mirror the same structure of each yielded example. It will be a concatenation of all of the examples in the iterator.</p>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.ForwardStep","title":"<code>ForwardStep = Callable[[MegatronModelType, DataT], DataT]</code>  <code>module-attribute</code>","text":"<p>Megatron-compatible forward pass function.</p>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule","title":"<code>BionemoLightningModule</code>","text":"<p>               Bases: <code>Generic[MegatronModelType, MegatronLossType]</code>, <code>LightningModule</code>, <code>IOMixin</code>, <code>ConnectorMixin</code>, <code>LightningPassthroughPredictionMixin</code></p> <p>Reusable PyTorch Lightning module for Megatron models that is compatible with NeMo's conventions.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class BionemoLightningModule(\n    Generic[MegatronModelType, MegatronLossType],\n    pl.LightningModule,\n    nlio.IOMixin,\n    nlio.ConnectorMixin,\n    LightningPassthroughPredictionMixin,\n):\n    \"\"\"Reusable PyTorch Lightning module for Megatron models that is compatible with NeMo's conventions.\"\"\"\n\n    def __init__(\n        self,\n        config: BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n        forward_step: ForwardStep,\n        data_step: DataStep,\n        optimizer: MegatronOptimizerModule,\n        model_transform: Optional[Callable[[MegatronModelType], MegatronModelType]] = None,\n        **model_construct_args,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Args:\n            config: Serializable configuration object that allows one to construct a new model instance and loss\n                function. Necessary for Megatron-based training as the model itself cannot be serialized and\n                distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.\n            forward_step: Performs forward pass using the model and a batch of data.\n            data_step: Custom batch-creating function for the model.\n            optimizer: Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning\n                rate.\n            model_construct_args: Optional. Any arguments necessary to construct the model in the `config`'s\n                `configure_model` method.\n            model_transform: Optional. The model transform function.\n            **model_construct_args: Optional. Arguments necessary for the supplied model configuration's\n                `configure_model` method, which will make an instance of the model.\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.module_construct_args: Optional[dict[str, Any]] = model_construct_args\n        # ***must** be set up in configure_model() -- megatron constraint\n        # also, must be called `module`: nemo expects the actual model to be stored this way\n        self.module: Optional[MegatronModelType] = None\n        self.loss_reduction_class: type[MegatronLossType] = config.get_loss_reduction_class()\n        self.optim = optimizer\n        self.optim.connect(self)  # This will bind the `configure_optimizers` method\n        self._data_step = data_step\n        self._forward_step = forward_step\n        self.model_transform = model_transform\n\n        # configure metrics\n        self.train_metric = self.config.train_metric.get_instance() if self.config.train_metric else None\n        self.valid_metric = self.config.valid_metric.get_instance() if self.config.valid_metric else None\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Updates internal state: instantiates the model from the object's config, assigns to `model` attribute.\n\n        NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.\n\n        Raises:\n            ValueError iff the internal config's configure_model method returns None.\n        \"\"\"\n        if self.module is None:\n            model: MegatronModelType = (\n                self.config.configure_model(**self.module_construct_args)\n                if self.module_construct_args is not None\n                else self.config.configure_model()\n            )\n            self.module = model\n\n        if self.module is None:\n            raise ValueError(\"Invalid semantics: configure_model method **MUST** initialize the model.\")\n\n    def is_on_logging_device(self):\n        \"\"\"Return True if last stage of pipeline parallel and first tensor parallel rank.\"\"\"\n        return parallel_state.is_pipeline_last_stage() and parallel_state.get_tensor_model_parallel_rank() == 0\n\n    def forward(self, *args, **kwargs) -&gt; DataT:\n        \"\"\"Call the forward method of the underlying model, and return whatever it outputs.\"\"\"\n        # safe to do because configure_model is idempotent\n        self.configure_model()\n        assert self.module is not None, \"ERROR: configure_model() method has been incorrectly overridden!\"\n        prediction = self.module(*args, **kwargs)  # for now just pass through to the underlying model\n        return prediction\n\n    def data_step(self, dataloader_iter: Iterator[DataT]) -&gt; DataT:  # noqa: D102\n        return self._data_step(dataloader_iter)\n\n    def forward_step(self, batch) -&gt; Tensor:\n        \"\"\"Megatron-required: the training forward step for the model, which is required to produce the loss.\n\n        Normally, the forward pass of a model means its inference. Loss is computed using the predictions\n        from the forward pass against labels. Megatron unfortunately conflates these two different concepts\n        and instead has models \"forward\" method produce the loss. See the Megatron docs for details:\n        https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170\n\n        To get actual predictions, use the :func:`forward` method instead.\n        \"\"\"\n        # safe to do because configure_model is idempotent\n        self.configure_model()\n        assert self.module is not None\n        return self._forward_step(self.module, batch)\n\n    def update_metric(\n        self, batch, outputs, metric, task: Literal[\"pretraining\", \"classification\", \"regression\"]\n    ) -&gt; None:\n        \"\"\"Update metric for logging.\"\"\"\n        match task:\n            case \"pretraining\":\n                logits = outputs[\"token_logits\"].detach().transpose(0, 1)  #  [s, b, v] -&gt; [b, s, v]\n                metric(logits, batch[\"labels\"])\n            case \"classification\":\n                classification_output = outputs[\"classification_output\"]\n                num_classes = classification_output.shape[-1]\n                metric(\n                    classification_output.reshape(-1, num_classes),\n                    batch[\"labels\"].reshape(-1),\n                )\n            case \"regression\":\n                regression_output = outputs[\"regression_output\"]\n                metric(regression_output, batch[\"labels\"])\n            case _:\n                raise NotImplementedError(f\"unrecognized task {task}\")\n\n    def training_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n        outputs = self.forward_step(batch)\n        if self.train_metric is not None:\n            if self.is_on_logging_device():\n                self.update_metric(batch, outputs, self.train_metric, self.config.train_metric.task)\n\n            self.log(\n                self.config.train_metric.metric_name,\n                self.train_metric,\n                on_step=True,\n                on_epoch=False,\n                prog_bar=True,\n            )\n\n        return outputs\n\n    def validation_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n        outputs = self.forward_step(batch)\n        if self.valid_metric is not None and self.is_on_logging_device():\n            self.update_metric(batch, outputs, self.valid_metric, self.config.valid_metric.task)\n\n        return outputs\n\n    def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"Alias for forward_step.\"\"\"\n        if len(batch) == 0:\n            return\n        return self.forward_step(batch)\n\n    def training_loss_reduction(self) -&gt; MegatronLossType:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\"\"\"\n        return self.loss_reduction_class()\n\n    def validation_loss_reduction(self) -&gt; MegatronLossType:  # noqa: D102\n        return self.loss_reduction_class(validation_step=True)\n\n    def test_loss_reduction(self) -&gt; MegatronLossType:  # noqa: D102\n        return self.loss_reduction_class(validation_step=True)\n\n    def on_validation_epoch_end(self):  # noqa: D102\n        if self.valid_metric is None:\n            return\n\n        if self.trainer.sanity_checking:\n            self.valid_metric.reset()  # clean up sanity runs\n            return\n\n        self.log(\n            self.config.valid_metric.metric_name,\n            self.valid_metric,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.__init__","title":"<code>__init__(config, forward_step, data_step, optimizer, model_transform=None, **model_construct_args)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BionemoTrainableModelConfig[MegatronModelType, MegatronLossType]</code> <p>Serializable configuration object that allows one to construct a new model instance and loss function. Necessary for Megatron-based training as the model itself cannot be serialized and distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.</p> required <code>forward_step</code> <code>ForwardStep</code> <p>Performs forward pass using the model and a batch of data.</p> required <code>data_step</code> <code>DataStep</code> <p>Custom batch-creating function for the model.</p> required <code>optimizer</code> <code>MegatronOptimizerModule</code> <p>Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning rate.</p> required <code>model_construct_args</code> <p>Optional. Any arguments necessary to construct the model in the <code>config</code>'s <code>configure_model</code> method.</p> <code>{}</code> <code>model_transform</code> <code>Optional[Callable[[MegatronModelType], MegatronModelType]]</code> <p>Optional. The model transform function.</p> <code>None</code> <code>**model_construct_args</code> <p>Optional. Arguments necessary for the supplied model configuration's <code>configure_model</code> method, which will make an instance of the model.</p> <code>{}</code> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def __init__(\n    self,\n    config: BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n    forward_step: ForwardStep,\n    data_step: DataStep,\n    optimizer: MegatronOptimizerModule,\n    model_transform: Optional[Callable[[MegatronModelType], MegatronModelType]] = None,\n    **model_construct_args,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Args:\n        config: Serializable configuration object that allows one to construct a new model instance and loss\n            function. Necessary for Megatron-based training as the model itself cannot be serialized and\n            distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.\n        forward_step: Performs forward pass using the model and a batch of data.\n        data_step: Custom batch-creating function for the model.\n        optimizer: Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning\n            rate.\n        model_construct_args: Optional. Any arguments necessary to construct the model in the `config`'s\n            `configure_model` method.\n        model_transform: Optional. The model transform function.\n        **model_construct_args: Optional. Arguments necessary for the supplied model configuration's\n            `configure_model` method, which will make an instance of the model.\n    \"\"\"\n    super().__init__()\n    self.config = config\n    self.module_construct_args: Optional[dict[str, Any]] = model_construct_args\n    # ***must** be set up in configure_model() -- megatron constraint\n    # also, must be called `module`: nemo expects the actual model to be stored this way\n    self.module: Optional[MegatronModelType] = None\n    self.loss_reduction_class: type[MegatronLossType] = config.get_loss_reduction_class()\n    self.optim = optimizer\n    self.optim.connect(self)  # This will bind the `configure_optimizers` method\n    self._data_step = data_step\n    self._forward_step = forward_step\n    self.model_transform = model_transform\n\n    # configure metrics\n    self.train_metric = self.config.train_metric.get_instance() if self.config.train_metric else None\n    self.valid_metric = self.config.valid_metric.get_instance() if self.config.valid_metric else None\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.configure_model","title":"<code>configure_model()</code>","text":"<p>Updates internal state: instantiates the model from the object's config, assigns to <code>model</code> attribute.</p> <p>NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Updates internal state: instantiates the model from the object's config, assigns to `model` attribute.\n\n    NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.\n\n    Raises:\n        ValueError iff the internal config's configure_model method returns None.\n    \"\"\"\n    if self.module is None:\n        model: MegatronModelType = (\n            self.config.configure_model(**self.module_construct_args)\n            if self.module_construct_args is not None\n            else self.config.configure_model()\n        )\n        self.module = model\n\n    if self.module is None:\n        raise ValueError(\"Invalid semantics: configure_model method **MUST** initialize the model.\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Call the forward method of the underlying model, and return whatever it outputs.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; DataT:\n    \"\"\"Call the forward method of the underlying model, and return whatever it outputs.\"\"\"\n    # safe to do because configure_model is idempotent\n    self.configure_model()\n    assert self.module is not None, \"ERROR: configure_model() method has been incorrectly overridden!\"\n    prediction = self.module(*args, **kwargs)  # for now just pass through to the underlying model\n    return prediction\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.forward_step","title":"<code>forward_step(batch)</code>","text":"<p>Megatron-required: the training forward step for the model, which is required to produce the loss.</p> <p>Normally, the forward pass of a model means its inference. Loss is computed using the predictions from the forward pass against labels. Megatron unfortunately conflates these two different concepts and instead has models \"forward\" method produce the loss. See the Megatron docs for details: https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170</p> <p>To get actual predictions, use the :func:<code>forward</code> method instead.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward_step(self, batch) -&gt; Tensor:\n    \"\"\"Megatron-required: the training forward step for the model, which is required to produce the loss.\n\n    Normally, the forward pass of a model means its inference. Loss is computed using the predictions\n    from the forward pass against labels. Megatron unfortunately conflates these two different concepts\n    and instead has models \"forward\" method produce the loss. See the Megatron docs for details:\n    https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170\n\n    To get actual predictions, use the :func:`forward` method instead.\n    \"\"\"\n    # safe to do because configure_model is idempotent\n    self.configure_model()\n    assert self.module is not None\n    return self._forward_step(self.module, batch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.is_on_logging_device","title":"<code>is_on_logging_device()</code>","text":"<p>Return True if last stage of pipeline parallel and first tensor parallel rank.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def is_on_logging_device(self):\n    \"\"\"Return True if last stage of pipeline parallel and first tensor parallel rank.\"\"\"\n    return parallel_state.is_pipeline_last_stage() and parallel_state.get_tensor_model_parallel_rank() == 0\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.predict_step","title":"<code>predict_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward_step.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"Alias for forward_step.\"\"\"\n    if len(batch) == 0:\n        return\n    return self.forward_step(batch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.training_loss_reduction","title":"<code>training_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def training_loss_reduction(self) -&gt; MegatronLossType:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\"\"\"\n    return self.loss_reduction_class()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.training_step","title":"<code>training_step(batch, batch_idx=None)</code>","text":"<p>In mcore the loss-function is part of the forward-pass when labels are provided.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def training_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n    outputs = self.forward_step(batch)\n    if self.train_metric is not None:\n        if self.is_on_logging_device():\n            self.update_metric(batch, outputs, self.train_metric, self.config.train_metric.task)\n\n        self.log(\n            self.config.train_metric.metric_name,\n            self.train_metric,\n            on_step=True,\n            on_epoch=False,\n            prog_bar=True,\n        )\n\n    return outputs\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.update_metric","title":"<code>update_metric(batch, outputs, metric, task)</code>","text":"<p>Update metric for logging.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def update_metric(\n    self, batch, outputs, metric, task: Literal[\"pretraining\", \"classification\", \"regression\"]\n) -&gt; None:\n    \"\"\"Update metric for logging.\"\"\"\n    match task:\n        case \"pretraining\":\n            logits = outputs[\"token_logits\"].detach().transpose(0, 1)  #  [s, b, v] -&gt; [b, s, v]\n            metric(logits, batch[\"labels\"])\n        case \"classification\":\n            classification_output = outputs[\"classification_output\"]\n            num_classes = classification_output.shape[-1]\n            metric(\n                classification_output.reshape(-1, num_classes),\n                batch[\"labels\"].reshape(-1),\n            )\n        case \"regression\":\n            regression_output = outputs[\"regression_output\"]\n            metric(regression_output, batch[\"labels\"])\n        case _:\n            raise NotImplementedError(f\"unrecognized task {task}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx=None)</code>","text":"<p>In mcore the loss-function is part of the forward-pass when labels are provided.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def validation_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n    outputs = self.forward_step(batch)\n    if self.valid_metric is not None and self.is_on_logging_device():\n        self.update_metric(batch, outputs, self.valid_metric, self.config.valid_metric.task)\n\n    return outputs\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.LightningPassthroughPredictionMixin","title":"<code>LightningPassthroughPredictionMixin</code>","text":"<p>A mixin that allows your model to do inference on the predict step by hijacking nemo's loss reduction mechanism.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class LightningPassthroughPredictionMixin:\n    \"\"\"A mixin that allows your model to do inference on the predict step by hijacking nemo's loss reduction mechanism.\"\"\"\n\n    def predict_loss_reduction(self) -&gt; PassthroughLossReduction:\n        \"\"\"For the predict step, pass through the forward pass output.\"\"\"\n        return PassthroughLossReduction()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.LightningPassthroughPredictionMixin.predict_loss_reduction","title":"<code>predict_loss_reduction()</code>","text":"<p>For the predict step, pass through the forward pass output.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def predict_loss_reduction(self) -&gt; PassthroughLossReduction:\n    \"\"\"For the predict step, pass through the forward pass output.\"\"\"\n    return PassthroughLossReduction()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction","title":"<code>PassthroughLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code>, <code>Generic[DataT]</code></p> <p>A workaround for nemo/megatron to perform inference.</p> <p>Internally in NeMo2.0 the forward step is always expected to return a loss reduction class, and forward is expected to return a loss. This class hijacks that mechanism to instead pass through the forward output unperturbed as the loss (to enable inference in the predict step), and then the reduce method is used to collate the batch of forward outputs into a single batch. This supports the model forward output being a tensor, dict, tuple, or list of tensors. The inner type must always be a Tensor.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class PassthroughLossReduction(MegatronLossReduction, Generic[DataT]):\n    \"\"\"A workaround for nemo/megatron to perform inference.\n\n    Internally in NeMo2.0 the forward step is always expected to return a loss reduction class, and forward is\n    expected to return a loss. This class hijacks that mechanism to instead pass through the forward output unperturbed\n    as the loss (to enable inference in the predict step), and then the reduce method is used to collate the batch of\n    forward outputs into a single batch. This supports the model forward output being a tensor, dict, tuple, or list of\n    tensors. The inner type _must always be a Tensor_.\n    \"\"\"\n\n    def forward(self, batch: DataT, forward_out: DataT) -&gt; Tuple[Tensor, DataT]:\n        \"\"\"Passes through the `forward_out` value as the 2nd tuple element.\n\n        Args:\n            batch: The batch of data that was passed through the model to generate output. NOTE: this value is ignored.\n            forward_out: The output from your model's forward pass.\n\n        Returns:\n            A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).\n        \"\"\"\n        return torch.zeros((1, 1)), forward_out\n\n    def reduce(self, forward_out: List[DataT]) -&gt; DataT:\n        \"\"\"Collates list of model's outputs into a single output.\"\"\"\n        return batch_collator(forward_out)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Passes through the <code>forward_out</code> value as the 2nd tuple element.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DataT</code> <p>The batch of data that was passed through the model to generate output. NOTE: this value is ignored.</p> required <code>forward_out</code> <code>DataT</code> <p>The output from your model's forward pass.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, DataT]</code> <p>A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward(self, batch: DataT, forward_out: DataT) -&gt; Tuple[Tensor, DataT]:\n    \"\"\"Passes through the `forward_out` value as the 2nd tuple element.\n\n    Args:\n        batch: The batch of data that was passed through the model to generate output. NOTE: this value is ignored.\n        forward_out: The output from your model's forward pass.\n\n    Returns:\n        A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).\n    \"\"\"\n    return torch.zeros((1, 1)), forward_out\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction.reduce","title":"<code>reduce(forward_out)</code>","text":"<p>Collates list of model's outputs into a single output.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def reduce(self, forward_out: List[DataT]) -&gt; DataT:\n    \"\"\"Collates list of model's outputs into a single output.\"\"\"\n    return batch_collator(forward_out)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.batch_collator","title":"<code>batch_collator(batches, batch_dim=0, seq_dim=1, batch_dim_key_defaults={'token_logits': 1}, seq_dim_key_defaults={'token_logits': 0})</code>","text":"<p>Takes a sequence of batches and collates them into a single batch.</p> <pre><code>This is distinct from the standard pytorch default_collator since it does\nnot add the batch dimension, it's assumed the batch\ndimension is already present in the input, as would be the case when\nparallelizing across minibatches.\n</code></pre> <p>IMPORTANT: The underlying data primitive must be a torch Tensor. The input to this function is a recurisve type, there can be any amount of nesting between dictionaries, tuples, and lists, as long as the inner type is a n-d Tensor.</p> <p>Examples:</p> <p>Outer container = Dict:     [{'a': Tensor([1]), 'b': Tensor([2])}, {'a': Tensor([2]), 'b': Tensor([3])}] -&gt; {'a': Tensor([1, 2]), 'b': Tensor([2, 3])} Outer container = List:     [[Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]] -&gt; [Tensor([1, 2]), Tensor([2, 3])] Outer container = Tuple:     ([Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]) -&gt; (Tensor([1, 2]), Tensor([2, 3]))</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Optional[Sequence[ReductionT]]</code> <p>sequence of batches to collate into a single batch.</p> required <code>batch_dim</code> <code>int</code> <p>If you know that the batch dim for the batch you are concatenating is not the 0th dimension (for example it is sequence first) then supply that dimension.</p> <code>0</code> <code>seq_dim</code> <code>int</code> <p>If you know that the sequence dim for the batch you are concatenating is not the 1st dimension (for example it is sequence first) then supply that dimension. This is used for padding to the max length.</p> <code>1</code> <code>batch_dim_key_defaults</code> <code>dictionary of keys to integers</code> <p>If your batch is a dictionary and you know that some keys have non-standard (0) batch dimensions, supply those here. By default \"token_logits\" has batch dim 1 and otherwise all keys are assumed to have batch dim 0.</p> <code>{'token_logits': 1}</code> <code>seq_dim_key_defaults</code> <code>dictionary of keys to integers</code> <p>If your batch is a dictionary and you know that some keys have non-standard (1) sequence dimensions, supply those here. By default \"token_logits\" has seq dim 0 and otherwise all keys are assumed to have seq dim 1.</p> <code>{'token_logits': 0}</code> <p>Returns:</p> Type Description <code>Optional[ReductionT]</code> <p>A single batch of the same type as the elements of your input sequence.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def batch_collator(\n    batches: Optional[Union[Tuple[ReductionT], List[ReductionT]]],\n    batch_dim: int = 0,\n    seq_dim: int = 1,\n    batch_dim_key_defaults: dict[str, int] = {\"token_logits\": 1},\n    seq_dim_key_defaults: dict[str, int] = {\"token_logits\": 0},\n) -&gt; Optional[ReductionT]:\n    \"\"\"Takes a sequence of batches and collates them into a single batch.\n\n        This is distinct from the standard pytorch default_collator since it does\n        not add the batch dimension, it's assumed the batch\n        dimension is already present in the input, as would be the case when\n        parallelizing across minibatches.\n\n    IMPORTANT: The underlying data primitive _must_ be a torch Tensor. The input to this function is a recurisve type,\n    there can be any amount of nesting between dictionaries, tuples, and lists, as long as the inner type is a n-d Tensor.\n\n    Examples:\n        Outer container = Dict:\n            [{'a': Tensor([1]), 'b': Tensor([2])}, {'a': Tensor([2]), 'b': Tensor([3])}] -&gt; {'a': Tensor([1, 2]), 'b': Tensor([2, 3])}\n        Outer container = List:\n            [[Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]] -&gt; [Tensor([1, 2]), Tensor([2, 3])]\n        Outer container = Tuple:\n            ([Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]) -&gt; (Tensor([1, 2]), Tensor([2, 3]))\n\n    Args:\n        batches (Optional[Sequence[ReductionT]]): sequence of batches to collate into a single batch.\n        batch_dim: If you know that the batch dim for the batch you are concatenating is not the 0th dimension (for\n            example it is sequence first) then supply that dimension.\n        seq_dim: If you know that the sequence dim for the batch you are concatenating is not the 1st dimension (for\n            example it is sequence first) then supply that dimension. This is used for padding to the max length.\n        batch_dim_key_defaults (dictionary of keys to integers): If your batch is a dictionary and you know that some\n            keys have non-standard (0) batch dimensions, supply those here. By default \"token_logits\" has batch dim 1\n            and otherwise all keys are assumed to have batch dim 0.\n        seq_dim_key_defaults (dictionary of keys to integers): If your batch is a dictionary and you know that some\n            keys have non-standard (1) sequence dimensions, supply those here. By default \"token_logits\" has seq dim 0\n            and otherwise all keys are assumed to have seq dim 1.\n\n    Returns:\n        A single batch of the same type as the elements of your input sequence.\n    \"\"\"\n    match batches:\n        # Handle base-cases for batch concatenation, either a list of None or a list of tensors\n        case [None, *_]:\n            return None\n        case [Tensor(), *_]:\n            # First shortcut if all tensors are 1D (they have at least one batch dim, and it must be at 0)\n            if len(batches) &gt; 0 and isinstance(batches[0], Tensor) and batches[0].ndim == 1:\n                return torch.cat(batches, dim=0)\n            # Find max sequence length across all tensors\n            max_seq_len = max(batch.size(seq_dim) for batch in batches)\n            # Pad each tensor to max length along seq_dim\n            padded_batches = []\n            for batch in batches:\n                # Initialize padding tuple - needs 2 values per dim, starting from last dim\n                # e.g. for 3D tensor: [left_pad_dim2, right_pad_dim2, left_pad_dim1, right_pad_dim1, left_pad_dim0, right_pad_dim0]\n                pad_size = [0] * (2 * batch.ndim)\n                # Calculate padding needed at end of sequence dimension\n                pad_amount = max_seq_len - batch.size(seq_dim)\n                # Pad end of sequence dimension by putting padding amount in correct position\n                # For seq_dim=1 in 3D tensor: [0, 0, 0, pad_amount, 0, 0]\n                pad_size[2 * (batch.ndim - 1 - seq_dim) + 1] = pad_amount\n                padded_batch = torch.nn.functional.pad(batch, tuple(pad_size))\n                padded_batches.append(padded_batch)\n            padded_batch = torch.cat(padded_batches, dim=batch_dim)\n            assert padded_batch.size(seq_dim) == max_seq_len\n            return padded_batch\n        # Next 3 calls are the recursive calls into the sub-structures of the batch. We handle dictionaries, tuples, and lists\n        case [dict(), *_]:\n            return {\n                key: batch_collator(\n                    [batch[key] for batch in batches],\n                    batch_dim=batch_dim_key_defaults.get(key, batch_dim),\n                    seq_dim=seq_dim_key_defaults.get(key, seq_dim),\n                    batch_dim_key_defaults=batch_dim_key_defaults,\n                    seq_dim_key_defaults=seq_dim_key_defaults,\n                )\n                for key in batches[0]\n            }\n        case [tuple(), *_]:\n            return tuple(\n                batch_collator(\n                    [batch[i] for batch in batches],\n                    batch_dim=batch_dim,\n                    seq_dim=seq_dim,\n                    batch_dim_key_defaults=batch_dim_key_defaults,\n                    seq_dim_key_defaults=seq_dim_key_defaults,\n                )\n                for i in range(len(batches[0]))\n            )\n        case [list(), *_]:\n            return [\n                batch_collator(\n                    [batch[i] for batch in batches],\n                    batch_dim=batch_dim,\n                    seq_dim=seq_dim,\n                    batch_dim_key_defaults=batch_dim_key_defaults,\n                    seq_dim_key_defaults=seq_dim_key_defaults,\n                )\n                for i in range(len(batches[0]))\n            ]\n        # Final cases shouldn't happen, an empty sequence (no batches), or \"other\".\n        case []:\n            raise ValueError(\"Cannot process an empty sequence\")\n        case _:\n            raise ValueError(\"Unsupported input structure in batch_collator\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.default_megatron_optimizer","title":"<code>default_megatron_optimizer()</code>","text":"<p>Default distributed optimizer uses Adam with a 1e-4 learning rate.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def default_megatron_optimizer() -&gt; MegatronOptimizerModule:\n    \"\"\"Default distributed optimizer uses Adam with a 1e-4 learning rate.\"\"\"\n    return MegatronOptimizerModule(\n        config=OptimizerConfig(lr=1e-4, optimizer=\"adam\", use_distributed_optimizer=True),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.some_first","title":"<code>some_first(seq)</code>","text":"<p>Returns the first non-None value from the sequence or fails</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def some_first(seq: Iterable[Optional[T]]) -&gt; T:\n    \"\"\"Returns the first non-None value from the sequence or fails\"\"\"  # noqa: D415\n    for s in seq:\n        if s is not None:\n            return s\n    raise ValueError(\"non-None value not found\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/","title":"Train","text":""},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.NsysConfig","title":"<code>NsysConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for nsys profiling.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>class NsysConfig(BaseModel):\n    \"\"\"Configuration for nsys profiling.\"\"\"\n\n    start_step: int = 0\n    end_step: Optional[int] = None\n    ranks: list[int] = field(default_factory=lambda: [0])\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.nemo_logger_factory","title":"<code>nemo_logger_factory(experiment_config, wandb_config)</code>","text":"<p>Creates and returns a NeMoLogger instance configured based on the provided experiment and wandb configurations.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_config</code> <code>ExperimentConfig</code> <p>Configuration object containing experiment settings such as result directory, experiment name, checkpoint settings, and logger preferences.</p> required <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>Optional configuration object for Weights and Biases logging.</p> required <p>Returns:</p> Type Description <code>NeMoLogger</code> <p>nl.NeMoLogger: An instance of NeMoLogger configured with the specified settings.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>def nemo_logger_factory(experiment_config: ExperimentConfig, wandb_config: Optional[WandbConfig]) -&gt; nl.NeMoLogger:\n    \"\"\"Creates and returns a NeMoLogger instance configured based on the provided experiment and wandb configurations.\n\n    Args:\n        experiment_config (ExperimentConfig): Configuration object containing experiment settings such as\n            result directory, experiment name, checkpoint settings, and logger preferences.\n        wandb_config (Optional[WandbConfig]): Optional configuration object for Weights and Biases logging.\n\n    Returns:\n        nl.NeMoLogger: An instance of NeMoLogger configured with the specified settings.\n    \"\"\"\n    if experiment_config.create_checkpoint_callback:\n        checkpoint_callback = nl_callbacks.ModelCheckpoint(\n            save_last=experiment_config.save_last_checkpoint,\n            monitor=experiment_config.metric_to_monitor_for_checkpoints,\n            save_top_k=experiment_config.save_top_k,\n            every_n_train_steps=experiment_config.save_every_n_steps,\n            always_save_context=True,\n            filename=\"{epoch}-{val_loss:.2f}-{step}-{consumed_samples}\",  # Including step and consumed_samples in the checkpoint filename prevents duplicate filenames and bugs related to this.\n        )\n    else:\n        checkpoint_callback = None\n\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=experiment_config.result_dir,\n        name=experiment_config.experiment_name,\n        initialize_tensorboard_logger=experiment_config.create_tensorboard_logger,\n        wandb_config=wandb_config,\n        ckpt_callback=checkpoint_callback,\n    )\n    return nemo_logger\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.setup_trainer","title":"<code>setup_trainer(parallel_config, training_config, callbacks=None, nsys_config=None)</code>","text":"<p>Set up the trainer for model training using the specified parallel and training configurations.</p> <p>Parameters:</p> Name Type Description Default <code>parallel_config</code> <code>ParallelConfig</code> <p>Configuration for parallelism, including tensor and pipeline model parallel sizes,                               number of devices, and number of nodes.</p> required <code>training_config</code> <code>TrainingConfig</code> <p>Configuration for training, including maximum steps, accelerator type,                               validation batch limit, validation check interval, and precision.</p> required <code>callbacks</code> <code>list</code> <p>List of callback functions to be used during training. Defaults to None,                         in which case default callbacks (RichModelSummary and LearningRateMonitor) are used.</p> <code>None</code> <code>nsys_config</code> <code>NsysConfig</code> <p>Configuration for nsys profiling. If None, is disabled.</p> <code>None</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>nl.Trainer: Configured trainer object ready for model training.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>def setup_trainer(\n    parallel_config: ParallelConfig,\n    training_config: TrainingConfig,\n    callbacks=None,\n    nsys_config: NsysConfig | None = None,\n) -&gt; nl.Trainer:\n    \"\"\"Set up the trainer for model training using the specified parallel and training configurations.\n\n    Args:\n        parallel_config (ParallelConfig): Configuration for parallelism, including tensor and pipeline model parallel sizes,\n                                          number of devices, and number of nodes.\n        training_config (TrainingConfig): Configuration for training, including maximum steps, accelerator type,\n                                          validation batch limit, validation check interval, and precision.\n        callbacks (list, optional): List of callback functions to be used during training. Defaults to None,\n                                    in which case default callbacks (RichModelSummary and LearningRateMonitor) are used.\n        nsys_config (NsysConfig, optional): Configuration for nsys profiling. If None, is disabled.\n\n    Returns:\n        nl.Trainer: Configured trainer object ready for model training.\n    \"\"\"\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=parallel_config.tensor_model_parallel_size,\n        pipeline_model_parallel_size=parallel_config.pipeline_model_parallel_size,\n        pipeline_dtype=get_autocast_dtype(training_config.precision),\n        ddp=DistributedDataParallelConfig(\n            check_for_nan_in_grad=True,\n            overlap_grad_reduce=True,\n            overlap_param_gather=False,  # TODO waiting for NeMo fix\n            average_in_collective=True,\n            use_distributed_optimizer=True,\n        ),\n        find_unused_parameters=True,\n        gradient_as_bucket_view=True,\n        ckpt_include_optimizer=True,\n        ckpt_async_save=True,\n        ckpt_parallel_load=True,\n    )\n    if callbacks is None:\n        callbacks = [\n            RichModelSummary(max_depth=4),\n            LearningRateMonitor(),\n        ]\n\n    if training_config.gc_interval &gt; 0:\n        callbacks.append(\n            nl_callbacks.GarbageCollectionCallback(\n                gc_interval_train=training_config.gc_interval, gc_interval_val=training_config.gc_interval\n            )\n        )\n\n    if nsys_config:\n        if nsys_config.end_step is None:\n            nsys_config.end_step = training_config.max_steps\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=nsys_config.start_step,\n                end_step=nsys_config.end_step,\n                ranks=nsys_config.ranks,\n                gen_shape=True,\n            )\n        )\n\n    trainer = nl.Trainer(\n        devices=parallel_config.num_devices,\n        max_steps=training_config.max_steps,\n        accelerator=training_config.accelerator,\n        strategy=strategy,\n        limit_val_batches=training_config.limit_val_batches,\n        val_check_interval=training_config.val_check_interval,\n        num_nodes=parallel_config.num_nodes,\n        callbacks=callbacks,\n        plugins=nl.MegatronMixedPrecision(\n            precision=training_config.precision,\n            params_dtype=get_autocast_dtype(training_config.precision),\n            pipeline_dtype=get_autocast_dtype(training_config.precision),\n            grad_reduce_in_fp32=False,\n            autocast_enabled=False,\n        ),\n        enable_checkpointing=training_config.enable_checkpointing,\n    )\n    return trainer\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.train","title":"<code>train(bionemo_exposed_model_config, data_config, parallel_config, training_config, optim_config, experiment_config, wandb_config, nsys_config=None, resume_if_exists=True)</code>","text":"<p>Train a BioNemo model using the provided configurations. Uses the ExposedModelConfig and DataConfig as the primary variants for this method.</p> <p>Parameters:</p> Name Type Description Default <code>bionemo_exposed_model_config</code> <code>ExposedModelConfig</code> <p>Configuration for the exposed BioNemo model.</p> required <code>data_config</code> <code>DataConfig[DataModuleT]</code> <p>Configuration for the data module.</p> required <code>parallel_config</code> <code>ParallelConfig</code> <p>Configuration for parallel training.</p> required <code>training_config</code> <code>TrainingConfig</code> <p>Configuration for training parameters.</p> required <code>optim_config</code> <code>OptimizerSchedulerConfig</code> <p>Configuration for the optimizer and scheduler.</p> required <code>experiment_config</code> <code>ExperimentConfig</code> <p>Configuration for the experiment.</p> required <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>Configuration for Weights and Biases logging.n</p> required <code>nsys_config</code> <code>Optional[NsysConfig]</code> <p>Configuration for nsys profiling. If None, is disabled.</p> <code>None</code> <code>resume_if_exists</code> <code>bool</code> <p>Flag to resume training if a checkpoint exists. Defaults to True.</p> <code>True</code> Source code in <code>bionemo/llm/train.py</code> <pre><code>def train(\n    bionemo_exposed_model_config: ExposedModelConfig,\n    data_config: DataConfig[DataModuleT],\n    parallel_config: ParallelConfig,\n    training_config: TrainingConfig,\n    optim_config: OptimizerSchedulerConfig,\n    experiment_config: ExperimentConfig,\n    wandb_config: Optional[WandbConfig],\n    nsys_config: Optional[NsysConfig] = None,\n    resume_if_exists: bool = True,\n):\n    \"\"\"Train a BioNemo model using the provided configurations. Uses the ExposedModelConfig and DataConfig as the primary variants for this method.\n\n    Args:\n        bionemo_exposed_model_config (ExposedModelConfig): Configuration for the exposed BioNemo model.\n        data_config (DataConfig[DataModuleT]): Configuration for the data module.\n        parallel_config (ParallelConfig): Configuration for parallel training.\n        training_config (TrainingConfig): Configuration for training parameters.\n        optim_config (OptimizerSchedulerConfig): Configuration for the optimizer and scheduler.\n        experiment_config (ExperimentConfig): Configuration for the experiment.\n        wandb_config (Optional[WandbConfig]): Configuration for Weights and Biases logging.n\n        nsys_config (Optional[NsysConfig], optional): Configuration for nsys profiling. If None, is disabled.\n        resume_if_exists (bool, optional): Flag to resume training if a checkpoint exists. Defaults to True.\n    \"\"\"\n    bionemo_model_config = bionemo_exposed_model_config.exposed_to_internal_bionemo_model_config()\n    pathlib.Path(data_config.result_dir).mkdir(parents=True, exist_ok=True)\n\n    if experiment_config.save_every_n_steps != training_config.val_check_interval:\n        logging.warning(\"Mutating training_config.save_every_n_steps to be equal to val_check_interval.\")\n        experiment_config.save_every_n_steps = training_config.val_check_interval\n\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=data_config.micro_batch_size,\n        num_nodes=parallel_config.num_nodes,\n        devices=parallel_config.num_devices,\n        accumulate_grad_batches=parallel_config.accumulate_grad_batches,\n        tensor_model_parallel_size=parallel_config.tensor_model_parallel_size,\n        pipeline_model_parallel_size=parallel_config.pipeline_model_parallel_size,\n    )\n\n    data: DataModuleT = data_config.construct_data_module(global_batch_size)\n    # TODO BioBertDataModule or BioBertTokenizer abstractions. We know all DataModuleT in this case has data.tokenizer,\n    # although this constraint is not documented.\n\n    # TODO: need an abstraction for LrSchedulerConfig\n    if optim_config.lr_scheduler == \"cosine\":\n        lr_scheduler = CosineAnnealingScheduler(\n            max_steps=training_config.max_steps if optim_config.max_steps is None else optim_config.max_steps,\n            min_lr=optim_config.lr / 100,\n            warmup_steps=int(math.ceil(training_config.max_steps * optim_config.cosine_rampup_frac)),\n            interval=optim_config.interval,\n            monitor=optim_config.monitor,\n            constant_steps=int(math.ceil(training_config.max_steps * optim_config.cosine_hold_frac)),\n        )\n    elif optim_config.lr_scheduler == \"warmup_anneal\":\n        lr_scheduler = WarmupAnnealDecayHoldScheduler(\n            warmup_steps=optim_config.warmup_steps,\n            max_steps=training_config.max_steps if optim_config.max_steps is None else optim_config.max_steps,\n            max_lr=optim_config.lr,\n            min_lr=optim_config.lr / 10.0,\n            anneal_percentage=0.10,\n        )\n    else:\n        raise NotImplementedError(f\"Scheduler {optim_config.lr_scheduler} not implemented.\")\n\n    optimizer = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=optim_config.lr,\n            optimizer=optim_config.optimizer,\n            use_distributed_optimizer=True,\n            fp16=bionemo_model_config.fp16,\n            bf16=bionemo_model_config.bf16,\n        ),\n        lr_scheduler=lr_scheduler,\n    )\n\n    model: BionemoLightningModule = biobert_lightning_module(\n        config=bionemo_model_config,\n        tokenizer=data.tokenizer,\n        optimizer=optimizer,\n    )\n    trainer: nl.Trainer = setup_trainer(parallel_config, training_config, nsys_config=nsys_config)\n    nemo_logger: nl.NeMoLogger = nemo_logger_factory(experiment_config, wandb_config=wandb_config)\n\n    llm.train(\n        model=model,\n        data=data,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=resume_if_exists,\n            resume_ignore_no_checkpoint=True,\n        ),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/collate/","title":"Collate","text":""},{"location":"API_reference/bionemo/llm/data/collate/#bionemo.llm.data.collate.bert_padding_collate_fn","title":"<code>bert_padding_collate_fn(batch, padding_value, min_length=None, max_length=None)</code>","text":"<p>Padding collate function for BERT dataloaders.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>List of samples.</p> required <code>padding_value</code> <code>int</code> <p>The tokenizer's pad token ID.</p> required <code>min_length</code> <code>int | None</code> <p>Minimum length of the output batch; tensors will be padded to this length. If not provided, no extra padding beyond the max_length will be added.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Maximum length of the sequence. If not provided, tensors will be padded to the longest sequence in the batch.</p> <code>None</code> Source code in <code>bionemo/llm/data/collate.py</code> <pre><code>def bert_padding_collate_fn(\n    batch: Sequence[types.BertSample],\n    padding_value: int,\n    min_length: int | None = None,\n    max_length: int | None = None,\n) -&gt; types.BertSample:\n    \"\"\"Padding collate function for BERT dataloaders.\n\n    Args:\n        batch (list): List of samples.\n        padding_value (int, optional): The tokenizer's pad token ID.\n        min_length: Minimum length of the output batch; tensors will be padded to this length. If not\n            provided, no extra padding beyond the max_length will be added.\n        max_length: Maximum length of the sequence. If not provided, tensors will be padded to the\n            longest sequence in the batch.\n    \"\"\"\n    padding_values = {\n        \"text\": padding_value,\n        \"types\": 0,\n        \"attention_mask\": False,\n        \"labels\": MLM_LOSS_IGNORE_INDEX,  # This should match the masked value used in the MLM loss mask.\n        \"loss_mask\": False,\n        \"is_random\": 0,\n    }\n    return padding_collate_fn(\n        batch=batch,  # type: ignore[assignment]\n        padding_values=padding_values,\n        min_length=min_length,\n        max_length=max_length,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/collate/#bionemo.llm.data.collate.padding_collate_fn","title":"<code>padding_collate_fn(batch, padding_values, min_length=None, max_length=None)</code>","text":"<p>Collate function with padding.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Sequence[_T]</code> <p>List of samples, each of which is a dictionary of tensors.</p> required <code>padding_values</code> <code>dict[str, int]</code> <p>A dictionary of padding values for each tensor key.</p> required <code>min_length</code> <code>int | None</code> <p>Minimum length of the output batch; tensors will be padded to this length. If not provided, no extra padding beyond the max_length will be added.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Maximum length of the sequence. If not provided, tensors will be padded to the longest sequence in the batch.</p> <code>None</code> <p>Returns:</p> Type Description <code>_T</code> <p>A collated batch with the same dictionary input structure.</p> Source code in <code>bionemo/llm/data/collate.py</code> <pre><code>def padding_collate_fn(\n    batch: Sequence[_T],\n    padding_values: dict[str, int],\n    min_length: int | None = None,\n    max_length: int | None = None,\n) -&gt; _T:\n    \"\"\"Collate function with padding.\n\n    Args:\n        batch: List of samples, each of which is a dictionary of tensors.\n        padding_values: A dictionary of padding values for each tensor key.\n        min_length: Minimum length of the output batch; tensors will be padded to this length. If not\n            provided, no extra padding beyond the max_length will be added.\n        max_length: Maximum length of the sequence. If not provided, tensors will be padded to the\n            longest sequence in the batch.\n\n    Returns:\n        A collated batch with the same dictionary input structure.\n    \"\"\"\n    global _warned_once\n    keys: set[str] | None = None\n\n    if len(batch) == 0:  # empty batches passed through in DDP inference\n        return {}\n\n    for entry in batch:\n        # First check that we have sane batches where keys align with each other.\n        if keys is None:\n            keys = set(entry.keys())\n        else:\n            if set(entry.keys()) != keys:\n                raise ValueError(f\"All keys in inputs must match each other. Got: {[sorted(e.keys()) for e in batch]}\")\n        if entry.keys() != padding_values.keys():\n            if not _warned_once:\n                extra_keys = {k for k in entry.keys() if k not in padding_values}\n                missing_keys = {k for k in padding_values.keys() if k not in entry}\n                logger.warning(\n                    f\"Extra keys in batch that will not be padded: {extra_keys}. Missing keys in batch: {missing_keys}\"\n                )\n                _warned_once = True\n\n    def _pad(tensors, padding_value):\n        if max_length is not None:\n            tensors = [t[:max_length] for t in tensors]\n        batched_tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n        if min_length is None:\n            return batched_tensors\n        return torch.nn.functional.pad(batched_tensors, (0, min_length - batched_tensors.size(1)), value=padding_value)\n\n    return {\n        k: _pad([s[k] for s in batch], padding_values[k])\n        if k in padding_values\n        else torch.stack([s[k] for s in batch])\n        for k in batch[0].keys()\n    }  # type: ignore[return-value]\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule","title":"<code>MegatronDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A mixin that adds a <code>state_dict</code> and <code>load_state_dict</code> method for datamodule training resumption in NeMo.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>class MegatronDataModule(pl.LightningDataModule):\n    \"\"\"A mixin that adds a `state_dict` and `load_state_dict` method for datamodule training resumption in NeMo.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Set init_global_step to 0 for datamodule resumption.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.init_global_step = 0\n\n    def update_init_global_step(self):\n        \"\"\"Please always call this when you get a new dataloader... if you forget, your resumption will not work.\"\"\"\n        self.init_global_step = self.trainer.global_step  # Update the init_global_step whenever we re-init training\n        self.data_sampler.init_global_step = (\n            self.init_global_step\n        )  # Update the init_global_step whenever we re-init training\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\n\n        Returns:\n            A dictionary containing datamodule state.\n\n        \"\"\"\n        consumed_samples = self.data_sampler.compute_consumed_samples(self.trainer.global_step - self.init_global_step)\n        return {\"consumed_samples\": consumed_samples}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.\n\n        Args:\n            state_dict: the datamodule state returned by ``state_dict``.\n\n        \"\"\"\n        try:\n            from megatron.core.num_microbatches_calculator import update_num_microbatches\n\n        except (ImportError, ModuleNotFoundError):\n            logging.warning(\"Megatron num_microbatches_calculator not found, using Apex version.\")\n            from apex.transformer.pipeline_parallel.utils import update_num_microbatches\n\n        consumed_samples = state_dict[\"consumed_samples\"]\n        self.data_sampler.init_consumed_samples = consumed_samples\n        self.data_sampler.prev_consumed_samples = consumed_samples\n\n        update_num_microbatches(\n            consumed_samples=consumed_samples,\n            consistency_check=False,\n        )\n        self.data_sampler.if_first_step = 1\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Set init_global_step to 0 for datamodule resumption.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Set init_global_step to 0 for datamodule resumption.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.init_global_step = 0\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>the datamodule state returned by <code>state_dict</code>.</p> required Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.\n\n    Args:\n        state_dict: the datamodule state returned by ``state_dict``.\n\n    \"\"\"\n    try:\n        from megatron.core.num_microbatches_calculator import update_num_microbatches\n\n    except (ImportError, ModuleNotFoundError):\n        logging.warning(\"Megatron num_microbatches_calculator not found, using Apex version.\")\n        from apex.transformer.pipeline_parallel.utils import update_num_microbatches\n\n    consumed_samples = state_dict[\"consumed_samples\"]\n    self.data_sampler.init_consumed_samples = consumed_samples\n    self.data_sampler.prev_consumed_samples = consumed_samples\n\n    update_num_microbatches(\n        consumed_samples=consumed_samples,\n        consistency_check=False,\n    )\n    self.data_sampler.if_first_step = 1\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.state_dict","title":"<code>state_dict()</code>","text":"<p>Called when saving a checkpoint, implement to generate and save datamodule state.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing datamodule state.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\n\n    Returns:\n        A dictionary containing datamodule state.\n\n    \"\"\"\n    consumed_samples = self.data_sampler.compute_consumed_samples(self.trainer.global_step - self.init_global_step)\n    return {\"consumed_samples\": consumed_samples}\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.update_init_global_step","title":"<code>update_init_global_step()</code>","text":"<p>Please always call this when you get a new dataloader... if you forget, your resumption will not work.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def update_init_global_step(self):\n    \"\"\"Please always call this when you get a new dataloader... if you forget, your resumption will not work.\"\"\"\n    self.init_global_step = self.trainer.global_step  # Update the init_global_step whenever we re-init training\n    self.data_sampler.init_global_step = (\n        self.init_global_step\n    )  # Update the init_global_step whenever we re-init training\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/","title":"Label2id tokenizer","text":""},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer","title":"<code>Label2IDTokenizer</code>","text":"<p>               Bases: <code>TokenizerSpec</code></p> <p>Initializes simple Char Tokenizer.</p> <p>Intended to be used for extracting class labels for classification models such as secondary structure prediction model, where each class is encoded with a character (ex. \"C\", \"H\", \"E\")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokenizer = Label2IDTokenizer()\n&gt;&gt;&gt; seqs = ['CHE', 'CCC', 'EHH']\n&gt;&gt;&gt; tokenizer = tokenizer.build_vocab(s)\n</code></pre> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>class Label2IDTokenizer(TokenizerSpec):\n    \"\"\"Initializes simple Char Tokenizer.\n\n    Intended to be used for extracting class labels\n    for classification models such as secondary\n    structure prediction model, where each class is\n    encoded with a character (ex. \"C\", \"H\", \"E\")\n\n    Examples:\n            &gt;&gt;&gt; tokenizer = Label2IDTokenizer()\n            &gt;&gt;&gt; seqs = ['CHE', 'CCC', 'EHH']\n            &gt;&gt;&gt; tokenizer = tokenizer.build_vocab(s)\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        super().__init__()\n        self.vocab: Dict[str, int] = {}\n        self.decode_vocab: Dict[int, str] = {id_: token for token, id_ in self.vocab.items()}\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Return the size of the vocab being used.\"\"\"\n        return len(self.vocab)\n\n    def text_to_tokens(self, text: str) -&gt; List[str]:  # noqa: D102\n        return list(text)\n\n    def tokens_to_text(self, tokens: List[str]) -&gt; str:  # noqa: D102\n        return \"\".join(tokens)\n\n    def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:\n        \"\"\"Convert tokens to indexes/ids.\n\n        Args:\n            tokens: Containing tokens\n        Returns:\n            Containing ID's for each token\n        \"\"\"\n        ids = []\n        for token in tokens:\n            id_ = self.vocab.get(token)\n            if id_ is None:\n                raise ValueError(f\"Do not recognize token: {token}\")\n            else:\n                ids.append(id_)\n        return ids\n\n    def ids_to_tokens(self, ids: List[int]) -&gt; List[str]:\n        \"\"\"Convert Ids to tokens.\n\n        Args:\n            ids: Containg ids for each token\n        Returns:\n            Containing tokens\n        \"\"\"\n        tokens = []\n        for id_ in ids:\n            token = self.decode_vocab.get(id_)\n            if token is None:\n                raise ValueError(f\"Do not recognize ID: {id_}\")\n            tokens.append(token)\n        return tokens\n\n    def text_to_ids(self, text: str) -&gt; List[int]:\n        \"\"\"Converts text to ids.\n\n        Args:\n            text (str): String containing text to convert\n        Returns:\n            (List[int]): Id's corresponding to the tokenization\n            of the text\n        \"\"\"\n        tokens = self.text_to_tokens(text)\n        return self.tokens_to_ids(tokens)\n\n    def ids_to_text(self, ids: List[int]) -&gt; str:  # noqa: D102\n        tokens = self.ids_to_tokens(ids)\n        return self.tokens_to_text(tokens)\n\n    def build_vocab(self, strings: Union[str, Iterable[str]]) -&gt; \"Label2IDTokenizer\":\n        \"\"\"Builds the vocabulary of the tokenizer from strings\n        Args:\n            strings: (Union[str, Iterable[str]]): Strings to\n                build the vocabulary with. If a string is supplied,\n                then the vocabulary is built from the single string.\n                Otherwise, the vocabulary is progressively built\n                from all the strings in `strings`.\n        \"\"\"  # noqa: D205\n        if isinstance(strings, str):\n            strings = [strings]\n\n        for string in strings:\n            for token in string:\n                if token not in self.vocab:\n                    self.vocab[token] = len(self.vocab)\n                    self.decode_vocab[self.vocab[token]] = token\n\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.vocab_size","title":"<code>vocab_size</code>  <code>property</code>","text":"<p>Return the size of the vocab being used.</p>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.build_vocab","title":"<code>build_vocab(strings)</code>","text":"<p>Builds the vocabulary of the tokenizer from strings Args:     strings: (Union[str, Iterable[str]]): Strings to         build the vocabulary with. If a string is supplied,         then the vocabulary is built from the single string.         Otherwise, the vocabulary is progressively built         from all the strings in <code>strings</code>.</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def build_vocab(self, strings: Union[str, Iterable[str]]) -&gt; \"Label2IDTokenizer\":\n    \"\"\"Builds the vocabulary of the tokenizer from strings\n    Args:\n        strings: (Union[str, Iterable[str]]): Strings to\n            build the vocabulary with. If a string is supplied,\n            then the vocabulary is built from the single string.\n            Otherwise, the vocabulary is progressively built\n            from all the strings in `strings`.\n    \"\"\"  # noqa: D205\n    if isinstance(strings, str):\n        strings = [strings]\n\n    for string in strings:\n        for token in string:\n            if token not in self.vocab:\n                self.vocab[token] = len(self.vocab)\n                self.decode_vocab[self.vocab[token]] = token\n\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.ids_to_tokens","title":"<code>ids_to_tokens(ids)</code>","text":"<p>Convert Ids to tokens.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[int]</code> <p>Containg ids for each token</p> required <p>Returns:     Containing tokens</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def ids_to_tokens(self, ids: List[int]) -&gt; List[str]:\n    \"\"\"Convert Ids to tokens.\n\n    Args:\n        ids: Containg ids for each token\n    Returns:\n        Containing tokens\n    \"\"\"\n    tokens = []\n    for id_ in ids:\n        token = self.decode_vocab.get(id_)\n        if token is None:\n            raise ValueError(f\"Do not recognize ID: {id_}\")\n        tokens.append(token)\n    return tokens\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.text_to_ids","title":"<code>text_to_ids(text)</code>","text":"<p>Converts text to ids.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>String containing text to convert</p> required <p>Returns:     (List[int]): Id's corresponding to the tokenization     of the text</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def text_to_ids(self, text: str) -&gt; List[int]:\n    \"\"\"Converts text to ids.\n\n    Args:\n        text (str): String containing text to convert\n    Returns:\n        (List[int]): Id's corresponding to the tokenization\n        of the text\n    \"\"\"\n    tokens = self.text_to_tokens(text)\n    return self.tokens_to_ids(tokens)\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.tokens_to_ids","title":"<code>tokens_to_ids(tokens)</code>","text":"<p>Convert tokens to indexes/ids.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>Containing tokens</p> required <p>Returns:     Containing ID's for each token</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:\n    \"\"\"Convert tokens to indexes/ids.\n\n    Args:\n        tokens: Containing tokens\n    Returns:\n        Containing ID's for each token\n    \"\"\"\n    ids = []\n    for token in tokens:\n        id_ = self.vocab.get(token)\n        if id_ is None:\n            raise ValueError(f\"Do not recognize token: {token}\")\n        else:\n            ids.append(id_)\n    return ids\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/","title":"Masking","text":""},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.BertMaskConfig","title":"<code>BertMaskConfig</code>  <code>dataclass</code>","text":"<p>Configuration for masking tokens in a BERT-style model.</p> <p>Attributes:</p> Name Type Description <code>mask_prob</code> <code>float</code> <p>Probability of masking a token.</p> <code>mask_token_prob</code> <code>float</code> <p>Probability of replacing a masked token with the mask token.</p> <code>random_token_prob</code> <code>float</code> <p>Probability of replacing a masked token with a random token.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>@dataclass(frozen=True)\nclass BertMaskConfig:\n    \"\"\"Configuration for masking tokens in a BERT-style model.\n\n    Attributes:\n        mask_prob: Probability of masking a token.\n        mask_token_prob: Probability of replacing a masked token with the mask token.\n        random_token_prob: Probability of replacing a masked token with a random token.\n    \"\"\"\n\n    tokenizer: Tokenizer\n    random_tokens: range\n    mask_prob: float = 0.15\n    mask_token_prob: float = 0.8\n    random_token_prob: float = 0.1\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Check that the sum of `mask_token_prob` and `random_token_prob` is less than or equal to 1.0.\n\n        Raises:\n            ValueError: If the sum of `mask_token_prob` and `random_token_prob` is greater than 1.0.\n        \"\"\"\n        if self.random_token_prob + self.mask_token_prob &gt; 1.0:\n            raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.BertMaskConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check that the sum of <code>mask_token_prob</code> and <code>random_token_prob</code> is less than or equal to 1.0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sum of <code>mask_token_prob</code> and <code>random_token_prob</code> is greater than 1.0.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Check that the sum of `mask_token_prob` and `random_token_prob` is less than or equal to 1.0.\n\n    Raises:\n        ValueError: If the sum of `mask_token_prob` and `random_token_prob` is greater than 1.0.\n    \"\"\"\n    if self.random_token_prob + self.mask_token_prob &gt; 1.0:\n        raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.add_cls_and_eos_tokens","title":"<code>add_cls_and_eos_tokens(sequence, labels, loss_mask, cls_token=None, eos_token=None)</code>","text":"<p>Prepends the CLS token and appends the EOS token to the masked sequence, updating the loss mask and labels.</p> <p>These labels should never be masked, so this is done after the masking step.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Tensor</code> <p>The input (likely masked) sequence.</p> required <code>labels</code> <code>Tensor</code> <p>The true values of the input sequence at the mask positions.</p> required <code>loss_mask</code> <code>Tensor</code> <p>A boolean tensor indicating which tokens should be included in the loss.</p> required <code>cls_token</code> <code>int | None</code> <p>The token to use for the CLS token. If None, no CLS token is added.</p> <code>None</code> <code>eos_token</code> <code>int | None</code> <p>The token to use for the EOS token. If None, no EOS token is added.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>The same input tensors with the CLS and EOS tokens added, and the labels and loss_mask updated accordingly.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def add_cls_and_eos_tokens(\n    sequence: torch.Tensor,\n    labels: torch.Tensor,\n    loss_mask: torch.Tensor,\n    cls_token: int | None = None,\n    eos_token: int | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Prepends the CLS token and appends the EOS token to the masked sequence, updating the loss mask and labels.\n\n    These labels should never be masked, so this is done after the masking step.\n\n    Args:\n        sequence: The input (likely masked) sequence.\n        labels: The true values of the input sequence at the mask positions.\n        loss_mask: A boolean tensor indicating which tokens should be included in the loss.\n        cls_token: The token to use for the CLS token. If None, no CLS token is added.\n        eos_token: The token to use for the EOS token. If None, no EOS token is added.\n\n    Returns:\n        The same input tensors with the CLS and EOS tokens added, and the labels and loss_mask updated accordingly.\n    \"\"\"\n    # Prepend the CLS token and append the EOS token, and update the loss mask and labels accordingly.\n    sequence = torch.cat(\n        [\n            torch.tensor([cls_token], dtype=sequence.dtype)\n            if cls_token is not None\n            else torch.tensor([], dtype=sequence.dtype),\n            sequence,\n            torch.tensor([eos_token], dtype=sequence.dtype)\n            if eos_token is not None\n            else torch.tensor([], dtype=sequence.dtype),\n        ]\n    )\n\n    labels = torch.cat(\n        [\n            torch.tensor([-1], dtype=labels.dtype) if cls_token is not None else torch.tensor([], dtype=labels.dtype),\n            labels,\n            torch.tensor([-1], dtype=labels.dtype) if eos_token is not None else torch.tensor([], dtype=labels.dtype),\n        ]\n    )\n\n    loss_mask = torch.cat(\n        [\n            torch.tensor([False]) if cls_token is not None else torch.tensor([], dtype=loss_mask.dtype),\n            loss_mask,\n            torch.tensor([False]) if eos_token is not None else torch.tensor([], dtype=loss_mask.dtype),\n        ]\n    )\n\n    return sequence, labels, loss_mask\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.apply_bert_pretraining_mask","title":"<code>apply_bert_pretraining_mask(tokenized_sequence, random_seed, mask_config)</code>","text":"<p>Applies the pretraining mask to a tokenized sequence.</p> <p>Parameters:</p> Name Type Description Default <code>tokenized_sequence</code> <code>Tensor</code> <p>Tokenized protein sequence.</p> required <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>mask_config</code> <code>BertMaskConfig</code> <p>Configuration for masking tokens in a BERT-style model.</p> required <p>Returns:</p> Name Type Description <code>masked_sequence</code> <code>Tensor</code> <p>The tokenized sequence with some tokens masked.</p> <code>labels</code> <code>Tensor</code> <p>A tensor the same shape as <code>masked_sequence</code> containing labels for the masked tokens, with -1 for non-masked tokens.</p> <code>loss_mask</code> <code>Tensor</code> <p>A boolean tensor the same shape as <code>masked_sequence</code>, where 'True' indicates which tokens should be included in the loss.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def apply_bert_pretraining_mask(\n    tokenized_sequence: torch.Tensor, random_seed: int, mask_config: BertMaskConfig\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Applies the pretraining mask to a tokenized sequence.\n\n    Args:\n        tokenized_sequence: Tokenized protein sequence.\n        random_seed: Random seed for reproducibility.\n        mask_config: Configuration for masking tokens in a BERT-style model.\n\n    Returns:\n        masked_sequence:\n            The tokenized sequence with some tokens masked.\n        labels:\n            A tensor the same shape as `masked_sequence` containing labels for the masked tokens, with -1 for non-masked\n            tokens.\n        loss_mask:\n            A boolean tensor the same shape as `masked_sequence`, where 'True' indicates which tokens should be included\n            in the loss.\n    \"\"\"\n    if mask_config.tokenizer.mask_token_id is None:\n        raise ValueError(\"Tokenizer must have a mask token.\")\n\n    if mask_config.random_token_prob + mask_config.mask_token_prob &gt; 1.0:\n        raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n\n    # Set the seed so that __getitem__(idx) is always deterministic.\n    # This is required by Megatron-LM's parallel strategies.\n    generator = torch.Generator().manual_seed(random_seed)\n\n    mask_stop_1 = mask_config.mask_prob * mask_config.mask_token_prob\n    mask_stop_2 = mask_config.mask_prob * (mask_config.mask_token_prob + mask_config.random_token_prob)\n\n    random_draws = torch.rand(tokenized_sequence.shape, generator=generator)  # Random draws for each token in [0, 1).\n\n    # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n    # (identity). We don't want to mask special tokens.\n    loss_mask = ~torch.isin(tokenized_sequence, torch.tensor(mask_config.tokenizer.all_special_ids))\n    loss_mask &amp;= random_draws &lt; mask_config.mask_prob\n\n    # The first `mask_token_prob` fraction of the `mask_prob` tokens are replaced with the mask token.\n    mask_token_mask = (random_draws &lt; mask_stop_1) &amp; loss_mask\n\n    # The next `random_token_prob` fraction of the `mask_prob` tokens are replaced with a random token.\n    random_token_mask = ((random_draws &gt;= mask_stop_1) &amp; (random_draws &lt; mask_stop_2)) &amp; loss_mask\n\n    # The remaining tokens are implicitly left as-is, representing an identity mask.\n\n    # Mask the tokens.\n    masked_sequence = tokenized_sequence.clone()\n    masked_sequence[mask_token_mask] = mask_config.tokenizer.mask_token_id\n    num_random_tokens: int = random_token_mask.sum().item()  # type: ignore[assignment]\n    masked_sequence[random_token_mask] = torch.randint(\n        low=mask_config.random_tokens.start,\n        high=mask_config.random_tokens.stop,\n        size=(num_random_tokens,),\n        dtype=masked_sequence.dtype,\n        generator=generator,\n    )\n\n    # Create the labels for the masked tokens.\n    labels = tokenized_sequence.clone()\n    labels[~loss_mask] = -100  # Ignore loss for non-masked tokens.\n\n    return masked_sequence, labels, loss_mask\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/types/","title":"Types","text":""},{"location":"API_reference/bionemo/llm/data/types/#bionemo.llm.data.types.BertSample","title":"<code>BertSample</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The type expected by NeMo/Megatron for a single dataset item.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Tensor</code> <p>The tokenized, masked input text.</p> <code>types</code> <code>Tensor</code> <p>The token type ids, if applicable.</p> <code>attention_mask</code> <code>Tensor</code> <p>A mask over all valid tokens, excluding padding.</p> <code>labels</code> <code>Tensor</code> <p>The true values of the masked tokens at each position covered by loss_mask.</p> <code>loss_mask</code> <code>Tensor</code> <p>The mask over the text indicating which tokens are masked and should be predicted.</p> <code>is_random</code> <code>Tensor</code> <p>??</p> Source code in <code>bionemo/llm/data/types.py</code> <pre><code>class BertSample(TypedDict):\n    \"\"\"The type expected by NeMo/Megatron for a single dataset item.\n\n    Attributes:\n        text: The tokenized, masked input text.\n        types: The token type ids, if applicable.\n        attention_mask: A mask over all valid tokens, excluding padding.\n        labels: The true values of the masked tokens at each position covered by loss_mask.\n        loss_mask: The mask over the text indicating which tokens are masked and should be predicted.\n        is_random: ??\n    \"\"\"\n\n    text: Tensor\n    types: Tensor\n    attention_mask: Tensor\n    labels: Tensor\n    loss_mask: Tensor\n    is_random: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/types/#bionemo.llm.data.types.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Required attributes for a tokenizers provided to apply_bert_pretraining_mask.</p> Source code in <code>bionemo/llm/data/types.py</code> <pre><code>class Tokenizer(Protocol):\n    \"\"\"Required attributes for a tokenizers provided to apply_bert_pretraining_mask.\"\"\"\n\n    @property\n    def mask_token_id(self) -&gt; int | None:  # noqa: D102\n        ...\n\n    @property\n    def all_special_ids(self) -&gt; list[int]:  # noqa: D102\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/","title":"Config","text":""},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto","title":"<code>IOMixinProto</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol for the get/set hparam functions of the IOMixin class from NeMo.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>class IOMixinProto(Protocol):\n    \"\"\"A Protocol for the get/set hparam functions of the IOMixin class from NeMo.\"\"\"\n\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Set the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n        ...\n\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Get the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto.get_hparam","title":"<code>get_hparam(attribute)</code>","text":"<p>Get the value of an attribute in the config attached to the class by the IOMixin.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Get the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>","text":"<p>Set the value of an attribute in the config attached to the class by the IOMixin.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Set the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoModelConfig","title":"<code>MegatronBioNeMoModelConfig</code>","text":"<p>               Bases: <code>BionemoModelConfig[MegatronModelType]</code>, <code>TransformerConfig</code>, <code>WillHaveGetSetHparam</code></p> <p>A ModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>class MegatronBioNeMoModelConfig(BionemoModelConfig[MegatronModelType], TransformerConfig, iom.WillHaveGetSetHparam):\n    \"\"\"A ModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.\"\"\"\n\n    model_cls: Type[MegatronModelType]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig","title":"<code>MegatronBioNeMoTrainableModelConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MegatronBioNeMoModelConfig[MegatronModelType]</code>, <code>BionemoTrainableModelConfig[MegatronModelType, MegatronLossType]</code>, <code>Generic[MegatronModelType, MegatronLossType]</code></p> <p>A TrainableModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>@dataclass\nclass MegatronBioNeMoTrainableModelConfig(\n    MegatronBioNeMoModelConfig[MegatronModelType],\n    BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n    Generic[MegatronModelType, MegatronLossType],\n):\n    \"\"\"A TrainableModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.\"\"\"\n\n    initial_ckpt_path: str | None = None\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n    override_parent_fields: List[str] = field(default_factory=lambda: _OVERRIDE_BIONEMO_CONFIG_DEFAULTS)\n\n    def load_settings_from_checkpoint(self, initial_ckpt_path: str) -&gt; None:\n        \"\"\"Load settings into self from the checkpoint saved in self.\n\n        Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper\n        parameters in this config, as well as the associated attributes in self in case they were modified post-init.\n\n        Args:\n            initial_ckpt_path: The path to the checkpoint to load, note that everything is loaded from this checkpoint\n                other than the settings in self.override_parent_fields.\n\n        Returns:\n            None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into\n                a checkpoint are updated.\n        \"\"\"\n        logger.warning(f\"Loading {self.initial_ckpt_path}\")\n        # 1. get the config from the trainer io context by querying the `model.config` subpath of the trainer.\n        initial_config: MegatronBioNeMoTrainableModelConfig = io.load_context(\n            path=Path(initial_ckpt_path) / \"context\", subpath=\"model.config\"\n        )  # type: ignore\n        initial_fields = {f.name for f in fields(initial_config)}\n        my_fields = [f.name for f in fields(self)]\n        skip_fields = set(self.override_parent_fields)\n        override_fields = [f for f in my_fields if f in initial_fields and f not in skip_fields]\n        override_mutate_possibly_extra_mutated_fiddle(self, initial_config, override_fields)\n\n    def update_model_from_checkpoint(self, model: MegatronModelType, initial_ckpt_path: str) -&gt; None:\n        \"\"\"Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.\n\n        Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in\n            self.initial_ckpt_skip_keys_with_these_prefixes.\n\n        Args:\n            model: The Megatron model to update.\n            initial_ckpt_path: The path to the megatron checkpoint to load.\n\n        Returns:\n            None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring\n                any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.\n        \"\"\"\n        load_weights_sharded_inplace_nemo2_to_mcore(\n            model=model,  # type: ignore\n            distributed_checkpoint_dir=initial_ckpt_path,\n            skip_keys_with_these_prefixes=set(self.initial_ckpt_skip_keys_with_these_prefixes),\n        )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig.load_settings_from_checkpoint","title":"<code>load_settings_from_checkpoint(initial_ckpt_path)</code>","text":"<p>Load settings into self from the checkpoint saved in self.</p> <p>Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper parameters in this config, as well as the associated attributes in self in case they were modified post-init.</p> <p>Parameters:</p> Name Type Description Default <code>initial_ckpt_path</code> <code>str</code> <p>The path to the checkpoint to load, note that everything is loaded from this checkpoint other than the settings in self.override_parent_fields.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into a checkpoint are updated.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def load_settings_from_checkpoint(self, initial_ckpt_path: str) -&gt; None:\n    \"\"\"Load settings into self from the checkpoint saved in self.\n\n    Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper\n    parameters in this config, as well as the associated attributes in self in case they were modified post-init.\n\n    Args:\n        initial_ckpt_path: The path to the checkpoint to load, note that everything is loaded from this checkpoint\n            other than the settings in self.override_parent_fields.\n\n    Returns:\n        None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into\n            a checkpoint are updated.\n    \"\"\"\n    logger.warning(f\"Loading {self.initial_ckpt_path}\")\n    # 1. get the config from the trainer io context by querying the `model.config` subpath of the trainer.\n    initial_config: MegatronBioNeMoTrainableModelConfig = io.load_context(\n        path=Path(initial_ckpt_path) / \"context\", subpath=\"model.config\"\n    )  # type: ignore\n    initial_fields = {f.name for f in fields(initial_config)}\n    my_fields = [f.name for f in fields(self)]\n    skip_fields = set(self.override_parent_fields)\n    override_fields = [f for f in my_fields if f in initial_fields and f not in skip_fields]\n    override_mutate_possibly_extra_mutated_fiddle(self, initial_config, override_fields)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig.update_model_from_checkpoint","title":"<code>update_model_from_checkpoint(model, initial_ckpt_path)</code>","text":"<p>Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.</p> <p>Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in     self.initial_ckpt_skip_keys_with_these_prefixes.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MegatronModelType</code> <p>The Megatron model to update.</p> required <code>initial_ckpt_path</code> <code>str</code> <p>The path to the megatron checkpoint to load.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def update_model_from_checkpoint(self, model: MegatronModelType, initial_ckpt_path: str) -&gt; None:\n    \"\"\"Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.\n\n    Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in\n        self.initial_ckpt_skip_keys_with_these_prefixes.\n\n    Args:\n        model: The Megatron model to update.\n        initial_ckpt_path: The path to the megatron checkpoint to load.\n\n    Returns:\n        None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring\n            any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.\n    \"\"\"\n    load_weights_sharded_inplace_nemo2_to_mcore(\n        model=model,  # type: ignore\n        distributed_checkpoint_dir=initial_ckpt_path,\n        skip_keys_with_these_prefixes=set(self.initial_ckpt_skip_keys_with_these_prefixes),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.TorchmetricsConfig","title":"<code>TorchmetricsConfig</code>  <code>dataclass</code>","text":"<p>TorchmetricsConfig to instantiate torchmetrics.Metric class.</p> <p>Fiddle requires all objects in config serializable and torchmetric.Metric is not. Its instantiation must be deferred into BionemoLightningModule.init. Only support torchmetrics currently, e.g. users can provide 'text.Perplexity' to 'class_path' to use 'torchmetrics.text.Perplexity'.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>@dataclass\nclass TorchmetricsConfig:\n    \"\"\"TorchmetricsConfig to instantiate torchmetrics.Metric class.\n\n    Fiddle requires all objects in config serializable and torchmetric.Metric is not. Its instantiation must be deferred into BionemoLightningModule.__init__.\n    Only support torchmetrics currently, e.g. users can provide 'text.Perplexity' to 'class_path' to use 'torchmetrics.text.Perplexity'.\n    \"\"\"\n\n    class_path: str\n    task: Literal[\"lm\", \"classification\", \"regression\"]\n    metric_name: str\n    kwargs: Optional[dict[str, Any]] = None\n\n    def __post_init__(self):\n        \"\"\"__post_init__ in dataclass.\"\"\"\n        self.kwargs = {} if self.kwargs is None else self.kwargs\n\n    def get_instance(self) -&gt; torchmetrics.Metric:\n        \"\"\"Dynamically imports and instantiates the metric class.\"\"\"\n        if \".\" in self.class_path:\n            module_path, class_name = self.class_path.rsplit(\".\", 1)\n            module = importlib.import_module(f\"torchmetrics.{module_path}\")\n        else:\n            class_name = self.class_path\n            module = importlib.import_module(\"torchmetrics\")\n\n        cls_ = getattr(module, class_name)\n        return cls_(**self.kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.TorchmetricsConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>post_init in dataclass.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"__post_init__ in dataclass.\"\"\"\n    self.kwargs = {} if self.kwargs is None else self.kwargs\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.TorchmetricsConfig.get_instance","title":"<code>get_instance()</code>","text":"<p>Dynamically imports and instantiates the metric class.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def get_instance(self) -&gt; torchmetrics.Metric:\n    \"\"\"Dynamically imports and instantiates the metric class.\"\"\"\n    if \".\" in self.class_path:\n        module_path, class_name = self.class_path.rsplit(\".\", 1)\n        module = importlib.import_module(f\"torchmetrics.{module_path}\")\n    else:\n        class_name = self.class_path\n        module = importlib.import_module(\"torchmetrics\")\n\n    cls_ = getattr(module, class_name)\n    return cls_(**self.kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.override_mutate_possibly_extra_mutated_fiddle","title":"<code>override_mutate_possibly_extra_mutated_fiddle(target_cfg, source_cfg, maybe_mutated_elements_to_clone)</code>","text":"<p>Override the values of the target config with the values of the source config for the given elements.</p> <p>This will modify the tracked init hyper-parameter values, as well as modifying the associated attributes in     self incase they were modified later by post_init code.</p> <p>Parameters:</p> Name Type Description Default <code>target_cfg</code> <code>IOMixinProto</code> <p>The config to update.</p> required <code>source_cfg</code> <code>IOMixinProto</code> <p>The config to copy values from.</p> required <code>maybe_mutated_elements_to_clone</code> <code>List[str]</code> <p>The list of elements to copy from the source config to the target config.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the target config is updated in place.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def override_mutate_possibly_extra_mutated_fiddle(\n    target_cfg: IOMixinProto, source_cfg: IOMixinProto, maybe_mutated_elements_to_clone: List[str]\n) -&gt; None:\n    \"\"\"Override the values of the target config with the values of the source config for the given elements.\n\n    This will modify the tracked init hyper-parameter values, as well as modifying the associated attributes in\n        self incase they were modified later by post_init code.\n\n    Args:\n        target_cfg: The config to update.\n        source_cfg: The config to copy values from.\n        maybe_mutated_elements_to_clone: The list of elements to copy from the source config to the target config.\n\n    Returns:\n        None, the target config is updated in place.\n    \"\"\"\n    for f in maybe_mutated_elements_to_clone:\n        # 1. Update the tracked config values. Note that the associated attribute in self may have been modified\n        #  post-init, so we don't want to change the value in self here. We do that separately next.\n        target_cfg.set_hparam(f, source_cfg.get_hparam(f), also_change_value=False)\n        # 2. Update the lazily untracked values (if the same variable name is used post-init)\n        setattr(target_cfg, f, getattr(source_cfg, f))\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/","title":"Layers","text":""},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.ESM2QueryScaling","title":"<code>ESM2QueryScaling</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>class ESM2QueryScaling(torch.nn.Module):  # noqa: D101\n    def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n        \"\"\"A custom layer that scales quary values.\n\n        This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2\n        which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()\n\n        Args:\n            config (TransformerConfig): The megatron config. This is used for computing projection_size\n        \"\"\"\n        super().__init__()\n        projection_size = config.kv_channels * config.num_attention_heads\n        self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)\n        self.sqrt_val = math.sqrt(self.hidden_size_per_attention_head)\n\n    @torch.compile\n    def forward(self, query, *args, **kwargs):  # noqa: D102\n        return query / self.sqrt_val\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.ESM2QueryScaling.__init__","title":"<code>__init__(config, *args, **kwargs)</code>","text":"<p>A custom layer that scales quary values.</p> <p>This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2 which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>The megatron config. This is used for computing projection_size</p> required Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n    \"\"\"A custom layer that scales quary values.\n\n    This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2\n    which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()\n\n    Args:\n        config (TransformerConfig): The megatron config. This is used for computing projection_size\n    \"\"\"\n    super().__init__()\n    projection_size = config.kv_channels * config.num_attention_heads\n    self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)\n    self.sqrt_val = math.sqrt(self.hidden_size_per_attention_head)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.TELayerNorm","title":"<code>TELayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code></p> Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>class TELayerNorm(te.pytorch.LayerNorm):  # noqa: D101\n    def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n        \"\"\"A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.\n            This allows this method to be used in a megatron layerspec.\n\n        Args:\n            config (TransformerConfig): The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma.\n                The rest of the config is not used.\n        \"\"\"  # noqa: D205\n        # Eps tends to get passed through properly, as does hidden_size, but not other params from the config.\n        super().__init__(\n            *args,\n            zero_centered_gamma=config.layernorm_zero_centered_gamma,\n            sequence_parallel=config.sequence_parallel,\n            **kwargs,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.TELayerNorm.__init__","title":"<code>__init__(config, *args, **kwargs)</code>","text":"<p>A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.     This allows this method to be used in a megatron layerspec.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma. The rest of the config is not used.</p> required Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n    \"\"\"A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.\n        This allows this method to be used in a megatron layerspec.\n\n    Args:\n        config (TransformerConfig): The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma.\n            The rest of the config is not used.\n    \"\"\"  # noqa: D205\n    # Eps tends to get passed through properly, as does hidden_size, but not other params from the config.\n    super().__init__(\n        *args,\n        zero_centered_gamma=config.layernorm_zero_centered_gamma,\n        sequence_parallel=config.sequence_parallel,\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/","title":"Loss","text":""},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction","title":"<code>BERTMLMLossWithReduction</code>","text":"<p>               Bases: <code>_Nemo2CompatibleLossReduceMixin</code>, <code>MegatronLossReduction</code></p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class BERTMLMLossWithReduction(_Nemo2CompatibleLossReduceMixin, MegatronLossReduction):  # noqa: D101\n    def __init__(\n        self,\n        validation_step: bool = False,\n        val_drop_last: bool = True,\n    ) -&gt; None:\n        \"\"\"Initializes the Model class.\n\n        Args:\n            validation_step (bool, optional): Whether this object is being applied to the validation step. Defaults to False.\n            val_drop_last (bool, optional): Whether the last batch is configured to be dropped during validation. Defaults to True.\n        \"\"\"\n        # TODO(@jomitchell): Track down how we handle test. This is a common pattern in NeMo2, but these parameters seem likely\n        #  to change in the future.\n        super().__init__()\n        self.validation_step = validation_step\n        self.val_drop_last = val_drop_last\n\n    def forward(\n        self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n        \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently. In the future this will be extended\n            to handle other loss types like sequence loss if it is present in the forward_out and batch.\n\n        Args:\n            batch (Dict[str, Tensor]): The batch of data. Each tensor should be of shape [batch_size, *, *],\n                and match the corresponding dimension for that particular key in the batch output.\n                For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n            forward_out (Dict[str, Tensor]): The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n        Taken from:\n        https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976 .\n        \"\"\"  # noqa: D205\n        if \"labels\" not in batch:\n            raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n        # NOTE: token_logits is [sequence, batch] but labels and other fiels, including the loss are [batch, sequence]\n        unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])  # [b s]\n\n        # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n        # compute loss\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            # reduce the loss across the micro batch per valid token\n            loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n        else:\n            # reduce the loss across the micro batch per valid token.\n            # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n            #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n            #  other necessary keys to the batch. Thanks!\n            loss_for_microbatch = masked_token_loss_context_parallel(\n                unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n            )\n\n        # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n        #  reducing the loss across the data parallel group.\n        if self.validation_step and not self.val_drop_last:\n            num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n            if loss_for_microbatch.isnan():\n                # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n                #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n                #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n                if batch[\"loss_mask\"].count_nonzero() != 0:\n                    raise ValueError(\"Got NaN loss with non-empty input\")\n                loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n            else:\n                loss_sum_for_microbatch = (\n                    num_valid_tokens_in_microbatch * loss_for_microbatch\n                )  # sum over all valid tokens\n\n            # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n            loss_sum_and_microbatch_size_all_gpu = torch.cat(\n                [\n                    loss_sum_for_microbatch.clone().detach().view(1),\n                    Tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n                ]\n            )\n            torch.distributed.all_reduce(\n                loss_sum_and_microbatch_size_all_gpu,\n                group=parallel_state.get_data_parallel_group(),\n                op=torch.distributed.ReduceOp.SUM,\n            )\n            return loss_for_microbatch * cp_size, {\n                \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n            }\n\n        # average the losses across the data parallel group, but also return the unreduced loss\n        reduced_loss = average_losses_across_data_parallel_group([loss_for_microbatch])\n        return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction.__init__","title":"<code>__init__(validation_step=False, val_drop_last=True)</code>","text":"<p>Initializes the Model class.</p> <p>Parameters:</p> Name Type Description Default <code>validation_step</code> <code>bool</code> <p>Whether this object is being applied to the validation step. Defaults to False.</p> <code>False</code> <code>val_drop_last</code> <code>bool</code> <p>Whether the last batch is configured to be dropped during validation. Defaults to True.</p> <code>True</code> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def __init__(\n    self,\n    validation_step: bool = False,\n    val_drop_last: bool = True,\n) -&gt; None:\n    \"\"\"Initializes the Model class.\n\n    Args:\n        validation_step (bool, optional): Whether this object is being applied to the validation step. Defaults to False.\n        val_drop_last (bool, optional): Whether the last batch is configured to be dropped during validation. Defaults to True.\n    \"\"\"\n    # TODO(@jomitchell): Track down how we handle test. This is a common pattern in NeMo2, but these parameters seem likely\n    #  to change in the future.\n    super().__init__()\n    self.validation_step = validation_step\n    self.val_drop_last = val_drop_last\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Computes loss of <code>labels</code> in the batch vs <code>token_logits</code> in the forward output currently. In the future this will be extended     to handle other loss types like sequence loss if it is present in the forward_out and batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>The batch of data. Each tensor should be of shape [batch_size, , ], and match the corresponding dimension for that particular key in the batch output. For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>The forward output from the model. Each tensor should be of shape [batch_size, , ]</p> required <p>Taken from: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976 .</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def forward(\n    self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n    \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently. In the future this will be extended\n        to handle other loss types like sequence loss if it is present in the forward_out and batch.\n\n    Args:\n        batch (Dict[str, Tensor]): The batch of data. Each tensor should be of shape [batch_size, *, *],\n            and match the corresponding dimension for that particular key in the batch output.\n            For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n        forward_out (Dict[str, Tensor]): The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n    Taken from:\n    https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976 .\n    \"\"\"  # noqa: D205\n    if \"labels\" not in batch:\n        raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n    # NOTE: token_logits is [sequence, batch] but labels and other fiels, including the loss are [batch, sequence]\n    unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])  # [b s]\n\n    # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n    # compute loss\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        # reduce the loss across the micro batch per valid token\n        loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n    else:\n        # reduce the loss across the micro batch per valid token.\n        # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n        #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n        #  other necessary keys to the batch. Thanks!\n        loss_for_microbatch = masked_token_loss_context_parallel(\n            unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n        )\n\n    # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n    #  reducing the loss across the data parallel group.\n    if self.validation_step and not self.val_drop_last:\n        num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n        if loss_for_microbatch.isnan():\n            # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n            #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n            #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n            if batch[\"loss_mask\"].count_nonzero() != 0:\n                raise ValueError(\"Got NaN loss with non-empty input\")\n            loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n        else:\n            loss_sum_for_microbatch = (\n                num_valid_tokens_in_microbatch * loss_for_microbatch\n            )  # sum over all valid tokens\n\n        # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n        loss_sum_and_microbatch_size_all_gpu = torch.cat(\n            [\n                loss_sum_for_microbatch.clone().detach().view(1),\n                Tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n            ]\n        )\n        torch.distributed.all_reduce(\n            loss_sum_and_microbatch_size_all_gpu,\n            group=parallel_state.get_data_parallel_group(),\n            op=torch.distributed.ReduceOp.SUM,\n        )\n        return loss_for_microbatch * cp_size, {\n            \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n        }\n\n    # average the losses across the data parallel group, but also return the unreduced loss\n    reduced_loss = average_losses_across_data_parallel_group([loss_for_microbatch])\n    return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.DataParallelGroupLossAndIO","title":"<code>DataParallelGroupLossAndIO</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Average losses across the data parallel group + the original batch and inference output.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class DataParallelGroupLossAndIO(TypedDict):\n    \"\"\"Average losses across the data parallel group + the original batch and inference output.\"\"\"\n\n    avg: Tensor\n    batch: dict[str, Tensor]\n    forward_out: dict[str, Tensor]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.PerTokenLossDict","title":"<code>PerTokenLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tensor dictionary for loss.</p> <p>This is the return type for a loss that is computed per token in the batch, supporting microbatches of varying sizes.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class PerTokenLossDict(TypedDict):\n    \"\"\"Tensor dictionary for loss.\n\n    This is the return type for a loss that is computed per token in the batch, supporting microbatches of varying sizes.\n    \"\"\"\n\n    loss_sum_and_microbatch_size: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.SameSizeLossDict","title":"<code>SameSizeLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tensor dictionary for loss.</p> <p>This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class SameSizeLossDict(TypedDict):\n    \"\"\"Tensor dictionary for loss.\n\n    This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.\n    \"\"\"\n\n    avg: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss._Nemo2CompatibleLossReduceMixin","title":"<code>_Nemo2CompatibleLossReduceMixin</code>","text":"<p>This is a mixin class that provides a general purpose reduce function that is compatible with NeMo2.0 and Megatron-LM. Mix this into your loss class to satisfy the abstract <code>reduce</code> method, unless you need more customization. Before you import this to another file, please refactor to remove the private <code>_</code> prefix. For now we assume that this is local to this file and not something a user would want to import elsewhere. If you do need it, then this assumption was incorrect so please refactor accordingly.</p> <p>Since this overrides an abstract parent class, this needs to be put first in the inheritance list to ensure that the correct method is called.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class _Nemo2CompatibleLossReduceMixin:\n    \"\"\"This is a mixin class that provides a general purpose reduce function that is compatible with NeMo2.0 and Megatron-LM.\n    Mix this into your loss class to satisfy the abstract `reduce` method, unless you need more\n    customization. Before you import this to another file, please refactor to remove the private `_` prefix.\n    For now we assume that this is local to this file and not something a user would want to import elsewhere.\n    If you do need it, then this assumption was incorrect so please refactor accordingly.\n\n    Since this overrides an abstract parent class, this needs to be put first in the inheritance list to ensure that the correct method is called.\n    \"\"\"  # noqa: D205\n\n    def old_reduce(self, losses_reduced_per_micro_batch: List[PerTokenLossDict | SameSizeLossDict]) -&gt; Tensor:\n        if losses_reduced_per_micro_batch:\n            if \"avg\" in losses_reduced_per_micro_batch[0]:\n                loss_tensors_list: list[Tensor] = [\n                    loss_reduced[\"avg\"] for loss_reduced in losses_reduced_per_micro_batch\n                ]\n                loss_tensor = torch.concat(loss_tensors_list)\n\n                return loss_tensor.mean()\n\n            loss_sum_tensors_list: List[Tensor] = [\n                loss_sum[\"loss_sum_and_microbatch_size\"]\n                for loss_sum in losses_reduced_per_micro_batch\n                if loss_sum[\"loss_sum_and_microbatch_size\"][1] &gt; 0\n            ]\n            dummy_tensor = Tensor([0.0, 0.0]).cuda()\n            loss_sum = (\n                torch.vstack(loss_sum_tensors_list).sum(dim=0) if len(loss_sum_tensors_list) &gt; 0 else dummy_tensor\n            )\n            return loss_sum\n\n        # If losses_reduced_per_micro_batch is empty, return a dummy tensor.\n        dummy_tensor = Tensor(0.0).cuda()\n        return dummy_tensor\n\n    # NOTE: this method reduces across microbatches and cross-device reduction is handled in forward method\n    def reduce(self, losses_reduced_per_micro_batch: List[PerTokenLossDict | SameSizeLossDict]) -&gt; Tensor:\n        # NOTE(SKH) This requires two passes over the data instead of one in the `loss_sum_and_microbatch_size` case.\n\n        # Expect two elements: losses, num_tokens. We only care about the num_tokens index.\n        NUM_TOKENS_IDX = 1\n\n        if not losses_reduced_per_micro_batch:  # model returns zero by default in NeMo2.0\n            dummy_tensor = Tensor(0.0).cuda()\n            return dummy_tensor\n\n        # do the gather\n        keys = list(losses_reduced_per_micro_batch[0].keys())\n        assert (\n            sum((\"avg\" in keys, \"loss_sum_and_microbatch_size\" in keys)) == 1\n        ), \"Expected only either 'avg' or 'loss_sum_and_microbatch_size' in keys but got both\"\n        key: Literal[\"avg\", \"loss_sum_and_microbatch_size\"] = (\n            \"avg\" if \"avg\" in keys else \"loss_sum_and_microbatch_size\"\n        )\n\n        loss_tensors_list: list[Tensor] = [loss_reduced[key] for loss_reduced in losses_reduced_per_micro_batch]\n        # switch on the keys and allow other keys to pass through\n        if key == \"avg\":\n            return torch.concat(loss_tensors_list).mean()\n        elif key == \"loss_sum_and_microbatch_size\":\n            loss_sum_tensors_list = [\n                loss_sum for loss_sum in losses_reduced_per_micro_batch if loss_tensors_list[NUM_TOKENS_IDX] &gt; 0\n            ]\n            if len(loss_sum_tensors_list) == 0:\n                # If we get no result, return zero.\n                dummy_tensor = Tensor([0.0, 0.0]).cuda()\n                return dummy_tensor\n            else:\n                # otherwise do a sum reduction.\n                loss_sum = torch.vstack(loss_sum_tensors_list).sum(dim=0)\n                return loss_sum\n        else:\n            raise ValueError(f\"Unexpected: key must either be 'avg' or 'loss_sum_and_microbatch_size', not {key=}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.unreduced_token_loss_fn","title":"<code>unreduced_token_loss_fn(logits, labels, cross_entropy_loss_fusion=False)</code>","text":"<p>Computes the unreduced token loss given the logits and labels without regard to the loss mask.</p> <p>WARNING: This function does not apply a loss mask. Also, it does inplace operation on the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted logits of shape [sequence_length, batch_size, num_classes].</p> required <code>labels</code> <code>Tensor</code> <p>The true labels of shape [batch_size, sequence_length].</p> required <code>cross_entropy_loss_fusion</code> <code>bool</code> <p>If True, use the fused kernel version of vocab parallel cross entropy. This should generally be preferred for speed as it packs more operations into a single kernel on the GPU. However some users have observed reduced training stability when using this method.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The unreduced token loss of shape [batch_size, sequence_length].</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def unreduced_token_loss_fn(logits: Tensor, labels: Tensor, cross_entropy_loss_fusion: bool = False) -&gt; Tensor:\n    \"\"\"Computes the unreduced token loss given the logits and labels without regard to the loss mask.\n\n    WARNING: This function does not apply a loss mask. Also, it does inplace operation on the inputs.\n\n    Args:\n        logits (Tensor): The predicted logits of shape [sequence_length, batch_size, num_classes].\n        labels (Tensor): The true labels of shape [batch_size, sequence_length].\n        cross_entropy_loss_fusion (bool): If True, use the fused kernel version of vocab parallel cross entropy. This\n            should generally be preferred for speed as it packs more operations into a single kernel on the GPU. However\n            some users have observed reduced training stability when using this method.\n\n    Returns:\n        Tensor: The unreduced token loss of shape [batch_size, sequence_length].\n    \"\"\"\n    labels = labels.transpose(0, 1).contiguous()  # [b, s] -&gt; [s, b]\n    if cross_entropy_loss_fusion:\n        loss = fused_vocab_parallel_cross_entropy(logits, labels)\n    else:\n        loss = tensor_parallel.vocab_parallel_cross_entropy(logits, labels)\n    # [s b] =&gt; [b, s]\n    loss = loss.transpose(0, 1).contiguous()\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/","title":"Lr scheduler","text":""},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.SchedulerOutput","title":"<code>SchedulerOutput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the scheduler method.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class SchedulerOutput(TypedDict):\n    \"\"\"Output of the scheduler method.\"\"\"\n\n    optimizer: MegatronOptimizerModule\n    lr_scheduler: dict\n    monitor: str\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold","title":"<code>WarmupAnnealDecayHold</code>","text":"<p>               Bases: <code>_LRScheduler</code></p> <p>Warmup Anneal Decay Hold learning rate scheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class WarmupAnnealDecayHold(_LRScheduler):\n    \"\"\"Warmup Anneal Decay Hold learning rate scheduler.\"\"\"\n\n    def __init__(\n        self,\n        optimizer: MegatronOptimizerModule,\n        *,\n        warmup_steps: Optional[int] = None,\n        max_steps: Optional[int] = None,\n        max_lr: Optional[float] = None,\n        min_lr: float = 4e-5,\n        anneal_percentage: float = 0.10,\n        last_epoch: int = -1,\n    ) -&gt; None:\n        \"\"\"Initializes the WarmupAnnealDecayHold learning rate scheduler.\n\n        Args:\n            optimizer: Optimizer to apply the learning rate scheduler.\n            warmup_steps (int): Number of steps for the linear warm-up.\n            max_steps (int): Total number of training steps.\n            max_lr (float): Peak learning rate to be achieved after warm-up.\n            min_lr (float): Minimum learning rate.\n            anneal_percentage (float): Percentage of the max_lr to hold after decay.\n            last_epoch (int): The index of the last epoch.\n        \"\"\"\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.anneal_percentage = anneal_percentage\n        self.last_epoch = last_epoch\n\n        for group in optimizer.param_groups:\n            group.setdefault(\"initial_lr\", max_lr)\n\n        super(WarmupAnnealDecayHold, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self) -&gt; List[float]:\n        \"\"\"Get the learning rate at the current step.\"\"\"\n        step_num = self.last_epoch\n        if step_num &lt; self.warmup_steps:\n            lr = self.min_lr + (self.max_lr - self.min_lr) * step_num / self.warmup_steps\n        else:\n            decay_steps = self.max_steps - self.warmup_steps\n            lr = self.max_lr * (1 - (step_num - self.warmup_steps) / decay_steps)\n            lr = max(lr, self.max_lr * self.anneal_percentage)\n\n        return [lr for _ in self.optimizer.param_groups]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold.__init__","title":"<code>__init__(optimizer, *, warmup_steps=None, max_steps=None, max_lr=None, min_lr=4e-05, anneal_percentage=0.1, last_epoch=-1)</code>","text":"<p>Initializes the WarmupAnnealDecayHold learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>MegatronOptimizerModule</code> <p>Optimizer to apply the learning rate scheduler.</p> required <code>warmup_steps</code> <code>int</code> <p>Number of steps for the linear warm-up.</p> <code>None</code> <code>max_steps</code> <code>int</code> <p>Total number of training steps.</p> <code>None</code> <code>max_lr</code> <code>float</code> <p>Peak learning rate to be achieved after warm-up.</p> <code>None</code> <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>4e-05</code> <code>anneal_percentage</code> <code>float</code> <p>Percentage of the max_lr to hold after decay.</p> <code>0.1</code> <code>last_epoch</code> <code>int</code> <p>The index of the last epoch.</p> <code>-1</code> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def __init__(\n    self,\n    optimizer: MegatronOptimizerModule,\n    *,\n    warmup_steps: Optional[int] = None,\n    max_steps: Optional[int] = None,\n    max_lr: Optional[float] = None,\n    min_lr: float = 4e-5,\n    anneal_percentage: float = 0.10,\n    last_epoch: int = -1,\n) -&gt; None:\n    \"\"\"Initializes the WarmupAnnealDecayHold learning rate scheduler.\n\n    Args:\n        optimizer: Optimizer to apply the learning rate scheduler.\n        warmup_steps (int): Number of steps for the linear warm-up.\n        max_steps (int): Total number of training steps.\n        max_lr (float): Peak learning rate to be achieved after warm-up.\n        min_lr (float): Minimum learning rate.\n        anneal_percentage (float): Percentage of the max_lr to hold after decay.\n        last_epoch (int): The index of the last epoch.\n    \"\"\"\n    self.warmup_steps = warmup_steps\n    self.max_steps = max_steps\n    self.max_lr = max_lr\n    self.min_lr = min_lr\n    self.anneal_percentage = anneal_percentage\n    self.last_epoch = last_epoch\n\n    for group in optimizer.param_groups:\n        group.setdefault(\"initial_lr\", max_lr)\n\n    super(WarmupAnnealDecayHold, self).__init__(optimizer, last_epoch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold.get_lr","title":"<code>get_lr()</code>","text":"<p>Get the learning rate at the current step.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def get_lr(self) -&gt; List[float]:\n    \"\"\"Get the learning rate at the current step.\"\"\"\n    step_num = self.last_epoch\n    if step_num &lt; self.warmup_steps:\n        lr = self.min_lr + (self.max_lr - self.min_lr) * step_num / self.warmup_steps\n    else:\n        decay_steps = self.max_steps - self.warmup_steps\n        lr = self.max_lr * (1 - (step_num - self.warmup_steps) / decay_steps)\n        lr = max(lr, self.max_lr * self.anneal_percentage)\n\n    return [lr for _ in self.optimizer.param_groups]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler","title":"<code>WarmupAnnealDecayHoldScheduler</code>","text":"<p>               Bases: <code>LRSchedulerModule</code></p> <p>Warmup Policy Learning Rate Scheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class WarmupAnnealDecayHoldScheduler(LRSchedulerModule):\n    \"\"\"Warmup Policy Learning Rate Scheduler.\"\"\"\n\n    def __init__(\n        self,\n        warmup_steps: int = 2000,\n        max_steps: int = 500_000,\n        max_lr: float = 4e-4,\n        min_lr: float = 4e-5,\n        anneal_percentage: float = 0.10,\n        interval: str = \"step\",\n        frequency: int = 1,\n        monitor: str = \"val_loss\",\n    ) -&gt; None:\n        \"\"\"Initializes the WarmupAnnealDecayHoldScheduler.\"\"\"\n        super().__init__()\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.anneal_percentage = anneal_percentage\n        self.interval = interval\n        self.frequency = frequency\n        self.monitor = monitor\n\n    def scheduler(self, model: MegatronBioBertModel, optimizer: MegatronOptimizerModule) -&gt; SchedulerOutput:\n        \"\"\"Returns the scheduler output.\"\"\"\n        lr_scheduler = WarmupAnnealDecayHold(\n            optimizer,\n            warmup_steps=self.warmup_steps,\n            max_steps=self.max_steps,\n            max_lr=self.max_lr,\n            min_lr=self.min_lr,\n            anneal_percentage=self.anneal_percentage,\n        )\n        return {\n            \"optimizer\": optimizer,\n            # REQUIRED: The scheduler instance\n            \"lr_scheduler\": {\n                \"scheduler\": lr_scheduler,\n                # `interval` is the unit of the scheduler's step size, could also be 'step'.\n                # 'epoch' updates the scheduler on epoch end whereas 'step'\n                # updates it after a optimizer update.\n                \"interval\": self.interval,\n                # How many epochs/steps should pass between calls to\n                # `scheduler.step()`. 1 corresponds to updating the learning\n                # rate after every epoch/step.\n                \"frequency\": self.frequency,\n            },\n            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n            \"monitor\": self.monitor,\n        }\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler.__init__","title":"<code>__init__(warmup_steps=2000, max_steps=500000, max_lr=0.0004, min_lr=4e-05, anneal_percentage=0.1, interval='step', frequency=1, monitor='val_loss')</code>","text":"<p>Initializes the WarmupAnnealDecayHoldScheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def __init__(\n    self,\n    warmup_steps: int = 2000,\n    max_steps: int = 500_000,\n    max_lr: float = 4e-4,\n    min_lr: float = 4e-5,\n    anneal_percentage: float = 0.10,\n    interval: str = \"step\",\n    frequency: int = 1,\n    monitor: str = \"val_loss\",\n) -&gt; None:\n    \"\"\"Initializes the WarmupAnnealDecayHoldScheduler.\"\"\"\n    super().__init__()\n    self.warmup_steps = warmup_steps\n    self.max_steps = max_steps\n    self.max_lr = max_lr\n    self.min_lr = min_lr\n    self.anneal_percentage = anneal_percentage\n    self.interval = interval\n    self.frequency = frequency\n    self.monitor = monitor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler.scheduler","title":"<code>scheduler(model, optimizer)</code>","text":"<p>Returns the scheduler output.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def scheduler(self, model: MegatronBioBertModel, optimizer: MegatronOptimizerModule) -&gt; SchedulerOutput:\n    \"\"\"Returns the scheduler output.\"\"\"\n    lr_scheduler = WarmupAnnealDecayHold(\n        optimizer,\n        warmup_steps=self.warmup_steps,\n        max_steps=self.max_steps,\n        max_lr=self.max_lr,\n        min_lr=self.min_lr,\n        anneal_percentage=self.anneal_percentage,\n    )\n    return {\n        \"optimizer\": optimizer,\n        # REQUIRED: The scheduler instance\n        \"lr_scheduler\": {\n            \"scheduler\": lr_scheduler,\n            # `interval` is the unit of the scheduler's step size, could also be 'step'.\n            # 'epoch' updates the scheduler on epoch end whereas 'step'\n            # updates it after a optimizer update.\n            \"interval\": self.interval,\n            # How many epochs/steps should pass between calls to\n            # `scheduler.step()`. 1 corresponds to updating the learning\n            # rate after every epoch/step.\n            \"frequency\": self.frequency,\n        },\n        # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n        \"monitor\": self.monitor,\n    }\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/","title":"Lightning","text":""},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertBatch","title":"<code>BertBatch</code>","text":"<p>               Bases: <code>BertBatchCore</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertBatch(BertBatchCore, total=False):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertBatchCore","title":"<code>BertBatchCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertBatchCore(TypedDict):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    text: Tensor\n    attention_mask: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertModel","title":"<code>BertModel</code>","text":"<p>               Bases: <code>Protocol[DataT]</code></p> <p>Interface for BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertModel(Protocol[DataT]):\n    \"\"\"Interface for BERT-like models.\"\"\"\n\n    def forward(\n        self, input_ids: Tensor, attention_mask: Tensor, packed_seq_params: Optional[PackedSeqParams] = None\n    ) -&gt; DataT:\n        \"\"\"Inference for BERT-like models.\n\n        Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input,\n        and the original sequence lengths if the sequences are packed into a dense batch.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertModel.forward","title":"<code>forward(input_ids, attention_mask, packed_seq_params=None)</code>","text":"<p>Inference for BERT-like models.</p> <p>Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input, and the original sequence lengths if the sequences are packed into a dense batch.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def forward(\n    self, input_ids: Tensor, attention_mask: Tensor, packed_seq_params: Optional[PackedSeqParams] = None\n) -&gt; DataT:\n    \"\"\"Inference for BERT-like models.\n\n    Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input,\n    and the original sequence lengths if the sequences are packed into a dense batch.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BioBertLightningModule","title":"<code>BioBertLightningModule</code>","text":"<p>               Bases: <code>BionemoLightningModule</code></p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BioBertLightningModule(BionemoLightningModule):\n    def __init__(\n        self,\n        *args,\n        data_step_function: DataStepFunction = biobert_data_step,\n        forward_step_function: ForwardStepFunction = bert_forward_step,\n        **kwargs,\n    ):\n        \"\"\"DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints.\n        This maps the old name `forward_step_function` to the new name `forward_step` and `data_step_function` to\n        `data_step`.\n\n        Args:\n            *args: all args are passed through to BionemoLightningModule\n            data_step_function (DataStepFunction, optional): The data step function. Defaults to biobert_data_step.\n            forward_step_function (ForwardStepFunction, optional): The forward step function. Defaults to bert_forward_step.\n            **kwargs: all other kwargs are passed through to BionemoLightningModule.\n        \"\"\"  # noqa: D205\n        super().__init__(*args, forward_step=forward_step_function, data_step=data_step_function, **kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BioBertLightningModule.__init__","title":"<code>__init__(*args, data_step_function=biobert_data_step, forward_step_function=bert_forward_step, **kwargs)</code>","text":"<p>DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints. This maps the old name <code>forward_step_function</code> to the new name <code>forward_step</code> and <code>data_step_function</code> to <code>data_step</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>all args are passed through to BionemoLightningModule</p> <code>()</code> <code>data_step_function</code> <code>DataStepFunction</code> <p>The data step function. Defaults to biobert_data_step.</p> <code>biobert_data_step</code> <code>forward_step_function</code> <code>ForwardStepFunction</code> <p>The forward step function. Defaults to bert_forward_step.</p> <code>bert_forward_step</code> <code>**kwargs</code> <p>all other kwargs are passed through to BionemoLightningModule.</p> <code>{}</code> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    data_step_function: DataStepFunction = biobert_data_step,\n    forward_step_function: ForwardStepFunction = bert_forward_step,\n    **kwargs,\n):\n    \"\"\"DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints.\n    This maps the old name `forward_step_function` to the new name `forward_step` and `data_step_function` to\n    `data_step`.\n\n    Args:\n        *args: all args are passed through to BionemoLightningModule\n        data_step_function (DataStepFunction, optional): The data step function. Defaults to biobert_data_step.\n        forward_step_function (ForwardStepFunction, optional): The forward step function. Defaults to bert_forward_step.\n        **kwargs: all other kwargs are passed through to BionemoLightningModule.\n    \"\"\"  # noqa: D205\n    super().__init__(*args, forward_step=forward_step_function, data_step=data_step_function, **kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.SequenceBatch","title":"<code>SequenceBatch</code>","text":"<p>               Bases: <code>SequenceBatchCore</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class SequenceBatch(SequenceBatchCore, total=False):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens_argmin: Tensor\n    max_seqlen: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.SequenceBatchCore","title":"<code>SequenceBatchCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class SequenceBatchCore(TypedDict):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.bert_default_optimizer","title":"<code>bert_default_optimizer(model)</code>","text":"<p>Returns the default optimizer for the BERT model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The BERT model.</p> required <p>Returns:</p> Type Description <code>FusedAdam</code> <p>The default optimizer initialized for this BERT module's parameters.</p> <code>FusedAdam</code> <p>Uses a learning rate of 1e-4 and weight decay of 1e-2.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def bert_default_optimizer(model: torch.nn.Module) -&gt; FusedAdam:\n    \"\"\"Returns the default optimizer for the BERT model.\n\n    Args:\n        model: The BERT model.\n\n    Returns:\n        The default optimizer initialized for this BERT module's parameters.\n        Uses a learning rate of 1e-4 and weight decay of 1e-2.\n    \"\"\"\n    return FusedAdam(model.parameters(), lr=1e-4, weight_decay=0.01)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.bert_forward_step","title":"<code>bert_forward_step(model, batch)</code>","text":"<p>Performs the model's forward pass using the batch, for Megatron compatibility.</p> <p>This subsets the batch keys to the ones actually used by forward pass of the model, and then calls the model's forward pass. if \"cu_seqsens\" are defined in the batch, then the packed sequence parameters are also passed to the model for forward pass efficiency.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def bert_forward_step(model: BertModel[DataT], batch: BertBatch) -&gt; DataT:\n    \"\"\"Performs the model's forward pass using the batch, for Megatron compatibility.\n\n    This subsets the batch keys to the ones actually used by forward pass of the model, and then calls the model's\n    forward pass. if \"cu_seqsens\" are defined in the batch, then the packed sequence parameters are also passed to the\n    model for forward pass efficiency.\n    \"\"\"\n    if \"cu_seqlens\" in batch:\n        forward_results = model.forward(\n            input_ids=batch[\"text\"],\n            attention_mask=batch[\"attention_mask\"],\n            packed_seq_params=get_packed_seq_params(cast(SequenceBatch, batch)),\n        )\n    else:\n        forward_results = model.forward(input_ids=batch[\"text\"], attention_mask=batch[\"attention_mask\"])\n    # TODO support losses that also include the binary head, this means doing something more fancy than the one\n    #      default GPT reduction function above MaskedTokenLossReduction()\n    return forward_results\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.biobert_data_step","title":"<code>biobert_data_step(dataloader_iter)</code>","text":"<p>Preprocesses a batch of data for the GeneFormer model, and ingest a single batch of data from the dataloader iterator.     only necessary batch keys are subsetted and passed to the model's forward pass, and the loss forward pass, depending on stage.     TODO document how parallel_state pipeline stages work.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader_iter</code> <p>An iterator over the dataloader.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Dict[str, Tensor]</code> <p>A dictionary of this batch limiting to relevant keys.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def biobert_data_step(dataloader_iter) -&gt; Dict[str, Tensor]:\n    \"\"\"Preprocesses a batch of data for the GeneFormer model, and ingest a single batch of data from the dataloader iterator.\n        only necessary batch keys are subsetted and passed to the model's forward pass, and the loss forward pass, depending on stage.\n        TODO document how parallel_state pipeline stages work.\n\n    Args:\n        dataloader_iter: An iterator over the dataloader.\n\n    Returns:\n        output: A dictionary of this batch limiting to relevant keys.\n\n    \"\"\"  # noqa: D205\n    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842\n\n    batch = next(dataloader_iter)\n\n    if isinstance(batch, tuple) and len(batch) == 3:\n        _batch: dict = batch[0]\n    else:\n        _batch = batch\n\n    required_keys = set()\n    required_keys.add(\"attention_mask\")\n    if parallel_state.is_pipeline_first_stage():\n        required_keys.add(\"text\")\n    if parallel_state.is_pipeline_last_stage():\n        required_keys.update((\"labels\", \"loss_mask\", \"types\", \"is_random\"))\n    # if self.get_attention_mask_from_fusion:\n    #     required_keys.remove('attention_mask')\n\n    _batch = {key: val.cuda(non_blocking=True) if key in required_keys else None for key, val in _batch.items()}\n    # slice batch along sequence dimension for context parallelism\n    output = get_batch_on_this_context_parallel_rank(_batch)\n\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.biobert_lightning_module","title":"<code>biobert_lightning_module(config, optimizer=None, tokenizer=None, data_step=biobert_data_step, forward_step=bert_forward_step, model_transform=None, **model_construct_args)</code>","text":"<p>A pytorch lightning module for BioBert-derived models.</p> <p>This module is designed to be used with the Megatron-LM strategy and nemo 2.0 conventions. To change your loss, pass in a different config object that returns a different loss reduction class. To change your model and what it outputs, pass in a different config object that returns a different model. Do not modify this function unless you need to change higher level logic. You may need to modify the various step and forward functions towards the bottom of this file to handle new/different keys in the batch. In the future some of those functions may need to be refactored out into the config object or a different place so that they live closer to the model definition.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def biobert_lightning_module(\n    config: BioBertConfig[MegatronBioBertModel, MegatronLossReduction],\n    optimizer: Optional[MegatronOptimizerModule] = None,\n    tokenizer: Optional[TokenizerSpec | PreTrainedTokenizerBase] = None,\n    data_step: DataStep = biobert_data_step,\n    forward_step: ForwardStep = bert_forward_step,\n    model_transform: Optional[Callable] = None,\n    **model_construct_args,\n) -&gt; BionemoLightningModule[MegatronBioBertModel, MegatronLossReduction]:\n    \"\"\"A pytorch lightning module for BioBert-derived models.\n\n    This module is designed to be used with the Megatron-LM strategy and nemo 2.0 conventions.\n    To change your loss, pass in a different config object that returns a different loss reduction class.\n    To change your model and what it outputs, pass in a different config object that returns a different model.\n    Do not modify this function unless you need to change higher level logic. You may need to modify the various step\n    and forward functions towards the bottom of this file to handle new/different keys in the batch. In the future some\n    of those functions may need to be refactored out into the config object or a different place so that they live\n    closer to the model definition.\n    \"\"\"\n    return BionemoLightningModule(\n        config=config,\n        optimizer=optimizer if optimizer is not None else default_megatron_optimizer(),\n        data_step=data_step,\n        forward_step=forward_step,\n        tokenizer=tokenizer,\n        model_transform=model_transform,\n        **model_construct_args,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.get_batch_on_this_context_parallel_rank","title":"<code>get_batch_on_this_context_parallel_rank(batch, in_place=True)</code>","text":"<p>Ensures that the input batch is in the right format for context parallel rank.</p> <p>Modifies the batch data based on the context parallel rank, if the context parallel world size is greater than 1. Otherwise, the batch is returned as-is.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>The input batch data.</p> required <code>in_place</code> <code>bool</code> <p>If true, then the input is mutated. The returned dict is a reference to the input.       Otherwise, the input data is always shallow-copied and this copy is modified and returned.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Tensor]</code> <p>The modified batch data based on the context parallel rank.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def get_batch_on_this_context_parallel_rank(batch: Dict[str, Tensor], in_place: bool = True) -&gt; Dict[str, Tensor]:\n    \"\"\"Ensures that the input batch is in the right format for context parallel rank.\n\n    Modifies the batch data based on the context parallel rank, if the context parallel world size is greater than 1.\n    Otherwise, the batch is returned as-is.\n\n\n    Args:\n        batch: The input batch data.\n        in_place: If true, then the input is mutated. The returned dict is a reference to the input.\n                  Otherwise, the input data is always shallow-copied and this copy is modified and returned.\n\n    Returns:\n        dict: The modified batch data based on the context parallel rank.\n    \"\"\"\n    if not in_place:\n        batch: dict[str, Tensor] = dict(**batch)\n\n    if cp_size := parallel_state.get_context_parallel_world_size() &gt; 1:\n        num_valid_tokens_in_ub: Tensor | None = None\n        if \"loss_mask\" in batch and batch[\"loss_mask\"] is not None:\n            num_valid_tokens_in_ub = batch[\"loss_mask\"].sum()\n\n        cp_rank = parallel_state.get_context_parallel_rank()\n        for key, val in batch.items():\n            if val is not None:\n                seq_dim = 1 if key != \"attention_mask\" else 2\n                _val = val.view(\n                    *val.shape[0:seq_dim],\n                    2 * cp_size,\n                    val.shape[seq_dim] // (2 * cp_size),\n                    *val.shape[(seq_dim + 1) :],\n                )\n                index = torch.tensor([cp_rank, (2 * cp_size - cp_rank - 1)], device=\"cpu\", pin_memory=True).cuda(\n                    non_blocking=True\n                )\n                _val = _val.index_select(seq_dim, index)\n                _val = _val.view(*val.shape[0:seq_dim], -1, *_val.shape[(seq_dim + 2) :])\n                batch[key] = _val\n        batch[\"num_valid_tokens_in_ub\"] = num_valid_tokens_in_ub  # type: ignore\n\n    return batch\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.get_packed_seq_params","title":"<code>get_packed_seq_params(batch)</code>","text":"<p>Get the packed sequence parameters for the given batch.</p> <p>This function should only be called if <code>cu_seqlens</code> is defined in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SequenceBatch</code> <p>The input batch to pack.</p> required <p>Returns:</p> Name Type Description <code>PackedSeqParams</code> <code>PackedSeqParams</code> <p>The packed sequence parameters containing the following attributes: - cu_seqlens_q (Tensor): The sequence lengths for query. - cu_seqlens_kv (Tensor): The sequence lengths for key and value. - max_seqlen_q (Tensor, optional): The maximum sequence length for query. - max_seqlen_kv (Tensor, optional): The maximum sequence length for key and value. - qkv_format (str): The format of query, key, and value tensors.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def get_packed_seq_params(batch: SequenceBatch) -&gt; PackedSeqParams:\n    \"\"\"Get the packed sequence parameters for the given batch.\n\n    This function should only be called if `cu_seqlens` is defined in the batch.\n\n    Args:\n        batch: The input batch to pack.\n\n    Returns:\n        PackedSeqParams: The packed sequence parameters containing the following attributes:\n            - cu_seqlens_q (Tensor): The sequence lengths for query.\n            - cu_seqlens_kv (Tensor): The sequence lengths for key and value.\n            - max_seqlen_q (Tensor, optional): The maximum sequence length for query.\n            - max_seqlen_kv (Tensor, optional): The maximum sequence length for key and value.\n            - qkv_format (str): The format of query, key, and value tensors.\n\n    \"\"\"\n    cu_seqlens = batch[\"cu_seqlens\"].squeeze()  # remove batch size dimension (mbs=1)\n    # remove -1 \"paddings\" added in collate_fn\n    if cu_seqlens_argmin := batch.get(\"cu_seqlens_argmin\", None) is not None:\n        # pre-compute cu_seqlens_argmin in dataset class for perf\n        cu_seqlens = cu_seqlens[: cu_seqlens_argmin.item()]\n    else:\n        cu_seqlens = cu_seqlens[: torch.argmin(cu_seqlens)]\n\n    # pre-compute max_seqlens in dataset class for perf\n    max_seqlen = batch[\"max_seqlen\"].squeeze() if \"max_seqlen\" in batch else None\n\n    # these args are passed eventually into TEDotProductAttention.forward()\n    return PackedSeqParams(\n        cu_seqlens_q=cu_seqlens,\n        cu_seqlens_kv=cu_seqlens,\n        max_seqlen_q=max_seqlen,\n        max_seqlen_kv=max_seqlen,\n        qkv_format=\"thd\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/","title":"Model","text":""},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModelType","title":"<code>MegatronBioBertModelType = TypeVar('MegatronBioBertModelType', bound=MegatronBioBertModel)</code>  <code>module-attribute</code>","text":"<p>A megatron model that is or extends the MegatronBioBertModel.</p>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.PositionEmbeddingKinds","title":"<code>PositionEmbeddingKinds = Literal['learned_absolute', 'rope']</code>  <code>module-attribute</code>","text":"<p>Kinds of supported positional embeddings.</p>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertConfig","title":"<code>BioBertConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MegatronBioNeMoTrainableModelConfig[MegatronBioBertModelType, MegatronLossType]</code></p> <p>Config class for BioBert model, responsible for the partial configuration of Transformer models.</p> <p>NOTE: do not use this config directly, define a child config that overrides items from this parent config</p> <p><code>configure_model()</code> is ultimately called by the LightningModule using PTL lightning module hooks.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>@dataclass\nclass BioBertConfig(\n    MegatronBioNeMoTrainableModelConfig[MegatronBioBertModelType, MegatronLossType],\n):\n    \"\"\"Config class for BioBert model, responsible for the partial configuration of Transformer models.\n\n    NOTE: do not use this config directly, define a child config that overrides items from this parent config\n\n    `configure_model()` is ultimately called by the LightningModule using PTL lightning module hooks.\n    \"\"\"\n\n    # From megatron.core.models.gpt.bert_model.GPTModel\n    kv_channels: int | None = None\n    fp16_lm_cross_entropy: bool = False\n    apply_rope_fusion: bool = True\n    parallel_output: bool = True\n    bias_dropout_fusion: bool = True\n    bias_activation_fusion: bool = True\n    masked_softmax_fusion: bool = True\n    persist_layer_norm: bool = True\n    get_attention_mask_from_fusion: bool = True\n    share_embeddings_and_output_weights: bool = False  # try True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\"\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n    hidden_size: int = 512\n    num_attention_heads: int = 8\n    num_layers: int = 6\n    init_method_std: float = 0.02\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n\n    optimizer_fn: Optional[Callable[[\"MegatronBioBertModel\"], Optimizer]] = None\n    # TODO (@skothenhill,@georgea) update to use the nemo2 checkpoint mixins\n    #  support HF (requires weight interleaving on qkv layer) and nemo1 checkpoints ideally.\n    # TODO (@skothenhill,@jstjohn) come up with a nice way of doing fine-tuning checkpoint loading,\n    #  where some acceptible layers (eg lm_head) may or may not be absent from the model, and others\n    #  (like a new head) may be new and missing from the initial checkpoint.\n    nemo1_ckpt_path: Optional[str] = None\n\n    initial_ckpt_path: Optional[str] = None\n    # TODO(@jstjohn, @skothenhill) Was this supposed to be only on the child?\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n    # Used if initializing from a checkpoint, set this to any fields you want to override rather than re-set.\n    #  by default all fields will be overridden.\n    override_parent_fields: List[str] = field(default_factory=lambda: _OVERRIDE_BIOBERT_CONFIG_DEFAULTS)\n    return_embeddings: bool = False\n    include_embeddings: bool = False\n    return_only_hidden_states: bool = False\n    include_hiddens: bool = False  # Include hidden layers in the output of the model\n    include_input_ids: bool = False\n    skip_logits: bool = False  # useful for inference\n    core_attention_override: Type[torch.nn.Module] | None = None\n\n    # loss reduction class\n    loss_reduction_class: Type[MegatronLossType] = BERTMLMLossWithReduction\n\n    # metric logging\n    train_metric: Optional[TorchmetricsConfig] = None\n    valid_metric: Optional[TorchmetricsConfig] = None\n\n    def configure_model(self, tokenizer: AutoTokenizer) -&gt; MegatronBioBertModelType:  # noqa: D102\n        vp_size = self.virtual_pipeline_model_parallel_size\n        if vp_size:\n            p_size = self.pipeline_model_parallel_size\n            assert (\n                self.num_layers // p_size\n            ) % vp_size == 0, \"Make sure the number of model chunks is the same across all pipeline stages.\"\n\n        # The local specs all require the standard full attention mask.\n        use_full_attention_mask: bool = \"transformer_engine\" not in self.biobert_spec_option\n        do_next_sentence = False\n        if self.model_cls is None:\n            raise ValueError(\n                f\"You must supply `model_cls` to the {type(self)} for module to initialization in `configure_model`.\"\n            )\n\n        if self.initial_ckpt_path:\n            self.load_settings_from_checkpoint(self.initial_ckpt_path)\n\n        model = self.model_cls(\n            self,\n            transformer_layer_spec=get_biobert_spec(\n                self.biobert_spec_option,\n                qk_layernorm=self.qk_layernorm,\n                core_attention=self.core_attention_override,\n            ),\n            num_tokentypes=2 if do_next_sentence else 0,\n            vocab_size=get_vocab_size(self, tokenizer.vocab_size, self.make_vocab_size_divisible_by),\n            max_sequence_length=self.seq_length,\n            tokenizer=tokenizer,\n            fp16_lm_cross_entropy=self.fp16_lm_cross_entropy,\n            parallel_output=self.parallel_output,\n            share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,\n            position_embedding_type=self.position_embedding_type,\n            rotary_percent=self.rotary_percent,\n            seq_len_interpolation_factor=self.seq_len_interpolation_factor,\n            return_embeddings=self.return_embeddings,\n            include_embeddings=self.include_embeddings,\n            pre_process=parallel_state.is_pipeline_first_stage(),\n            post_process=parallel_state.is_pipeline_last_stage(),  # set to False for inference\n            add_binary_head=do_next_sentence,\n            use_full_attention_mask=use_full_attention_mask,\n            include_hiddens=self.include_hiddens,\n            skip_logits=self.skip_logits,\n            include_input_ids=self.include_input_ids,\n        )\n        # TODO (@skothenhill) this is a hack to load the old checkpoint.\n        # This should be removed once we have a proper checkpoint conversion\n        # see NeMo/nemo/collections/llm/gpt/model/mixtral.py for how we should do it.\n        # We should eventually have an adapter for nemo1 checkpoints, HF checkpoints (at least for ESM2 @georgea)\n        # and an adapter may also be the right way to handle expected missing/extra keys when importing\n        # a checkpoint for fine-tuning (eg ignore misisng lm_head, if not there in model, etc).\n        if self.nemo1_ckpt_path is not None:\n            assert self.initial_ckpt_path is None, \"Mutually exclusive checkpoint path used twice\"\n            te_mapping = \"transformer_engine\" in self.biobert_spec_option.value\n            with tarfile.open(self.nemo1_ckpt_path, \"r\") as old_ckpt:\n                ckpt_file = old_ckpt.extractfile(\"./model_weights.ckpt\")\n                if ckpt_file is None:\n                    raise ValueError(f\"Failure to read checkpoint file: {old_ckpt}/model_weights/ckpt\")\n                old_weights = torch.load(ckpt_file)\n                new_state_dict_from_old = {}\n                for k, v in old_weights.items():\n                    new_key = nemo1_to_nemo2_biobert_key_mapping(k, new_model_prefix=\"\", te_mapping=te_mapping)\n                    new_state_dict_from_old[new_key] = v\n                # TE adds non-null ._extra_state objects to layers, which store some kind of buffer bits\n                #  so we need to allow those to pass through if we're loading from bionemo1 which did not\n                #  use TE.\n                model.load_state_dict(new_state_dict_from_old, strict=not te_mapping)\n        if self.initial_ckpt_path is not None:\n            assert self.nemo1_ckpt_path is None, \"Mutually exclusive checkpoint path used twice\"\n            self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n\n        # TODO (@jstjohn) come up with a cleaner way in the biobert module to return hidden states.\n        #  maybe a suite of options like hugging face has so a user can ask for several or only one thing.\n        if self.return_only_hidden_states:\n            # this applies the final layernorm in the encoder to the hidden states which was\n            #  the default in nemo1.\n            model.post_process = False\n            model.encoder.post_process = True\n            model.encoder.post_layer_norm = True\n        return model\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:  # noqa: D102\n        # You could optionally return a different loss reduction class here based on the config settings.\n        return self.loss_reduction_class\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertOutput","title":"<code>BioBertOutput</code>","text":"<p>               Bases: <code>BioBertOutputCore</code></p> <p>The megatron bionemo bert model inference type.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class BioBertOutput(BioBertOutputCore, total=False):\n    \"\"\"The megatron bionemo bert model inference type.\"\"\"\n\n    hidden_states: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertOutputCore","title":"<code>BioBertOutputCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Keys always present in the bionemo bert model inference output.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class BioBertOutputCore(TypedDict):\n    \"\"\"Keys always present in the bionemo bert model inference output.\"\"\"\n\n    token_logits: Tensor\n    binary_logits: Optional[Tensor]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel","title":"<code>MegatronBioBertModel</code>","text":"<p>               Bases: <code>LanguageModule</code></p> <p>Transformer language model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>transformer config</p> required <code>num_tokentypes</code> <code>int</code> <p>Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.</p> required <code>transformer_layer_spec</code> <code>ModuleSpec</code> <p>Specifies module to use for transformer layers</p> required <code>vocab_size</code> <code>int</code> <p>vocabulary size</p> required <code>max_sequence_length</code> <code>int</code> <p>maximum size of sequence. This is used for positional embedding</p> required <code>pre_process</code> <code>bool</code> <p>Include embedding layer (used with pipeline parallelism)</p> <code>True</code> <code>post_process</code> <code>bool</code> <p>Include an output layer (used with pipeline parallelism)</p> <code>True</code> <code>parallel_output</code> <code>bool</code> <p>Do not gather the outputs, keep them split across tensor parallel ranks</p> <code>True</code> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>When True, input embeddings and output logit weights are shared. Defaults to False.</p> <code>False</code> <code>position_embedding_type</code> <code>PositionEmbeddingKinds</code> <p>Position embedding type. Options [\"learned_absolute\", \"rope\"]. Defaults is 'learned_absolute'.</p> <code>'learned_absolute'</code> <code>rotary_percent</code> <code>float</code> <p>Percent of rotary dimension to use for rotary position embeddings. Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.</p> <code>1.0</code> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class MegatronBioBertModel(LanguageModule):\n    \"\"\"Transformer language model.\n\n    Args:\n        config: transformer config\n        num_tokentypes: Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n        transformer_layer_spec: Specifies module to use for transformer layers\n        vocab_size: vocabulary size\n        max_sequence_length: maximum size of sequence. This is used for positional embedding\n        pre_process: Include embedding layer (used with pipeline parallelism)\n        post_process: Include an output layer (used with pipeline parallelism)\n        parallel_output: Do not gather the outputs, keep them split across tensor parallel ranks\n        share_embeddings_and_output_weights: When True, input embeddings and output logit weights are shared.\n            Defaults to False.\n        position_embedding_type: Position embedding type. Options [\"learned_absolute\", \"rope\"].\n            Defaults is 'learned_absolute'.\n        rotary_percent: Percent of rotary dimension to use for rotary position embeddings.\n            Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n    \"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        config: TransformerConfig,\n        num_tokentypes: int,\n        transformer_layer_spec: ModuleSpec,\n        vocab_size: int,\n        max_sequence_length: int,\n        tokenizer: Optional[AutoTokenizer] = None,\n        pre_process: bool = True,\n        post_process: bool = True,\n        fp16_lm_cross_entropy: bool = False,\n        parallel_output: bool = True,\n        share_embeddings_and_output_weights: bool = False,\n        position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\",\n        rotary_percent: float = 1.0,\n        seq_len_interpolation_factor: Optional[float] = None,\n        add_binary_head: bool = False,\n        return_embeddings: bool = False,\n        include_embeddings: bool = False,\n        use_full_attention_mask: bool = False,\n        include_hiddens: bool = False,\n        include_input_ids: bool = False,\n        skip_logits: bool = False,  # Useful for inference time.\n    ):\n        # TODO (@jstjohn) come up with a cleaner way for this model to return a set of things the user wants.\n        #  hidden states, embeddings, logits, etc. The defaults should work for training but we need to make it\n        #  customizable and easy to tell how to make it work well for inference as well as trouble shooting.\n        #  Also make sure that everything returned that the user wants gets transposed to the b,s,h format.\n        super(MegatronBioBertModel, self).__init__(config=config)\n        self.post_process = post_process\n        self.add_binary_head = add_binary_head\n        self.skip_logits = skip_logits\n        if return_embeddings:\n            assert self.post_process, \"only return embeddings on the last pipeline stage\"\n        # `b` = batch, `s` = sequence.\n        # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n        #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n        self.use_full_attention_mask = use_full_attention_mask\n        self.config: TransformerConfig = config\n        self.transformer_layer_spec: ModuleSpec = transformer_layer_spec\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.tokenizer = tokenizer\n        self.pre_process = pre_process\n        self.post_process = post_process\n        self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n        self.parallel_output = parallel_output\n        self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n        self.position_embedding_type = position_embedding_type\n        self.add_binary_head = add_binary_head\n        self.return_embeddings = return_embeddings\n        self.include_embeddings = include_embeddings\n        self.include_hiddens = include_hiddens\n        self.include_input_ids = include_input_ids\n        self.skip_logits = skip_logits\n\n        # megatron core pipelining currently depends on model type\n        self.model_type = ModelType.encoder_or_decoder\n        # Embeddings.\n        if self.pre_process:\n            self.register_buffer(\n                \"bert_position_id_tensor\",\n                torch.arange(max_sequence_length, dtype=torch.long, requires_grad=False).unsqueeze(0),\n                persistent=False,\n            )\n            self.embedding = LanguageModelEmbedding(\n                config=self.config,\n                vocab_size=self.vocab_size,\n                max_sequence_length=self.max_sequence_length,\n                position_embedding_type=position_embedding_type,\n                num_tokentypes=num_tokentypes,\n            )\n\n        if self.position_embedding_type == \"rope\":\n            self.rotary_pos_emb = RotaryEmbedding(\n                kv_channels=self.config.kv_channels,\n                rotary_percent=rotary_percent,\n                rotary_interleaved=self.config.rotary_interleaved,\n                # bug in megatron: they list the type as `float` but they default to `None` so it should be `Optional[float]`\n                seq_len_interpolation_factor=seq_len_interpolation_factor,  # type: ignore\n            )\n\n        # Transformer.\n        self.encoder = TransformerBlock(\n            config=self.config,\n            spec=self.transformer_layer_spec,\n            pre_process=self.pre_process,\n            post_process=self.post_process,  # NOTE: in bionemo1 this is hard-coded to True\n        )\n\n        # Output\n        if post_process:\n            # TODO: Make sure you are passing in the mpu_vocab_size properly\n            if self.config.defer_embedding_wgrad_compute:\n                # The embedding activation buffer preserves a reference to the input activations\n                # of the final embedding projection layer GEMM. It will hold the activations for\n                # all the micro-batches of a global batch for the last pipeline stage. Once we are\n                # done with all the back props for all the microbatches for the last pipeline stage,\n                # it will be in the pipeline flush stage. During this pipeline flush we use the\n                # input activations stored in embedding activation buffer and gradient outputs\n                # stored in gradient buffer to calculate the weight gradients for the embedding\n                # final linear layer.\n                self.embedding_activation_buffer = []\n                self.grad_output_buffer = []\n            else:\n                self.embedding_activation_buffer = None\n                self.grad_output_buffer = None\n\n            self.lm_head = BertLMHead(\n                config.hidden_size,\n                config,\n            )\n\n            self.output_layer = tensor_parallel.ColumnParallelLinear(\n                config.hidden_size,\n                self.vocab_size,\n                config=config,\n                init_method=config.init_method,\n                is_expert=False,\n                bias=True,\n                skip_bias_add=False,\n                gather_output=not self.parallel_output,\n                skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n                embedding_activation_buffer=self.embedding_activation_buffer,\n                grad_output_buffer=self.grad_output_buffer,\n            )\n\n            self.binary_head = None\n            if self.add_binary_head:\n                # TODO: Shoudl switch this to TE ?\n                self.binary_head = get_linear_layer(\n                    config.hidden_size, 2, config.init_method, config.perform_initialization\n                )\n                self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n\n        if self.pre_process or self.post_process:\n            self.setup_embeddings_and_output_layer()\n\n    def bert_extended_attention_mask(self, attention_mask: Tensor) -&gt; Tensor:\n        \"\"\"Creates the extended attention mask\n\n        Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary\n\n        Args:\n            attention_mask (Tensor): The input attention mask\n\n        Returns:\n            Tensor: The extended binary attention mask\n        \"\"\"  # noqa: D415\n        # We create a 3D attention mask from a 2D tensor mask.\n        # [b, 1, s]\n        attention_mask_b1s = attention_mask.unsqueeze(1)\n\n        if self.use_full_attention_mask:\n            # [b, s, 1]\n            attention_mask_bs1 = attention_mask.unsqueeze(2)\n            # [b, s, s]\n            attention_mask_bss = attention_mask_b1s * attention_mask_bs1\n            # [b, 1, s, s]\n            extended_attention_mask = attention_mask_bss.unsqueeze(1)\n        else:\n            # Tensor Engine requires a 1x1xS attention mask which it internally\n            #  converts into a 1xSxS mask.\n            # [b, 1, 1, s]\n            extended_attention_mask = attention_mask_b1s.unsqueeze(1)\n\n        # Convert attention mask to binary, and flip the values from 0 to 1 and vice versa so that\n        #  extended_attention_mask._mask_fill(-1000) that megatron does internally result in\n        #  masking out pad positions.\n        extended_attention_mask = extended_attention_mask &lt; 0.5\n\n        return extended_attention_mask\n\n    def bert_position_ids(self, token_ids):  # noqa: D102\n        # Create position ids\n        seq_length = token_ids.size(1)\n        if seq_length != self.max_sequence_length:\n            return self.bert_position_id_tensor[:, :seq_length]\n        return self.bert_position_id_tensor  # No need to subset so skip the slice op\n\n    def embedding_forward(\n        self,\n        input_ids: Tensor,\n        position_ids: Tensor,\n        tokentype_ids: Optional[Tensor] = None,\n        attention_mask: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Produce embeddings.\"\"\"\n        return self.embedding(input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids)\n\n    def set_input_tensor(self, input_tensor: Tensor | list[Tensor]) -&gt; None:\n        \"\"\"Sets input tensor to the model.\n\n        See megatron.model.transformer.set_input_tensor()\n\n        Args:\n            input_tensor: Sets the input tensor for the model.\n\n        Raises:\n            ValueError: Iff the input tensor is a list that doesn't have exactly 1 tensor.\n        \"\"\"\n        # This is usually handled in schedules.py but some inference code still gives us non-lists or None.\n        if isinstance(input_tensor, list):\n            if len(input_tensor) != 1:\n                raise ValueError(f\"input_tensor should only be length 1 for gpt/bert, not length: {len(input_tensor)}\")\n            single_input_tensor: Tensor = input_tensor[0]\n        else:\n            single_input_tensor = input_tensor\n        self.encoder.set_input_tensor(single_input_tensor)\n\n    def forward(\n        self,\n        input_ids: Tensor,\n        attention_mask: Tensor,\n        tokentype_ids: Optional[Tensor] = None,\n        lm_labels: Optional[Tensor] = None,\n        inference_params: Any | None = None,\n        runtime_gather_output: Optional[bool] = None,\n    ) -&gt; BioBertOutput | Tensor:\n        \"\"\"Forward function of BERT model\n\n        Forward function of the BERT Model This function passes the input tensors\n        through the embedding layer, and then the encoder and finally into the post\n        processing layer (optional).\n\n        It either returns the Loss values if labels are given or the final hidden units.\n        \"\"\"  # noqa: D415\n        # TODO! If we upgrade to TE 1.7 why does bit flipping back to 1 help the loss in TE 1.7? It claimed that they now follow standards, did\n        #  nemo/megatron flip again internally to be compatible wtih TE somewhere?\n        #  change the following line to ~self.bert... and see if it helps if we upgrade to TE 1.7 and NeMo/Megatron have not compensated.\n        extended_attention_mask = self.bert_extended_attention_mask(attention_mask)\n\n        if parallel_state.is_pipeline_first_stage():\n            using_input_ids: Optional[Tensor] = input_ids\n            using_position_ids: Optional[Tensor] = self.bert_position_ids(input_ids)\n        else:\n            using_input_ids = None\n            using_position_ids = None\n\n        # Encoder embedding.\n        if self.pre_process:\n            encoder_input: Optional[Tensor] = self.embedding_forward(\n                input_ids=using_input_ids,\n                position_ids=using_position_ids,\n                tokentype_ids=tokentype_ids,\n                attention_mask=attention_mask,\n            )\n        else:\n            # intermediate stage of pipeline\n            # encoder will get hidden_states from encoder.input_tensor\n            encoder_input = None\n\n        # Rotary positional embeddings (Why not move this into BERT/GPTEmberdding ?)\n        rotary_pos_emb = None\n        if self.position_embedding_type == \"rope\":\n            rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n                inference_params,\n                self.encoder,\n                encoder_input,\n                self.config,\n                packed_seq_params=None,  # TODO @sichu: upstream to Megatron-LM\n            )\n            rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n        # Run encoder.\n        hidden_states = self.encoder(\n            hidden_states=encoder_input,\n            attention_mask=extended_attention_mask,\n            inference_params=inference_params,\n            rotary_pos_emb=rotary_pos_emb,\n        )\n\n        if not self.post_process:\n            return hidden_states\n\n        if self.add_binary_head:\n            pooled_output = self.pooler(hidden_states, 0)\n\n        if self.return_embeddings or self.include_embeddings:\n            embeddings = torch.transpose(hidden_states, 0, 1)\n            masks = torch.sum(attention_mask, dim=1)\n            # Collect masked embeddings.\n            output_embeddings = torch.zeros(\n                size=(embeddings.shape[0], embeddings.shape[2]),\n                dtype=embeddings.dtype,\n                device=torch.cuda.current_device(),\n            )\n            for i, (embedding, mask) in enumerate(zip(embeddings, masks)):\n                output_embeddings[i, :] = torch.mean(embedding[1 : mask - 1], dim=0)\n\n        if self.return_embeddings:\n            return output_embeddings\n\n        # logits and loss\n        output_weight = None\n        if self.share_embeddings_and_output_weights:\n            output_weight = self.shared_embedding_or_output_weight()\n\n        hidden_states_after_lm_head = self.lm_head(hidden_states=hidden_states)\n        if not self.skip_logits:\n            # TODO add , runtime_gather_output=runtime_gather_output once supported in ColumnParallelLinear\n            logits, _ = self.output_layer(hidden_states_after_lm_head, weight=output_weight)\n        else:\n            logits = None\n\n        binary_logits = None\n        if self.binary_head is not None:\n            binary_logits = self.binary_head(pooled_output)\n\n        output = {\"token_logits\": logits, \"binary_logits\": binary_logits}\n        if self.include_hiddens:\n            output[\"hidden_states\"] = hidden_states.transpose(0, 1).contiguous()  # [s b h] =&gt; [b s h]\n        if self.include_input_ids:\n            output[\"input_ids\"] = input_ids\n        if self.include_embeddings:\n            output[\"embeddings\"] = output_embeddings\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.bert_extended_attention_mask","title":"<code>bert_extended_attention_mask(attention_mask)</code>","text":"<p>Creates the extended attention mask</p> <p>Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary</p> <p>Parameters:</p> Name Type Description Default <code>attention_mask</code> <code>Tensor</code> <p>The input attention mask</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The extended binary attention mask</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def bert_extended_attention_mask(self, attention_mask: Tensor) -&gt; Tensor:\n    \"\"\"Creates the extended attention mask\n\n    Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary\n\n    Args:\n        attention_mask (Tensor): The input attention mask\n\n    Returns:\n        Tensor: The extended binary attention mask\n    \"\"\"  # noqa: D415\n    # We create a 3D attention mask from a 2D tensor mask.\n    # [b, 1, s]\n    attention_mask_b1s = attention_mask.unsqueeze(1)\n\n    if self.use_full_attention_mask:\n        # [b, s, 1]\n        attention_mask_bs1 = attention_mask.unsqueeze(2)\n        # [b, s, s]\n        attention_mask_bss = attention_mask_b1s * attention_mask_bs1\n        # [b, 1, s, s]\n        extended_attention_mask = attention_mask_bss.unsqueeze(1)\n    else:\n        # Tensor Engine requires a 1x1xS attention mask which it internally\n        #  converts into a 1xSxS mask.\n        # [b, 1, 1, s]\n        extended_attention_mask = attention_mask_b1s.unsqueeze(1)\n\n    # Convert attention mask to binary, and flip the values from 0 to 1 and vice versa so that\n    #  extended_attention_mask._mask_fill(-1000) that megatron does internally result in\n    #  masking out pad positions.\n    extended_attention_mask = extended_attention_mask &lt; 0.5\n\n    return extended_attention_mask\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.embedding_forward","title":"<code>embedding_forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Produce embeddings.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def embedding_forward(\n    self,\n    input_ids: Tensor,\n    position_ids: Tensor,\n    tokentype_ids: Optional[Tensor] = None,\n    attention_mask: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Produce embeddings.\"\"\"\n    return self.embedding(input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.forward","title":"<code>forward(input_ids, attention_mask, tokentype_ids=None, lm_labels=None, inference_params=None, runtime_gather_output=None)</code>","text":"<p>Forward function of BERT model</p> <p>Forward function of the BERT Model This function passes the input tensors through the embedding layer, and then the encoder and finally into the post processing layer (optional).</p> <p>It either returns the Loss values if labels are given or the final hidden units.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def forward(\n    self,\n    input_ids: Tensor,\n    attention_mask: Tensor,\n    tokentype_ids: Optional[Tensor] = None,\n    lm_labels: Optional[Tensor] = None,\n    inference_params: Any | None = None,\n    runtime_gather_output: Optional[bool] = None,\n) -&gt; BioBertOutput | Tensor:\n    \"\"\"Forward function of BERT model\n\n    Forward function of the BERT Model This function passes the input tensors\n    through the embedding layer, and then the encoder and finally into the post\n    processing layer (optional).\n\n    It either returns the Loss values if labels are given or the final hidden units.\n    \"\"\"  # noqa: D415\n    # TODO! If we upgrade to TE 1.7 why does bit flipping back to 1 help the loss in TE 1.7? It claimed that they now follow standards, did\n    #  nemo/megatron flip again internally to be compatible wtih TE somewhere?\n    #  change the following line to ~self.bert... and see if it helps if we upgrade to TE 1.7 and NeMo/Megatron have not compensated.\n    extended_attention_mask = self.bert_extended_attention_mask(attention_mask)\n\n    if parallel_state.is_pipeline_first_stage():\n        using_input_ids: Optional[Tensor] = input_ids\n        using_position_ids: Optional[Tensor] = self.bert_position_ids(input_ids)\n    else:\n        using_input_ids = None\n        using_position_ids = None\n\n    # Encoder embedding.\n    if self.pre_process:\n        encoder_input: Optional[Tensor] = self.embedding_forward(\n            input_ids=using_input_ids,\n            position_ids=using_position_ids,\n            tokentype_ids=tokentype_ids,\n            attention_mask=attention_mask,\n        )\n    else:\n        # intermediate stage of pipeline\n        # encoder will get hidden_states from encoder.input_tensor\n        encoder_input = None\n\n    # Rotary positional embeddings (Why not move this into BERT/GPTEmberdding ?)\n    rotary_pos_emb = None\n    if self.position_embedding_type == \"rope\":\n        rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n            inference_params,\n            self.encoder,\n            encoder_input,\n            self.config,\n            packed_seq_params=None,  # TODO @sichu: upstream to Megatron-LM\n        )\n        rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n    # Run encoder.\n    hidden_states = self.encoder(\n        hidden_states=encoder_input,\n        attention_mask=extended_attention_mask,\n        inference_params=inference_params,\n        rotary_pos_emb=rotary_pos_emb,\n    )\n\n    if not self.post_process:\n        return hidden_states\n\n    if self.add_binary_head:\n        pooled_output = self.pooler(hidden_states, 0)\n\n    if self.return_embeddings or self.include_embeddings:\n        embeddings = torch.transpose(hidden_states, 0, 1)\n        masks = torch.sum(attention_mask, dim=1)\n        # Collect masked embeddings.\n        output_embeddings = torch.zeros(\n            size=(embeddings.shape[0], embeddings.shape[2]),\n            dtype=embeddings.dtype,\n            device=torch.cuda.current_device(),\n        )\n        for i, (embedding, mask) in enumerate(zip(embeddings, masks)):\n            output_embeddings[i, :] = torch.mean(embedding[1 : mask - 1], dim=0)\n\n    if self.return_embeddings:\n        return output_embeddings\n\n    # logits and loss\n    output_weight = None\n    if self.share_embeddings_and_output_weights:\n        output_weight = self.shared_embedding_or_output_weight()\n\n    hidden_states_after_lm_head = self.lm_head(hidden_states=hidden_states)\n    if not self.skip_logits:\n        # TODO add , runtime_gather_output=runtime_gather_output once supported in ColumnParallelLinear\n        logits, _ = self.output_layer(hidden_states_after_lm_head, weight=output_weight)\n    else:\n        logits = None\n\n    binary_logits = None\n    if self.binary_head is not None:\n        binary_logits = self.binary_head(pooled_output)\n\n    output = {\"token_logits\": logits, \"binary_logits\": binary_logits}\n    if self.include_hiddens:\n        output[\"hidden_states\"] = hidden_states.transpose(0, 1).contiguous()  # [s b h] =&gt; [b s h]\n    if self.include_input_ids:\n        output[\"input_ids\"] = input_ids\n    if self.include_embeddings:\n        output[\"embeddings\"] = output_embeddings\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.set_input_tensor","title":"<code>set_input_tensor(input_tensor)</code>","text":"<p>Sets input tensor to the model.</p> <p>See megatron.model.transformer.set_input_tensor()</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor | list[Tensor]</code> <p>Sets the input tensor for the model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Iff the input tensor is a list that doesn't have exactly 1 tensor.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def set_input_tensor(self, input_tensor: Tensor | list[Tensor]) -&gt; None:\n    \"\"\"Sets input tensor to the model.\n\n    See megatron.model.transformer.set_input_tensor()\n\n    Args:\n        input_tensor: Sets the input tensor for the model.\n\n    Raises:\n        ValueError: Iff the input tensor is a list that doesn't have exactly 1 tensor.\n    \"\"\"\n    # This is usually handled in schedules.py but some inference code still gives us non-lists or None.\n    if isinstance(input_tensor, list):\n        if len(input_tensor) != 1:\n            raise ValueError(f\"input_tensor should only be length 1 for gpt/bert, not length: {len(input_tensor)}\")\n        single_input_tensor: Tensor = input_tensor[0]\n    else:\n        single_input_tensor = input_tensor\n    self.encoder.set_input_tensor(single_input_tensor)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/testing_utils/","title":"Testing utils","text":""},{"location":"API_reference/bionemo/llm/model/biobert/testing_utils/#bionemo.llm.model.biobert.testing_utils.compute_biobert_loss_singlegpu","title":"<code>compute_biobert_loss_singlegpu(trainer, pl_module)</code>","text":"<p>Computes the loss for BioBert models on a single GPU.</p> <p>This will not function in multi-gpu settings nor with models that do not conform to BioBert.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Lightning Trainer object.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule being trained.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The mean loss.</p> <p>See Also: - :class: BioBertModel</p> Source code in <code>bionemo/llm/model/biobert/testing_utils.py</code> <pre><code>def compute_biobert_loss_singlegpu(trainer: pl.Trainer, pl_module: pl.LightningModule):\n    \"\"\"Computes the loss for BioBert models on a single GPU.\n\n    This will not function in multi-gpu settings nor with models that do not conform to BioBert.\n\n    Args:\n        trainer (pl.Trainer): The Lightning Trainer object.\n        pl_module (pl.LightningModule): The LightningModule being trained.\n\n    Returns:\n        float: The mean loss.\n\n    See Also:\n    - :class: BioBertModel\n    \"\"\"\n    model = pl_module\n    dl = trainer.datamodule.val_dataloader()\n\n    n, loss = -1, 0.0\n    model.eval()\n    # batch = next(iter(dl))\n    batch = model.data_step(iter(dl))\n    result = model(\n        input_ids=batch[\"text\"].cuda(),  # 'tokens' also a valid input for MockGPTDataModule\n        attention_mask=batch[\"attention_mask\"].cuda(),\n    )\n    loss_mask = batch[\"loss_mask\"].cuda()\n    # Not guaranteed i guess?\n    logits = result[\"token_logits\"]\n    target = batch[\"labels\"].cuda()\n    loss += F.cross_entropy(logits[loss_mask].float(), target[loss_mask], reduction=\"sum\")\n    n += loss_mask.sum()\n\n    mean_loss: float = (loss / n).detach().cpu().numpy().item()\n    model.train()\n    return mean_loss\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/transformer_specs/","title":"Transformer specs","text":""},{"location":"API_reference/bionemo/llm/model/biobert/transformer_specs/#bionemo.llm.model.biobert.transformer_specs.BiobertSpecOption","title":"<code>BiobertSpecOption</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Options for the BiobertSpec. The spec defines the architecture of the transformer (BERT) block in the biobert model. This is a <code>str, Enum</code> type so that argparse can use the string names as choices.</p> Source code in <code>bionemo/llm/model/biobert/transformer_specs.py</code> <pre><code>class BiobertSpecOption(str, Enum):\n    \"\"\"Options for the BiobertSpec. The spec defines the architecture of the transformer (BERT) block in the biobert model.\n    This is a `str, Enum` type so that argparse can use the string names as choices.\n    \"\"\"  # noqa: D205\n\n    bert_layer_local_spec = \"bert_layer_local_spec\"\n    bert_layer_local_spec_with_qk_ln = \"bert_layer_local_spec_with_qk_ln\"\n    bert_layer_with_transformer_engine_spec = \"bert_layer_with_transformer_engine_spec\"\n    bert_layer_with_transformer_engine_and_qk_ln_spec = \"bert_layer_with_transformer_engine_and_qk_ln_spec\"\n    # ESM2 spec\n    esm2_bert_layer_local_spec = \"esm2_bert_layer_local_spec\"\n    esm2_bert_layer_with_transformer_engine_spec = \"esm2_bert_layer_with_transformer_engine_spec\"\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/transformer_specs/#bionemo.llm.model.biobert.transformer_specs.get_biobert_spec","title":"<code>get_biobert_spec(biobert_spec_option, qk_layernorm=False, core_attention=None)</code>","text":"<p>Get the spec for the Biobert model.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>The model type.</p> required <code>spec_option</code> <code>BiobertSpecOption</code> <p>The spec option.</p> required <p>Returns:</p> Name Type Description <code>TransformerConfig</code> <code>ModuleSpec</code> <p>The Biobert spec.</p> Source code in <code>bionemo/llm/model/biobert/transformer_specs.py</code> <pre><code>def get_biobert_spec(  # noqa: D417\n    biobert_spec_option: BiobertSpecOption,\n    qk_layernorm: bool = False,\n    core_attention: Optional[Type[Module]] = None,\n) -&gt; spec_utils.ModuleSpec:\n    \"\"\"Get the spec for the Biobert model.\n\n    Args:\n        model_type (ModelType): The model type.\n        spec_option (BiobertSpecOption): The spec option.\n\n    Returns:\n        TransformerConfig: The Biobert spec.\n    \"\"\"\n    #\n    # BEGIN define several specs that are a function of `qk_layernorm`\n    #\n\n    match biobert_spec_option:\n        case BiobertSpecOption.bert_layer_local_spec:\n            return bert_layer_specs.bert_layer_local_spec\n\n        case BiobertSpecOption.bert_layer_local_spec_with_qk_ln:\n            # Use this spec for an implementation using only modules in megatron core\n\n            if core_attention is None:\n                core_attention = DotProductAttention\n\n            bert_layer_local_spec_with_qk_ln = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    input_layernorm=FusedLayerNorm,\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=ColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=RowParallelLinear,\n                            q_layernorm=FusedLayerNorm if qk_layernorm else IdentityOp,\n                            k_layernorm=FusedLayerNorm if qk_layernorm else IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    pre_mlp_layernorm=FusedLayerNorm,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=ColumnParallelLinear,\n                            linear_fc2=RowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                    sharded_state_dict_keys_map={\n                        \"input_layernorm.\": \"self_attention.linear_qkv.layer_norm_\",\n                        \"pre_mlp_layernorm.\": \"mlp.linear_fc1.layer_norm_\",\n                    },\n                ),\n            )\n            return bert_layer_local_spec_with_qk_ln\n\n        case BiobertSpecOption.bert_layer_with_transformer_engine_spec:\n            return bert_layer_specs.bert_layer_with_transformer_engine_spec\n\n        case BiobertSpecOption.bert_layer_with_transformer_engine_and_qk_ln_spec:\n            if core_attention is None:\n                core_attention = TEDotProductAttention\n\n            bert_layer_with_transformer_engine_and_qk_ln_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=TELayerNormColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=TERowParallelLinear,\n                            q_layernorm=TELayerNorm if qk_layernorm else IdentityOp,\n                            k_layernorm=TELayerNorm if qk_layernorm else IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=TELayerNormColumnParallelLinear,\n                            linear_fc2=TERowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                ),\n            )\n            return bert_layer_with_transformer_engine_and_qk_ln_spec\n\n        case BiobertSpecOption.esm2_bert_layer_local_spec:\n            if core_attention is None:\n                raise ValueError(f\"Must supply core_attention with {BiobertSpecOption.esm2_bert_layer_local_spec} !\")\n\n            esm2_bert_layer_local_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    input_layernorm=FusedLayerNorm,\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=ColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=RowParallelLinear,\n                            q_layernorm=ESM2QueryScaling,\n                            k_layernorm=IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    pre_mlp_layernorm=FusedLayerNorm,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=ColumnParallelLinear,\n                            linear_fc2=RowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                    sharded_state_dict_keys_map={\n                        \"input_layernorm.\": \"self_attention.linear_qkv.layer_norm_\",\n                        \"pre_mlp_layernorm.\": \"mlp.linear_fc1.layer_norm_\",\n                    },\n                ),\n            )\n            return esm2_bert_layer_local_spec\n\n        case BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            if core_attention is None:\n                core_attention = TEDotProductAttention\n\n            esm2_bert_layer_local_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=TELayerNormColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=TERowParallelLinear,\n                            q_layernorm=ESM2QueryScaling,\n                            k_layernorm=IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=TELayerNormColumnParallelLinear,\n                            linear_fc2=TERowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                ),\n            )\n            return esm2_bert_layer_local_spec\n\n        case _:\n            raise NotImplementedError(f\"Spec option {biobert_spec_option} not implemented\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/","title":"Config models","text":""},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig","title":"<code>DataConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DataModuleT]</code>, <code>ABC</code></p> <p>Base class for all data configurations.</p> <p>This class is used to define the interface for all data configurations. It is used to define the data module that will be used in the training loop.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class DataConfig(BaseModel, Generic[DataModuleT], ABC):\n    \"\"\"Base class for all data configurations.\n\n    This class is used to define the interface for all data configurations. It is used to define the data module that\n    will be used in the training loop.\n    \"\"\"\n\n    micro_batch_size: int = 8\n    result_dir: str | pathlib.Path = \"./results\"\n    num_dataset_workers: int = 0\n    seq_length: int = 128\n\n    @field_serializer(\"result_dir\")\n    def serialize_paths(self, value: pathlib.Path) -&gt; str:  # noqa: D102\n        return serialize_path_or_str(value)\n\n    @field_validator(\"result_dir\")\n    def deserialize_paths(cls, value: str) -&gt; pathlib.Path:  # noqa: D102\n        return deserialize_str_to_path(value)\n\n    @abstractmethod\n    def construct_data_module(self, global_batch_size: int) -&gt; DataModuleT:\n        \"\"\"Construct the data module from the configuration. Cannot be defined generically.\"\"\"\n        ...\n\n    def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n        \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n        The following expression will always be true:\n\n        global_cfg.data_config == self\n        \"\"\"\n        return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig.construct_data_module","title":"<code>construct_data_module(global_batch_size)</code>  <code>abstractmethod</code>","text":"<p>Construct the data module from the configuration. Cannot be defined generically.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@abstractmethod\ndef construct_data_module(self, global_batch_size: int) -&gt; DataModuleT:\n    \"\"\"Construct the data module from the configuration. Cannot be defined generically.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig.custom_model_validator","title":"<code>custom_model_validator(global_cfg)</code>","text":"<p>Use custom implementation of this method to define the things inside global_config.</p> <p>The following expression will always be true:</p> <p>global_cfg.data_config == self</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n    \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n    The following expression will always be true:\n\n    global_cfg.data_config == self\n    \"\"\"\n    return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExperimentConfig","title":"<code>ExperimentConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for setting up and managing experiment parameters.</p> <p>Attributes:</p> Name Type Description <code>save_every_n_steps</code> <code>int</code> <p>Number of steps between saving checkpoints.</p> <code>result_dir</code> <code>str | Path</code> <p>Directory where results will be saved.</p> <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> <code>restore_from_checkpoint_path</code> <code>Optional[str]</code> <p>Path to restore from a checkpoint. Note: This does not invoke the checkpoint callback as expected.</p> <code>save_last_checkpoint</code> <code>bool</code> <p>Flag to save the last checkpoint. Default is True.</p> <code>metric_to_monitor_for_checkpoints</code> <code>str</code> <p>Metric to monitor for saving top-k checkpoints. Default is \"reduced_train_loss\".</p> <code>save_top_k</code> <code>int</code> <p>Number of top checkpoints to save based on the monitored metric. Default is 2.</p> <code>create_tensorboard_logger</code> <code>bool</code> <p>Flag to create a TensorBoard logger. Default is False.</p> <code>create_checkpoint_callback</code> <code>bool</code> <p>Flag to create a ModelCheckpoint callback</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ExperimentConfig(BaseModel):\n    \"\"\"Configuration class for setting up and managing experiment parameters.\n\n    Attributes:\n        save_every_n_steps (int): Number of steps between saving checkpoints.\n        result_dir (str | pathlib.Path): Directory where results will be saved.\n        experiment_name (str): Name of the experiment.\n        restore_from_checkpoint_path (Optional[str]): Path to restore from a checkpoint. Note: This does not invoke the checkpoint callback as expected.\n        save_last_checkpoint (bool): Flag to save the last checkpoint. Default is True.\n        metric_to_monitor_for_checkpoints (str): Metric to monitor for saving top-k checkpoints. Default is \"reduced_train_loss\".\n        save_top_k (int): Number of top checkpoints to save based on the monitored metric. Default is 2.\n        create_tensorboard_logger (bool): Flag to create a TensorBoard logger. Default is False.\n        create_checkpoint_callback (bool): Flag to create a ModelCheckpoint callback\n    \"\"\"\n\n    save_every_n_steps: int\n    result_dir: str | pathlib.Path\n    experiment_name: str\n    # NOTE: restore_from_checkpoint_path does not invoke the checkpoint callback in the way we'd like. Avoid using.\n    restore_from_checkpoint_path: Optional[str]\n    save_last_checkpoint: bool = True\n    metric_to_monitor_for_checkpoints: str = \"reduced_train_loss\"\n    save_top_k: int = 2\n    create_tensorboard_logger: bool = False\n    create_checkpoint_callback: bool = True\n\n    @field_serializer(\"result_dir\")\n    def serialize_paths(self, value: pathlib.Path) -&gt; str:  # noqa: D102\n        return serialize_path_or_str(value)\n\n    @field_validator(\"result_dir\")\n    def deserialize_paths(cls, value: str) -&gt; pathlib.Path:  # noqa: D102\n        return deserialize_str_to_path(value)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig","title":"<code>ExposedModelConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ModelConfigT]</code>, <code>ABC</code></p> <p>BioNeMo model configuration class, wraps TransformerConfig and friends.</p> <p>This class is used to define the interface for all model configurations. It is Exposed to guard against ill-typed or poorly defined fields in the underlying configuration objects. <code>ModelConfigT</code> declares the associated type of the underlying config (most commonly a BioBertGenericConfig, but could also be a TransformerConfig or something similar). Children should try to expose the minimal set of fields necessary for the user to configure the model while keeping the more esoteric configuration private to the underlying ModelConfigT.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ExposedModelConfig(BaseModel, Generic[ModelConfigT], ABC):\n    \"\"\"BioNeMo model configuration class, wraps TransformerConfig and friends.\n\n    This class is used to define the interface for all model configurations. It is **Exposed** to guard against ill-typed\n    or poorly defined fields in the underlying configuration objects. `ModelConfigT` declares the associated type of the\n    underlying config (most commonly a BioBertGenericConfig, but could also be a TransformerConfig or something similar).\n    Children should try to expose the minimal set of fields necessary for the user to configure the model while keeping\n    the more esoteric configuration private to the underlying ModelConfigT.\n    \"\"\"\n\n    # Restores weights from a pretrained checkpoint\n    initial_ckpt_path: Optional[str] = None\n    # Does not attempt to load keys with these prefixes (useful if you attached extra parameters and still want to load a set of weights)\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n\n    # Pydantic stuff to allow arbitrary types + validators + serializers\n    class Config:  # noqa: D106\n        arbitrary_types_allowed = True\n\n    def model_class(self) -&gt; Type[ModelConfigT]:\n        \"\"\"Returns the underlying model class that this config wraps.\"\"\"\n        raise NotImplementedError\n\n    def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n        \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n        The following expression will always be true:\n\n        global_cfg.bionemo_model_config == self\n        \"\"\"\n        return global_cfg\n\n    def exposed_to_internal_bionemo_model_config(self) -&gt; ModelConfigT:\n        \"\"\"Converts the exposed dataclass to the underlying Transformer config.\n\n        The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to\n        hide fields that are either not serializable by Pydantic or that we do not want to expose.\n        \"\"\"\n        cls: Type[ModelConfigT] = self.model_class()\n        model_dict = {}\n        for attr in self.model_fields:\n            if attr not in model_dict and attr in cls.__dataclass_fields__:\n                model_dict[attr] = getattr(self, attr)\n\n        # Now set fp16 and bf16 based on the precision for the underlying TransformerConfig=&gt;ParallelConfig\n        #   the only constraint is that both must not be true.\n        model_dict[\"bf16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"bf16-mixed\"]\n        model_dict[\"fp16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"16-mixed\"]\n        result = cls(**model_dict)\n\n        return result\n\n    # NOTE: See PrecisionTypes for a list of valid literals that may be deserialized.\n    params_dtype: torch.dtype\n    pipeline_dtype: torch.dtype\n    autocast_dtype: torch.dtype\n\n    num_layers: int = 6\n    hidden_size: int = 256\n    ffn_hidden_size: int = 512\n    num_attention_heads: int = 4\n    seq_length: int = 512\n    fp32_residual_connection: bool = False\n    hidden_dropout: float = 0.02\n    init_method_std: float = 0.02\n    kv_channels: Optional[int] = None\n    apply_query_key_layer_scaling: bool = False\n    make_vocab_size_divisible_by: int = 128\n    masked_softmax_fusion: bool = True\n    fp16_lm_cross_entropy: bool = False\n    gradient_accumulation_fusion: bool = False\n    layernorm_zero_centered_gamma: bool = False\n    layernorm_epsilon: float = 1.0e-12\n    activation_func: Callable[[torch.Tensor, Any], torch.Tensor] = F.gelu\n    qk_layernorm: bool = False\n    apply_residual_connection_post_layernorm: bool = False\n    bias_activation_fusion: bool = True\n    bias_dropout_fusion: bool = True\n    get_attention_mask_from_fusion: bool = False\n    attention_dropout: float = 0.1\n    share_embeddings_and_output_weights: bool = True\n    enable_autocast: bool = False\n    nemo1_ckpt_path: Optional[str] = None\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n\n    @field_serializer(\"biobert_spec_option\")\n    def serialize_spec_option(self, value: BiobertSpecOption) -&gt; str:  # noqa: D102\n        return value.value\n\n    @field_validator(\"biobert_spec_option\", mode=\"before\")\n    def deserialize_spec_option(cls, value: str) -&gt; BiobertSpecOption:  # noqa: D102\n        return BiobertSpecOption(value)\n\n    @field_validator(\"activation_func\", mode=\"before\")\n    @classmethod\n    def validate_activation_func(cls, activation_func: str) -&gt; Callable:\n        \"\"\"Validates the activation function, assumes this function exists in torch.nn.functional.\n\n        For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method\n        validates the provided activation function string and returns a callable function based on the validation\n        context using the provided validator in the base class.\n\n        Args:\n            activation_func (str): The activation function to be validated.\n            context (ValidationInfo): The context for validation.\n\n        Returns:\n            Callable: A callable function after validation.\n\n        See Also:\n            CUSTOM_ACTIVATION_FNS\n        \"\"\"\n        func = getattr(torch.nn.functional, activation_func.lower(), None)\n        if func is None and activation_func in CUSTOM_ACTIVATION_FNS:\n            func = CUSTOM_ACTIVATION_FNS[activation_func]\n            return func\n        elif func is None:\n            raise ValueError(\n                f\"activation_func must be a valid function in `torch.nn.functional`, got {activation_func=}\"\n            )\n        else:\n            return func\n\n    @field_serializer(\"activation_func\")\n    def serialize_activation_func(self, v: Callable[[torch.Tensor, Any], torch.Tensor]) -&gt; str:\n        \"\"\"Serializes a given activation function to its corresponding string representation.\n\n        By default, all activation functions from `torch.nn.functional` are serialized to their name. User defined\n        activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the\n        top of this file. This allows our Pydantic model to serialize and deserialize the activation function.\n\n        Args:\n            v (Callable[[torch.Tensor, Any], torch.Tensor]): The activation function to serialize.\n\n        Returns:\n            str: The name of the activation function if it is a standard PyTorch function,\n                 or the corresponding serialization key if it is a custom activation function.\n\n        Raises:\n            ValueError: If the activation function is not supported.\n        \"\"\"\n        func_name = v.__name__\n        func = getattr(torch.nn.functional, func_name, None)\n        if func is not None:\n            return func_name\n        elif func in REVERSE_CUSTOM_ACTIVATION_FNS:\n            return REVERSE_CUSTOM_ACTIVATION_FNS[func]  # Get the serialization key\n        else:\n            raise ValueError(f\"Unsupported activation function: {v}\")\n\n    @field_validator(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\", mode=\"before\")\n    @classmethod\n    def precision_validator(cls, v: dtypes.PrecisionTypes) -&gt; torch.dtype:\n        \"\"\"Validates the precision type and returns the corresponding torch dtype.\"\"\"\n        return dtypes.get_autocast_dtype(v)\n\n    @field_serializer(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\")\n    def serialize_dtypes(self, v: torch.dtype) -&gt; dtypes.PrecisionTypes:\n        \"\"\"Serializes the torch dtype to the corresponding precision type.\"\"\"\n        return dtypes.dtype_to_precision[v]\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.custom_model_validator","title":"<code>custom_model_validator(global_cfg)</code>","text":"<p>Use custom implementation of this method to define the things inside global_config.</p> <p>The following expression will always be true:</p> <p>global_cfg.bionemo_model_config == self</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n    \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n    The following expression will always be true:\n\n    global_cfg.bionemo_model_config == self\n    \"\"\"\n    return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.exposed_to_internal_bionemo_model_config","title":"<code>exposed_to_internal_bionemo_model_config()</code>","text":"<p>Converts the exposed dataclass to the underlying Transformer config.</p> <p>The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to hide fields that are either not serializable by Pydantic or that we do not want to expose.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def exposed_to_internal_bionemo_model_config(self) -&gt; ModelConfigT:\n    \"\"\"Converts the exposed dataclass to the underlying Transformer config.\n\n    The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to\n    hide fields that are either not serializable by Pydantic or that we do not want to expose.\n    \"\"\"\n    cls: Type[ModelConfigT] = self.model_class()\n    model_dict = {}\n    for attr in self.model_fields:\n        if attr not in model_dict and attr in cls.__dataclass_fields__:\n            model_dict[attr] = getattr(self, attr)\n\n    # Now set fp16 and bf16 based on the precision for the underlying TransformerConfig=&gt;ParallelConfig\n    #   the only constraint is that both must not be true.\n    model_dict[\"bf16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"bf16-mixed\"]\n    model_dict[\"fp16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"16-mixed\"]\n    result = cls(**model_dict)\n\n    return result\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.model_class","title":"<code>model_class()</code>","text":"<p>Returns the underlying model class that this config wraps.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def model_class(self) -&gt; Type[ModelConfigT]:\n    \"\"\"Returns the underlying model class that this config wraps.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.precision_validator","title":"<code>precision_validator(v)</code>  <code>classmethod</code>","text":"<p>Validates the precision type and returns the corresponding torch dtype.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_validator(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\", mode=\"before\")\n@classmethod\ndef precision_validator(cls, v: dtypes.PrecisionTypes) -&gt; torch.dtype:\n    \"\"\"Validates the precision type and returns the corresponding torch dtype.\"\"\"\n    return dtypes.get_autocast_dtype(v)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.serialize_activation_func","title":"<code>serialize_activation_func(v)</code>","text":"<p>Serializes a given activation function to its corresponding string representation.</p> <p>By default, all activation functions from <code>torch.nn.functional</code> are serialized to their name. User defined activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the top of this file. This allows our Pydantic model to serialize and deserialize the activation function.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Callable[[Tensor, Any], Tensor]</code> <p>The activation function to serialize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the activation function if it is a standard PyTorch function,  or the corresponding serialization key if it is a custom activation function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the activation function is not supported.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_serializer(\"activation_func\")\ndef serialize_activation_func(self, v: Callable[[torch.Tensor, Any], torch.Tensor]) -&gt; str:\n    \"\"\"Serializes a given activation function to its corresponding string representation.\n\n    By default, all activation functions from `torch.nn.functional` are serialized to their name. User defined\n    activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the\n    top of this file. This allows our Pydantic model to serialize and deserialize the activation function.\n\n    Args:\n        v (Callable[[torch.Tensor, Any], torch.Tensor]): The activation function to serialize.\n\n    Returns:\n        str: The name of the activation function if it is a standard PyTorch function,\n             or the corresponding serialization key if it is a custom activation function.\n\n    Raises:\n        ValueError: If the activation function is not supported.\n    \"\"\"\n    func_name = v.__name__\n    func = getattr(torch.nn.functional, func_name, None)\n    if func is not None:\n        return func_name\n    elif func in REVERSE_CUSTOM_ACTIVATION_FNS:\n        return REVERSE_CUSTOM_ACTIVATION_FNS[func]  # Get the serialization key\n    else:\n        raise ValueError(f\"Unsupported activation function: {v}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.serialize_dtypes","title":"<code>serialize_dtypes(v)</code>","text":"<p>Serializes the torch dtype to the corresponding precision type.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_serializer(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\")\ndef serialize_dtypes(self, v: torch.dtype) -&gt; dtypes.PrecisionTypes:\n    \"\"\"Serializes the torch dtype to the corresponding precision type.\"\"\"\n    return dtypes.dtype_to_precision[v]\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.validate_activation_func","title":"<code>validate_activation_func(activation_func)</code>  <code>classmethod</code>","text":"<p>Validates the activation function, assumes this function exists in torch.nn.functional.</p> <p>For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method validates the provided activation function string and returns a callable function based on the validation context using the provided validator in the base class.</p> <p>Parameters:</p> Name Type Description Default <code>activation_func</code> <code>str</code> <p>The activation function to be validated.</p> required <code>context</code> <code>ValidationInfo</code> <p>The context for validation.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A callable function after validation.</p> See Also <p>CUSTOM_ACTIVATION_FNS</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_validator(\"activation_func\", mode=\"before\")\n@classmethod\ndef validate_activation_func(cls, activation_func: str) -&gt; Callable:\n    \"\"\"Validates the activation function, assumes this function exists in torch.nn.functional.\n\n    For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method\n    validates the provided activation function string and returns a callable function based on the validation\n    context using the provided validator in the base class.\n\n    Args:\n        activation_func (str): The activation function to be validated.\n        context (ValidationInfo): The context for validation.\n\n    Returns:\n        Callable: A callable function after validation.\n\n    See Also:\n        CUSTOM_ACTIVATION_FNS\n    \"\"\"\n    func = getattr(torch.nn.functional, activation_func.lower(), None)\n    if func is None and activation_func in CUSTOM_ACTIVATION_FNS:\n        func = CUSTOM_ACTIVATION_FNS[activation_func]\n        return func\n    elif func is None:\n        raise ValueError(\n            f\"activation_func must be a valid function in `torch.nn.functional`, got {activation_func=}\"\n        )\n    else:\n        return func\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig","title":"<code>MainConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ExModelConfigT, DataConfigT]</code></p> <p>Main configuration class for BioNeMo. All serialized configs that are a valid MainConfig should be Runnable.</p> <p>This class is used to define the main configuration for BioNeMo. It defines the minimal pieces of configuration to execution a training job with the NeMo2 training api. It accepts two generic type parameters which users must define in their own environment for execution.</p> <p>Additionally, this class assumes that the configs for ExposedModelConfig and DataConfig may have custom validators implemented that operate on the entire MainConfig. This prevents the need from type based conditionals inside this class while still allowing for custom validation global logic to be implemented in the underlying classes. For example, some models may want to restrict their Datamodules seq_length to a certain value.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <p>Generic config type that contains instructions on instantiating the required DataModule.</p> required <code>parallel_config</code> <p>The parallel configuration for the model.</p> required <code>training_config</code> <p>The training configuration for the model.</p> required <code>bionemo_model_config</code> <p>Generic ExposedModelConfig type. This class hides extra configuration parameters in the underlying model configuration as well as providing</p> required <code>optim_config</code> <p>The optimizer/scheduler configuration for the model.</p> required <code>experiment_config</code> <p>The experiment configuration for the model.</p> required <code>wandb_config</code> <p>Optional, the wandb configuration for the model.</p> required Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class MainConfig(BaseModel, Generic[ExModelConfigT, DataConfigT]):\n    \"\"\"Main configuration class for BioNeMo. All serialized configs that are a valid MainConfig should be Runnable.\n\n    This class is used to define the main configuration for BioNeMo. It defines the minimal pieces of configuration\n    to execution a training job with the NeMo2 training api. It accepts two generic type parameters which users\n    must define in their own environment for execution.\n\n    Additionally, this class assumes that the configs for ExposedModelConfig and DataConfig may have custom validators\n    implemented that operate on the entire MainConfig. This prevents the need from type based conditionals inside this\n    class while still allowing for custom validation global logic to be implemented in the underlying classes. For example,\n    some models may want to restrict their Datamodules seq_length to a certain value.\n\n\n    Args:\n        data_config: Generic config type that contains instructions on instantiating the required DataModule.\n        parallel_config: The parallel configuration for the model.\n        training_config: The training configuration for the model.\n        bionemo_model_config: Generic ExposedModelConfig type. This class hides extra configuration parameters in the\n            underlying model configuration as well as providing\n        optim_config: The optimizer/scheduler configuration for the model.\n        experiment_config: The experiment configuration for the model.\n        wandb_config: Optional, the wandb configuration for the model.\n    \"\"\"\n\n    data_config: DataConfigT\n    parallel_config: ParallelConfig\n    training_config: TrainingConfig\n    bionemo_model_config: ExModelConfigT\n    optim_config: OptimizerSchedulerConfig\n    experiment_config: ExperimentConfig\n    wandb_config: Optional[WandbConfig] = None\n\n    @model_validator(mode=\"after\")\n    def validate_master_config(self) -&gt; \"MainConfig\":\n        \"\"\"Validates the master configuration object.\"\"\"\n        self.bionemo_model_config.seq_length = self.data_config.seq_length\n        return self\n\n    @model_validator(mode=\"after\")\n    def run_bionemo_model_config_model_validators(self) -&gt; \"MainConfig\":\n        \"\"\"Runs the model validators on the bionemo_model_config.\"\"\"\n        return self.bionemo_model_config.custom_model_validator(self)\n\n    @model_validator(mode=\"after\")\n    def run_data_config_model_validators(self) -&gt; \"MainConfig\":\n        \"\"\"Runs the model validators on the data_config.\"\"\"\n        return self.data_config.custom_model_validator(self)\n\n    @model_validator(mode=\"after\")\n    def validate_checkpointing_setting(self) -&gt; \"MainConfig\":\n        \"\"\"Validates the master configuration object.\"\"\"\n        self.training_config.enable_checkpointing = self.experiment_config.create_checkpoint_callback\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.run_bionemo_model_config_model_validators","title":"<code>run_bionemo_model_config_model_validators()</code>","text":"<p>Runs the model validators on the bionemo_model_config.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef run_bionemo_model_config_model_validators(self) -&gt; \"MainConfig\":\n    \"\"\"Runs the model validators on the bionemo_model_config.\"\"\"\n    return self.bionemo_model_config.custom_model_validator(self)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.run_data_config_model_validators","title":"<code>run_data_config_model_validators()</code>","text":"<p>Runs the model validators on the data_config.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef run_data_config_model_validators(self) -&gt; \"MainConfig\":\n    \"\"\"Runs the model validators on the data_config.\"\"\"\n    return self.data_config.custom_model_validator(self)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.validate_checkpointing_setting","title":"<code>validate_checkpointing_setting()</code>","text":"<p>Validates the master configuration object.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_checkpointing_setting(self) -&gt; \"MainConfig\":\n    \"\"\"Validates the master configuration object.\"\"\"\n    self.training_config.enable_checkpointing = self.experiment_config.create_checkpoint_callback\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.validate_master_config","title":"<code>validate_master_config()</code>","text":"<p>Validates the master configuration object.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_master_config(self) -&gt; \"MainConfig\":\n    \"\"\"Validates the master configuration object.\"\"\"\n    self.bionemo_model_config.seq_length = self.data_config.seq_length\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.OptimizerSchedulerConfig","title":"<code>OptimizerSchedulerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the optimizer and learning rate scheduler.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>Learning rate for the optimizer. Default is 1e-4.</p> <code>optimizer</code> <code>str</code> <p>Type of optimizer to use. Default is \"adam\".</p> <code>interval</code> <code>str</code> <p>Interval for updating the learning rate scheduler. Default is \"step\".</p> <code>monitor</code> <code>str</code> <p>Metric to monitor for learning rate adjustments. Default is \"val_loss\".</p> <code>interval</code> <code>str</code> <p>Interval for updating the learning rate scheduler. Default is \"step\".</p> <code>monitor</code> <code>str</code> <p>Metric to monitor for learning rate adjustments. Default is \"val_loss\".</p> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps for use with the warmup annealing learning rate scheduler. Default is 0.</p> <code>lr_scheduler</code> <code>Literal['warmup_anneal', 'cosine']</code> <p>Type of learning rate scheduler to use. Default is 'warmup_anneal'. NOTE this is likely to change.</p> <code>max_steps</code> <code>Optional[int]</code> <p>max_steps used in optimizer. Default to None which uses max_steps from TrainingConfig.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class OptimizerSchedulerConfig(BaseModel):\n    \"\"\"Configuration for the optimizer and learning rate scheduler.\n\n    Attributes:\n        lr (float): Learning rate for the optimizer. Default is 1e-4.\n        optimizer (str): Type of optimizer to use. Default is \"adam\".\n        interval (str): Interval for updating the learning rate scheduler. Default is \"step\".\n        monitor (str): Metric to monitor for learning rate adjustments. Default is \"val_loss\".\n        interval (str): Interval for updating the learning rate scheduler. Default is \"step\".\n        monitor (str): Metric to monitor for learning rate adjustments. Default is \"val_loss\".\n        warmup_steps (int): Number of warmup steps for use with the warmup annealing learning rate scheduler. Default is 0.\n        lr_scheduler (Literal['warmup_anneal', 'cosine']): Type of learning rate scheduler to use. Default is 'warmup_anneal'. NOTE this is likely to change.\n        max_steps (Optional[int]): max_steps used in optimizer. Default to None which uses max_steps from TrainingConfig.\n    \"\"\"\n\n    lr: float = 1e-4\n    optimizer: str = \"adam\"\n    interval: str = \"step\"\n    monitor: str = \"val_loss\"\n    cosine_rampup_frac: float = 0.01\n    cosine_hold_frac: float = 0.05\n    warmup_steps: int = 0\n    lr_scheduler: Literal[\"warmup_anneal\", \"cosine\"] = \"warmup_anneal\"\n    max_steps: Optional[int] = None\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ParallelConfig","title":"<code>ParallelConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>ParallelConfig is a configuration class for setting up parallelism in model training.</p> <p>Attributes:</p> Name Type Description <code>tensor_model_parallel_size</code> <code>int</code> <p>The size of the tensor model parallelism. Default is 1.</p> <code>pipeline_model_parallel_size</code> <code>int</code> <p>The size of the pipeline model parallelism. Default is 1.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>The number of batches to accumulate gradients over. Default is 1.</p> <code>ddp</code> <code>Literal['megatron']</code> <p>The distributed data parallel method to use. Default is \"megatron\".</p> <code>remove_unused_parameters</code> <code>bool</code> <p>Whether to remove unused parameters. Default is True.</p> <code>num_devices</code> <code>int</code> <p>The number of devices to use. Default is 1.</p> <code>num_nodes</code> <code>int</code> <p>The number of nodes to use. Default is 1.</p> <p>Methods:</p> Name Description <code>validate_devices</code> <p>Validates the number of devices based on the tensor and pipeline model parallel sizes.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ParallelConfig(BaseModel):\n    \"\"\"ParallelConfig is a configuration class for setting up parallelism in model training.\n\n    Attributes:\n        tensor_model_parallel_size (int): The size of the tensor model parallelism. Default is 1.\n        pipeline_model_parallel_size (int): The size of the pipeline model parallelism. Default is 1.\n        accumulate_grad_batches (int): The number of batches to accumulate gradients over. Default is 1.\n        ddp (Literal[\"megatron\"]): The distributed data parallel method to use. Default is \"megatron\".\n        remove_unused_parameters (bool): Whether to remove unused parameters. Default is True.\n        num_devices (int): The number of devices to use. Default is 1.\n        num_nodes (int): The number of nodes to use. Default is 1.\n\n    Methods:\n        validate_devices(): Validates the number of devices based on the tensor and pipeline model parallel sizes.\n    \"\"\"\n\n    tensor_model_parallel_size: int = 1\n    pipeline_model_parallel_size: int = 1\n    accumulate_grad_batches: int = 1\n    ddp: Literal[\"megatron\"] = \"megatron\"\n    remove_unused_parameters: bool = True\n    num_devices: int = 1\n    num_nodes: int = 1\n\n    @model_validator(mode=\"after\")\n    def validate_devices(self):\n        \"\"\"Validates the number of devices based on the tensor and pipeline model parallel sizes.\"\"\"\n        if self.num_devices &lt; self.tensor_model_parallel_size * self.pipeline_model_parallel_size:\n            raise ValueError(\"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\")\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ParallelConfig.validate_devices","title":"<code>validate_devices()</code>","text":"<p>Validates the number of devices based on the tensor and pipeline model parallel sizes.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_devices(self):\n    \"\"\"Validates the number of devices based on the tensor and pipeline model parallel sizes.\"\"\"\n    if self.num_devices &lt; self.tensor_model_parallel_size * self.pipeline_model_parallel_size:\n        raise ValueError(\"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\")\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.TrainingConfig","title":"<code>TrainingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TrainingConfig is a configuration class for training models.</p> <p>Attributes:</p> Name Type Description <code>max_steps</code> <code>int</code> <p>The maximum number of training steps.</p> <code>limit_val_batches</code> <code>int | float</code> <p>The number of validation batches to use. Can be a fraction or a count.</p> <code>val_check_interval</code> <code>int</code> <p>The interval (in steps) at which to check validation.</p> <code>precision</code> <code>Literal['32', 'bf16-mixed', '16-mixed']</code> <p>The precision to use for training. Defaults to \"bf16-mixed\".</p> <code>accelerator</code> <code>str</code> <p>The type of accelerator to use for training. Defaults to \"gpu\".</p> <code>gc_interval</code> <code>int</code> <p>The interval of global steps at which to run synchronized garbage collection. Useful for synchronizing garbage collection when performing distributed training. Defaults to 0.</p> <code>include_perplexity</code> <code>bool</code> <p>Whether to include perplexity in the validation logs. Defaults to False.</p> <code>enable_checkpointing</code> <code>bool</code> <p>Whether to enable checkpointing and configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint. Corresponds to the same parameter name in pl.Trainer</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"TrainingConfig is a configuration class for training models.\n\n    Attributes:\n        max_steps (int): The maximum number of training steps.\n        limit_val_batches (int | float): The number of validation batches to use. Can be a fraction or a count.\n        val_check_interval (int): The interval (in steps) at which to check validation.\n        precision (Literal[\"32\", \"bf16-mixed\", \"16-mixed\"], optional): The precision to use for training. Defaults to \"bf16-mixed\".\n        accelerator (str, optional): The type of accelerator to use for training. Defaults to \"gpu\".\n        gc_interval (int, optional): The interval of global steps at which to run synchronized garbage collection. Useful for synchronizing garbage collection when performing distributed training. Defaults to 0.\n        include_perplexity (bool, optional): Whether to include perplexity in the validation logs. Defaults to False.\n        enable_checkpointing (bool, optional): Whether to enable checkpointing and configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint. Corresponds to the same parameter name in pl.Trainer\n    \"\"\"\n\n    max_steps: int\n    limit_val_batches: int | float  # Because this can be a fraction or a count...\n    val_check_interval: int\n    precision: Literal[\"32\", \"bf16-mixed\", \"16-mixed\"] = \"bf16-mixed\"\n    accelerator: str = \"gpu\"\n    # NOTE: VERY important for distributed training performance.\n    gc_interval: int = 0\n    log_train_ppl: bool = False\n    log_val_ppl: bool = True\n    enable_checkpointing: bool = True\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.deserialize_str_to_path","title":"<code>deserialize_str_to_path(path)</code>","text":"<p>General purpose deserialize for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_validator.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def deserialize_str_to_path(path: str) -&gt; pathlib.Path:\n    \"\"\"General purpose deserialize for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_validator.\"\"\"\n    return pathlib.Path(path)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.serialize_path_or_str","title":"<code>serialize_path_or_str(path)</code>","text":"<p>General purpose serialization for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_serializer.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def serialize_path_or_str(path: str | pathlib.Path) -&gt; str:\n    \"\"\"General purpose serialization for string/path objects. Since YAML has no native representation for pathlib.Path, we serialize to strings. Import this method as a @field_serializer.\"\"\"\n    if isinstance(path, pathlib.Path):\n        return str(path)\n    elif isinstance(path, str):\n        return path\n    else:\n        raise ValueError(f\"Expected str or pathlib.Path, got {type(path)}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/callbacks/","title":"Callbacks","text":""},{"location":"API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter","title":"<code>PredictionWriter</code>","text":"<p>               Bases: <code>BasePredictionWriter</code>, <code>Callback</code></p> <p>A callback that writes predictions to disk at specified intervals during training.</p> Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>class PredictionWriter(BasePredictionWriter, pl.Callback):\n    \"\"\"A callback that writes predictions to disk at specified intervals during training.\"\"\"\n\n    def __init__(\n        self,\n        output_dir: str | os.PathLike,\n        write_interval: IntervalT,\n        batch_dim_key_defaults: dict[str, int] | None = None,\n        seq_dim_key_defaults: dict[str, int] | None = None,\n    ):\n        \"\"\"Initializes the callback.\n\n        Args:\n            output_dir: The directory where predictions will be written.\n            write_interval: The interval at which predictions will be written. (batch, epoch)\n            batch_dim_key_defaults: The default batch dimension for each key, if different from the standard 0.\n            seq_dim_key_defaults: The default sequence dimension for each key, if different from the standard 1.\n        \"\"\"\n        super().__init__(write_interval)\n        self.output_dir = str(output_dir)\n        self.batch_dim_key_defaults = batch_dim_key_defaults\n        self.seq_dim_key_defaults = seq_dim_key_defaults\n\n    def write_on_batch_end(\n        self,\n        trainer: pl.Trainer,\n        pl_module: pl.LightningModule,\n        prediction: Any,\n        batch_indices: Sequence[int],\n        batch: Any,\n        batch_idx: int,\n        dataloader_idx: int,\n    ) -&gt; None:\n        \"\"\"Writes predictions to disk at the end of each batch.\n\n        Args:\n            trainer: The Trainer instance.\n            pl_module: The LightningModule instance.\n            prediction: The prediction made by the model.\n            batch_indices: The indices of the batch.\n            batch: The batch data.\n            batch_idx: The index of the batch.\n            dataloader_idx: The index of the dataloader.\n        \"\"\"\n        # this will create N (num processes) files in `output_dir` each containing\n        # the predictions of it's respective rank\n        result_path = os.path.join(self.output_dir, f\"predictions__rank_{trainer.global_rank}__batch_{batch_idx}.pt\")\n\n        # batch_indices is not captured due to a lightning bug when return_predictions = False\n        # we use input IDs in the prediction to map the result to input\n        torch.save(prediction, result_path)\n        logging.info(f\"Inference predictions are stored in {result_path}\\n{prediction.keys()}\")\n\n    def write_on_epoch_end(\n        self,\n        trainer: pl.Trainer,\n        pl_module: pl.LightningModule,\n        predictions: Any,\n        batch_indices: Sequence[int],\n    ) -&gt; None:\n        \"\"\"Writes predictions to disk at the end of each epoch.\n\n        Args:\n            trainer: The Trainer instance.\n            pl_module: The LightningModule instance.\n            predictions: The predictions made by the model.\n            batch_indices: The indices of the batch.\n        \"\"\"\n        # this will create N (num processes) files in `output_dir` each containing\n        # the predictions of it's respective rank\n        result_path = os.path.join(self.output_dir, f\"predictions__rank_{trainer.global_rank}.pt\")\n\n        # collate multiple batches / ignore empty ones\n        collate_kwargs = {}\n        if self.batch_dim_key_defaults is not None:\n            collate_kwargs[\"batch_dim_key_defaults\"] = self.batch_dim_key_defaults\n        if self.seq_dim_key_defaults is not None:\n            collate_kwargs[\"seq_dim_key_defaults\"] = self.seq_dim_key_defaults\n        prediction = batch_collator([item for item in predictions if item is not None], **collate_kwargs)\n\n        # batch_indices is not captured due to a lightning bug when return_predictions = False\n        # we use input IDs in the prediction to map the result to input\n        torch.save(prediction, result_path)\n        if isinstance(prediction, dict):\n            keys = prediction.keys()\n        else:\n            keys = \"tensor\"\n        logging.info(f\"Inference predictions are stored in {result_path}\\n{keys}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.__init__","title":"<code>__init__(output_dir, write_interval, batch_dim_key_defaults=None, seq_dim_key_defaults=None)</code>","text":"<p>Initializes the callback.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | PathLike</code> <p>The directory where predictions will be written.</p> required <code>write_interval</code> <code>IntervalT</code> <p>The interval at which predictions will be written. (batch, epoch)</p> required <code>batch_dim_key_defaults</code> <code>dict[str, int] | None</code> <p>The default batch dimension for each key, if different from the standard 0.</p> <code>None</code> <code>seq_dim_key_defaults</code> <code>dict[str, int] | None</code> <p>The default sequence dimension for each key, if different from the standard 1.</p> <code>None</code> Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str | os.PathLike,\n    write_interval: IntervalT,\n    batch_dim_key_defaults: dict[str, int] | None = None,\n    seq_dim_key_defaults: dict[str, int] | None = None,\n):\n    \"\"\"Initializes the callback.\n\n    Args:\n        output_dir: The directory where predictions will be written.\n        write_interval: The interval at which predictions will be written. (batch, epoch)\n        batch_dim_key_defaults: The default batch dimension for each key, if different from the standard 0.\n        seq_dim_key_defaults: The default sequence dimension for each key, if different from the standard 1.\n    \"\"\"\n    super().__init__(write_interval)\n    self.output_dir = str(output_dir)\n    self.batch_dim_key_defaults = batch_dim_key_defaults\n    self.seq_dim_key_defaults = seq_dim_key_defaults\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.write_on_batch_end","title":"<code>write_on_batch_end(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx)</code>","text":"<p>Writes predictions to disk at the end of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Trainer instance.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule instance.</p> required <code>prediction</code> <code>Any</code> <p>The prediction made by the model.</p> required <code>batch_indices</code> <code>Sequence[int]</code> <p>The indices of the batch.</p> required <code>batch</code> <code>Any</code> <p>The batch data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the dataloader.</p> required Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>def write_on_batch_end(\n    self,\n    trainer: pl.Trainer,\n    pl_module: pl.LightningModule,\n    prediction: Any,\n    batch_indices: Sequence[int],\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int,\n) -&gt; None:\n    \"\"\"Writes predictions to disk at the end of each batch.\n\n    Args:\n        trainer: The Trainer instance.\n        pl_module: The LightningModule instance.\n        prediction: The prediction made by the model.\n        batch_indices: The indices of the batch.\n        batch: The batch data.\n        batch_idx: The index of the batch.\n        dataloader_idx: The index of the dataloader.\n    \"\"\"\n    # this will create N (num processes) files in `output_dir` each containing\n    # the predictions of it's respective rank\n    result_path = os.path.join(self.output_dir, f\"predictions__rank_{trainer.global_rank}__batch_{batch_idx}.pt\")\n\n    # batch_indices is not captured due to a lightning bug when return_predictions = False\n    # we use input IDs in the prediction to map the result to input\n    torch.save(prediction, result_path)\n    logging.info(f\"Inference predictions are stored in {result_path}\\n{prediction.keys()}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/callbacks/#bionemo.llm.utils.callbacks.PredictionWriter.write_on_epoch_end","title":"<code>write_on_epoch_end(trainer, pl_module, predictions, batch_indices)</code>","text":"<p>Writes predictions to disk at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Trainer instance.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule instance.</p> required <code>predictions</code> <code>Any</code> <p>The predictions made by the model.</p> required <code>batch_indices</code> <code>Sequence[int]</code> <p>The indices of the batch.</p> required Source code in <code>bionemo/llm/utils/callbacks.py</code> <pre><code>def write_on_epoch_end(\n    self,\n    trainer: pl.Trainer,\n    pl_module: pl.LightningModule,\n    predictions: Any,\n    batch_indices: Sequence[int],\n) -&gt; None:\n    \"\"\"Writes predictions to disk at the end of each epoch.\n\n    Args:\n        trainer: The Trainer instance.\n        pl_module: The LightningModule instance.\n        predictions: The predictions made by the model.\n        batch_indices: The indices of the batch.\n    \"\"\"\n    # this will create N (num processes) files in `output_dir` each containing\n    # the predictions of it's respective rank\n    result_path = os.path.join(self.output_dir, f\"predictions__rank_{trainer.global_rank}.pt\")\n\n    # collate multiple batches / ignore empty ones\n    collate_kwargs = {}\n    if self.batch_dim_key_defaults is not None:\n        collate_kwargs[\"batch_dim_key_defaults\"] = self.batch_dim_key_defaults\n    if self.seq_dim_key_defaults is not None:\n        collate_kwargs[\"seq_dim_key_defaults\"] = self.seq_dim_key_defaults\n    prediction = batch_collator([item for item in predictions if item is not None], **collate_kwargs)\n\n    # batch_indices is not captured due to a lightning bug when return_predictions = False\n    # we use input IDs in the prediction to map the result to input\n    torch.save(prediction, result_path)\n    if isinstance(prediction, dict):\n        keys = prediction.keys()\n    else:\n        keys = \"tensor\"\n    logging.info(f\"Inference predictions are stored in {result_path}\\n{keys}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/","title":"Datamodule utils","text":""},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.float_or_int_or_none","title":"<code>float_or_int_or_none(value)</code>","text":"<p>Converts a given value into a float, int, or None.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, float, int, None]</code> <p>A value that can be either a string, float, int, or None.</p> required <p>Returns:</p> Type Description <code>Union[float, int, None]</code> <p>Union[float, int, None]: A float, int, or None based on the input value.</p> <p>If the input value is None or \"None\", it returns None. If the input value is an int or float, it returns the same value. If the input value is a string, it tries to convert it into an int if possible, otherwise into a float.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def float_or_int_or_none(value: Union[str, float, int, None]) -&gt; Union[float, int, None]:\n    \"\"\"Converts a given value into a float, int, or None.\n\n    Args:\n        value (Union[str, float, int, None]): A value that can be either a string, float, int, or None.\n\n    Returns:\n        Union[float, int, None]: A float, int, or None based on the input value.\n\n    If the input value is None or \"None\", it returns None.\n    If the input value is an int or float, it returns the same value.\n    If the input value is a string, it tries to convert it into an int if possible, otherwise into a float.\n    \"\"\"\n    if value is None or value == \"None\":\n        return\n    if isinstance(value, (int, float)):\n        return value\n    if value.isdigit():\n        return int(value)\n    return float(value)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.infer_global_batch_size","title":"<code>infer_global_batch_size(micro_batch_size, num_nodes, devices, accumulate_grad_batches=1, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, context_model_parallel_size=1)</code>","text":"<p>Infers the global batch size based on the micro batch size, number of nodes, devices, accumulation of gradient batches, and model parallel sizes.</p> <p>Parameters:</p> Name Type Description Default <code>micro_batch_size</code> <code>int</code> <p>The micro batch size.</p> required <code>num_nodes</code> <code>int</code> <p>The number of nodes.</p> required <code>devices</code> <code>int</code> <p>The number of devices.</p> required <code>accumulate_grad_batches</code> <code>int</code> <p>The accumulation of gradient batches. Defaults to 1.</p> <code>1</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>The tensor model parallel size. Defaults to 1.</p> <code>1</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>The pipeline model parallel size. Defaults to 1.</p> <code>1</code> <code>context_model_parallel_size</code> <code>int</code> <p>The context model parallel size. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The global batch size.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def infer_global_batch_size(\n    micro_batch_size: int,\n    num_nodes: int,\n    devices: int,\n    accumulate_grad_batches: int = 1,\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    context_model_parallel_size: int = 1,\n) -&gt; int:\n    \"\"\"Infers the global batch size based on the micro batch size, number of nodes, devices, accumulation of gradient batches, and model parallel sizes.\n\n    Args:\n        micro_batch_size (int): The micro batch size.\n        num_nodes (int): The number of nodes.\n        devices (int): The number of devices.\n        accumulate_grad_batches (int): The accumulation of gradient batches. Defaults to 1.\n        tensor_model_parallel_size (int): The tensor model parallel size. Defaults to 1.\n        pipeline_model_parallel_size (int): The pipeline model parallel size. Defaults to 1.\n        context_model_parallel_size (int): The context model parallel size. Defaults to 1.\n\n    Returns:\n        int: The global batch size.\n    \"\"\"\n    if not all(\n        isinstance(arg, int)\n        for arg in [\n            micro_batch_size,\n            num_nodes,\n            devices,\n            accumulate_grad_batches,\n            tensor_model_parallel_size,\n            pipeline_model_parallel_size,\n            context_model_parallel_size,\n        ]\n    ):\n        raise ValueError(\n            f\"All arguments must be of type int, got {type(micro_batch_size)}, {type(num_nodes)}, {type(devices)}, \"\n            f\"{type(accumulate_grad_batches)}, {type(tensor_model_parallel_size)}, {type(pipeline_model_parallel_size)}, and {type(context_model_parallel_size)}\"\n        )\n    if micro_batch_size &lt;= 0:\n        raise ValueError(f\"micro_batch_size must be greater than 0, got {micro_batch_size}\")\n    if num_nodes &lt;= 0:\n        raise ValueError(f\"num_nodes must be greater than 0, got {num_nodes}\")\n    if devices &lt;= 0:\n        raise ValueError(f\"devices must be greater than 0, got {devices}\")\n    if accumulate_grad_batches &lt;= 0:\n        raise ValueError(f\"accumulate_grad_batches must be greater than 0, got {accumulate_grad_batches}\")\n    if tensor_model_parallel_size &lt;= 0:\n        raise ValueError(f\"tensor_model_parallel_size must be greater than 0, got {tensor_model_parallel_size}\")\n    if pipeline_model_parallel_size &lt;= 0:\n        raise ValueError(f\"pipeline_model_parallel_size must be greater than 0, got {pipeline_model_parallel_size}\")\n    if context_model_parallel_size &lt;= 0:\n        raise ValueError(f\"context_model_parallel_size must be greater than 0, got {context_model_parallel_size}\")\n\n    world_size = num_nodes * devices\n    if world_size % (tensor_model_parallel_size * pipeline_model_parallel_size * context_model_parallel_size) != 0:\n        raise ValueError(\n            f\"world_size must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size * context_model_parallel_size, \"\n            f\"got {world_size} and TP{tensor_model_parallel_size} * PP{pipeline_model_parallel_size} * CP{context_model_parallel_size}\"\n        )\n\n    model_parallel_size = tensor_model_parallel_size * pipeline_model_parallel_size * context_model_parallel_size\n    data_parallel_size = world_size // model_parallel_size\n    global_batch_size = micro_batch_size * data_parallel_size * accumulate_grad_batches\n    return global_batch_size\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.infer_num_samples","title":"<code>infer_num_samples(limit_batches, num_samples_in_dataset, global_batch_size, stage)</code>","text":"<p>Infers the number of samples based on the limit_batches parameter, the length of the dataset, and the global batch size.</p> <p>Parameters:</p> Name Type Description Default <code>limit_batches</code> <code>Union[float, int, str, None]</code> <p>The limit on the number of batches. Can be a float between 0 and 1, an integer, a string, or None. If None, defaults to 1.0.</p> required <code>num_samples_in_dataset</code> <code>int</code> <p>The number of samples in the dataset.</p> required <code>global_batch_size</code> <code>int</code> <p>The global batch size.</p> required <code>stage</code> <code>str</code> <p>The stage of the training.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of samples from the limit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the limited number of samples is less than the global batch size, or if the limit_batches parameter is invalid.</p> <p>If limit_batches is a float between 0 and 1, the number of samples is inferred as a fraction of the number of samples in the dataset. If limit_batches is an integer greater than or equal to 1, the number of limited samples is inferred as the product of limit_batches and global batch size. If limit_batches is None, it defaults to 1.0, indicating that all dataset samples should be used.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def infer_num_samples(\n    limit_batches: Union[float, int, str, None], num_samples_in_dataset: int, global_batch_size: int, stage: str\n):\n    \"\"\"Infers the number of samples based on the limit_batches parameter, the length of the dataset, and the global batch size.\n\n    Args:\n        limit_batches (Union[float, int, str, None]): The limit on the number of batches. Can be a float\n            between 0 and 1, an integer, a string, or None. If None, defaults to 1.0.\n        num_samples_in_dataset (int): The number of samples in the dataset.\n        global_batch_size (int): The global batch size.\n        stage (str): The stage of the training.\n\n    Returns:\n        int: The number of samples from the limit.\n\n    Raises:\n        ValueError: If the limited number of samples is less than the global batch size, or if the\n            limit_batches parameter is invalid.\n\n    If limit_batches is a float between 0 and 1, the number of samples is inferred as a fraction of the number of samples\n    in the dataset. If limit_batches is an integer greater than or equal to 1, the number of limited samples is inferred\n    as the product of limit_batches and global batch size. If limit_batches is None, it defaults to 1.0, indicating that\n    all dataset samples should be used.\n    \"\"\"\n    limit_batches = 1.0 if limit_batches is None else limit_batches  # validation data does not require upsampling\n    if 0 &lt; limit_batches &lt;= 1.0 and isinstance(limit_batches, float):\n        num_limited_samples = int(num_samples_in_dataset * limit_batches)\n        if num_limited_samples &lt; global_batch_size:\n            raise ValueError(\n                \"The limited number of %s samples %s is less than the global batch size %s\"\n                % (stage, num_limited_samples, global_batch_size)\n            )\n    elif limit_batches &gt;= 1 and isinstance(limit_batches, int):\n        num_limited_samples = int(limit_batches * global_batch_size)\n    else:\n        raise ValueError(\"Invalid choice of limit_%s_batches size: %s\" % (stage, limit_batches))\n\n    return num_limited_samples\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.parse_kwargs_to_arglist","title":"<code>parse_kwargs_to_arglist(kwargs)</code>","text":"<p>Converts a dictionary of keyword arguments into a list of command-line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>A dictionary where keys are argument names and values are argument values.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings, where each string is a command-line argument in the format '--argument-name value'.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def parse_kwargs_to_arglist(kwargs: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Converts a dictionary of keyword arguments into a list of command-line arguments.\n\n    Args:\n        kwargs (Dict[str, Any]): A dictionary where keys are argument names and values are argument values.\n\n    Returns:\n        A list of strings, where each string is a command-line argument in the format '--argument-name value'.\n    \"\"\"\n    arglist = []\n    for k, v in kwargs.items():\n        arglist.extend([f\"--{k.replace('_', '-')}\", str(v)])\n    return arglist\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/","title":"Iomixin utils","text":""},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters","title":"<code>IOMixinWithGettersSetters</code>","text":"<p>               Bases: <code>WillHaveGetSetHparam</code>, <code>IOMixin</code></p> <p>An implementation of WillHaveGetSetHparam which makes use of the io.IOMixin.io added to your classes.</p> <p>This enables you to mutate the hyper-parameters of your classes which will later be saved in configs.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>class IOMixinWithGettersSetters(WillHaveGetSetHparam, io.IOMixin):\n    \"\"\"An implementation of WillHaveGetSetHparam which makes use of the io.IOMixin.__io__ added to your classes.\n\n    This enables you to mutate the hyper-parameters of your classes which will later be saved in configs.\n    \"\"\"\n\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n        If you would like to only change the saved hyper-param\n            for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n            entities by deterministic rules after init, then use `also_change_value=False` to only update the\n            hyper-parameter.\n\n        Args:\n            attribute: The element name to modify within the saved init settings for self\n            value: New parameter for the saved init settings\n            also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n                value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n                do not set this and modify the self attribute separately in the normal pythonic way.\n\n        Returns:\n            None.\n        \"\"\"\n        # Change the attribute of self and also change the io tracker so it gets updated in the config\n        if also_change_value:\n            setattr(self, attribute, value)\n        setattr(self.__io__, attribute, value)\n\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n        Args:\n            attribute: The element name to look up within the saved init settings for self\n        Returns:\n            Value\n        Raises:\n            KeyError if the attribute does not exist in the saved init settings.\n        \"\"\"\n        if attribute not in dir(self.__io__):\n            raise KeyError(\n                f\"Attribute '{attribute}' not found in hyper-parameters. Options: {sorted(self.get_hparams().keys())}\"\n            )\n        return getattr(self.__io__, attribute)\n\n    def get_non_default_hparams(self) -&gt; List[str]:\n        \"\"\"Returns a list of hyper-parameters that have been changed from their default values.\n\n        Returns:\n            List[str]: A list of hyper-parameters that have been changed from their default values.\n        \"\"\"\n        return [k for k in self.__io__.__dict__[\"__argument_history__\"].keys() if k != \"__fn_or_cls__\"]\n\n    def get_hparams(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n        Returns:\n            Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n        \"\"\"\n        return {k: getattr(self.__io__, k) for k in self.get_non_default_hparams()}\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_hparam","title":"<code>get_hparam(attribute)</code>","text":"<p>Looks up the saved hyper-parameter for the io mixed class.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to look up within the saved init settings for self</p> required <p>Returns:     Value Raises:     KeyError if the attribute does not exist in the saved init settings.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n    Args:\n        attribute: The element name to look up within the saved init settings for self\n    Returns:\n        Value\n    Raises:\n        KeyError if the attribute does not exist in the saved init settings.\n    \"\"\"\n    if attribute not in dir(self.__io__):\n        raise KeyError(\n            f\"Attribute '{attribute}' not found in hyper-parameters. Options: {sorted(self.get_hparams().keys())}\"\n        )\n    return getattr(self.__io__, attribute)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_hparams","title":"<code>get_hparams()</code>","text":"<p>Returns the hyper-parameters of init in a dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of the init hyper-parameters on this object.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_hparams(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n    Returns:\n        Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n    \"\"\"\n    return {k: getattr(self.__io__, k) for k in self.get_non_default_hparams()}\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_non_default_hparams","title":"<code>get_non_default_hparams()</code>","text":"<p>Returns a list of hyper-parameters that have been changed from their default values.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of hyper-parameters that have been changed from their default values.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_non_default_hparams(self) -&gt; List[str]:\n    \"\"\"Returns a list of hyper-parameters that have been changed from their default values.\n\n    Returns:\n        List[str]: A list of hyper-parameters that have been changed from their default values.\n    \"\"\"\n    return [k for k in self.__io__.__dict__[\"__argument_history__\"].keys() if k != \"__fn_or_cls__\"]\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>","text":"<p>Mutates the saved hyper-parameter for the io mixed class.</p> <p>If you would like to only change the saved hyper-param     for example in the case of loading a dataclass where the same variables are mutated to other non-savable     entities by deterministic rules after init, then use <code>also_change_value=False</code> to only update the     hyper-parameter.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to modify within the saved init settings for self</p> required <code>value</code> <code>Any</code> <p>New parameter for the saved init settings</p> required <code>also_change_value</code> <code>bool</code> <p>If you also want to mutate the attribute of this same name in self to be the desired value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then do not set this and modify the self attribute separately in the normal pythonic way.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n    If you would like to only change the saved hyper-param\n        for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n        entities by deterministic rules after init, then use `also_change_value=False` to only update the\n        hyper-parameter.\n\n    Args:\n        attribute: The element name to modify within the saved init settings for self\n        value: New parameter for the saved init settings\n        also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n            value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n            do not set this and modify the self attribute separately in the normal pythonic way.\n\n    Returns:\n        None.\n    \"\"\"\n    # Change the attribute of self and also change the io tracker so it gets updated in the config\n    if also_change_value:\n        setattr(self, attribute, value)\n    setattr(self.__io__, attribute, value)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam","title":"<code>WillHaveGetSetHparam</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An ABC that states that a particular class will have our mutatable IO Mixin variant added to it.</p> <p>This is a placeholder until a similar piece of functionality is added in NeMo.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>You must implement set_hparam, get_hparam, and get_hparams</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>class WillHaveGetSetHparam(ABC):\n    \"\"\"An ABC that states that a particular class _will_ have our mutatable IO Mixin variant added to it.\n\n    This is a placeholder until a similar piece of functionality is added in NeMo.\n\n\n    Raises:\n        NotImplementedError: You must implement set_hparam, get_hparam, and get_hparams\n    \"\"\"\n\n    @abstractmethod\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n        If you would like to only change the saved hyper-param\n            for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n            entities by deterministic rules after init, then use `also_change_value=False` to only update the\n            hyper-parameter.\n\n        Args:\n            attribute: The element name to modify within the saved init settings for self\n            value: New parameter for the saved init settings\n            also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n                value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n                do not set this and modify the self attribute separately in the normal pythonic way.\n\n        Returns:\n            None.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n        Args:\n            attribute: The element name to look up within the saved init settings for self\n        Returns:\n            Value\n        Raises:\n            KeyError if the attribute does not exist in the saved init settings.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_hparams(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n        Returns:\n            Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.get_hparam","title":"<code>get_hparam(attribute)</code>  <code>abstractmethod</code>","text":"<p>Looks up the saved hyper-parameter for the io mixed class.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to look up within the saved init settings for self</p> required <p>Returns:     Value Raises:     KeyError if the attribute does not exist in the saved init settings.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n    Args:\n        attribute: The element name to look up within the saved init settings for self\n    Returns:\n        Value\n    Raises:\n        KeyError if the attribute does not exist in the saved init settings.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.get_hparams","title":"<code>get_hparams()</code>  <code>abstractmethod</code>","text":"<p>Returns the hyper-parameters of init in a dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of the init hyper-parameters on this object.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef get_hparams(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n    Returns:\n        Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>  <code>abstractmethod</code>","text":"<p>Mutates the saved hyper-parameter for the io mixed class.</p> <p>If you would like to only change the saved hyper-param     for example in the case of loading a dataclass where the same variables are mutated to other non-savable     entities by deterministic rules after init, then use <code>also_change_value=False</code> to only update the     hyper-parameter.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to modify within the saved init settings for self</p> required <code>value</code> <code>Any</code> <p>New parameter for the saved init settings</p> required <code>also_change_value</code> <code>bool</code> <p>If you also want to mutate the attribute of this same name in self to be the desired value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then do not set this and modify the self attribute separately in the normal pythonic way.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n    If you would like to only change the saved hyper-param\n        for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n        entities by deterministic rules after init, then use `also_change_value=False` to only update the\n        hyper-parameter.\n\n    Args:\n        attribute: The element name to modify within the saved init settings for self\n        value: New parameter for the saved init settings\n        also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n            value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n            do not set this and modify the self attribute separately in the normal pythonic way.\n\n    Returns:\n        None.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/logger_utils/","title":"Logger utils","text":""},{"location":"API_reference/bionemo/llm/utils/logger_utils/#bionemo.llm.utils.logger_utils.WandbConfig","title":"<code>WandbConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Note: <code>name</code> controls the exp name is handled by the NeMoLogger so it is ommitted here. <code>directory</code> is also omitted since it is set by the NeMoLogger.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <p>The team posting this run (default: your username or your default team)</p> required <code>project</code> <p>The name of the project to which this run will belong.</p> required <code>tags</code> <p>Tags associated with this run.</p> required <code>group</code> <p>A unique string shared by all runs in a given group</p> required <code>job_type</code> <p>Type of run, which is useful when you're grouping runs together into larger experiments.</p> required <code>offline</code> <p>Run offline (data can be streamed later to wandb servers).</p> required <code>id</code> <p>Sets the version, mainly used to resume a previous run.</p> required <code>anonymous</code> <p>Enables or explicitly disables anonymous logging.</p> required Source code in <code>bionemo/llm/utils/logger_utils.py</code> <pre><code>class WandbConfig(BaseModel):\n    \"\"\"Note: `name` controls the exp name is handled by the NeMoLogger so it is ommitted here.\n    `directory` is also omitted since it is set by the NeMoLogger.\n\n    Args:\n        entity: The team posting this run (default: your username or your default team)\n        project: The name of the project to which this run will belong.\n        tags: Tags associated with this run.\n        group: A unique string shared by all runs in a given group\n        job_type: Type of run, which is useful when you're grouping runs together into larger experiments.\n        offline: Run offline (data can be streamed later to wandb servers).\n        id: Sets the version, mainly used to resume a previous run.\n        anonymous: Enables or explicitly disables anonymous logging.\n    \"\"\"  # noqa: D205\n\n    entity: str | None  # The team posting this run (default: your username or your default team)\n    project: str  # The name of the project to which this run will belong.\n    # name: #Display name for the run. \"This is handled by NeMoLogger\"\n    # save_dir: #Path where data is saved. \"This is handled by NeMoLogger\"\n    tags: List[str] | None  # Tags associated with this run.\n    group: str | None  # A unique string shared by all runs in a given group.\n    job_type: str | None = (\n        None  # Type of run, which is useful when you're grouping runs together into larger experiments.\n    )\n    offline: bool  # Run offline (data can be streamed later to wandb servers).\n    id: str | None  # Sets the version, mainly used to resume a previous run.\n    anonymous: bool  # Enables or explicitly disables anonymous logging.\n    log_model: bool  # Save checkpoints in wandb dir to upload on W&amp;B servers.\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/logger_utils/#bionemo.llm.utils.logger_utils.setup_nemo_lightning_logger","title":"<code>setup_nemo_lightning_logger(name='default-name', root_dir='./results', initialize_tensorboard_logger=False, wandb_config=None, ckpt_callback=None, **kwargs)</code>","text":"<p>Setup the logger for the experiment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the experiment. Results go into <code>root_dir</code>/<code>name</code></p> <code>'default-name'</code> <code>root_dir</code> <code>str | Path</code> <p>The root directory to create the <code>name</code> directory in for saving run results.</p> <code>'./results'</code> <code>initialize_tensorboard_logger</code> <code>bool</code> <p>Whether to initialize the tensorboard logger.</p> <code>False</code> <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>The remaining configuration options for the wandb logger.</p> <code>None</code> <code>ckpt_callback</code> <code>Optional[ModelCheckpoint]</code> <p>The checkpoint callback to use, must be a child of the pytorch lightning ModelCheckpoint callback. NOTE the type annotation in the underlying NeMoCheckpoint constructor is incorrect.</p> <code>None</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>The kwargs for the NeMoLogger.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>NeMoLogger</code> <code>NeMoLogger</code> <p>NeMo logger instance.</p> Source code in <code>bionemo/llm/utils/logger_utils.py</code> <pre><code>def setup_nemo_lightning_logger(\n    name: str = \"default-name\",\n    root_dir: str | pathlib.Path = \"./results\",\n    initialize_tensorboard_logger: bool = False,\n    wandb_config: Optional[WandbConfig] = None,\n    ckpt_callback: Optional[nemo_callbacks.ModelCheckpoint] = None,\n    **kwargs: Dict[str, Any],\n) -&gt; NeMoLogger:\n    \"\"\"Setup the logger for the experiment.\n\n    Arguments:\n        name: The name of the experiment. Results go into `root_dir`/`name`\n        root_dir: The root directory to create the `name` directory in for saving run results.\n        initialize_tensorboard_logger: Whether to initialize the tensorboard logger.\n        wandb_config: The remaining configuration options for the wandb logger.\n        ckpt_callback: The checkpoint callback to use, must be a child of the pytorch lightning ModelCheckpoint callback.\n            NOTE the type annotation in the underlying NeMoCheckpoint constructor is incorrect.\n        **kwargs: The kwargs for the NeMoLogger.\n\n    Returns:\n        NeMoLogger: NeMo logger instance.\n    \"\"\"\n    # The directory that the logger will save to\n    save_dir = pathlib.Path(root_dir) / name\n    if wandb_config is not None:\n        wandb_logger = WandbLogger(save_dir=save_dir, name=name, **wandb_config.model_dump())\n    else:\n        wandb_logger = None\n        logging.warning(\"WandB is currently turned off.\")\n    if initialize_tensorboard_logger:\n        tb_logger = TensorBoardLogger(save_dir=save_dir, name=name)\n    else:\n        tb_logger = None\n        logging.warning(\"User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\")\n    logger: NeMoLogger = NeMoLogger(\n        name=name,\n        log_dir=str(root_dir),\n        tensorboard=tb_logger,\n        wandb=wandb_logger,\n        ckpt=ckpt_callback,\n        use_datetime_version=False,\n        version=\"dev\",\n        **kwargs,\n    )\n    # Needed so that the trainer can find an output directory for the profiler\n    logger.save_dir = save_dir\n    return logger\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/megatron_utils/","title":"Megatron utils","text":""},{"location":"API_reference/bionemo/llm/utils/megatron_utils/#bionemo.llm.utils.megatron_utils.is_only_data_parallel","title":"<code>is_only_data_parallel()</code>","text":"<p>Checks to see if you are in a distributed megatron environment with only data parallelism active.</p> <p>This is useful if you are working on a model, loss, etc and you know that you do not yet support megatron model parallelism. You can test that the only kind of parallelism in use is data parallelism.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if data parallel is the only parallel mode, False otherwise.</p> Source code in <code>bionemo/llm/utils/megatron_utils.py</code> <pre><code>def is_only_data_parallel() -&gt; bool:\n    \"\"\"Checks to see if you are in a distributed megatron environment with only data parallelism active.\n\n    This is useful if you are working on a model, loss, etc and you know that you do not yet support megatron model\n    parallelism. You can test that the only kind of parallelism in use is data parallelism.\n\n    Returns:\n        True if data parallel is the only parallel mode, False otherwise.\n    \"\"\"\n    if not (torch.distributed.is_available() and parallel_state.is_initialized()):\n        raise RuntimeError(\"This function is only defined within an initialized megatron parallel environment.\")\n    # Idea: when world_size == data_parallel_world_size, then you know that you are fully DDP, which means you are not\n    #  using model parallelism (meaning virtual GPUs composed of several underlying GPUs that you need to reduce over).\n\n    world_size: int = torch.distributed.get_world_size()\n    dp_world_size: int = parallel_state.get_data_parallel_world_size()\n    return world_size == dp_world_size\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/","title":"Remote","text":""},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.FTPRemoteResource","title":"<code>FTPRemoteResource</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RemoteResource</code></p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>class FTPRemoteResource(RemoteResource):  # noqa: D101\n    def download_resource(self, overwrite=False) -&gt; str:\n        \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n        Returns: the fully qualified destination filename.\n        \"\"\"\n        self.exists_or_create_destination_directory()\n\n        if not self.check_exists() or overwrite:\n            request.urlretrieve(self.url, self.fully_qualified_dest_filename)\n\n        self.check_exists()\n        return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.FTPRemoteResource.download_resource","title":"<code>download_resource(overwrite=False)</code>","text":"<p>Downloads the resource to its specified fully_qualified_dest name.</p> <p>Returns: the fully qualified destination filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def download_resource(self, overwrite=False) -&gt; str:\n    \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n    Returns: the fully qualified destination filename.\n    \"\"\"\n    self.exists_or_create_destination_directory()\n\n    if not self.check_exists() or overwrite:\n        request.urlretrieve(self.url, self.fully_qualified_dest_filename)\n\n    self.check_exists()\n    return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource","title":"<code>RemoteResource</code>  <code>dataclass</code>","text":"<p>Responsible for downloading remote files, along with optional processing of downloaded files for downstream usecases.</p> <p>Each object is invoked through either its constructor (setting up the destination and checksum), or through a pre-configured class method. <code>download_resource()</code> contains the core functionality, which is to download the file at <code>url</code> to the fully qualified filename. Class methods can be used to further configure this process.</p> Receive <p>a file, its checksum, a destination directory, and a root directory</p> <p>Our dataclass then provides some useful things:     - fully qualified destination folder (property)     - fully qualified destination file (property)     - check_exists()     - download_resource()</p> <p>Form the fully qualified destination folder. Create a fully qualified path for the file</p> <p>(all lives in the download routine) Check that the fq destination folder exists, otherwise create it Download the file. Checksum the download. Done.</p> <p>Postprocessing should be their own method with their own configuration.</p> Example usage <p>Attributes:</p> Name Type Description <code>dest_directory</code> <code>str</code> <p>The directory to place the desired file upon completing the download. Should have the form {dest_directory}/{dest_filename}</p> <code>dest_filename</code> <code>str</code> <p>The desired name for the file upon completing the download.</p> <code>checksum</code> <code>Optional[str]</code> <p>checksum associated with the file located at url. If set to None, check_exists only checks for the existance of <code>{dest_directory}/{dest_filename}</code></p> <code>url</code> <code>Optional[str]</code> <p>URL of the file to download</p> <code>root_directory</code> <code>str | PathLike</code> <p>the bottom-level directory, the fully qualified path is formed by joining root_directory, dest_directory, and dest_filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>@dataclass\nclass RemoteResource:\n    \"\"\"Responsible for downloading remote files, along with optional processing of downloaded files for downstream usecases.\n\n    Each object is invoked through either its constructor (setting up the destination and checksum), or through a pre-configured class method.\n    `download_resource()` contains the core functionality, which is to download the file at `url` to the fully qualified filename. Class methods\n    can be used to further configure this process.\n\n    Receive:\n        a file, its checksum, a destination directory, and a root directory\n\n        Our dataclass then provides some useful things:\n            - fully qualified destination folder (property)\n            - fully qualified destination file (property)\n            - check_exists()\n            - download_resource()\n\n        Form the fully qualified destination folder.\n        Create a fully qualified path for the file\n\n        (all lives in the download routine)\n        Check that the fq destination folder exists, otherwise create it\n        Download the file.\n        Checksum the download.\n        Done.\n\n        Postprocessing should be their own method with their own configuration.\n\n    Example usage:\n        &gt;&gt;&gt; # The following will download and preprocess the prepackaged resources.\n        &gt;&gt;&gt; GRCh38Ensembl99ResourcePreparer().prepare()\n        &gt;&gt;&gt; Hg38chromResourcePreparer().prepare()\n        &gt;&gt;&gt; GRCh38p13_ResourcePreparer().prepare()\n\n\n    Attributes:\n        dest_directory: The directory to place the desired file upon completing the download. Should have the form {dest_directory}/{dest_filename}\n        dest_filename: The desired name for the file upon completing the download.\n        checksum: checksum associated with the file located at url. If set to None, check_exists only checks for the existance of `{dest_directory}/{dest_filename}`\n        url: URL of the file to download\n        root_directory: the bottom-level directory, the fully qualified path is formed by joining root_directory, dest_directory, and dest_filename.\n    \"\"\"\n\n    checksum: Optional[str]\n    dest_filename: str\n    dest_directory: str\n    root_directory: str | os.PathLike = BIONEMO_CACHE_DIR\n    url: Optional[str] = None\n\n    @property\n    def fully_qualified_dest_folder(self):  # noqa: D102\n        return Path(self.root_directory) / self.dest_directory\n\n    @property\n    def fully_qualified_dest_filename(self):\n        \"\"\"Returns the fully qualified destination path of the file.\n\n        Example:\n            /tmp/my_folder/file.tar.gz\n        \"\"\"\n        return os.path.join(self.fully_qualified_dest_folder, self.dest_filename)\n\n    def exists_or_create_destination_directory(self, exist_ok=True):\n        \"\"\"Checks that the `fully_qualified_destination_directory` exists, if it does not, the directory is created (or fails).\n\n        exists_ok: Triest to create `fully_qualified_dest_folder` if it doesnt already exist.\n        \"\"\"\n        os.makedirs(self.fully_qualified_dest_folder, exist_ok=exist_ok)\n\n    @staticmethod\n    def get_env_tmpdir():\n        \"\"\"Convenience method that exposes the environment TMPDIR variable.\"\"\"\n        return os.environ.get(\"TMPDIR\", \"/tmp\")\n\n    def download_resource(self, overwrite=False) -&gt; str:\n        \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n        Returns: the fully qualified destination filename.\n        \"\"\"\n        self.exists_or_create_destination_directory()\n\n        if not self.check_exists() or overwrite:\n            logging.info(f\"Downloading resource: {self.url}\")\n            with requests.get(self.url, stream=True) as r, open(self.fully_qualified_dest_filename, \"wb\") as fd:\n                r.raise_for_status()\n                for bytes in r:\n                    fd.write(bytes)\n        else:\n            logging.info(f\"Resource already exists, skipping download: {self.url}\")\n\n        self.check_exists()\n        return self.fully_qualified_dest_filename\n\n    def check_exists(self):\n        \"\"\"Returns true if `fully_qualified_dest_filename` exists and the checksum matches `self.checksum`\"\"\"  # noqa: D415\n        if os.path.exists(self.fully_qualified_dest_filename):\n            with open(self.fully_qualified_dest_filename, \"rb\") as fd:\n                data = fd.read()\n                result = md5(data).hexdigest()\n            if self.checksum is None:\n                logging.info(\"No checksum provided, filename exists. Assuming it is complete.\")\n                matches = True\n            else:\n                matches = result == self.checksum\n            return matches\n\n        return False\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource--the-following-will-download-and-preprocess-the-prepackaged-resources","title":"The following will download and preprocess the prepackaged resources.","text":"<p>GRCh38Ensembl99ResourcePreparer().prepare() Hg38chromResourcePreparer().prepare() GRCh38p13_ResourcePreparer().prepare()</p>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.fully_qualified_dest_filename","title":"<code>fully_qualified_dest_filename</code>  <code>property</code>","text":"<p>Returns the fully qualified destination path of the file.</p> Example <p>/tmp/my_folder/file.tar.gz</p>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.check_exists","title":"<code>check_exists()</code>","text":"<p>Returns true if <code>fully_qualified_dest_filename</code> exists and the checksum matches <code>self.checksum</code></p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def check_exists(self):\n    \"\"\"Returns true if `fully_qualified_dest_filename` exists and the checksum matches `self.checksum`\"\"\"  # noqa: D415\n    if os.path.exists(self.fully_qualified_dest_filename):\n        with open(self.fully_qualified_dest_filename, \"rb\") as fd:\n            data = fd.read()\n            result = md5(data).hexdigest()\n        if self.checksum is None:\n            logging.info(\"No checksum provided, filename exists. Assuming it is complete.\")\n            matches = True\n        else:\n            matches = result == self.checksum\n        return matches\n\n    return False\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.download_resource","title":"<code>download_resource(overwrite=False)</code>","text":"<p>Downloads the resource to its specified fully_qualified_dest name.</p> <p>Returns: the fully qualified destination filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def download_resource(self, overwrite=False) -&gt; str:\n    \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n    Returns: the fully qualified destination filename.\n    \"\"\"\n    self.exists_or_create_destination_directory()\n\n    if not self.check_exists() or overwrite:\n        logging.info(f\"Downloading resource: {self.url}\")\n        with requests.get(self.url, stream=True) as r, open(self.fully_qualified_dest_filename, \"wb\") as fd:\n            r.raise_for_status()\n            for bytes in r:\n                fd.write(bytes)\n    else:\n        logging.info(f\"Resource already exists, skipping download: {self.url}\")\n\n    self.check_exists()\n    return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.exists_or_create_destination_directory","title":"<code>exists_or_create_destination_directory(exist_ok=True)</code>","text":"<p>Checks that the <code>fully_qualified_destination_directory</code> exists, if it does not, the directory is created (or fails).</p> <p>exists_ok: Triest to create <code>fully_qualified_dest_folder</code> if it doesnt already exist.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def exists_or_create_destination_directory(self, exist_ok=True):\n    \"\"\"Checks that the `fully_qualified_destination_directory` exists, if it does not, the directory is created (or fails).\n\n    exists_ok: Triest to create `fully_qualified_dest_folder` if it doesnt already exist.\n    \"\"\"\n    os.makedirs(self.fully_qualified_dest_folder, exist_ok=exist_ok)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.get_env_tmpdir","title":"<code>get_env_tmpdir()</code>  <code>staticmethod</code>","text":"<p>Convenience method that exposes the environment TMPDIR variable.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>@staticmethod\ndef get_env_tmpdir():\n    \"\"\"Convenience method that exposes the environment TMPDIR variable.\"\"\"\n    return os.environ.get(\"TMPDIR\", \"/tmp\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/weight_utils/","title":"Weight utils","text":""},{"location":"API_reference/bionemo/llm/utils/weight_utils/#bionemo.llm.utils.weight_utils.load_weights_sharded_inplace_nemo2_to_mcore","title":"<code>load_weights_sharded_inplace_nemo2_to_mcore(model, distributed_checkpoint_dir, skip_keys_with_these_prefixes)</code>","text":"<p>Given a megatron module, this function will determine which keys/subsets of weights to load given the     parallel/distributed state. This operates assuming a checkpoint was saved by a nemo2 trainer which places     the <code>module.</code> prefix on all key names, but we are then going to load directly in to the megatron module     without the <code>module.</code> prefix. Note that if there are any extra keys that you do not want to search the     checkpoint for, for example if you add new layers/heads onto your module, you need to supply the prefix     path to those keys in your model and they will be ignored. This latter feature is key for flexible fine-tuning     strategies where you load weights partially from other models with partially overlapping structures.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MegatronModelType</code> <p>Megatron model that you want to load weights into.</p> required <code>distributed_checkpoint_dir</code> <code>str | Path</code> <p>description</p> required <code>skip_keys_with_these_prefixes</code> <code>Set[str]</code> <p>description</p> required Source code in <code>bionemo/llm/utils/weight_utils.py</code> <pre><code>def load_weights_sharded_inplace_nemo2_to_mcore(\n    model: MegatronModelType, distributed_checkpoint_dir: str | Path, skip_keys_with_these_prefixes: Set[str]\n) -&gt; None:\n    \"\"\"Given a megatron module, this function will determine which keys/subsets of weights to load given the\n        parallel/distributed state. This operates assuming a checkpoint was saved by a nemo2 trainer which places\n        the `module.` prefix on all key names, but we are then going to load directly in to the megatron module\n        without the `module.` prefix. Note that if there are any _extra_ keys that you do not want to search the\n        checkpoint for, for example if you add new layers/heads onto your module, you need to supply the prefix\n        path to those keys in your model and they will be ignored. This latter feature is key for flexible fine-tuning\n        strategies where you load weights partially from other models with partially overlapping structures.\n\n    Args:\n        model: Megatron model that you want to load weights into.\n        distributed_checkpoint_dir: _description_\n        skip_keys_with_these_prefixes: _description_\n    \"\"\"  # noqa: D205\n    sharded_state_dict = {\n        _munge_key_megatron_to_nemo2(k): _munge_sharded_tensor_key_megatron_to_nemo2(v)\n        for k, v in model.sharded_state_dict().items()\n        if not _key_in_filter(k, skip_keys_with_these_prefixes) and \"_extra_state\" not in k\n    }\n    dist_checkpointing.load(\n        sharded_state_dict=sharded_state_dict,\n        checkpoint_dir=str(Path(distributed_checkpoint_dir) / \"weights\"),\n        strict=dist_checkpointing.serialization.StrictHandling.ASSUME_OK_UNEXPECTED,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/weight_utils/#bionemo.llm.utils.weight_utils.nemo1_to_nemo2_biobert_key_mapping","title":"<code>nemo1_to_nemo2_biobert_key_mapping(old_key, new_model_prefix='module', old_model_prefix='model', te_mapping=False)</code>","text":"<p>This function is used to map the keys from the old nemo BERT models to the new BioBERT models</p> <p>Parameters:</p> Name Type Description Default <code>old_key</code> <code>str</code> <p>old key we want to map to the expected new key name.</p> required <code>new_model_prefix</code> <code>str</code> <p>The new key for the base weights. If you point this at the core megatron model set it to \"\". For the regular nemo2 lightning module following standards, set it to \"module\". Defaults to \"module\".</p> <code>'module'</code> <code>old_model_prefix</code> <code>str</code> <p>The previous saved weight prefix. Defaults to \"model\" which was the standard in nemo1.</p> <code>'model'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>New key name</p> Source code in <code>bionemo/llm/utils/weight_utils.py</code> <pre><code>def nemo1_to_nemo2_biobert_key_mapping(  # noqa: D417\n    old_key: str,\n    new_model_prefix: str = \"module\",\n    old_model_prefix: str = \"model\",\n    te_mapping: bool = False,\n) -&gt; str:\n    \"\"\"This function is used to map the keys from the old nemo BERT models to the new BioBERT models\n\n    Args:\n        old_key (str): old key we want to map to the expected new key name.\n        new_model_prefix (str, optional): The new key for the base weights.\n            If you point this at the core megatron model set it to \"\".\n            For the regular nemo2 lightning module following standards, set it to \"module\".\n            Defaults to \"module\".\n        old_model_prefix (str, optional): The previous saved weight prefix. Defaults to \"model\" which was the standard in nemo1.\n\n    Returns:\n        str: New key name\n    \"\"\"  # noqa: D415\n    # add the . to the end of the input prefixes if they are not the empty string,\n    #  unless the user has already done so.\n    if old_model_prefix != \"\":\n        old_model_prefix = f\"{old_model_prefix.rstrip('.')}.\"\n    if new_model_prefix != \"\":\n        new_model_prefix = f\"{new_model_prefix.rstrip('.')}.\"\n\n    # This function is used to map the keys from the old nemo BERT models to the new BioBERT models\n    base_rename = old_key.replace(f\"{old_model_prefix}language_model.\", f\"{new_model_prefix}\")\n    base_rename = base_rename.replace(f\"{old_model_prefix}\", f\"{new_model_prefix}\")\n    if \"dense_h_to_4h\" in base_rename:\n        return base_rename.replace(\"dense_h_to_4h\", \"linear_fc1\")\n    if \"dense_4h_to_h\" in base_rename:\n        return base_rename.replace(\"dense_4h_to_h\", \"linear_fc2\")\n    if \"query_key_value\" in base_rename:\n        return base_rename.replace(\"query_key_value\", \"linear_qkv\")\n    if \"self_attention.dense\" in base_rename:\n        #  This is definitely the linear_proj and not the qkv. The linear_proj shapes are 256x256\n        #   which match dense but not query_key_value\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_proj.weight'].shape\n        #  torch.Size([256, 256])\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_qkv.weight'].shape\n        # torch.Size([768, 256])\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_qkv.bias'].shape\n        # torch.Size([768])\n        return base_rename.replace(\"self_attention.dense\", \"self_attention.linear_proj\")\n    if \"lm_head.bias\" in base_rename:\n        return base_rename.replace(\"lm_head.bias\", \"output_layer.bias\")\n    if \"lm_head.weight\" in base_rename:\n        return base_rename.replace(\"lm_head.weight\", \"output_layer.weight\")\n    if \"lm_head.layernorm\" in base_rename:\n        return base_rename.replace(\"lm_head.layernorm\", \"lm_head.layer_norm\")\n\n    if \"post_attention_layernorm\" in base_rename:\n        base_rename = base_rename.replace(\"post_attention_layernorm\", \"pre_mlp_layernorm\")\n\n    # Handle the transformer engine spec's differences in layer naming and where things like layernorm are stored.\n    #  TE moves layernorm from  an object that's part of the main attention layer to being an internal component of\n    #  the linear layers, probably for efficiency/fusion of some sort.\n    if te_mapping:\n        if \".input_layernorm.weight\" in base_rename:\n            return base_rename.replace(\".input_layernorm.weight\", \".self_attention.linear_qkv.layer_norm_weight\")\n        if \".input_layernorm.bias\" in base_rename:\n            return base_rename.replace(\".input_layernorm.bias\", \".self_attention.linear_qkv.layer_norm_bias\")\n        if \".pre_mlp_layernorm.bias\" in base_rename:\n            return base_rename.replace(\".pre_mlp_layernorm.bias\", \".mlp.linear_fc1.layer_norm_bias\")\n        if \".pre_mlp_layernorm.weight\" in base_rename:\n            return base_rename.replace(\".pre_mlp_layernorm.weight\", \".mlp.linear_fc1.layer_norm_weight\")\n    return base_rename\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/distribution/","title":"Distribution","text":""},{"location":"API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution","title":"<code>DiscretePriorDistribution</code>","text":"<p>               Bases: <code>PriorDistribution</code></p> <p>An abstract base class representing a discrete prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>class DiscretePriorDistribution(PriorDistribution):\n    \"\"\"An abstract base class representing a discrete prior distribution.\"\"\"\n\n    def __init__(self, num_classes: int, prior_dist: Tensor):\n        \"\"\"Initializes a DiscretePriorDistribution instance.\n\n        Args:\n        num_classes (int): The number of classes in the discrete distribution.\n        prior_dist (Tensor): The prior distribution over the classes.\n\n        Returns:\n        None\n        \"\"\"\n        self.num_classes = num_classes\n        self.prior_dist = prior_dist\n\n    def get_num_classes(self) -&gt; int:\n        \"\"\"Getter for num_classes.\"\"\"\n        return self.num_classes\n\n    def get_prior_dist(self) -&gt; Tensor:\n        \"\"\"Getter for prior_dist.\"\"\"\n        return self.prior_dist\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution.__init__","title":"<code>__init__(num_classes, prior_dist)</code>","text":"<p>Initializes a DiscretePriorDistribution instance.</p> <p>Args: num_classes (int): The number of classes in the discrete distribution. prior_dist (Tensor): The prior distribution over the classes.</p> <p>Returns: None</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>def __init__(self, num_classes: int, prior_dist: Tensor):\n    \"\"\"Initializes a DiscretePriorDistribution instance.\n\n    Args:\n    num_classes (int): The number of classes in the discrete distribution.\n    prior_dist (Tensor): The prior distribution over the classes.\n\n    Returns:\n    None\n    \"\"\"\n    self.num_classes = num_classes\n    self.prior_dist = prior_dist\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution.get_num_classes","title":"<code>get_num_classes()</code>","text":"<p>Getter for num_classes.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>def get_num_classes(self) -&gt; int:\n    \"\"\"Getter for num_classes.\"\"\"\n    return self.num_classes\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.DiscretePriorDistribution.get_prior_dist","title":"<code>get_prior_dist()</code>","text":"<p>Getter for prior_dist.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>def get_prior_dist(self) -&gt; Tensor:\n    \"\"\"Getter for prior_dist.\"\"\"\n    return self.prior_dist\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.PriorDistribution","title":"<code>PriorDistribution</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class representing a prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>class PriorDistribution(ABC):\n    \"\"\"An abstract base class representing a prior distribution.\"\"\"\n\n    @abstractmethod\n    def sample(self, shape: Tuple, mask: Optional[Tensor] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generates a specified number of samples from the time distribution.\n\n        Args:\n        shape (Tuple): The shape of the samples to generate.\n        mask (Optional[Tensor], optional): A tensor indicating which samples should be masked. Defaults to None.\n        device (str, optional): The device on which to generate the samples. Defaults to \"cpu\".\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/distribution/#bionemo.moco.distributions.prior.distribution.PriorDistribution.sample","title":"<code>sample(shape, mask=None, device='cpu')</code>  <code>abstractmethod</code>","text":"<p>Generates a specified number of samples from the time distribution.</p> <p>Args: shape (Tuple): The shape of the samples to generate. mask (Optional[Tensor], optional): A tensor indicating which samples should be masked. Defaults to None. device (str, optional): The device on which to generate the samples. Defaults to \"cpu\".</p> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/distribution.py</code> <pre><code>@abstractmethod\ndef sample(self, shape: Tuple, mask: Optional[Tensor] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Generates a specified number of samples from the time distribution.\n\n    Args:\n    shape (Tuple): The shape of the samples to generate.\n    mask (Optional[Tensor], optional): A tensor indicating which samples should be masked. Defaults to None.\n    device (str, optional): The device on which to generate the samples. Defaults to \"cpu\".\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/gaussian/","title":"Gaussian","text":""},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/gaussian/#bionemo.moco.distributions.prior.continuous.gaussian.GaussianPrior","title":"<code>GaussianPrior</code>","text":"<p>               Bases: <code>PriorDistribution</code></p> <p>A subclass representing a Gaussian prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/gaussian.py</code> <pre><code>class GaussianPrior(PriorDistribution):\n    \"\"\"A subclass representing a Gaussian prior distribution.\"\"\"\n\n    def __init__(\n        self,\n        mean: Float = 0.0,\n        std: Float = 1.0,\n        center: Bool = False,\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; None:\n        \"\"\"Gaussian prior distribution.\n\n        Args:\n            mean (Float): The mean of the Gaussian distribution. Defaults to 0.0.\n            std (Float): The standard deviation of the Gaussian distribution. Defaults to 1.0.\n            center (bool): Whether to center the samples around the mean. Defaults to False.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        self.mean = mean\n        self.std = std\n        self.center = center\n        self.rng_generator = rng_generator\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples from the Gaussian prior distribution.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        samples = torch.randn(*shape, device=device, generator=rng_generator)\n        if self.std != 1:\n            samples = samples * self.std\n        if self.mean != 0:\n            samples = samples + self.mean\n\n        if self.center:\n            samples = remove_center_of_mass(samples, mask)\n        if mask is not None:\n            samples = samples * mask.unsqueeze(-1)\n        return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/gaussian/#bionemo.moco.distributions.prior.continuous.gaussian.GaussianPrior.__init__","title":"<code>__init__(mean=0.0, std=1.0, center=False, rng_generator=None)</code>","text":"<p>Gaussian prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Float</code> <p>The mean of the Gaussian distribution. Defaults to 0.0.</p> <code>0.0</code> <code>std</code> <code>Float</code> <p>The standard deviation of the Gaussian distribution. Defaults to 1.0.</p> <code>1.0</code> <code>center</code> <code>bool</code> <p>Whether to center the samples around the mean. Defaults to False.</p> <code>False</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/prior/continuous/gaussian.py</code> <pre><code>def __init__(\n    self,\n    mean: Float = 0.0,\n    std: Float = 1.0,\n    center: Bool = False,\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; None:\n    \"\"\"Gaussian prior distribution.\n\n    Args:\n        mean (Float): The mean of the Gaussian distribution. Defaults to 0.0.\n        std (Float): The standard deviation of the Gaussian distribution. Defaults to 1.0.\n        center (bool): Whether to center the samples around the mean. Defaults to False.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    self.mean = mean\n    self.std = std\n    self.center = center\n    self.rng_generator = rng_generator\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/gaussian/#bionemo.moco.distributions.prior.continuous.gaussian.GaussianPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the Gaussian prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/gaussian.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples from the Gaussian prior distribution.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    samples = torch.randn(*shape, device=device, generator=rng_generator)\n    if self.std != 1:\n        samples = samples * self.std\n    if self.mean != 0:\n        samples = samples + self.mean\n\n    if self.center:\n        samples = remove_center_of_mass(samples, mask)\n    if mask is not None:\n        samples = samples * mask.unsqueeze(-1)\n    return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/harmonic/","title":"Harmonic","text":""},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/harmonic/#bionemo.moco.distributions.prior.continuous.harmonic.LinearHarmonicPrior","title":"<code>LinearHarmonicPrior</code>","text":"<p>               Bases: <code>PriorDistribution</code></p> <p>A subclass representing a Linear Harmonic prior distribution from Jing et al. https://arxiv.org/abs/2304.02198.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/harmonic.py</code> <pre><code>class LinearHarmonicPrior(PriorDistribution):\n    \"\"\"A subclass representing a Linear Harmonic prior distribution from Jing et al. https://arxiv.org/abs/2304.02198.\"\"\"\n\n    def __init__(\n        self,\n        length: Optional[int] = None,\n        distance: Float = 3.8,\n        center: Bool = False,\n        rng_generator: Optional[torch.Generator] = None,\n        device: Union[str, torch.device] = \"cpu\",\n    ) -&gt; None:\n        \"\"\"Linear Harmonic prior distribution.\n\n        Args:\n            length (Optional[int]): The number of points in a batch.\n            distance (Float): RMS distance between adjacent points in the line graph.\n            center (bool): Whether to center the samples around the mean. Defaults to False.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        self.distance = distance\n        self.length = length\n        self.center = center\n        self.rng_generator = rng_generator\n        self.device = device\n        if length:\n            self._calculate_terms(length, device)\n\n    def _calculate_terms(self, N, device):\n        a = 3 / (self.distance * self.distance)\n        J = torch.zeros(N, N)\n        for i, j in zip(torch.arange(N - 1), torch.arange(1, N)):\n            J[i, i] += a\n            J[j, j] += a\n            J[i, j] = J[j, i] = -a\n        D, P = torch.linalg.eigh(J)\n        D_inv = 1 / D\n        D_inv[0] = 0\n        self.P, self.D_inv = P.to(device), D_inv.to(device)\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples from the Harmonic prior distribution.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n\n        samples = torch.randn(*shape, device=device, generator=rng_generator)\n        N = shape[-2]\n\n        if N != self.length:\n            self._calculate_terms(N, device)\n\n        std = torch.sqrt(self.D_inv.to(device)).unsqueeze(-1)\n        samples = self.P.to(device) @ (std * samples)\n        # torch broadcasting avoids shape errors NxN @ (N x 1 * B x N x D)\n        if self.center:\n            samples = remove_center_of_mass(samples, mask)\n\n        if mask is not None:\n            samples = samples * mask.unsqueeze(-1)\n        return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/harmonic/#bionemo.moco.distributions.prior.continuous.harmonic.LinearHarmonicPrior.__init__","title":"<code>__init__(length=None, distance=3.8, center=False, rng_generator=None, device='cpu')</code>","text":"<p>Linear Harmonic prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>Optional[int]</code> <p>The number of points in a batch.</p> <code>None</code> <code>distance</code> <code>Float</code> <p>RMS distance between adjacent points in the line graph.</p> <code>3.8</code> <code>center</code> <code>bool</code> <p>Whether to center the samples around the mean. Defaults to False.</p> <code>False</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/distributions/prior/continuous/harmonic.py</code> <pre><code>def __init__(\n    self,\n    length: Optional[int] = None,\n    distance: Float = 3.8,\n    center: Bool = False,\n    rng_generator: Optional[torch.Generator] = None,\n    device: Union[str, torch.device] = \"cpu\",\n) -&gt; None:\n    \"\"\"Linear Harmonic prior distribution.\n\n    Args:\n        length (Optional[int]): The number of points in a batch.\n        distance (Float): RMS distance between adjacent points in the line graph.\n        center (bool): Whether to center the samples around the mean. Defaults to False.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    self.distance = distance\n    self.length = length\n    self.center = center\n    self.rng_generator = rng_generator\n    self.device = device\n    if length:\n        self._calculate_terms(length, device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/harmonic/#bionemo.moco.distributions.prior.continuous.harmonic.LinearHarmonicPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the Harmonic prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/continuous/harmonic.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples from the Harmonic prior distribution.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n\n    samples = torch.randn(*shape, device=device, generator=rng_generator)\n    N = shape[-2]\n\n    if N != self.length:\n        self._calculate_terms(N, device)\n\n    std = torch.sqrt(self.D_inv.to(device)).unsqueeze(-1)\n    samples = self.P.to(device) @ (std * samples)\n    # torch broadcasting avoids shape errors NxN @ (N x 1 * B x N x D)\n    if self.center:\n        samples = remove_center_of_mass(samples, mask)\n\n    if mask is not None:\n        samples = samples * mask.unsqueeze(-1)\n    return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/moco/distributions/prior/continuous/utils/#bionemo.moco.distributions.prior.continuous.utils.remove_center_of_mass","title":"<code>remove_center_of_mass(data, mask=None)</code>","text":"<p>Calculates the center of mass (CoM) of the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data with shape (..., nodes, features).</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional binary mask to apply to the data with shape (..., nodes) to mask out interaction from CoM calculation. Defaults to None.</p> <code>None</code> <p>Returns: The CoM of the data with shape (..., 1, features).</p> Source code in <code>bionemo/moco/distributions/prior/continuous/utils.py</code> <pre><code>def remove_center_of_mass(data: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Calculates the center of mass (CoM) of the given data.\n\n    Args:\n        data: The input data with shape (..., nodes, features).\n        mask: An optional binary mask to apply to the data with shape (..., nodes) to mask out interaction from CoM calculation. Defaults to None.\n\n    Returns:\n    The CoM of the data with shape (..., 1, features).\n    \"\"\"\n    if mask is None:\n        com = data.mean(dim=-2, keepdim=True)\n    else:\n        masked_data = data * mask.unsqueeze(-1)\n        num_nodes = mask.sum(dim=-1, keepdim=True).unsqueeze(-1)\n        com = masked_data.sum(dim=-2, keepdim=True) / num_nodes\n    return data - com\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/custom/","title":"Custom","text":""},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/custom/#bionemo.moco.distributions.prior.discrete.custom.DiscreteCustomPrior","title":"<code>DiscreteCustomPrior</code>","text":"<p>               Bases: <code>DiscretePriorDistribution</code></p> <p>A subclass representing a discrete custom prior distribution.</p> <p>This class allows for the creation of a prior distribution with a custom probability mass function defined by the <code>prior_dist</code> tensor. For example if my data has 4 classes and I want [.3, .2, .4, .1] as the probabilities of the 4 classes.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/custom.py</code> <pre><code>class DiscreteCustomPrior(DiscretePriorDistribution):\n    \"\"\"A subclass representing a discrete custom prior distribution.\n\n    This class allows for the creation of a prior distribution with a custom\n    probability mass function defined by the `prior_dist` tensor. For example if my data has 4 classes and I want [.3, .2, .4, .1] as the probabilities of the 4 classes.\n    \"\"\"\n\n    def __init__(self, prior_dist: Tensor, num_classes: int = 10) -&gt; None:\n        \"\"\"Initializes a DiscreteCustomPrior distribution.\n\n        Args:\n            prior_dist: A tensor representing the probability mass function of the prior distribution.\n            num_classes: The number of classes in the prior distribution. Defaults to 10.\n\n        Note:\n            The `prior_dist` tensor should have a sum close to 1.0, as it represents a probability mass function.\n        \"\"\"\n        super().__init__(num_classes, prior_dist)\n        if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n            raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Samples from the discrete custom prior distribution.\n\n        Args:\n            shape: A tuple specifying the shape of the samples to generate.\n            mask: An optional tensor mask to apply to the samples, broadcastable to the sample shape. Defaults to None.\n            device: The device on which to generate the samples, specified as a string or a :class:`torch.device`. Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples drawn from the prior distribution.\n        \"\"\"\n        samples = (\n            torch.multinomial(self.prior_dist, math.prod(shape), replacement=True, generator=rng_generator)\n            .to(device)\n            .reshape(shape)\n        )\n        if mask is not None:\n            samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n        return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/custom/#bionemo.moco.distributions.prior.discrete.custom.DiscreteCustomPrior.__init__","title":"<code>__init__(prior_dist, num_classes=10)</code>","text":"<p>Initializes a DiscreteCustomPrior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>prior_dist</code> <code>Tensor</code> <p>A tensor representing the probability mass function of the prior distribution.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes in the prior distribution. Defaults to 10.</p> <code>10</code> Note <p>The <code>prior_dist</code> tensor should have a sum close to 1.0, as it represents a probability mass function.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/custom.py</code> <pre><code>def __init__(self, prior_dist: Tensor, num_classes: int = 10) -&gt; None:\n    \"\"\"Initializes a DiscreteCustomPrior distribution.\n\n    Args:\n        prior_dist: A tensor representing the probability mass function of the prior distribution.\n        num_classes: The number of classes in the prior distribution. Defaults to 10.\n\n    Note:\n        The `prior_dist` tensor should have a sum close to 1.0, as it represents a probability mass function.\n    \"\"\"\n    super().__init__(num_classes, prior_dist)\n    if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n        raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/custom/#bionemo.moco.distributions.prior.discrete.custom.DiscreteCustomPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Samples from the discrete custom prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>A tuple specifying the shape of the samples to generate.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional tensor mask to apply to the samples, broadcastable to the sample shape. Defaults to None.</p> <code>None</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to generate the samples, specified as a string or a :class:<code>torch.device</code>. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of samples drawn from the prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/custom.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Samples from the discrete custom prior distribution.\n\n    Args:\n        shape: A tuple specifying the shape of the samples to generate.\n        mask: An optional tensor mask to apply to the samples, broadcastable to the sample shape. Defaults to None.\n        device: The device on which to generate the samples, specified as a string or a :class:`torch.device`. Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples drawn from the prior distribution.\n    \"\"\"\n    samples = (\n        torch.multinomial(self.prior_dist, math.prod(shape), replacement=True, generator=rng_generator)\n        .to(device)\n        .reshape(shape)\n    )\n    if mask is not None:\n        samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n    return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/mask/","title":"Mask","text":""},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior","title":"<code>DiscreteMaskedPrior</code>","text":"<p>               Bases: <code>DiscretePriorDistribution</code></p> <p>A subclass representing a Discrete Masked prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>class DiscreteMaskedPrior(DiscretePriorDistribution):\n    \"\"\"A subclass representing a Discrete Masked prior distribution.\"\"\"\n\n    def __init__(self, num_classes: int = 10, mask_dim: Optional[int] = None, inclusive: bool = True) -&gt; None:\n        \"\"\"Discrete Masked prior distribution.\n\n        Theres 3 ways I can think of defining the problem that are hard to mesh together.\n\n        1. [..., M, ....] inclusive anywhere --&gt; exisiting LLM tokenizer where the mask has a specific location not at the end\n        2. [......, M] inclusive on end --&gt; mask_dim = None with inclusive set to True default stick on the end\n        3. [.....] + [M] exclusive --&gt; the number of classes representes the number of data classes and one wishes to add a separate MASK dimension.\n            - Note the pad_sample function is provided to help add this extra external dimension.\n\n        Args:\n            num_classes (int): The number of classes in the distribution. Defaults to 10.\n            mask_dim (int): The index for the mask token. Defaults to num_classes - 1 if inclusive or num_classes if exclusive.\n            inclusive (bool): Whether the mask is included in the specified number of classes.\n                                If True, the mask is considered as one of the classes.\n                                If False, the mask is considered as an additional class. Defaults to True.\n        \"\"\"\n        if inclusive:\n            if mask_dim is None:\n                mask_dim = num_classes - 1\n            else:\n                if mask_dim &gt;= num_classes:\n                    raise ValueError(\n                        \"As Inclusive accounts for the mask as one of the specified num_classes, the provided mask_dim cannot be &gt;= to num_classes\"\n                    )\n            prior_dist = torch.zeros((num_classes))\n            prior_dist[-1] = 1.0\n            super().__init__(num_classes, prior_dist)\n            self.mask_dim = mask_dim\n        else:\n            prior_dist = torch.zeros((num_classes + 1))\n            prior_dist[-1] = 1.0\n            super().__init__(num_classes + 1, prior_dist)\n            self.mask_dim = num_classes\n        if torch.sum(self.prior_dist).item() - 1.0 &gt;= 1e-5:\n            raise ValueError(\"Invalid probability distribution. Must sum to 1.0\")\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        samples = torch.ones(shape, dtype=torch.int64, device=device) * self.mask_dim\n        if mask is not None:\n            samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n        return samples\n\n    def is_masked(self, sample: Tensor) -&gt; Tensor:\n        \"\"\"Creates a mask for whether a state is masked.\n\n        Args:\n            sample (Tensor): The sample to check.\n\n        Returns:\n            Tensor: A float tensor indicating whether the sample is masked.\n        \"\"\"\n        return (sample == self.mask_dim).float()\n\n    def pad_sample(self, sample: Tensor) -&gt; Tensor:\n        \"\"\"Pads the input sample with zeros along the last dimension.\n\n        Args:\n            sample (Tensor): The input sample to be padded.\n\n        Returns:\n            Tensor: The padded sample.\n        \"\"\"\n        # Create a zeros tensor with the same shape as the original tensor, except the last dimension is 1\n        zeros = torch.zeros((*sample.shape[:-1], 1), dtype=torch.float, device=sample.device)\n        # Concatenate along the last dimension to make the shape (..., N+1)\n        padded_sample = torch.cat((sample, zeros), dim=-1)\n        return padded_sample\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.__init__","title":"<code>__init__(num_classes=10, mask_dim=None, inclusive=True)</code>","text":"<p>Discrete Masked prior distribution.</p> <p>Theres 3 ways I can think of defining the problem that are hard to mesh together.</p> <ol> <li>[..., M, ....] inclusive anywhere --&gt; exisiting LLM tokenizer where the mask has a specific location not at the end</li> <li>[......, M] inclusive on end --&gt; mask_dim = None with inclusive set to True default stick on the end</li> <li>[.....] + [M] exclusive --&gt; the number of classes representes the number of data classes and one wishes to add a separate MASK dimension.<ul> <li>Note the pad_sample function is provided to help add this extra external dimension.</li> </ul> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes in the distribution. Defaults to 10.</p> <code>10</code> <code>mask_dim</code> <code>int</code> <p>The index for the mask token. Defaults to num_classes - 1 if inclusive or num_classes if exclusive.</p> <code>None</code> <code>inclusive</code> <code>bool</code> <p>Whether the mask is included in the specified number of classes.                 If True, the mask is considered as one of the classes.                 If False, the mask is considered as an additional class. Defaults to True.</p> <code>True</code> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def __init__(self, num_classes: int = 10, mask_dim: Optional[int] = None, inclusive: bool = True) -&gt; None:\n    \"\"\"Discrete Masked prior distribution.\n\n    Theres 3 ways I can think of defining the problem that are hard to mesh together.\n\n    1. [..., M, ....] inclusive anywhere --&gt; exisiting LLM tokenizer where the mask has a specific location not at the end\n    2. [......, M] inclusive on end --&gt; mask_dim = None with inclusive set to True default stick on the end\n    3. [.....] + [M] exclusive --&gt; the number of classes representes the number of data classes and one wishes to add a separate MASK dimension.\n        - Note the pad_sample function is provided to help add this extra external dimension.\n\n    Args:\n        num_classes (int): The number of classes in the distribution. Defaults to 10.\n        mask_dim (int): The index for the mask token. Defaults to num_classes - 1 if inclusive or num_classes if exclusive.\n        inclusive (bool): Whether the mask is included in the specified number of classes.\n                            If True, the mask is considered as one of the classes.\n                            If False, the mask is considered as an additional class. Defaults to True.\n    \"\"\"\n    if inclusive:\n        if mask_dim is None:\n            mask_dim = num_classes - 1\n        else:\n            if mask_dim &gt;= num_classes:\n                raise ValueError(\n                    \"As Inclusive accounts for the mask as one of the specified num_classes, the provided mask_dim cannot be &gt;= to num_classes\"\n                )\n        prior_dist = torch.zeros((num_classes))\n        prior_dist[-1] = 1.0\n        super().__init__(num_classes, prior_dist)\n        self.mask_dim = mask_dim\n    else:\n        prior_dist = torch.zeros((num_classes + 1))\n        prior_dist[-1] = 1.0\n        super().__init__(num_classes + 1, prior_dist)\n        self.mask_dim = num_classes\n    if torch.sum(self.prior_dist).item() - 1.0 &gt;= 1e-5:\n        raise ValueError(\"Invalid probability distribution. Must sum to 1.0\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.is_masked","title":"<code>is_masked(sample)</code>","text":"<p>Creates a mask for whether a state is masked.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Tensor</code> <p>The sample to check.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A float tensor indicating whether the sample is masked.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def is_masked(self, sample: Tensor) -&gt; Tensor:\n    \"\"\"Creates a mask for whether a state is masked.\n\n    Args:\n        sample (Tensor): The sample to check.\n\n    Returns:\n        Tensor: A float tensor indicating whether the sample is masked.\n    \"\"\"\n    return (sample == self.mask_dim).float()\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.pad_sample","title":"<code>pad_sample(sample)</code>","text":"<p>Pads the input sample with zeros along the last dimension.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Tensor</code> <p>The input sample to be padded.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The padded sample.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def pad_sample(self, sample: Tensor) -&gt; Tensor:\n    \"\"\"Pads the input sample with zeros along the last dimension.\n\n    Args:\n        sample (Tensor): The input sample to be padded.\n\n    Returns:\n        Tensor: The padded sample.\n    \"\"\"\n    # Create a zeros tensor with the same shape as the original tensor, except the last dimension is 1\n    zeros = torch.zeros((*sample.shape[:-1], 1), dtype=torch.float, device=sample.device)\n    # Concatenate along the last dimension to make the shape (..., N+1)\n    padded_sample = torch.cat((sample, zeros), dim=-1)\n    return padded_sample\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/mask/#bionemo.moco.distributions.prior.discrete.mask.DiscreteMaskedPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/mask.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    samples = torch.ones(shape, dtype=torch.int64, device=device) * self.mask_dim\n    if mask is not None:\n        samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n    return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/uniform/","title":"Uniform","text":""},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/uniform/#bionemo.moco.distributions.prior.discrete.uniform.DiscreteUniformPrior","title":"<code>DiscreteUniformPrior</code>","text":"<p>               Bases: <code>DiscretePriorDistribution</code></p> <p>A subclass representing a discrete uniform prior distribution.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/uniform.py</code> <pre><code>class DiscreteUniformPrior(DiscretePriorDistribution):\n    \"\"\"A subclass representing a discrete uniform prior distribution.\"\"\"\n\n    def __init__(self, num_classes: int = 10) -&gt; None:\n        \"\"\"Initializes a discrete uniform prior distribution.\n\n        Args:\n            num_classes (int): The number of classes in the discrete uniform distribution. Defaults to 10.\n        \"\"\"\n        prior_dist = torch.ones((num_classes)) * 1 / num_classes\n        super().__init__(num_classes, prior_dist)\n        if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n            raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n\n    def sample(\n        self,\n        shape: Tuple,\n        mask: Optional[Tensor] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generates a specified number of samples.\n\n        Args:\n            shape (Tuple): The shape of the samples to generate.\n            device (str): cpu or gpu.\n            mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A tensor of samples.\n        \"\"\"\n        samples = torch.randint(0, self.num_classes, shape, device=device, generator=rng_generator)\n        if mask is not None:\n            samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n        return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/uniform/#bionemo.moco.distributions.prior.discrete.uniform.DiscreteUniformPrior.__init__","title":"<code>__init__(num_classes=10)</code>","text":"<p>Initializes a discrete uniform prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes in the discrete uniform distribution. Defaults to 10.</p> <code>10</code> Source code in <code>bionemo/moco/distributions/prior/discrete/uniform.py</code> <pre><code>def __init__(self, num_classes: int = 10) -&gt; None:\n    \"\"\"Initializes a discrete uniform prior distribution.\n\n    Args:\n        num_classes (int): The number of classes in the discrete uniform distribution. Defaults to 10.\n    \"\"\"\n    prior_dist = torch.ones((num_classes)) * 1 / num_classes\n    super().__init__(num_classes, prior_dist)\n    if torch.sum(self.prior_dist).item() - 1.0 &gt; 1e-5:\n        raise ValueError(\"Prior distribution probabilities do not sum up to 1.0\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/prior/discrete/uniform/#bionemo.moco.distributions.prior.discrete.uniform.DiscreteUniformPrior.sample","title":"<code>sample(shape, mask=None, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Tuple</code> <p>The shape of the samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the samples. Defaults to None.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Tensor</code> <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/prior/discrete/uniform.py</code> <pre><code>def sample(\n    self,\n    shape: Tuple,\n    mask: Optional[Tensor] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Tensor:\n    \"\"\"Generates a specified number of samples.\n\n    Args:\n        shape (Tuple): The shape of the samples to generate.\n        device (str): cpu or gpu.\n        mask (Optional[Tensor]): An optional mask to apply to the samples. Defaults to None.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A tensor of samples.\n    \"\"\"\n    samples = torch.randint(0, self.num_classes, shape, device=device, generator=rng_generator)\n    if mask is not None:\n        samples = samples * mask[(...,) + (None,) * (len(samples.shape) - len(mask.shape))]\n    return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/beta/","title":"Beta","text":""},{"location":"API_reference/bionemo/moco/distributions/time/beta/#bionemo.moco.distributions.time.beta.BetaTimeDistribution","title":"<code>BetaTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a beta time distribution.</p> Source code in <code>bionemo/moco/distributions/time/beta.py</code> <pre><code>class BetaTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a beta time distribution.\"\"\"\n\n    def __init__(\n        self,\n        p1: Float = 2.0,\n        p2: Float = 1.0,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a BetaTimeDistribution object.\n\n        Args:\n            p1 (Float): The first shape parameter of the beta distribution.\n            p2 (Float): The second shape parameter of the beta distribution.\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n        self.dist = torch.distributions.Beta(p1, p2)\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        if isinstance(n_samples, int):\n            n_samples = torch.Size([n_samples])\n        elif isinstance(n_samples, tuple):\n            n_samples = torch.Size(n_samples)\n        time_step = self.dist.sample(n_samples).to(device=device)\n        if self.min_t and self.max_t and self.min_t &gt; 0:\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = float_time_to_index(time_step, self.nsteps)\n        return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/beta/#bionemo.moco.distributions.time.beta.BetaTimeDistribution.__init__","title":"<code>__init__(p1=2.0, p2=1.0, min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a BetaTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Float</code> <p>The first shape parameter of the beta distribution.</p> <code>2.0</code> <code>p2</code> <code>Float</code> <p>The second shape parameter of the beta distribution.</p> <code>1.0</code> <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/beta.py</code> <pre><code>def __init__(\n    self,\n    p1: Float = 2.0,\n    p2: Float = 1.0,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a BetaTimeDistribution object.\n\n    Args:\n        p1 (Float): The first shape parameter of the beta distribution.\n        p2 (Float): The second shape parameter of the beta distribution.\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n    self.dist = torch.distributions.Beta(p1, p2)\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/beta/#bionemo.moco.distributions.time.beta.BetaTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/beta.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    if isinstance(n_samples, int):\n        n_samples = torch.Size([n_samples])\n    elif isinstance(n_samples, tuple):\n        n_samples = torch.Size(n_samples)\n    time_step = self.dist.sample(n_samples).to(device=device)\n    if self.min_t and self.max_t and self.min_t &gt; 0:\n        time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = float_time_to_index(time_step, self.nsteps)\n    return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/distribution/","title":"Distribution","text":""},{"location":"API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.MixTimeDistribution","title":"<code>MixTimeDistribution</code>","text":"<p>An abstract base class representing a mixed time distribution.</p> <p>uniform_dist = UniformTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False) beta_dist = BetaTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False, p1=2.0, p2=1.0) mix_dist = MixTimeDistribution(uniform_dist, beta_dist, mix_fraction=0.5)</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>class MixTimeDistribution:\n    \"\"\"An abstract base class representing a mixed time distribution.\n\n    uniform_dist = UniformTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False)\n    beta_dist = BetaTimeDistribution(min_t=0.0, max_t=1.0, discrete_time=False, p1=2.0, p2=1.0)\n    mix_dist = MixTimeDistribution(uniform_dist, beta_dist, mix_fraction=0.5)\n    \"\"\"\n\n    def __init__(self, dist1: TimeDistribution, dist2: TimeDistribution, mix_fraction: Float):\n        \"\"\"Initializes a MixTimeDistribution object.\n\n        Args:\n            dist1 (TimeDistribution): The first time distribution.\n            dist2 (TimeDistribution): The second time distribution.\n            mix_fraction (Float): The fraction of samples to draw from dist1. Must be between 0 and 1.\n        \"\"\"\n        if not 0 &lt;= mix_fraction &lt;= 1:\n            raise ValueError(\"mix_fraction must be between 0 and 1\")\n        self.dist1 = dist1\n        self.dist2 = dist2\n        self.mix_fraction = mix_fraction\n\n    def sample(\n        self, n_samples: int, device: Union[str, torch.device] = \"cpu\", rng_generator: Optional[torch.Generator] = None\n    ) -&gt; Float:\n        \"\"\"Generates a specified number of samples from the mixed time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A list or array of samples.\n        \"\"\"\n        samples_dist1 = self.dist1.sample(n_samples, device)\n        samples_dist2 = self.dist2.sample(n_samples, device)\n        mix = torch.rand(n_samples, device=device, generator=rng_generator)\n        return torch.where(mix &lt; self.mix_fraction, samples_dist1, samples_dist2)\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.MixTimeDistribution.__init__","title":"<code>__init__(dist1, dist2, mix_fraction)</code>","text":"<p>Initializes a MixTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>dist1</code> <code>TimeDistribution</code> <p>The first time distribution.</p> required <code>dist2</code> <code>TimeDistribution</code> <p>The second time distribution.</p> required <code>mix_fraction</code> <code>Float</code> <p>The fraction of samples to draw from dist1. Must be between 0 and 1.</p> required Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>def __init__(self, dist1: TimeDistribution, dist2: TimeDistribution, mix_fraction: Float):\n    \"\"\"Initializes a MixTimeDistribution object.\n\n    Args:\n        dist1 (TimeDistribution): The first time distribution.\n        dist2 (TimeDistribution): The second time distribution.\n        mix_fraction (Float): The fraction of samples to draw from dist1. Must be between 0 and 1.\n    \"\"\"\n    if not 0 &lt;= mix_fraction &lt;= 1:\n        raise ValueError(\"mix_fraction must be between 0 and 1\")\n    self.dist1 = dist1\n    self.dist2 = dist2\n    self.mix_fraction = mix_fraction\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.MixTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the mixed time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Float</code> <code>Float</code> <p>A list or array of samples.</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>def sample(\n    self, n_samples: int, device: Union[str, torch.device] = \"cpu\", rng_generator: Optional[torch.Generator] = None\n) -&gt; Float:\n    \"\"\"Generates a specified number of samples from the mixed time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A list or array of samples.\n    \"\"\"\n    samples_dist1 = self.dist1.sample(n_samples, device)\n    samples_dist2 = self.dist2.sample(n_samples, device)\n    mix = torch.rand(n_samples, device=device, generator=rng_generator)\n    return torch.where(mix &lt; self.mix_fraction, samples_dist1, samples_dist2)\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.TimeDistribution","title":"<code>TimeDistribution</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class representing a time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>min_t</code> <code>Optional[Float]</code> <p>Min continuous time.</p> <code>None</code> <code>max_t</code> <code>Optional[Float]</code> <p>Max continuous time.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>class TimeDistribution(ABC):\n    \"\"\"An abstract base class representing a time distribution.\n\n    Args:\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        min_t (Optional[Float]): Min continuous time.\n        max_t (Optional[Float]): Max continuous time.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        min_t: Optional[Float] = None,\n        max_t: Optional[Float] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a TimeDistribution object.\"\"\"\n        self.discrete_time = discrete_time\n        self.nsteps = nsteps\n        self.rng_generator = rng_generator\n        if discrete_time:\n            min_t = 0.0\n            max_t = 1.0\n            if nsteps is None:\n                raise ValueError(\"nsteps must not be None and must be specified for discrete time\")\n        if min_t is not None and isinstance(min_t, float):\n            if not 0 &lt;= min_t &lt; 1.0:\n                raise ValueError(\"min_t must be greater than or equal to 0 and less than 1.0\")\n        self.min_t = min_t\n        if max_t is not None and isinstance(max_t, float):\n            if not 0 &lt; max_t &lt;= 1.0:\n                raise ValueError(\"max_t must be greater than 0 and less than or equal to 1.0\")\n        self.max_t = max_t\n        if (\n            self.min_t is not None\n            and self.max_t is not None\n            and isinstance(self.min_t, float)\n            and isinstance(self.max_t, float)\n        ):\n            if self.min_t &gt;= self.max_t:\n                raise ValueError(\"min_t must be less than max_t\")\n\n    @abstractmethod\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ) -&gt; Float:\n        \"\"\"Generates a specified number of samples from the time distribution.\n\n        Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            Float: A list or array of samples.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.TimeDistribution.__init__","title":"<code>__init__(discrete_time=False, nsteps=None, min_t=None, max_t=None, rng_generator=None)</code>","text":"<p>Initializes a TimeDistribution object.</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>def __init__(\n    self,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    min_t: Optional[Float] = None,\n    max_t: Optional[Float] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a TimeDistribution object.\"\"\"\n    self.discrete_time = discrete_time\n    self.nsteps = nsteps\n    self.rng_generator = rng_generator\n    if discrete_time:\n        min_t = 0.0\n        max_t = 1.0\n        if nsteps is None:\n            raise ValueError(\"nsteps must not be None and must be specified for discrete time\")\n    if min_t is not None and isinstance(min_t, float):\n        if not 0 &lt;= min_t &lt; 1.0:\n            raise ValueError(\"min_t must be greater than or equal to 0 and less than 1.0\")\n    self.min_t = min_t\n    if max_t is not None and isinstance(max_t, float):\n        if not 0 &lt; max_t &lt;= 1.0:\n            raise ValueError(\"max_t must be greater than 0 and less than or equal to 1.0\")\n    self.max_t = max_t\n    if (\n        self.min_t is not None\n        and self.max_t is not None\n        and isinstance(self.min_t, float)\n        and isinstance(self.max_t, float)\n    ):\n        if self.min_t &gt;= self.max_t:\n            raise ValueError(\"min_t must be less than max_t\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/distribution/#bionemo.moco.distributions.time.distribution.TimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>  <code>abstractmethod</code>","text":"<p>Generates a specified number of samples from the time distribution.</p> <p>Args: n_samples (int): The number of samples to generate. device (str): cpu or gpu. rng_generator: An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <p>Returns:</p> Name Type Description <code>Float</code> <code>Float</code> <p>A list or array of samples.</p> Source code in <code>bionemo/moco/distributions/time/distribution.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n) -&gt; Float:\n    \"\"\"Generates a specified number of samples from the time distribution.\n\n    Args:\n    n_samples (int): The number of samples to generate.\n    device (str): cpu or gpu.\n    rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        Float: A list or array of samples.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/logit_normal/","title":"Logit normal","text":""},{"location":"API_reference/bionemo/moco/distributions/time/logit_normal/#bionemo.moco.distributions.time.logit_normal.LogitNormalTimeDistribution","title":"<code>LogitNormalTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a logit normal time distribution.</p> Source code in <code>bionemo/moco/distributions/time/logit_normal.py</code> <pre><code>class LogitNormalTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a logit normal time distribution.\"\"\"\n\n    def __init__(\n        self,\n        p1: Float = 0.0,\n        p2: Float = 1.0,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a BetaTimeDistribution object.\n\n        Args:\n            p1 (Float): The first shape parameter of the logit normal distribution i.e. the mean.\n            p2 (Float): The second shape parameter of the logit normal distribution i.e. the std.\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n        self.p1 = p1\n        self.p2 = p2\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        time_step = torch.randn(n_samples, device=device, generator=rng_generator) * self.p2 + self.p1\n        time_step = torch.nn.functional.sigmoid(time_step)\n        if self.min_t and self.max_t and (self.min_t &gt; 0 or self.max_t &lt; 1):\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = float_time_to_index(time_step, self.nsteps)\n        return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/logit_normal/#bionemo.moco.distributions.time.logit_normal.LogitNormalTimeDistribution.__init__","title":"<code>__init__(p1=0.0, p2=1.0, min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a BetaTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Float</code> <p>The first shape parameter of the logit normal distribution i.e. the mean.</p> <code>0.0</code> <code>p2</code> <code>Float</code> <p>The second shape parameter of the logit normal distribution i.e. the std.</p> <code>1.0</code> <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/logit_normal.py</code> <pre><code>def __init__(\n    self,\n    p1: Float = 0.0,\n    p2: Float = 1.0,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a BetaTimeDistribution object.\n\n    Args:\n        p1 (Float): The first shape parameter of the logit normal distribution i.e. the mean.\n        p2 (Float): The second shape parameter of the logit normal distribution i.e. the std.\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n    self.p1 = p1\n    self.p2 = p2\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/logit_normal/#bionemo.moco.distributions.time.logit_normal.LogitNormalTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/logit_normal.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    time_step = torch.randn(n_samples, device=device, generator=rng_generator) * self.p2 + self.p1\n    time_step = torch.nn.functional.sigmoid(time_step)\n    if self.min_t and self.max_t and (self.min_t &gt; 0 or self.max_t &lt; 1):\n        time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = float_time_to_index(time_step, self.nsteps)\n    return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/uniform/","title":"Uniform","text":""},{"location":"API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.SymmetricUniformTimeDistribution","title":"<code>SymmetricUniformTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a uniform time distribution.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>class SymmetricUniformTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a uniform time distribution.\"\"\"\n\n    def __init__(\n        self,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a UniformTimeDistribution object.\n\n        Args:\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        if not isinstance(n_samples, int):\n            n_samples = n_samples[0]\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = torch.randint(\n                0, self.nsteps, size=(n_samples // 2 + 1,), device=device, generator=rng_generator\n            )\n            time_step = torch.cat([time_step, self.nsteps - time_step - 1], dim=0)[:n_samples]\n        else:\n            time_step = torch.rand(n_samples // 2 + 1, device=device, generator=rng_generator)\n            time_step = torch.cat([time_step, 1 - time_step], dim=0)[:n_samples]\n            if self.min_t and self.max_t and self.min_t &gt; 0:\n                time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.SymmetricUniformTimeDistribution.__init__","title":"<code>__init__(min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a UniformTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def __init__(\n    self,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a UniformTimeDistribution object.\n\n    Args:\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.SymmetricUniformTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    if not isinstance(n_samples, int):\n        n_samples = n_samples[0]\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = torch.randint(\n            0, self.nsteps, size=(n_samples // 2 + 1,), device=device, generator=rng_generator\n        )\n        time_step = torch.cat([time_step, self.nsteps - time_step - 1], dim=0)[:n_samples]\n    else:\n        time_step = torch.rand(n_samples // 2 + 1, device=device, generator=rng_generator)\n        time_step = torch.cat([time_step, 1 - time_step], dim=0)[:n_samples]\n        if self.min_t and self.max_t and self.min_t &gt; 0:\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.UniformTimeDistribution","title":"<code>UniformTimeDistribution</code>","text":"<p>               Bases: <code>TimeDistribution</code></p> <p>A class representing a uniform time distribution.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>class UniformTimeDistribution(TimeDistribution):\n    \"\"\"A class representing a uniform time distribution.\"\"\"\n\n    def __init__(\n        self,\n        min_t: Float = 0.0,\n        max_t: Float = 1.0,\n        discrete_time: Bool = False,\n        nsteps: Optional[int] = None,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes a UniformTimeDistribution object.\n\n        Args:\n            min_t (Float): The minimum time value.\n            max_t (Float): The maximum time value.\n            discrete_time (Bool): Whether the time is discrete.\n            nsteps (Optional[int]): Number of nsteps for discretization.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n\n    def sample(\n        self,\n        n_samples: Union[int, Tuple[int, ...], torch.Size],\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n        Args:\n            n_samples (int): The number of samples to generate.\n            device (str): cpu or gpu.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n        Returns:\n            A tensor of samples.\n        \"\"\"\n        if rng_generator is None:\n            rng_generator = self.rng_generator\n        if self.discrete_time:\n            if self.nsteps is None:\n                raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n            time_step = torch.randint(\n                0,\n                self.nsteps,\n                size=(n_samples,) if isinstance(n_samples, int) else n_samples,\n                device=device,\n                generator=rng_generator,\n            )\n        else:\n            time_step = torch.rand(n_samples, device=device, generator=rng_generator)\n            if self.min_t and self.max_t and self.min_t &gt; 0:\n                time_step = time_step * (self.max_t - self.min_t) + self.min_t\n        return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.UniformTimeDistribution.__init__","title":"<code>__init__(min_t=0.0, max_t=1.0, discrete_time=False, nsteps=None, rng_generator=None)</code>","text":"<p>Initializes a UniformTimeDistribution object.</p> <p>Parameters:</p> Name Type Description Default <code>min_t</code> <code>Float</code> <p>The minimum time value.</p> <code>0.0</code> <code>max_t</code> <code>Float</code> <p>The maximum time value.</p> <code>1.0</code> <code>discrete_time</code> <code>Bool</code> <p>Whether the time is discrete.</p> <code>False</code> <code>nsteps</code> <code>Optional[int]</code> <p>Number of nsteps for discretization.</p> <code>None</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def __init__(\n    self,\n    min_t: Float = 0.0,\n    max_t: Float = 1.0,\n    discrete_time: Bool = False,\n    nsteps: Optional[int] = None,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes a UniformTimeDistribution object.\n\n    Args:\n        min_t (Float): The minimum time value.\n        max_t (Float): The maximum time value.\n        discrete_time (Bool): Whether the time is discrete.\n        nsteps (Optional[int]): Number of nsteps for discretization.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(discrete_time, nsteps, min_t, max_t, rng_generator)\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/uniform/#bionemo.moco.distributions.time.uniform.UniformTimeDistribution.sample","title":"<code>sample(n_samples, device='cpu', rng_generator=None)</code>","text":"<p>Generates a specified number of samples from the uniform time distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>device</code> <code>str</code> <p>cpu or gpu.</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tensor of samples.</p> Source code in <code>bionemo/moco/distributions/time/uniform.py</code> <pre><code>def sample(\n    self,\n    n_samples: Union[int, Tuple[int, ...], torch.Size],\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Generates a specified number of samples from the uniform time distribution.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        device (str): cpu or gpu.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n\n    Returns:\n        A tensor of samples.\n    \"\"\"\n    if rng_generator is None:\n        rng_generator = self.rng_generator\n    if self.discrete_time:\n        if self.nsteps is None:\n            raise ValueError(\"nsteps cannot be None for discrete time sampling\")\n        time_step = torch.randint(\n            0,\n            self.nsteps,\n            size=(n_samples,) if isinstance(n_samples, int) else n_samples,\n            device=device,\n            generator=rng_generator,\n        )\n    else:\n        time_step = torch.rand(n_samples, device=device, generator=rng_generator)\n        if self.min_t and self.max_t and self.min_t &gt; 0:\n            time_step = time_step * (self.max_t - self.min_t) + self.min_t\n    return time_step\n</code></pre>"},{"location":"API_reference/bionemo/moco/distributions/time/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/moco/distributions/time/utils/#bionemo.moco.distributions.time.utils.float_time_to_index","title":"<code>float_time_to_index(time, num_time_steps)</code>","text":"<p>Convert a float time value to a time index.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>Tensor</code> <p>A tensor of float time values in the range [0, 1].</p> required <code>num_time_steps</code> <code>int</code> <p>The number of discrete time steps.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor of time indices corresponding to the input float time values.</p> Source code in <code>bionemo/moco/distributions/time/utils.py</code> <pre><code>def float_time_to_index(time: torch.Tensor, num_time_steps: int) -&gt; torch.Tensor:\n    \"\"\"Convert a float time value to a time index.\n\n    Args:\n        time (torch.Tensor): A tensor of float time values in the range [0, 1].\n        num_time_steps (int): The number of discrete time steps.\n\n    Returns:\n        torch.Tensor: A tensor of time indices corresponding to the input float time values.\n    \"\"\"\n    # Ensure time values are in the range [0, 1]\n    time = torch.clamp(time, 0.0, 1.0)\n\n    # Scale to the index range and round\n    indices = torch.round(time * (num_time_steps - 1)).to(torch.int64)\n\n    return indices\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/","title":"Base interpolant","text":""},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant","title":"<code>Interpolant</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class representing an Interpolant.</p> <p>This class serves as a foundation for creating interpolants that can be used in various applications, providing a basic structure and interface for interpolation-related operations.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>class Interpolant(ABC):\n    \"\"\"An abstract base class representing an Interpolant.\n\n    This class serves as a foundation for creating interpolants that can be used\n    in various applications, providing a basic structure and interface for\n    interpolation-related operations.\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the Interpolant class.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps.\n            prior_distribution (PriorDistribution): The prior distribution of the variable.\n            device (Union[str, torch.device], optional): The device on which to operate. Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        self.time_distribution = time_distribution\n        self.prior_distribution = prior_distribution\n        self.device = device\n        self.rng_generator = rng_generator\n\n    @abstractmethod\n    def interpolate(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Interpolate between x0 and x1 at the given time t.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def step(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Do one step integration.\"\"\"\n        pass\n\n    def general_step(self, method_name: str, kwargs: dict):\n        \"\"\"Calls a step method of the class by its name, passing the provided keyword arguments.\n\n        Args:\n            method_name (str): The name of the step method to call.\n            kwargs (dict): Keyword arguments to pass to the step method.\n\n        Returns:\n            The result of the step method call.\n\n        Raises:\n            ValueError: If the provided method name does not start with 'step'.\n            Exception: If the step method call fails. The error message includes a list of available step methods.\n\n        Note:\n            This method allows for dynamic invocation of step methods, providing flexibility in the class's usage.\n        \"\"\"\n        if not method_name.startswith(\"step\"):\n            raise ValueError(f\"Method name '{method_name}' does not start with 'step'\")\n\n        try:\n            # Get the step method by its name\n            func = getattr(self, method_name)\n            # Call the step method with the provided keyword arguments\n            return func(**kwargs)\n        except Exception as e:\n            # Get a list of available step methods\n            available_methods = \"\\n\".join([f\"  - {attr}\" for attr in dir(self) if attr.startswith(\"step\")])\n            # Create a detailed error message\n            error_message = f\"Error calling method '{method_name}': {e}\\nAvailable step methods:\\n{available_methods}\"\n            # Re-raise the exception with the detailed error message\n            raise type(e)(error_message)\n\n    def sample_prior(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Sample from prior distribution.\n\n        This method generates a sample from the prior distribution specified by the\n        `prior_distribution` attribute.\n\n        Returns:\n            Tensor: The generated sample from the prior distribution.\n        \"\"\"\n        # Ensure the device is specified, default to self.device if not provided\n        if \"device\" not in kwargs:\n            kwargs[\"device\"] = self.device\n        kwargs[\"rng_generator\"] = self.rng_generator\n        # Sample from the prior distribution\n        return self.prior_distribution.sample(*args, **kwargs)\n\n    def sample_time(self, *args, **kwargs) -&gt; Tensor:\n        \"\"\"Sample from time distribution.\"\"\"\n        # Ensure the device is specified, default to self.device if not provided\n        if \"device\" not in kwargs:\n            kwargs[\"device\"] = self.device\n        kwargs[\"rng_generator\"] = self.rng_generator\n        # Sample from the time distribution\n        return self.time_distribution.sample(*args, **kwargs)\n\n    def to_device(self, device: str):\n        \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n        Args:\n            device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n        Note:\n            This method is used to transfer the internal state of the DDPM interpolant to a different device.\n            It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n        \"\"\"\n        self.device = device\n        for attr_name in dir(self):\n            if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n                setattr(self, attr_name, getattr(self, attr_name).to(device))\n        return self\n\n    def clean_mask_center(self, data: Tensor, mask: Optional[Tensor] = None, center: Bool = False) -&gt; Tensor:\n        \"\"\"Returns a clean tensor that has been masked and/or centered based on the function arguments.\n\n        Args:\n            data: The input data with shape (..., nodes, features).\n            mask: An optional mask to apply to the data with shape (..., nodes). If provided, it is used to calculate the CoM. Defaults to None.\n            center: A boolean indicating whether to center the data around the calculated CoM. Defaults to False.\n\n        Returns:\n            The data with shape (..., nodes, features) either centered around the CoM if `center` is True or unchanged if `center` is False.\n        \"\"\"\n        if mask is not None:\n            data = data * mask.unsqueeze(-1)\n        if not center:\n            return data\n        if mask is None:\n            num_nodes = torch.tensor(data.shape[1], device=data.device)\n        else:\n            num_nodes = torch.clamp(mask.sum(dim=-1), min=1)  # clamp used to prevent divide by 0\n        com = data.sum(dim=-2) / num_nodes.unsqueeze(-1)\n        return data - com.unsqueeze(-2)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.__init__","title":"<code>__init__(time_distribution, prior_distribution, device='cpu', rng_generator=None)</code>","text":"<p>Initializes the Interpolant class.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable.</p> required <code>device</code> <code>Union[str, device]</code> <p>The device on which to operate. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the Interpolant class.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps.\n        prior_distribution (PriorDistribution): The prior distribution of the variable.\n        device (Union[str, torch.device], optional): The device on which to operate. Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    self.time_distribution = time_distribution\n    self.prior_distribution = prior_distribution\n    self.device = device\n    self.rng_generator = rng_generator\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.clean_mask_center","title":"<code>clean_mask_center(data, mask=None, center=False)</code>","text":"<p>Returns a clean tensor that has been masked and/or centered based on the function arguments.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data with shape (..., nodes, features).</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data with shape (..., nodes). If provided, it is used to calculate the CoM. Defaults to None.</p> <code>None</code> <code>center</code> <code>Bool</code> <p>A boolean indicating whether to center the data around the calculated CoM. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The data with shape (..., nodes, features) either centered around the CoM if <code>center</code> is True or unchanged if <code>center</code> is False.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def clean_mask_center(self, data: Tensor, mask: Optional[Tensor] = None, center: Bool = False) -&gt; Tensor:\n    \"\"\"Returns a clean tensor that has been masked and/or centered based on the function arguments.\n\n    Args:\n        data: The input data with shape (..., nodes, features).\n        mask: An optional mask to apply to the data with shape (..., nodes). If provided, it is used to calculate the CoM. Defaults to None.\n        center: A boolean indicating whether to center the data around the calculated CoM. Defaults to False.\n\n    Returns:\n        The data with shape (..., nodes, features) either centered around the CoM if `center` is True or unchanged if `center` is False.\n    \"\"\"\n    if mask is not None:\n        data = data * mask.unsqueeze(-1)\n    if not center:\n        return data\n    if mask is None:\n        num_nodes = torch.tensor(data.shape[1], device=data.device)\n    else:\n        num_nodes = torch.clamp(mask.sum(dim=-1), min=1)  # clamp used to prevent divide by 0\n    com = data.sum(dim=-2) / num_nodes.unsqueeze(-1)\n    return data - com.unsqueeze(-2)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.general_step","title":"<code>general_step(method_name, kwargs)</code>","text":"<p>Calls a step method of the class by its name, passing the provided keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>method_name</code> <code>str</code> <p>The name of the step method to call.</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments to pass to the step method.</p> required <p>Returns:</p> Type Description <p>The result of the step method call.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided method name does not start with 'step'.</p> <code>Exception</code> <p>If the step method call fails. The error message includes a list of available step methods.</p> Note <p>This method allows for dynamic invocation of step methods, providing flexibility in the class's usage.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def general_step(self, method_name: str, kwargs: dict):\n    \"\"\"Calls a step method of the class by its name, passing the provided keyword arguments.\n\n    Args:\n        method_name (str): The name of the step method to call.\n        kwargs (dict): Keyword arguments to pass to the step method.\n\n    Returns:\n        The result of the step method call.\n\n    Raises:\n        ValueError: If the provided method name does not start with 'step'.\n        Exception: If the step method call fails. The error message includes a list of available step methods.\n\n    Note:\n        This method allows for dynamic invocation of step methods, providing flexibility in the class's usage.\n    \"\"\"\n    if not method_name.startswith(\"step\"):\n        raise ValueError(f\"Method name '{method_name}' does not start with 'step'\")\n\n    try:\n        # Get the step method by its name\n        func = getattr(self, method_name)\n        # Call the step method with the provided keyword arguments\n        return func(**kwargs)\n    except Exception as e:\n        # Get a list of available step methods\n        available_methods = \"\\n\".join([f\"  - {attr}\" for attr in dir(self) if attr.startswith(\"step\")])\n        # Create a detailed error message\n        error_message = f\"Error calling method '{method_name}': {e}\\nAvailable step methods:\\n{available_methods}\"\n        # Re-raise the exception with the detailed error message\n        raise type(e)(error_message)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.interpolate","title":"<code>interpolate(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Interpolate between x0 and x1 at the given time t.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>@abstractmethod\ndef interpolate(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Interpolate between x0 and x1 at the given time t.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.sample_prior","title":"<code>sample_prior(*args, **kwargs)</code>","text":"<p>Sample from prior distribution.</p> <p>This method generates a sample from the prior distribution specified by the <code>prior_distribution</code> attribute.</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The generated sample from the prior distribution.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def sample_prior(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Sample from prior distribution.\n\n    This method generates a sample from the prior distribution specified by the\n    `prior_distribution` attribute.\n\n    Returns:\n        Tensor: The generated sample from the prior distribution.\n    \"\"\"\n    # Ensure the device is specified, default to self.device if not provided\n    if \"device\" not in kwargs:\n        kwargs[\"device\"] = self.device\n    kwargs[\"rng_generator\"] = self.rng_generator\n    # Sample from the prior distribution\n    return self.prior_distribution.sample(*args, **kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.sample_time","title":"<code>sample_time(*args, **kwargs)</code>","text":"<p>Sample from time distribution.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def sample_time(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Sample from time distribution.\"\"\"\n    # Ensure the device is specified, default to self.device if not provided\n    if \"device\" not in kwargs:\n        kwargs[\"device\"] = self.device\n    kwargs[\"rng_generator\"] = self.rng_generator\n    # Sample from the time distribution\n    return self.time_distribution.sample(*args, **kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.step","title":"<code>step(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Do one step integration.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>@abstractmethod\ndef step(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"Do one step integration.\"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.Interpolant.to_device","title":"<code>to_device(device)</code>","text":"<p>Moves all internal tensors to the specified device and updates the <code>self.device</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").</p> required Note <p>This method is used to transfer the internal state of the DDPM interpolant to a different device. It updates the <code>self.device</code> attribute to reflect the new device and moves all internal tensors to the specified device.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def to_device(self, device: str):\n    \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n    Args:\n        device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n    Note:\n        This method is used to transfer the internal state of the DDPM interpolant to a different device.\n        It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n    \"\"\"\n    self.device = device\n    for attr_name in dir(self):\n        if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n            setattr(self, attr_name, getattr(self, attr_name).to(device))\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.PredictionType","title":"<code>PredictionType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An enumeration representing the type of prediction a Denoising Diffusion Probabilistic Model (DDPM) can be used for.</p> <p>DDPMs are versatile models that can be utilized for various prediction tasks, including:</p> <ul> <li>Data: Predicting the original data distribution from a noisy input.</li> <li>Noise: Predicting the noise that was added to the original data to obtain the input.</li> <li>Velocity: Predicting the velocity or rate of change of the data, particularly useful for modeling temporal dynamics.</li> </ul> <p>These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>class PredictionType(Enum):\n    \"\"\"An enumeration representing the type of prediction a Denoising Diffusion Probabilistic Model (DDPM) can be used for.\n\n    DDPMs are versatile models that can be utilized for various prediction tasks, including:\n\n    - **Data**: Predicting the original data distribution from a noisy input.\n    - **Noise**: Predicting the noise that was added to the original data to obtain the input.\n    - **Velocity**: Predicting the velocity or rate of change of the data, particularly useful for modeling temporal dynamics.\n\n    These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.\n    \"\"\"\n\n    DATA = \"data\"\n    NOISE = \"noise\"\n    VELOCITY = \"velocity\"\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.pad_like","title":"<code>pad_like(source, target)</code>","text":"<p>Pads the dimensions of the source tensor to match the dimensions of the target tensor.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Tensor</code> <p>The tensor to be padded.</p> required <code>target</code> <code>Tensor</code> <p>The tensor that the source tensor should match in dimensions.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The padded source tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source tensor has more dimensions than the target tensor.</p> Example <p>source = torch.tensor([1, 2, 3])  # shape: (3,) target = torch.tensor([[1, 2], [4, 5], [7, 8]])  # shape: (3, 2) padded_source = pad_like(source, target)  # shape: (3, 1)</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def pad_like(source: Tensor, target: Tensor) -&gt; Tensor:\n    \"\"\"Pads the dimensions of the source tensor to match the dimensions of the target tensor.\n\n    Args:\n        source (Tensor): The tensor to be padded.\n        target (Tensor): The tensor that the source tensor should match in dimensions.\n\n    Returns:\n        Tensor: The padded source tensor.\n\n    Raises:\n        ValueError: If the source tensor has more dimensions than the target tensor.\n\n    Example:\n        &gt;&gt;&gt; source = torch.tensor([1, 2, 3])  # shape: (3,)\n        &gt;&gt;&gt; target = torch.tensor([[1, 2], [4, 5], [7, 8]])  # shape: (3, 2)\n        &gt;&gt;&gt; padded_source = pad_like(source, target)  # shape: (3, 1)\n    \"\"\"\n    if source.ndim == target.ndim:\n        return source\n    elif source.ndim &gt; target.ndim:\n        raise ValueError(f\"Cannot pad {source.shape} to {target.shape}\")\n    return source.view(list(source.shape) + [1] * (target.ndim - source.ndim))\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/base_interpolant/#bionemo.moco.interpolants.base_interpolant.string_to_enum","title":"<code>string_to_enum(value, enum_type)</code>","text":"<p>Converts a string to an enum value of the specified type. If the input is already an enum instance, it is returned as-is.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, E]</code> <p>The string to convert or an existing enum instance.</p> required <code>enum_type</code> <code>Type[E]</code> <p>The enum type to convert to.</p> required <p>Returns:</p> Name Type Description <code>E</code> <code>AnyEnum</code> <p>The corresponding enum value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the string does not correspond to any enum member.</p> Source code in <code>bionemo/moco/interpolants/base_interpolant.py</code> <pre><code>def string_to_enum(value: Union[str, AnyEnum], enum_type: Type[AnyEnum]) -&gt; AnyEnum:\n    \"\"\"Converts a string to an enum value of the specified type. If the input is already an enum instance, it is returned as-is.\n\n    Args:\n        value (Union[str, E]): The string to convert or an existing enum instance.\n        enum_type (Type[E]): The enum type to convert to.\n\n    Returns:\n        E: The corresponding enum value.\n\n    Raises:\n        ValueError: If the string does not correspond to any enum member.\n    \"\"\"\n    if isinstance(value, enum_type):\n        # If the value is already an enum, return it\n        return value\n\n    try:\n        # Match the value to the Enum, case-insensitively\n        return enum_type(value)\n    except ValueError:\n        # Raise a helpful error if the value is invalid\n        valid_values = [e.value for e in enum_type]\n        raise ValueError(f\"Invalid value '{value}'. Expected one of {valid_values}.\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/batch_augmentation/","title":"Batch augmentation","text":""},{"location":"API_reference/bionemo/moco/interpolants/batch_augmentation/#bionemo.moco.interpolants.batch_augmentation.BatchDataAugmentation","title":"<code>BatchDataAugmentation</code>","text":"<p>Facilitates the creation of batch augmentation objects based on specified optimal transport types.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to use for computations (e.g., 'cpu', 'cuda').</p> required <code>num_threads</code> <code>int</code> <p>The number of threads to utilize.</p> required Source code in <code>bionemo/moco/interpolants/batch_augmentation.py</code> <pre><code>class BatchDataAugmentation:\n    \"\"\"Facilitates the creation of batch augmentation objects based on specified optimal transport types.\n\n    Args:\n        device (str): The device to use for computations (e.g., 'cpu', 'cuda').\n        num_threads (int): The number of threads to utilize.\n    \"\"\"\n\n    def __init__(self, device, num_threads):\n        \"\"\"Initializes a BatchAugmentation instance.\n\n        Args:\n            device (str): Device for computation.\n            num_threads (int): Number of threads to use.\n        \"\"\"\n        self.device = device\n        self.num_threads = num_threads\n\n    def create(self, method_type: AugmentationType):\n        \"\"\"Creates a batch augmentation object of the specified type.\n\n        Args:\n            method_type (AugmentationType): The type of optimal transport method.\n\n        Returns:\n            The augmentation object if the type is supported, otherwise **None**.\n        \"\"\"\n        if method_type == AugmentationType.EXACT_OT:\n            augmentation = OTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n        elif method_type == AugmentationType.KABSCH:\n            augmentation = KabschAugmentation()\n        elif method_type == AugmentationType.EQUIVARIANT_OT:\n            augmentation = EquivariantOTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n        else:\n            return None\n        return augmentation\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/batch_augmentation/#bionemo.moco.interpolants.batch_augmentation.BatchDataAugmentation.__init__","title":"<code>__init__(device, num_threads)</code>","text":"<p>Initializes a BatchAugmentation instance.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>Device for computation.</p> required <code>num_threads</code> <code>int</code> <p>Number of threads to use.</p> required Source code in <code>bionemo/moco/interpolants/batch_augmentation.py</code> <pre><code>def __init__(self, device, num_threads):\n    \"\"\"Initializes a BatchAugmentation instance.\n\n    Args:\n        device (str): Device for computation.\n        num_threads (int): Number of threads to use.\n    \"\"\"\n    self.device = device\n    self.num_threads = num_threads\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/batch_augmentation/#bionemo.moco.interpolants.batch_augmentation.BatchDataAugmentation.create","title":"<code>create(method_type)</code>","text":"<p>Creates a batch augmentation object of the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>method_type</code> <code>AugmentationType</code> <p>The type of optimal transport method.</p> required <p>Returns:</p> Type Description <p>The augmentation object if the type is supported, otherwise None.</p> Source code in <code>bionemo/moco/interpolants/batch_augmentation.py</code> <pre><code>def create(self, method_type: AugmentationType):\n    \"\"\"Creates a batch augmentation object of the specified type.\n\n    Args:\n        method_type (AugmentationType): The type of optimal transport method.\n\n    Returns:\n        The augmentation object if the type is supported, otherwise **None**.\n    \"\"\"\n    if method_type == AugmentationType.EXACT_OT:\n        augmentation = OTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n    elif method_type == AugmentationType.KABSCH:\n        augmentation = KabschAugmentation()\n    elif method_type == AugmentationType.EQUIVARIANT_OT:\n        augmentation = EquivariantOTSampler(method=\"exact\", device=self.device, num_threads=self.num_threads)\n    else:\n        return None\n    return augmentation\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/","title":"Continuous flow matching","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher","title":"<code>ContinuousFlowMatcher</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Continuous Flow Matching interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching import ContinuousFlowMatcher\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\nflow_matcher = ContinuousFlowMatcher(\n    time_distribution = UniformTimeDistribution(...),\n    prior_distribution = GaussianPrior(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = flow_matcher.sample_time(batch_size)\n    noise = flow_matcher.sample_prior(data.shape)\n    data, time, noise = flow_matcher.apply_augmentation(noise, data) # Optional, only for OT\n    xt = flow_matcher.interpolate(data, time, noise)\n    flow = flow_matcher.calculate_target(data, noise)\n\n    u_pred = model(xt, time)\n    loss = flow_matcher.loss(u_pred, flow)\n    loss.backward()\n\n# Generation\nx_pred = flow_matcher.sample_prior(data.shape)\ninference_sched = LinearInferenceSchedule(...)\nfor t in inference_sched.generate_schedule():\n    time = inference_sched.pad_time(x_pred.shape[0], t)\n    u_hat = model(x_pred, time)\n    x_pred = flow_matcher.step(u_hat, x_pred, time)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>class ContinuousFlowMatcher(Interpolant):\n    \"\"\"A Continuous Flow Matching interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching import ContinuousFlowMatcher\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n    flow_matcher = ContinuousFlowMatcher(\n        time_distribution = UniformTimeDistribution(...),\n        prior_distribution = GaussianPrior(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = flow_matcher.sample_time(batch_size)\n        noise = flow_matcher.sample_prior(data.shape)\n        data, time, noise = flow_matcher.apply_augmentation(noise, data) # Optional, only for OT\n        xt = flow_matcher.interpolate(data, time, noise)\n        flow = flow_matcher.calculate_target(data, noise)\n\n        u_pred = model(xt, time)\n        loss = flow_matcher.loss(u_pred, flow)\n        loss.backward()\n\n    # Generation\n    x_pred = flow_matcher.sample_prior(data.shape)\n    inference_sched = LinearInferenceSchedule(...)\n    for t in inference_sched.generate_schedule():\n        time = inference_sched.pad_time(x_pred.shape[0], t)\n        u_hat = model(x_pred, time)\n        x_pred = flow_matcher.step(u_hat, x_pred, time)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n        sigma: Float = 0,\n        augmentation_type: Optional[Union[AugmentationType, str]] = None,\n        augmentation_num_threads: int = 1,\n        data_scale: Float = 1.0,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n        eps: Float = 1e-5,\n    ):\n        \"\"\"Initializes the Continuous Flow Matching interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            prediction_type (PredictionType, optional): The type of prediction, either \"flow\" or another type. Defaults to PredictionType.DATA.\n            sigma (Float, optional): The standard deviation of the Gaussian noise added to the interpolated data. Defaults to 0.\n            augmentation_type (Optional[Union[AugmentationType, str]], optional): The type of optimal transport, if applicable. Defaults to None.\n            augmentation_num_threads:  Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n            data_scale (Float, optional): The scale factor for the data. Defaults to 1.0.\n            device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n            eps: Small float to prevent divide by zero\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        self.prediction_type = string_to_enum(prediction_type, PredictionType)\n        if self.prediction_type == PredictionType.NOISE:\n            raise ValueError(\"Prediction type cannot be NOISE for Continuous Flow Matching\")\n        self.sigma = sigma\n        self.augmentation_type = augmentation_type\n        self.data_scale = data_scale\n        self.eps = eps\n        if data_scale &lt;= 0:\n            raise ValueError(\"Data Scale must be &gt; 0\")\n        if augmentation_type is not None:\n            self.augmentation_type = augmentation_type = string_to_enum(augmentation_type, AugmentationType)\n            self.augmentation_sampler = self._build_augmentation_sampler(\n                method_type=augmentation_type, num_threads=augmentation_num_threads\n            )\n        self._loss_function = nn.MSELoss(reduction=\"none\")\n\n    def _build_augmentation_sampler(self, method_type: AugmentationType, num_threads: int = 1):\n        \"\"\"Build the optimal transport sampler for the given optimal transport type.\n\n        Args:\n            method_type (augmentation_type): The type of augmentation.\n            num_threads (int): The number of threads to use for the OT sampler, default to 1.\n\n        Returns:\n            The augmentation object.\n        \"\"\"\n        return BatchDataAugmentation(self.device, num_threads).create(method_type)\n\n    def apply_augmentation(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None, **kwargs) -&gt; tuple:\n        \"\"\"Sample and apply the optimal transport plan between batched (and masked) x0 and x1.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            **kwargs: Additional keyword arguments to be passed to self.augmentation_sampler.apply_augmentation or handled within this method.\n\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n        \"\"\"\n        if self.augmentation_sampler is None:\n            raise ValueError(\"Optimal Transport Sampler is not defined\")\n        return self.augmentation_sampler.apply_augmentation(x0, x1, mask=mask, **kwargs)\n\n    def undo_scale_data(self, data: Tensor) -&gt; Tensor:\n        \"\"\"Downscale the input data by the data scale factor.\n\n        Args:\n            data (Tensor): The input data to downscale.\n\n        Returns:\n            The downscaled data.\n        \"\"\"\n        return 1 / self.data_scale * data\n\n    def scale_data(self, data: Tensor) -&gt; Tensor:\n        \"\"\"Upscale the input data by the data scale factor.\n\n        Args:\n            data (Tensor): The input data to upscale.\n\n        Returns:\n            The upscaled data.\n        \"\"\"\n        return self.data_scale * data\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n        \"\"\"Get x_t with given time t from noise (x_0) and data (x_1).\n\n        Currently, we use the linear interpolation as defined in:\n            1. Rectified flow: https://arxiv.org/abs/2209.03003.\n            2. Conditional flow matching: https://arxiv.org/abs/2210.02747 (called conditional optimal transport).\n\n        Args:\n            noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n            t (Tensor): time, shape (batchsize)\n            data (Tensor): target, shape (batchsize, nodes, features)\n        \"\"\"\n        # Expand t to the same shape as noise: ones([b,n,f]) * t([b,1,1])\n        t = pad_like(t, data)\n        # Calculate x_t as the linear interpolation between noise and data\n        x_t = data * t + noise * (1.0 - t)\n        # Add Gaussian Noise\n        if self.sigma &gt; 0:\n            x_t += self.sigma * torch.randn(x_t.shape, device=x_t.device, generator=self.rng_generator)\n        return x_t\n\n    def calculate_target(self, data: Tensor, noise: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n        \"\"\"Get the target vector field at time t.\n\n        Args:\n            noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n            data (Tensor): target, shape (batchsize, nodes, features)\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            Tensor: The target vector field at time t.\n        \"\"\"\n        # Calculate the target vector field u_t(x_t|x_1) as the difference between data and noise because t~[0,1]\n        if self.prediction_type == PredictionType.VELOCITY:\n            u_t = data - noise\n        elif self.prediction_type == PredictionType.DATA:\n            u_t = data\n        else:\n            raise ValueError(\n                f\"Given prediction_type {self.prediction_type} is not supproted for Continuous Flow Matching.\"\n            )\n        if mask is not None:\n            u_t = u_t * mask.unsqueeze(-1)\n        return u_t\n\n    def process_vector_field_prediction(\n        self,\n        model_output: Tensor,\n        xt: Optional[Tensor] = None,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n    ):\n        \"\"\"Process the model output based on the prediction type to calculate vecotr field.\n\n        Args:\n            model_output (Tensor): The output of the model.\n            xt (Tensor): The input sample.\n            t (Tensor): The time step.\n            mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n        Returns:\n            The vector field prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not \"flow\" or \"data\".\n        \"\"\"\n        if self.prediction_type == PredictionType.VELOCITY:\n            pred_vector_field = model_output\n        elif self.prediction_type == PredictionType.DATA:\n            if xt is None or t is None:\n                raise ValueError(\"Xt and Time cannpt be None for vector field conversion\")\n            t = pad_like(t, model_output)\n            pred_vector_field = (model_output - xt) / (1 - t + self.eps)\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be `flow` or `data` \"\n                \"for Continuous Flow Matching.\"\n            )\n        if mask is not None:\n            pred_vector_field = pred_vector_field * mask.unsqueeze(-1)\n        return pred_vector_field\n\n    def process_data_prediction(\n        self,\n        model_output: Tensor,\n        xt: Optional[Tensor] = None,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n    ):\n        \"\"\"Process the model output based on the prediction type to generate clean data.\n\n        Args:\n            model_output (Tensor): The output of the model.\n            xt (Tensor): The input sample.\n            t (Tensor): The time step.\n            mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n        Returns:\n            The data prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not \"flow\".\n        \"\"\"\n        if self.prediction_type == PredictionType.VELOCITY:\n            if xt is None or t is None:\n                raise ValueError(\"Xt and time cannot be None\")\n            t = pad_like(t, model_output)\n            pred_data = xt + (1 - t) * model_output\n        elif self.prediction_type == PredictionType.DATA:\n            pred_data = model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be `flow` \" \"for Continuous Flow Matching.\"\n            )\n        if mask is not None:\n            pred_data = pred_data * mask.unsqueeze(-1)\n        return pred_data\n\n    def step(\n        self,\n        model_out: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n    ):\n        \"\"\"Perform a single ODE step integration using Euler method.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step.\n            xt (Tensor): The current intermediate state.\n            dt (Tensor): The time step size.\n            t (Tensor, optional): The current time. Defaults to None.\n            mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n            center (Bool, optional): Whether to center the output. Defaults to False.\n\n        Returns:\n            x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n        Notes:\n        - If a mask is provided, it is applied element-wise to the model output before scaling.\n        - The `clean` method is called on the updated state before it is returned.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n        dt = pad_like(dt, model_out)\n        delta_x = v_t * dt\n        x_next = xt + delta_x\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def step_score_stochastic(\n        self,\n        model_out: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        t: Tensor,\n        mask: Optional[Tensor] = None,\n        gt_mode: str = \"tan\",\n        gt_p: Float = 1.0,\n        gt_clamp: Optional[Float] = None,\n        score_temperature: Float = 1.0,\n        noise_temperature: Float = 1.0,\n        t_lim_ode: Float = 0.99,\n        center: Bool = False,\n    ):\n        r\"\"\"Perform a single SDE step integration using a score-based Langevin update.\n\n        d x_t = [v(x_t, t) + g(t) * s(x_t, t) * score_temperature] dt + \\sqrt{2 * g(t) * noise_temperature} dw_t.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step.\n            xt (Tensor): The current intermediate state.\n            dt (Tensor): The time step size.\n            t (Tensor, optional): The current time. Defaults to None.\n            mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n            gt_mode (str, optional): The mode for the gt function. Defaults to \"tan\".\n            gt_p (Float, optional): The parameter for the gt function. Defaults to 1.0.\n            gt_clamp: (Float, optional): Upper limit of gt term. Defaults to None.\n            score_temperature (Float, optional): The temperature for the score part of the step. Defaults to 1.0.\n            noise_temperature (Float, optional): The temperature for the stochastic part of the step. Defaults to 1.0.\n            t_lim_ode (Float, optional): The time limit for the ODE step. Defaults to 0.99.\n            center (Bool, optional): Whether to center the output. Defaults to False.\n\n        Returns:\n            x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n        Notes:\n            - If a mask is provided, it is applied element-wise to the model output before scaling.\n            - The `clean` method is called on the updated state before it is returned.\n        \"\"\"\n        if self.augmentation_type is not None:\n            raise ValueError(\"Optimal Transport violates the vector field to score conversion\")\n        if not isinstance(self.prior_distribution, GaussianPrior):\n            raise ValueError(\n                \"Prior distribution must be an instance of GaussianPrior to learn a proper score function\"\n            )\n        if t.min() &gt;= t_lim_ode:\n            return self.step(model_out, xt, dt, t, mask, center)\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n        dt = pad_like(dt, model_out)\n        t = pad_like(t, model_out)\n        score = self.vf_to_score(xt, v_t, t)\n        gt = self.get_gt(t, gt_mode, gt_p, gt_clamp)\n        eps = torch.randn(xt.shape, dtype=xt.dtype, device=xt.device, generator=self.rng_generator)\n        std_eps = torch.sqrt(2 * gt * noise_temperature * dt)\n        delta_x = (v_t + gt * score * score_temperature) * dt + std_eps * eps\n        x_next = xt + delta_x\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def loss(\n        self,\n        model_pred: Tensor,\n        target: Tensor,\n        t: Optional[Tensor] = None,\n        xt: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        target_type: Union[PredictionType, str] = PredictionType.DATA,\n    ):\n        \"\"\"Calculate the loss given the model prediction, data sample, time, and mask.\n\n        If target_type is FLOW loss = ||v_hat - (x1-x0)||**2\n        If target_type is DATA loss = ||x1_hat - x1||**2 * 1 / (1 - t)**2 as the target vector field = x1 - x0 = (1/(1-t)) * x1 - xt where xt = tx1 - (1-t)x0.\n        This functions supports any cominbation of prediction_type and target_type in {DATA, FLOW}.\n\n        Args:\n            model_pred (Tensor): The predicted output from the model.\n            target (Tensor): The target output for the model prediction.\n            t (Optional[Tensor], optional): The time for the model prediction. Defaults to None.\n            xt (Optional[Tensor], optional): The interpolated data. Defaults to None.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            target_type (PredictionType, optional): The type of the target output. Defaults to PredictionType.DATA.\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        target_type = string_to_enum(target_type, PredictionType)\n        if target_type == PredictionType.DATA:\n            model_pred = self.process_data_prediction(model_pred, xt, t, mask)\n        else:\n            model_pred = self.process_vector_field_prediction(model_pred, xt, t, mask)\n        raw_loss = self._loss_function(model_pred, target)\n\n        if mask is not None:\n            loss = raw_loss * mask.unsqueeze(-1)\n            n_elem = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n        else:\n            loss = torch.sum(raw_loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n        if target_type == PredictionType.DATA:\n            if t is None:\n                raise ValueError(\"Time cannot be None when using a time-based weighting\")\n            loss_weight = 1.0 / ((1.0 - t) ** 2 + self.eps)\n            loss = loss_weight * loss\n        return loss\n\n    def vf_to_score(\n        self,\n        x_t: Tensor,\n        v: Tensor,\n        t: Tensor,\n    ) -&gt; Tensor:\n        \"\"\"From Geffner et al. Computes score of noisy density given the vector field learned by flow matching.\n\n        With our interpolation scheme these are related by\n\n        v(x_t, t) = (1 / t) (x_t + scale_ref ** 2 * (1 - t) * s(x_t, t)),\n\n        or equivalently,\n\n        s(x_t, t) = (t * v(x_t, t) - x_t) / (scale_ref ** 2 * (1 - t)).\n\n        with scale_ref = 1\n\n        Args:\n            x_t: Noisy sample, shape [*, dim]\n            v: Vector field, shape [*, dim]\n            t: Interpolation time, shape [*] (must be &lt; 1)\n\n        Returns:\n            Score of intermediate density, shape [*, dim].\n        \"\"\"\n        assert torch.all(t &lt; 1.0), \"vf_to_score requires t &lt; 1 (strict)\"\n        t = pad_like(t, v)\n        num = t * v - x_t  # [*, dim]\n        den = 1.0 - t  # [*, 1]\n        score = num / den\n        return score  # [*, dim]\n\n    def get_gt(\n        self,\n        t: Tensor,\n        mode: str = \"tan\",\n        param: float = 1.0,\n        clamp_val: Optional[float] = None,\n        eps: float = 1e-2,\n    ) -&gt; Tensor:\n        \"\"\"From Geffner et al. Computes gt for different modes.\n\n        Args:\n            t: times where we'll evaluate, covers [0, 1), shape [nsteps]\n            mode: \"us\" or \"tan\"\n            param: parameterized transformation\n            clamp_val: value to clamp gt, no clamping if None\n            eps: small value leave as it is\n        \"\"\"\n\n        # Function to get variants for some gt mode\n        def transform_gt(gt, f_pow=1.0):\n            # 1.0 means no transformation\n            if f_pow == 1.0:\n                return gt\n\n            # First we somewhat normalize between 0 and 1\n            log_gt = torch.log(gt)\n            mean_log_gt = torch.mean(log_gt)\n            log_gt_centered = log_gt - mean_log_gt\n            normalized = torch.nn.functional.sigmoid(log_gt_centered)\n            # Transformation here\n            normalized = normalized**f_pow\n            # Undo normalization with the transformed variable\n            log_gt_centered_rec = torch.logit(normalized, eps=1e-6)\n            log_gt_rec = log_gt_centered_rec + mean_log_gt\n            gt_rec = torch.exp(log_gt_rec)\n            return gt_rec\n\n        # Numerical reasons for some schedule\n        t = torch.clamp(t, 0, 1 - self.eps)\n\n        if mode == \"us\":\n            num = 1.0 - t\n            den = t\n            gt = num / (den + eps)\n        elif mode == \"tan\":\n            num = torch.sin((1.0 - t) * torch.pi / 2.0)\n            den = torch.cos((1.0 - t) * torch.pi / 2.0)\n            gt = (torch.pi / 2.0) * num / (den + eps)\n        elif mode == \"1/t\":\n            num = 1.0\n            den = t\n            gt = num / (den + eps)\n        elif mode == \"1/t2\":\n            num = 1.0\n            den = t**2\n            gt = num / (den + eps)\n        elif mode == \"1/t1p5\":\n            num = 1.0\n            den = t**1.5\n            gt = num / (den + eps)\n        elif mode == \"2/t\":\n            num = 2.0\n            den = t\n            gt = num / (den + eps)\n        elif mode == \"2/t2\":\n            num = 2.0\n            den = t**2\n            gt = num / (den + eps)\n        elif mode == \"2/t1p5\":\n            num = 2.0\n            den = t**1.5\n            gt = num / (den + eps)\n        elif mode == \"1mt\":\n            gt = 1 - t\n        elif mode == \"t\":\n            gt = t\n        elif mode == \"ones\":\n            gt = 0 * t + 1\n        else:\n            raise NotImplementedError(f\"gt not implemented {mode}\")\n        gt = transform_gt(gt, f_pow=param)\n        gt = torch.clamp(gt, 0, clamp_val)  # If None no clamping\n        return gt  # [s]\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.__init__","title":"<code>__init__(time_distribution, prior_distribution, prediction_type=PredictionType.DATA, sigma=0, augmentation_type=None, augmentation_num_threads=1, data_scale=1.0, device='cpu', rng_generator=None, eps=1e-05)</code>","text":"<p>Initializes the Continuous Flow Matching interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>prediction_type</code> <code>PredictionType</code> <p>The type of prediction, either \"flow\" or another type. Defaults to PredictionType.DATA.</p> <code>DATA</code> <code>sigma</code> <code>Float</code> <p>The standard deviation of the Gaussian noise added to the interpolated data. Defaults to 0.</p> <code>0</code> <code>augmentation_type</code> <code>Optional[Union[AugmentationType, str]]</code> <p>The type of optimal transport, if applicable. Defaults to None.</p> <code>None</code> <code>augmentation_num_threads</code> <code>int</code> <p>Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.</p> <code>1</code> <code>data_scale</code> <code>Float</code> <p>The scale factor for the data. Defaults to 1.0.</p> <code>1.0</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> <code>eps</code> <code>Float</code> <p>Small float to prevent divide by zero</p> <code>1e-05</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n    sigma: Float = 0,\n    augmentation_type: Optional[Union[AugmentationType, str]] = None,\n    augmentation_num_threads: int = 1,\n    data_scale: Float = 1.0,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n    eps: Float = 1e-5,\n):\n    \"\"\"Initializes the Continuous Flow Matching interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        prediction_type (PredictionType, optional): The type of prediction, either \"flow\" or another type. Defaults to PredictionType.DATA.\n        sigma (Float, optional): The standard deviation of the Gaussian noise added to the interpolated data. Defaults to 0.\n        augmentation_type (Optional[Union[AugmentationType, str]], optional): The type of optimal transport, if applicable. Defaults to None.\n        augmentation_num_threads:  Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n        data_scale (Float, optional): The scale factor for the data. Defaults to 1.0.\n        device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        eps: Small float to prevent divide by zero\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    self.prediction_type = string_to_enum(prediction_type, PredictionType)\n    if self.prediction_type == PredictionType.NOISE:\n        raise ValueError(\"Prediction type cannot be NOISE for Continuous Flow Matching\")\n    self.sigma = sigma\n    self.augmentation_type = augmentation_type\n    self.data_scale = data_scale\n    self.eps = eps\n    if data_scale &lt;= 0:\n        raise ValueError(\"Data Scale must be &gt; 0\")\n    if augmentation_type is not None:\n        self.augmentation_type = augmentation_type = string_to_enum(augmentation_type, AugmentationType)\n        self.augmentation_sampler = self._build_augmentation_sampler(\n            method_type=augmentation_type, num_threads=augmentation_num_threads\n        )\n    self._loss_function = nn.MSELoss(reduction=\"none\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher._build_augmentation_sampler","title":"<code>_build_augmentation_sampler(method_type, num_threads=1)</code>","text":"<p>Build the optimal transport sampler for the given optimal transport type.</p> <p>Parameters:</p> Name Type Description Default <code>method_type</code> <code>augmentation_type</code> <p>The type of augmentation.</p> required <code>num_threads</code> <code>int</code> <p>The number of threads to use for the OT sampler, default to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>The augmentation object.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def _build_augmentation_sampler(self, method_type: AugmentationType, num_threads: int = 1):\n    \"\"\"Build the optimal transport sampler for the given optimal transport type.\n\n    Args:\n        method_type (augmentation_type): The type of augmentation.\n        num_threads (int): The number of threads to use for the OT sampler, default to 1.\n\n    Returns:\n        The augmentation object.\n    \"\"\"\n    return BatchDataAugmentation(self.device, num_threads).create(method_type)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, **kwargs)</code>","text":"<p>Sample and apply the optimal transport plan between batched (and masked) x0 and x1.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to self.augmentation_sampler.apply_augmentation or handled within this method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>tuple</code> <p>tuple of 2 tensors, represents the noise and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def apply_augmentation(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None, **kwargs) -&gt; tuple:\n    \"\"\"Sample and apply the optimal transport plan between batched (and masked) x0 and x1.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        **kwargs: Additional keyword arguments to be passed to self.augmentation_sampler.apply_augmentation or handled within this method.\n\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n    \"\"\"\n    if self.augmentation_sampler is None:\n        raise ValueError(\"Optimal Transport Sampler is not defined\")\n    return self.augmentation_sampler.apply_augmentation(x0, x1, mask=mask, **kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.calculate_target","title":"<code>calculate_target(data, noise, mask=None)</code>","text":"<p>Get the target vector field at time t.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>Tensor</code> <p>noise from prior(), shape (batchsize, nodes, features)</p> required <code>data</code> <code>Tensor</code> <p>target, shape (batchsize, nodes, features)</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The target vector field at time t.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def calculate_target(self, data: Tensor, noise: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Get the target vector field at time t.\n\n    Args:\n        noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n        data (Tensor): target, shape (batchsize, nodes, features)\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        Tensor: The target vector field at time t.\n    \"\"\"\n    # Calculate the target vector field u_t(x_t|x_1) as the difference between data and noise because t~[0,1]\n    if self.prediction_type == PredictionType.VELOCITY:\n        u_t = data - noise\n    elif self.prediction_type == PredictionType.DATA:\n        u_t = data\n    else:\n        raise ValueError(\n            f\"Given prediction_type {self.prediction_type} is not supproted for Continuous Flow Matching.\"\n        )\n    if mask is not None:\n        u_t = u_t * mask.unsqueeze(-1)\n    return u_t\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.get_gt","title":"<code>get_gt(t, mode='tan', param=1.0, clamp_val=None, eps=0.01)</code>","text":"<p>From Geffner et al. Computes gt for different modes.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>times where we'll evaluate, covers [0, 1), shape [nsteps]</p> required <code>mode</code> <code>str</code> <p>\"us\" or \"tan\"</p> <code>'tan'</code> <code>param</code> <code>float</code> <p>parameterized transformation</p> <code>1.0</code> <code>clamp_val</code> <code>Optional[float]</code> <p>value to clamp gt, no clamping if None</p> <code>None</code> <code>eps</code> <code>float</code> <p>small value leave as it is</p> <code>0.01</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def get_gt(\n    self,\n    t: Tensor,\n    mode: str = \"tan\",\n    param: float = 1.0,\n    clamp_val: Optional[float] = None,\n    eps: float = 1e-2,\n) -&gt; Tensor:\n    \"\"\"From Geffner et al. Computes gt for different modes.\n\n    Args:\n        t: times where we'll evaluate, covers [0, 1), shape [nsteps]\n        mode: \"us\" or \"tan\"\n        param: parameterized transformation\n        clamp_val: value to clamp gt, no clamping if None\n        eps: small value leave as it is\n    \"\"\"\n\n    # Function to get variants for some gt mode\n    def transform_gt(gt, f_pow=1.0):\n        # 1.0 means no transformation\n        if f_pow == 1.0:\n            return gt\n\n        # First we somewhat normalize between 0 and 1\n        log_gt = torch.log(gt)\n        mean_log_gt = torch.mean(log_gt)\n        log_gt_centered = log_gt - mean_log_gt\n        normalized = torch.nn.functional.sigmoid(log_gt_centered)\n        # Transformation here\n        normalized = normalized**f_pow\n        # Undo normalization with the transformed variable\n        log_gt_centered_rec = torch.logit(normalized, eps=1e-6)\n        log_gt_rec = log_gt_centered_rec + mean_log_gt\n        gt_rec = torch.exp(log_gt_rec)\n        return gt_rec\n\n    # Numerical reasons for some schedule\n    t = torch.clamp(t, 0, 1 - self.eps)\n\n    if mode == \"us\":\n        num = 1.0 - t\n        den = t\n        gt = num / (den + eps)\n    elif mode == \"tan\":\n        num = torch.sin((1.0 - t) * torch.pi / 2.0)\n        den = torch.cos((1.0 - t) * torch.pi / 2.0)\n        gt = (torch.pi / 2.0) * num / (den + eps)\n    elif mode == \"1/t\":\n        num = 1.0\n        den = t\n        gt = num / (den + eps)\n    elif mode == \"1/t2\":\n        num = 1.0\n        den = t**2\n        gt = num / (den + eps)\n    elif mode == \"1/t1p5\":\n        num = 1.0\n        den = t**1.5\n        gt = num / (den + eps)\n    elif mode == \"2/t\":\n        num = 2.0\n        den = t\n        gt = num / (den + eps)\n    elif mode == \"2/t2\":\n        num = 2.0\n        den = t**2\n        gt = num / (den + eps)\n    elif mode == \"2/t1p5\":\n        num = 2.0\n        den = t**1.5\n        gt = num / (den + eps)\n    elif mode == \"1mt\":\n        gt = 1 - t\n    elif mode == \"t\":\n        gt = t\n    elif mode == \"ones\":\n        gt = 0 * t + 1\n    else:\n        raise NotImplementedError(f\"gt not implemented {mode}\")\n    gt = transform_gt(gt, f_pow=param)\n    gt = torch.clamp(gt, 0, clamp_val)  # If None no clamping\n    return gt  # [s]\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x_t with given time t from noise (x_0) and data (x_1).</p> <p>Currently, we use the linear interpolation as defined in:     1. Rectified flow: https://arxiv.org/abs/2209.03003.     2. Conditional flow matching: https://arxiv.org/abs/2210.02747 (called conditional optimal transport).</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>Tensor</code> <p>noise from prior(), shape (batchsize, nodes, features)</p> required <code>t</code> <code>Tensor</code> <p>time, shape (batchsize)</p> required <code>data</code> <code>Tensor</code> <p>target, shape (batchsize, nodes, features)</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n    \"\"\"Get x_t with given time t from noise (x_0) and data (x_1).\n\n    Currently, we use the linear interpolation as defined in:\n        1. Rectified flow: https://arxiv.org/abs/2209.03003.\n        2. Conditional flow matching: https://arxiv.org/abs/2210.02747 (called conditional optimal transport).\n\n    Args:\n        noise (Tensor): noise from prior(), shape (batchsize, nodes, features)\n        t (Tensor): time, shape (batchsize)\n        data (Tensor): target, shape (batchsize, nodes, features)\n    \"\"\"\n    # Expand t to the same shape as noise: ones([b,n,f]) * t([b,1,1])\n    t = pad_like(t, data)\n    # Calculate x_t as the linear interpolation between noise and data\n    x_t = data * t + noise * (1.0 - t)\n    # Add Gaussian Noise\n    if self.sigma &gt; 0:\n        x_t += self.sigma * torch.randn(x_t.shape, device=x_t.device, generator=self.rng_generator)\n    return x_t\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.loss","title":"<code>loss(model_pred, target, t=None, xt=None, mask=None, target_type=PredictionType.DATA)</code>","text":"<p>Calculate the loss given the model prediction, data sample, time, and mask.</p> <p>If target_type is FLOW loss = ||v_hat - (x1-x0)||2 If target_type is DATA loss = ||x1_hat - x1||2 * 1 / (1 - t)**2 as the target vector field = x1 - x0 = (1/(1-t)) * x1 - xt where xt = tx1 - (1-t)x0. This functions supports any cominbation of prediction_type and target_type in {DATA, FLOW}.</p> <p>Parameters:</p> Name Type Description Default <code>model_pred</code> <code>Tensor</code> <p>The predicted output from the model.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction.</p> required <code>t</code> <code>Optional[Tensor]</code> <p>The time for the model prediction. Defaults to None.</p> <code>None</code> <code>xt</code> <code>Optional[Tensor]</code> <p>The interpolated data. Defaults to None.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>target_type</code> <code>PredictionType</code> <p>The type of the target output. Defaults to PredictionType.DATA.</p> <code>DATA</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def loss(\n    self,\n    model_pred: Tensor,\n    target: Tensor,\n    t: Optional[Tensor] = None,\n    xt: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    target_type: Union[PredictionType, str] = PredictionType.DATA,\n):\n    \"\"\"Calculate the loss given the model prediction, data sample, time, and mask.\n\n    If target_type is FLOW loss = ||v_hat - (x1-x0)||**2\n    If target_type is DATA loss = ||x1_hat - x1||**2 * 1 / (1 - t)**2 as the target vector field = x1 - x0 = (1/(1-t)) * x1 - xt where xt = tx1 - (1-t)x0.\n    This functions supports any cominbation of prediction_type and target_type in {DATA, FLOW}.\n\n    Args:\n        model_pred (Tensor): The predicted output from the model.\n        target (Tensor): The target output for the model prediction.\n        t (Optional[Tensor], optional): The time for the model prediction. Defaults to None.\n        xt (Optional[Tensor], optional): The interpolated data. Defaults to None.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        target_type (PredictionType, optional): The type of the target output. Defaults to PredictionType.DATA.\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    target_type = string_to_enum(target_type, PredictionType)\n    if target_type == PredictionType.DATA:\n        model_pred = self.process_data_prediction(model_pred, xt, t, mask)\n    else:\n        model_pred = self.process_vector_field_prediction(model_pred, xt, t, mask)\n    raw_loss = self._loss_function(model_pred, target)\n\n    if mask is not None:\n        loss = raw_loss * mask.unsqueeze(-1)\n        n_elem = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n    else:\n        loss = torch.sum(raw_loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n    if target_type == PredictionType.DATA:\n        if t is None:\n            raise ValueError(\"Time cannot be None when using a time-based weighting\")\n        loss_weight = 1.0 / ((1.0 - t) ** 2 + self.eps)\n        loss = loss_weight * loss\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.process_data_prediction","title":"<code>process_data_prediction(model_output, xt=None, t=None, mask=None)</code>","text":"<p>Process the model output based on the prediction type to generate clean data.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The input sample.</p> <code>None</code> <code>t</code> <code>Tensor</code> <p>The time step.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the model output. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>The data prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"flow\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def process_data_prediction(\n    self,\n    model_output: Tensor,\n    xt: Optional[Tensor] = None,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n):\n    \"\"\"Process the model output based on the prediction type to generate clean data.\n\n    Args:\n        model_output (Tensor): The output of the model.\n        xt (Tensor): The input sample.\n        t (Tensor): The time step.\n        mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n    Returns:\n        The data prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not \"flow\".\n    \"\"\"\n    if self.prediction_type == PredictionType.VELOCITY:\n        if xt is None or t is None:\n            raise ValueError(\"Xt and time cannot be None\")\n        t = pad_like(t, model_output)\n        pred_data = xt + (1 - t) * model_output\n    elif self.prediction_type == PredictionType.DATA:\n        pred_data = model_output\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be `flow` \" \"for Continuous Flow Matching.\"\n        )\n    if mask is not None:\n        pred_data = pred_data * mask.unsqueeze(-1)\n    return pred_data\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.process_vector_field_prediction","title":"<code>process_vector_field_prediction(model_output, xt=None, t=None, mask=None)</code>","text":"<p>Process the model output based on the prediction type to calculate vecotr field.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The input sample.</p> <code>None</code> <code>t</code> <code>Tensor</code> <p>The time step.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the model output. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>The vector field prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"flow\" or \"data\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def process_vector_field_prediction(\n    self,\n    model_output: Tensor,\n    xt: Optional[Tensor] = None,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n):\n    \"\"\"Process the model output based on the prediction type to calculate vecotr field.\n\n    Args:\n        model_output (Tensor): The output of the model.\n        xt (Tensor): The input sample.\n        t (Tensor): The time step.\n        mask (Optional[Tensor], optional): An optional mask to apply to the model output. Defaults to None.\n\n    Returns:\n        The vector field prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not \"flow\" or \"data\".\n    \"\"\"\n    if self.prediction_type == PredictionType.VELOCITY:\n        pred_vector_field = model_output\n    elif self.prediction_type == PredictionType.DATA:\n        if xt is None or t is None:\n            raise ValueError(\"Xt and Time cannpt be None for vector field conversion\")\n        t = pad_like(t, model_output)\n        pred_vector_field = (model_output - xt) / (1 - t + self.eps)\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be `flow` or `data` \"\n            \"for Continuous Flow Matching.\"\n        )\n    if mask is not None:\n        pred_vector_field = pred_vector_field * mask.unsqueeze(-1)\n    return pred_vector_field\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.scale_data","title":"<code>scale_data(data)</code>","text":"<p>Upscale the input data by the data scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data to upscale.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The upscaled data.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def scale_data(self, data: Tensor) -&gt; Tensor:\n    \"\"\"Upscale the input data by the data scale factor.\n\n    Args:\n        data (Tensor): The input data to upscale.\n\n    Returns:\n        The upscaled data.\n    \"\"\"\n    return self.data_scale * data\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.step","title":"<code>step(model_out, xt, dt, t=None, mask=None, center=False)</code>","text":"<p>Perform a single ODE step integration using Euler method.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model at the current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current intermediate state.</p> required <code>dt</code> <code>Tensor</code> <p>The time step size.</p> required <code>t</code> <code>Tensor</code> <p>The current time. Defaults to None.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>A mask to apply to the model output. Defaults to None.</p> <code>None</code> <code>center</code> <code>Bool</code> <p>Whether to center the output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>x_next</code> <code>Tensor</code> <p>The updated state of the system after the single step, x_(t+dt).</p> <p>Notes: - If a mask is provided, it is applied element-wise to the model output before scaling. - The <code>clean</code> method is called on the updated state before it is returned.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def step(\n    self,\n    model_out: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n):\n    \"\"\"Perform a single ODE step integration using Euler method.\n\n    Args:\n        model_out (Tensor): The output of the model at the current time step.\n        xt (Tensor): The current intermediate state.\n        dt (Tensor): The time step size.\n        t (Tensor, optional): The current time. Defaults to None.\n        mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n        center (Bool, optional): Whether to center the output. Defaults to False.\n\n    Returns:\n        x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n    Notes:\n    - If a mask is provided, it is applied element-wise to the model output before scaling.\n    - The `clean` method is called on the updated state before it is returned.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n    dt = pad_like(dt, model_out)\n    delta_x = v_t * dt\n    x_next = xt + delta_x\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.step_score_stochastic","title":"<code>step_score_stochastic(model_out, xt, dt, t, mask=None, gt_mode='tan', gt_p=1.0, gt_clamp=None, score_temperature=1.0, noise_temperature=1.0, t_lim_ode=0.99, center=False)</code>","text":"<p>Perform a single SDE step integration using a score-based Langevin update.</p> <p>d x_t = [v(x_t, t) + g(t) * s(x_t, t) * score_temperature] dt + \\sqrt{2 * g(t) * noise_temperature} dw_t.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model at the current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current intermediate state.</p> required <code>dt</code> <code>Tensor</code> <p>The time step size.</p> required <code>t</code> <code>Tensor</code> <p>The current time. Defaults to None.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>A mask to apply to the model output. Defaults to None.</p> <code>None</code> <code>gt_mode</code> <code>str</code> <p>The mode for the gt function. Defaults to \"tan\".</p> <code>'tan'</code> <code>gt_p</code> <code>Float</code> <p>The parameter for the gt function. Defaults to 1.0.</p> <code>1.0</code> <code>gt_clamp</code> <code>Optional[Float]</code> <p>(Float, optional): Upper limit of gt term. Defaults to None.</p> <code>None</code> <code>score_temperature</code> <code>Float</code> <p>The temperature for the score part of the step. Defaults to 1.0.</p> <code>1.0</code> <code>noise_temperature</code> <code>Float</code> <p>The temperature for the stochastic part of the step. Defaults to 1.0.</p> <code>1.0</code> <code>t_lim_ode</code> <code>Float</code> <p>The time limit for the ODE step. Defaults to 0.99.</p> <code>0.99</code> <code>center</code> <code>Bool</code> <p>Whether to center the output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>x_next</code> <code>Tensor</code> <p>The updated state of the system after the single step, x_(t+dt).</p> Notes <ul> <li>If a mask is provided, it is applied element-wise to the model output before scaling.</li> <li>The <code>clean</code> method is called on the updated state before it is returned.</li> </ul> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def step_score_stochastic(\n    self,\n    model_out: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    t: Tensor,\n    mask: Optional[Tensor] = None,\n    gt_mode: str = \"tan\",\n    gt_p: Float = 1.0,\n    gt_clamp: Optional[Float] = None,\n    score_temperature: Float = 1.0,\n    noise_temperature: Float = 1.0,\n    t_lim_ode: Float = 0.99,\n    center: Bool = False,\n):\n    r\"\"\"Perform a single SDE step integration using a score-based Langevin update.\n\n    d x_t = [v(x_t, t) + g(t) * s(x_t, t) * score_temperature] dt + \\sqrt{2 * g(t) * noise_temperature} dw_t.\n\n    Args:\n        model_out (Tensor): The output of the model at the current time step.\n        xt (Tensor): The current intermediate state.\n        dt (Tensor): The time step size.\n        t (Tensor, optional): The current time. Defaults to None.\n        mask (Optional[Tensor], optional): A mask to apply to the model output. Defaults to None.\n        gt_mode (str, optional): The mode for the gt function. Defaults to \"tan\".\n        gt_p (Float, optional): The parameter for the gt function. Defaults to 1.0.\n        gt_clamp: (Float, optional): Upper limit of gt term. Defaults to None.\n        score_temperature (Float, optional): The temperature for the score part of the step. Defaults to 1.0.\n        noise_temperature (Float, optional): The temperature for the stochastic part of the step. Defaults to 1.0.\n        t_lim_ode (Float, optional): The time limit for the ODE step. Defaults to 0.99.\n        center (Bool, optional): Whether to center the output. Defaults to False.\n\n    Returns:\n        x_next (Tensor): The updated state of the system after the single step, x_(t+dt).\n\n    Notes:\n        - If a mask is provided, it is applied element-wise to the model output before scaling.\n        - The `clean` method is called on the updated state before it is returned.\n    \"\"\"\n    if self.augmentation_type is not None:\n        raise ValueError(\"Optimal Transport violates the vector field to score conversion\")\n    if not isinstance(self.prior_distribution, GaussianPrior):\n        raise ValueError(\n            \"Prior distribution must be an instance of GaussianPrior to learn a proper score function\"\n        )\n    if t.min() &gt;= t_lim_ode:\n        return self.step(model_out, xt, dt, t, mask, center)\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    v_t = self.process_vector_field_prediction(model_out, xt, t, mask)\n    dt = pad_like(dt, model_out)\n    t = pad_like(t, model_out)\n    score = self.vf_to_score(xt, v_t, t)\n    gt = self.get_gt(t, gt_mode, gt_p, gt_clamp)\n    eps = torch.randn(xt.shape, dtype=xt.dtype, device=xt.device, generator=self.rng_generator)\n    std_eps = torch.sqrt(2 * gt * noise_temperature * dt)\n    delta_x = (v_t + gt * score * score_temperature) * dt + std_eps * eps\n    x_next = xt + delta_x\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.undo_scale_data","title":"<code>undo_scale_data(data)</code>","text":"<p>Downscale the input data by the data scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data to downscale.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The downscaled data.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def undo_scale_data(self, data: Tensor) -&gt; Tensor:\n    \"\"\"Downscale the input data by the data scale factor.\n\n    Args:\n        data (Tensor): The input data to downscale.\n\n    Returns:\n        The downscaled data.\n    \"\"\"\n    return 1 / self.data_scale * data\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching/#bionemo.moco.interpolants.continuous_time.continuous.continuous_flow_matching.ContinuousFlowMatcher.vf_to_score","title":"<code>vf_to_score(x_t, v, t)</code>","text":"<p>From Geffner et al. Computes score of noisy density given the vector field learned by flow matching.</p> <p>With our interpolation scheme these are related by</p> <p>v(x_t, t) = (1 / t) (x_t + scale_ref ** 2 * (1 - t) * s(x_t, t)),</p> <p>or equivalently,</p> <p>s(x_t, t) = (t * v(x_t, t) - x_t) / (scale_ref ** 2 * (1 - t)).</p> <p>with scale_ref = 1</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Tensor</code> <p>Noisy sample, shape [*, dim]</p> required <code>v</code> <code>Tensor</code> <p>Vector field, shape [*, dim]</p> required <code>t</code> <code>Tensor</code> <p>Interpolation time, shape [*] (must be &lt; 1)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Score of intermediate density, shape [*, dim].</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/continuous_flow_matching.py</code> <pre><code>def vf_to_score(\n    self,\n    x_t: Tensor,\n    v: Tensor,\n    t: Tensor,\n) -&gt; Tensor:\n    \"\"\"From Geffner et al. Computes score of noisy density given the vector field learned by flow matching.\n\n    With our interpolation scheme these are related by\n\n    v(x_t, t) = (1 / t) (x_t + scale_ref ** 2 * (1 - t) * s(x_t, t)),\n\n    or equivalently,\n\n    s(x_t, t) = (t * v(x_t, t) - x_t) / (scale_ref ** 2 * (1 - t)).\n\n    with scale_ref = 1\n\n    Args:\n        x_t: Noisy sample, shape [*, dim]\n        v: Vector field, shape [*, dim]\n        t: Interpolation time, shape [*] (must be &lt; 1)\n\n    Returns:\n        Score of intermediate density, shape [*, dim].\n    \"\"\"\n    assert torch.all(t &lt; 1.0), \"vf_to_score requires t &lt; 1 (strict)\"\n    t = pad_like(t, v)\n    num = t * v - x_t  # [*, dim]\n    den = 1.0 - t  # [*, 1]\n    score = num / den\n    return score  # [*, dim]\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/","title":"Vdm","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM","title":"<code>VDM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Variational Diffusion Models (VDM) interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.vdm import VDM\n&gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_snr_transforms import CosineSNRTransform\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\nvdm = VDM(\n    time_distribution = UniformTimeDistribution(...),\n    prior_distribution = GaussianPrior(...),\n    noise_schedule = CosineSNRTransform(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = vdm.sample_time(batch_size)\n    noise = vdm.sample_prior(data.shape)\n    xt = vdm.interpolate(data, noise, time)\n\n    x_pred = model(xt, time)\n    loss = vdm.loss(x_pred, data, time)\n    loss.backward()\n\n# Generation\nx_pred = vdm.sample_prior(data.shape)\nfor t in LinearInferenceSchedule(...).generate_schedule():\n    time = torch.full((batch_size,), t)\n    x_hat = model(x_pred, time)\n    x_pred = vdm.step(x_hat, time, x_pred)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>class VDM(Interpolant):\n    \"\"\"A Variational Diffusion Models (VDM) interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.vdm import VDM\n    &gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_snr_transforms import CosineSNRTransform\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\n\n    vdm = VDM(\n        time_distribution = UniformTimeDistribution(...),\n        prior_distribution = GaussianPrior(...),\n        noise_schedule = CosineSNRTransform(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = vdm.sample_time(batch_size)\n        noise = vdm.sample_prior(data.shape)\n        xt = vdm.interpolate(data, noise, time)\n\n        x_pred = model(xt, time)\n        loss = vdm.loss(x_pred, data, time)\n        loss.backward()\n\n    # Generation\n    x_pred = vdm.sample_prior(data.shape)\n    for t in LinearInferenceSchedule(...).generate_schedule():\n        time = torch.full((batch_size,), t)\n        x_hat = model(x_pred, time)\n        x_pred = vdm.step(x_hat, time, x_pred)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        noise_schedule: ContinuousSNRTransform,\n        prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n        device: Union[str, torch.device] = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the DDPM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            noise_schedule (ContinuousSNRTransform): The schedule of noise, defining the amount of noise added at each time step.\n            prediction_type (PredictionType, optional): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n            device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        if not isinstance(prior_distribution, GaussianPrior):\n            warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n        self.noise_schedule = noise_schedule\n        self.prediction_type = string_to_enum(prediction_type, PredictionType)\n        self._loss_function = nn.MSELoss(reduction=\"none\")\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor): noise from prior()\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        psi = pad_like(psi, data)\n        omega = pad_like(omega, data)\n        x_t = data * psi + noise * omega\n        return x_t\n\n    def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor, optional): noise from prior(). Defaults to None\n        \"\"\"\n        if noise is None:\n            noise = self.sample_prior(data.shape)\n        return self.interpolate(data, t, noise)\n\n    def process_data_prediction(self, model_output: Tensor, sample, t):\n        \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n        This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n        Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n        The conversion formulas are as follows:\n        - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n        - For \"data\" prediction type: `pred_data = model_output`\n        - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n        Args:\n            model_output (Tensor): The output of the model.\n            sample (Tensor): The input sample.\n            t (Tensor): The time step.\n\n        Returns:\n            The data prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_data = (sample - noise_scale * model_output) / data_scale\n        elif self.prediction_type == PredictionType.DATA:\n            pred_data = model_output\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n                f\" PredictionType.VELOCITY for vdm.\"\n            )\n        return pred_data\n\n    def process_noise_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n        \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n        Args:\n            model_output (Tensor): The output of the model.\n            sample (Tensor): The input sample.\n            t (Tensor): The time step.\n\n        Returns:\n            The input as noise if the prediction type is \"noise\".\n\n        Raises:\n            ValueError: If the prediction type is not \"noise\".\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_noise = model_output\n        elif self.prediction_type == PredictionType.DATA:\n            pred_noise = (sample - data_scale * model_output) / noise_scale\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n            pred_noise = (sample - data_scale * pred_data) / noise_scale\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n                \" `v_prediction`  for vdm.\"\n            )\n        return pred_noise\n\n    def step(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ):\n        \"\"\"Do one step integration.\n\n        Args:\n            model_out (Tensor): The output of the model.\n            xt (Tensor): The current data point.\n            t (Tensor): The current time step.\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n            center (bool): Whether to center the data. Defaults to False.\n            temperature (Float): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n        Note:\n            The temperature parameter controls the trade off between diversity and sample quality.\n            Decreasing the temperature sharpens the sampling distribtion to focus on more likely samples.\n            The impact of low temperature sampling must be ablated analytically.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        alpha_t, sigma_t = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n\n        if (t - dt &lt; 0).any():\n            raise ValueError(\n                \"Error in inference schedule: t - dt &lt; 0. Please ensure that your inference time schedule has shape T with the final t = dt to make s = 0\"\n            )\n\n        log_snr_s = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n        alpha_s, sigma_s = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr_s)\n        sigma_s_2 = sigma_s * sigma_s\n        sigma_t_2 = sigma_t * sigma_t\n        alpha_t_s = alpha_t / alpha_s\n        sigma_2_t_s = -torch.expm1(F.softplus(-log_snr_s) - F.softplus(-log_snr))  # Equation 63\n\n        omega_r = alpha_t_s * sigma_s_2 / sigma_t_2  # Equation 28\n        psi_r = alpha_s * sigma_2_t_s / sigma_t_2\n        std = sigma_2_t_s.sqrt() * sigma_s / sigma_t\n        nonzero_mask = (\n            t &gt; 0\n        ).float()  # based on the time this is always just ones. can leave for now to see if ever want to take extra step and only grab mean\n\n        psi_r = pad_like(psi_r, x_hat)\n        omega_r = pad_like(omega_r, x_hat)\n        std = pad_like(std, x_hat)\n        nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n        mean = psi_r * x_hat + omega_r * xt\n        eps = torch.randn_like(mean).to(model_out.device)\n        x_next = mean + nonzero_mask * std * eps * temperature\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n        \"\"\"Converts the data prediction to the estimated score function.\n\n        Args:\n            x_hat (tensor): The predicted data point.\n            xt (Tensor): The current data point.\n            t (Tensor): The time step.\n\n        Returns:\n            The estimated score function.\n        \"\"\"\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        psi = pad_like(psi, x_hat)\n        omega = pad_like(omega, x_hat)\n        score = psi * x_hat - xt\n        score = score / (omega * omega)\n        return score\n\n    def step_ddim(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        eta: Float = 0.0,\n        center: Bool = False,\n    ):\n        \"\"\"Do one step of DDIM sampling.\n\n        From the ddpm equations alpha_bar = alpha**2 and  1 - alpha**2 = sigma**2\n\n        Args:\n            model_out (Tensor): output of the model\n            t (Tensor): current time step\n            xt (Tensor): current data point\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n            eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n            center (Bool, optional): whether to center the data point. Defaults to False.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        data_pred = self.process_data_prediction(model_out, xt, t)\n        noise_pred = self.process_noise_prediction(model_out, xt, t)\n        eps = torch.randn_like(data_pred).to(model_out.device)\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        squared_alpha = log_snr.sigmoid()\n        squared_sigma = (-log_snr).sigmoid()\n        log_snr_prev = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n        squared_alpha_prev = log_snr_prev.sigmoid()\n        squared_sigma_prev = (-log_snr_prev).sigmoid()\n        sigma_t_2 = squared_sigma_prev / squared_sigma * (1 - squared_alpha / squared_alpha_prev)\n        psi_r = torch.sqrt(squared_alpha_prev)\n        omega_r = torch.sqrt(1 - squared_alpha_prev - eta * eta * sigma_t_2)\n\n        sigma_t_2 = pad_like(sigma_t_2, model_out)\n        psi_r = pad_like(psi_r, model_out)\n        omega_r = pad_like(omega_r, model_out)\n\n        mean = data_pred * psi_r + omega_r * noise_pred\n        x_next = mean + eta * sigma_t_2.sqrt() * eps\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def set_loss_weight_fn(self, fn: Callable):\n        \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n        Args:\n            fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n        \"\"\"\n        self.loss_weight = fn\n\n    def loss_weight(self, raw_loss: Tensor, t: Tensor, weight_type: str, dt: Float = 0.001) -&gt; Tensor:\n        \"\"\"Calculates the weight for the loss based on the given weight type.\n\n        This function computes the loss weight according to the specified `weight_type`.\n        The available weight types are:\n        - \"ones\": uniform weight of 1.0\n        - \"data_to_noise\": derived from Equation (9) of https://arxiv.org/pdf/2202.00512\n        - \"variational_objective\": based on the variational objective, see https://arxiv.org/pdf/2202.00512\n\n        Args:\n            raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n            t (Tensor): The time step.\n            weight_type (str): The type of weight to use. Can be \"ones\", \"data_to_noise\", or \"variational_objective\".\n            dt (Float, optional): The time step increment. Defaults to 0.001.\n\n        Returns:\n            Tensor: The weight for the loss.\n\n        Raises:\n            ValueError: If the weight type is not recognized.\n        \"\"\"\n        if weight_type == \"ones\":\n            schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n        elif weight_type == \"data_to_noise\":  #\n            log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n            psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n            schedule = (psi**2) / (omega**2)\n            for _ in range(raw_loss.ndim - 1):\n                schedule = schedule.unsqueeze(-1)\n        elif weight_type == \"variational_objective\":\n            # (1-SNR(t-1)/SNR(t)),\n            snr = torch.exp(self.noise_schedule.calculate_log_snr(t, device=t.device))\n            snr_m1 = torch.exp(self.noise_schedule.calculate_log_snr(t - dt, device=t.device))\n            schedule = 1 - snr_m1 / snr\n            for _ in range(raw_loss.ndim - 1):\n                schedule = schedule.unsqueeze(-1)\n        else:\n            raise ValueError(\"Invalid loss weight keyword\")\n        return schedule\n\n    def loss(\n        self,\n        model_pred: Tensor,\n        target: Tensor,\n        t: Tensor,\n        dt: Optional[Float] = 0.001,\n        mask: Optional[Tensor] = None,\n        weight_type: str = \"ones\",\n    ):\n        \"\"\"Calculates the loss given the model prediction, target, and time.\n\n        Args:\n            model_pred (Tensor): The predicted output from the model.\n            target (Tensor): The target output for the model prediction.\n            t (Tensor): The time at which the loss is calculated.\n            dt (Optional[Float], optional): The time step increment. Defaults to 0.001.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            weight_type (str, optional): The type of weight to use for the loss. Can be \"ones\", \"data_to_noise\", or \"variational_objective\". Defaults to \"ones\".\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        raw_loss = self._loss_function(model_pred, target)\n        update_weight = self.loss_weight(raw_loss, t, weight_type, dt)\n        loss = raw_loss * update_weight\n        if mask is not None:\n            loss = loss * mask.unsqueeze(-1)\n            n_elem = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n        else:\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n        return loss\n\n    def step_hybrid_sde(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n        equilibrium_rate: Float = 0.0,\n    ) -&gt; Tensor:\n        \"\"\"Do one step integration of Hybrid Langevin-Reverse Time SDE.\n\n        See section B.3 page 37 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n        and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n        Args:\n            model_out (Tensor): The output of the model.\n            xt (Tensor): The current data point.\n            t (Tensor): The current time step.\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n            center (bool, optional): Whether to center the data. Defaults to False.\n            temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n            equilibrium_rate (Float, optional): The rate of Langevin equilibration.  Scales the amount of Langevin dynamics per unit time. Best values are in the range [1.0, 5.0]. Defaults to 0.0.\n\n        Note:\n        For all step functions that use the SDE formulation its important to note that we are moving backwards in time which corresponds to an apparent sign change.\n        A clear example can be seen in slide 29 https://ernestryu.com/courses/FM/diffusion1.pdf.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        # Schedule coeffiecients\n        beta = self.noise_schedule.calculate_beta(t)\n        inverse_temperature = 1 / temperature  # lambda_0\n        langevin_factor = equilibrium_rate\n        # Temperature coefficients\n        lambda_t = (\n            inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n        )\n        # langevin_isothermal = True\n        lambda_langevin = inverse_temperature  # if langevin_isothermal else lambda_t\n\n        score_scale_t = lambda_t + lambda_langevin * langevin_factor / 2.0\n\n        eps = torch.randn_like(x_hat).to(model_out.device)\n        score = self.score(x_hat, xt, t)\n        beta = pad_like(beta, model_out)\n        score_scale_t = pad_like(score_scale_t, model_out)\n\n        gT = beta * ((-1 / 2) * xt - score_scale_t * score)\n        gW = torch.sqrt((1.0 + langevin_factor) * beta.abs()) * eps\n\n        x_next = xt + dt * gT + dt.sqrt() * gW\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def step_ode(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ) -&gt; Tensor:\n        \"\"\"Do one step integration of ODE.\n\n        See section B page 36 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n        and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n        Args:\n            model_out (Tensor): The output of the model.\n            xt (Tensor): The current data point.\n            t (Tensor): The current time step.\n            dt (Tensor): The time step increment.\n            mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n            center (bool, optional): Whether to center the data. Defaults to False.\n            temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        # Schedule coeffiecients\n        beta = self.noise_schedule.calculate_beta(t)\n        inverse_temperature = 1 / temperature\n        # Temperature coefficients\n        lambda_t = (\n            inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n        )\n\n        score = self.score(x_hat, xt, t)\n        beta = pad_like(beta, model_out)\n        lambda_t = pad_like(lambda_t, model_out)\n\n        gT = (-1 / 2) * beta * (xt + lambda_t * score)\n\n        x_next = xt + gT * dt\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, prediction_type=PredictionType.DATA, device='cpu', rng_generator=None)</code>","text":"<p>Initializes the DDPM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>noise_schedule</code> <code>ContinuousSNRTransform</code> <p>The schedule of noise, defining the amount of noise added at each time step.</p> required <code>prediction_type</code> <code>PredictionType</code> <p>The type of prediction, either \"data\" or another type. Defaults to \"data\".</p> <code>DATA</code> <code>device</code> <code>str</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    noise_schedule: ContinuousSNRTransform,\n    prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n    device: Union[str, torch.device] = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the DDPM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        noise_schedule (ContinuousSNRTransform): The schedule of noise, defining the amount of noise added at each time step.\n        prediction_type (PredictionType, optional): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n        device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    if not isinstance(prior_distribution, GaussianPrior):\n        warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n    self.noise_schedule = noise_schedule\n    self.prediction_type = string_to_enum(prediction_type, PredictionType)\n    self._loss_function = nn.MSELoss(reduction=\"none\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.forward_process","title":"<code>forward_process(data, t, noise=None)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior(). Defaults to None</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor, optional): noise from prior(). Defaults to None\n    \"\"\"\n    if noise is None:\n        noise = self.sample_prior(data.shape)\n    return self.interpolate(data, t, noise)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior()</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor): noise from prior()\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    psi = pad_like(psi, data)\n    omega = pad_like(omega, data)\n    x_t = data * psi + noise * omega\n    return x_t\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.loss","title":"<code>loss(model_pred, target, t, dt=0.001, mask=None, weight_type='ones')</code>","text":"<p>Calculates the loss given the model prediction, target, and time.</p> <p>Parameters:</p> Name Type Description Default <code>model_pred</code> <code>Tensor</code> <p>The predicted output from the model.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction.</p> required <code>t</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> required <code>dt</code> <code>Optional[Float]</code> <p>The time step increment. Defaults to 0.001.</p> <code>0.001</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>weight_type</code> <code>str</code> <p>The type of weight to use for the loss. Can be \"ones\", \"data_to_noise\", or \"variational_objective\". Defaults to \"ones\".</p> <code>'ones'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def loss(\n    self,\n    model_pred: Tensor,\n    target: Tensor,\n    t: Tensor,\n    dt: Optional[Float] = 0.001,\n    mask: Optional[Tensor] = None,\n    weight_type: str = \"ones\",\n):\n    \"\"\"Calculates the loss given the model prediction, target, and time.\n\n    Args:\n        model_pred (Tensor): The predicted output from the model.\n        target (Tensor): The target output for the model prediction.\n        t (Tensor): The time at which the loss is calculated.\n        dt (Optional[Float], optional): The time step increment. Defaults to 0.001.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        weight_type (str, optional): The type of weight to use for the loss. Can be \"ones\", \"data_to_noise\", or \"variational_objective\". Defaults to \"ones\".\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    raw_loss = self._loss_function(model_pred, target)\n    update_weight = self.loss_weight(raw_loss, t, weight_type, dt)\n    loss = raw_loss * update_weight\n    if mask is not None:\n        loss = loss * mask.unsqueeze(-1)\n        n_elem = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n    else:\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.loss_weight","title":"<code>loss_weight(raw_loss, t, weight_type, dt=0.001)</code>","text":"<p>Calculates the weight for the loss based on the given weight type.</p> <p>This function computes the loss weight according to the specified <code>weight_type</code>. The available weight types are: - \"ones\": uniform weight of 1.0 - \"data_to_noise\": derived from Equation (9) of https://arxiv.org/pdf/2202.00512 - \"variational_objective\": based on the variational objective, see https://arxiv.org/pdf/2202.00512</p> <p>Parameters:</p> Name Type Description Default <code>raw_loss</code> <code>Tensor</code> <p>The raw loss calculated from the model prediction and target.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <code>weight_type</code> <code>str</code> <p>The type of weight to use. Can be \"ones\", \"data_to_noise\", or \"variational_objective\".</p> required <code>dt</code> <code>Float</code> <p>The time step increment. Defaults to 0.001.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The weight for the loss.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the weight type is not recognized.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def loss_weight(self, raw_loss: Tensor, t: Tensor, weight_type: str, dt: Float = 0.001) -&gt; Tensor:\n    \"\"\"Calculates the weight for the loss based on the given weight type.\n\n    This function computes the loss weight according to the specified `weight_type`.\n    The available weight types are:\n    - \"ones\": uniform weight of 1.0\n    - \"data_to_noise\": derived from Equation (9) of https://arxiv.org/pdf/2202.00512\n    - \"variational_objective\": based on the variational objective, see https://arxiv.org/pdf/2202.00512\n\n    Args:\n        raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n        t (Tensor): The time step.\n        weight_type (str): The type of weight to use. Can be \"ones\", \"data_to_noise\", or \"variational_objective\".\n        dt (Float, optional): The time step increment. Defaults to 0.001.\n\n    Returns:\n        Tensor: The weight for the loss.\n\n    Raises:\n        ValueError: If the weight type is not recognized.\n    \"\"\"\n    if weight_type == \"ones\":\n        schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n    elif weight_type == \"data_to_noise\":  #\n        log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n        psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n        schedule = (psi**2) / (omega**2)\n        for _ in range(raw_loss.ndim - 1):\n            schedule = schedule.unsqueeze(-1)\n    elif weight_type == \"variational_objective\":\n        # (1-SNR(t-1)/SNR(t)),\n        snr = torch.exp(self.noise_schedule.calculate_log_snr(t, device=t.device))\n        snr_m1 = torch.exp(self.noise_schedule.calculate_log_snr(t - dt, device=t.device))\n        schedule = 1 - snr_m1 / snr\n        for _ in range(raw_loss.ndim - 1):\n            schedule = schedule.unsqueeze(-1)\n    else:\n        raise ValueError(\"Invalid loss weight keyword\")\n    return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.process_data_prediction","title":"<code>process_data_prediction(model_output, sample, t)</code>","text":"<p>Converts the model output to a data prediction based on the prediction type.</p> <p>This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512. Given the model output and the sample, we convert the output to a data prediction based on the prediction type. The conversion formulas are as follows: - For \"noise\" prediction type: <code>pred_data = (sample - noise_scale * model_output) / data_scale</code> - For \"data\" prediction type: <code>pred_data = model_output</code> - For \"v_prediction\" prediction type: <code>pred_data = data_scale * sample - noise_scale * model_output</code></p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>sample</code> <code>Tensor</code> <p>The input sample.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The data prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def process_data_prediction(self, model_output: Tensor, sample, t):\n    \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n    This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n    Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n    The conversion formulas are as follows:\n    - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n    - For \"data\" prediction type: `pred_data = model_output`\n    - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n    Args:\n        model_output (Tensor): The output of the model.\n        sample (Tensor): The input sample.\n        t (Tensor): The time step.\n\n    Returns:\n        The data prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_data = (sample - noise_scale * model_output) / data_scale\n    elif self.prediction_type == PredictionType.DATA:\n        pred_data = model_output\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n            f\" PredictionType.VELOCITY for vdm.\"\n        )\n    return pred_data\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.process_noise_prediction","title":"<code>process_noise_prediction(model_output, sample, t)</code>","text":"<p>Do the same as process_data_prediction but take the model output and convert to nosie.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>sample</code> <code>Tensor</code> <p>The input sample.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The input as noise if the prediction type is \"noise\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"noise\".</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def process_noise_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n    \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n    Args:\n        model_output (Tensor): The output of the model.\n        sample (Tensor): The input sample.\n        t (Tensor): The time step.\n\n    Returns:\n        The input as noise if the prediction type is \"noise\".\n\n    Raises:\n        ValueError: If the prediction type is not \"noise\".\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    data_scale, noise_scale = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_noise = model_output\n    elif self.prediction_type == PredictionType.DATA:\n        pred_noise = (sample - data_scale * model_output) / noise_scale\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n        pred_noise = (sample - data_scale * pred_data) / noise_scale\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n            \" `v_prediction`  for vdm.\"\n        )\n    return pred_noise\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.score","title":"<code>score(x_hat, xt, t)</code>","text":"<p>Converts the data prediction to the estimated score function.</p> <p>Parameters:</p> Name Type Description Default <code>x_hat</code> <code>tensor</code> <p>The predicted data point.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The estimated score function.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n    \"\"\"Converts the data prediction to the estimated score function.\n\n    Args:\n        x_hat (tensor): The predicted data point.\n        xt (Tensor): The current data point.\n        t (Tensor): The time step.\n\n    Returns:\n        The estimated score function.\n    \"\"\"\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    psi, omega = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    psi = pad_like(psi, x_hat)\n    omega = pad_like(omega, x_hat)\n    score = psi * x_hat - xt\n    score = score / (omega * omega)\n    return score\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.set_loss_weight_fn","title":"<code>set_loss_weight_fn(fn)</code>","text":"<p>Sets the loss_weight attribute of the instance to the given function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def set_loss_weight_fn(self, fn: Callable):\n    \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n    Args:\n        fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n    \"\"\"\n    self.loss_weight = fn\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step","title":"<code>step(model_out, t, xt, dt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data. Defaults to None.</p> <code>None</code> <code>center</code> <code>bool</code> <p>Whether to center the data. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Float</code> <p>The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <code>1.0</code> Note <p>The temperature parameter controls the trade off between diversity and sample quality. Decreasing the temperature sharpens the sampling distribtion to focus on more likely samples. The impact of low temperature sampling must be ablated analytically.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n):\n    \"\"\"Do one step integration.\n\n    Args:\n        model_out (Tensor): The output of the model.\n        xt (Tensor): The current data point.\n        t (Tensor): The current time step.\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool): Whether to center the data. Defaults to False.\n        temperature (Float): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n    Note:\n        The temperature parameter controls the trade off between diversity and sample quality.\n        Decreasing the temperature sharpens the sampling distribtion to focus on more likely samples.\n        The impact of low temperature sampling must be ablated analytically.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    alpha_t, sigma_t = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n\n    if (t - dt &lt; 0).any():\n        raise ValueError(\n            \"Error in inference schedule: t - dt &lt; 0. Please ensure that your inference time schedule has shape T with the final t = dt to make s = 0\"\n        )\n\n    log_snr_s = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n    alpha_s, sigma_s = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr_s)\n    sigma_s_2 = sigma_s * sigma_s\n    sigma_t_2 = sigma_t * sigma_t\n    alpha_t_s = alpha_t / alpha_s\n    sigma_2_t_s = -torch.expm1(F.softplus(-log_snr_s) - F.softplus(-log_snr))  # Equation 63\n\n    omega_r = alpha_t_s * sigma_s_2 / sigma_t_2  # Equation 28\n    psi_r = alpha_s * sigma_2_t_s / sigma_t_2\n    std = sigma_2_t_s.sqrt() * sigma_s / sigma_t\n    nonzero_mask = (\n        t &gt; 0\n    ).float()  # based on the time this is always just ones. can leave for now to see if ever want to take extra step and only grab mean\n\n    psi_r = pad_like(psi_r, x_hat)\n    omega_r = pad_like(omega_r, x_hat)\n    std = pad_like(std, x_hat)\n    nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n    mean = psi_r * x_hat + omega_r * xt\n    eps = torch.randn_like(mean).to(model_out.device)\n    x_next = mean + nonzero_mask * std * eps * temperature\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step_ddim","title":"<code>step_ddim(model_out, t, xt, dt, mask=None, eta=0.0, center=False)</code>","text":"<p>Do one step of DDIM sampling.</p> <p>From the ddpm equations alpha_bar = alpha2 and  1 - alpha2 = sigma**2</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>output of the model</p> required <code>t</code> <code>Tensor</code> <p>current time step</p> required <code>xt</code> <code>Tensor</code> <p>current data point</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask for the data point. Defaults to None.</p> <code>None</code> <code>eta</code> <code>Float</code> <p>DDIM sampling parameter. Defaults to 0.0.</p> <code>0.0</code> <code>center</code> <code>Bool</code> <p>whether to center the data point. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step_ddim(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    eta: Float = 0.0,\n    center: Bool = False,\n):\n    \"\"\"Do one step of DDIM sampling.\n\n    From the ddpm equations alpha_bar = alpha**2 and  1 - alpha**2 = sigma**2\n\n    Args:\n        model_out (Tensor): output of the model\n        t (Tensor): current time step\n        xt (Tensor): current data point\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n        eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n        center (Bool, optional): whether to center the data point. Defaults to False.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    data_pred = self.process_data_prediction(model_out, xt, t)\n    noise_pred = self.process_noise_prediction(model_out, xt, t)\n    eps = torch.randn_like(data_pred).to(model_out.device)\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    squared_alpha = log_snr.sigmoid()\n    squared_sigma = (-log_snr).sigmoid()\n    log_snr_prev = self.noise_schedule.calculate_log_snr(t - dt, device=t.device)\n    squared_alpha_prev = log_snr_prev.sigmoid()\n    squared_sigma_prev = (-log_snr_prev).sigmoid()\n    sigma_t_2 = squared_sigma_prev / squared_sigma * (1 - squared_alpha / squared_alpha_prev)\n    psi_r = torch.sqrt(squared_alpha_prev)\n    omega_r = torch.sqrt(1 - squared_alpha_prev - eta * eta * sigma_t_2)\n\n    sigma_t_2 = pad_like(sigma_t_2, model_out)\n    psi_r = pad_like(psi_r, model_out)\n    omega_r = pad_like(omega_r, model_out)\n\n    mean = data_pred * psi_r + omega_r * noise_pred\n    x_next = mean + eta * sigma_t_2.sqrt() * eps\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step_hybrid_sde","title":"<code>step_hybrid_sde(model_out, t, xt, dt, mask=None, center=False, temperature=1.0, equilibrium_rate=0.0)</code>","text":"<p>Do one step integration of Hybrid Langevin-Reverse Time SDE.</p> <p>See section B.3 page 37 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf. and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data. Defaults to None.</p> <code>None</code> <code>center</code> <code>bool</code> <p>Whether to center the data. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Float</code> <p>The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <code>1.0</code> <code>equilibrium_rate</code> <code>Float</code> <p>The rate of Langevin equilibration.  Scales the amount of Langevin dynamics per unit time. Best values are in the range [1.0, 5.0]. Defaults to 0.0.</p> <code>0.0</code> <p>Note: For all step functions that use the SDE formulation its important to note that we are moving backwards in time which corresponds to an apparent sign change. A clear example can be seen in slide 29 https://ernestryu.com/courses/FM/diffusion1.pdf.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step_hybrid_sde(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n    equilibrium_rate: Float = 0.0,\n) -&gt; Tensor:\n    \"\"\"Do one step integration of Hybrid Langevin-Reverse Time SDE.\n\n    See section B.3 page 37 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n    and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n    Args:\n        model_out (Tensor): The output of the model.\n        xt (Tensor): The current data point.\n        t (Tensor): The current time step.\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n        equilibrium_rate (Float, optional): The rate of Langevin equilibration.  Scales the amount of Langevin dynamics per unit time. Best values are in the range [1.0, 5.0]. Defaults to 0.0.\n\n    Note:\n    For all step functions that use the SDE formulation its important to note that we are moving backwards in time which corresponds to an apparent sign change.\n    A clear example can be seen in slide 29 https://ernestryu.com/courses/FM/diffusion1.pdf.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    # Schedule coeffiecients\n    beta = self.noise_schedule.calculate_beta(t)\n    inverse_temperature = 1 / temperature  # lambda_0\n    langevin_factor = equilibrium_rate\n    # Temperature coefficients\n    lambda_t = (\n        inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n    )\n    # langevin_isothermal = True\n    lambda_langevin = inverse_temperature  # if langevin_isothermal else lambda_t\n\n    score_scale_t = lambda_t + lambda_langevin * langevin_factor / 2.0\n\n    eps = torch.randn_like(x_hat).to(model_out.device)\n    score = self.score(x_hat, xt, t)\n    beta = pad_like(beta, model_out)\n    score_scale_t = pad_like(score_scale_t, model_out)\n\n    gT = beta * ((-1 / 2) * xt - score_scale_t * score)\n    gW = torch.sqrt((1.0 + langevin_factor) * beta.abs()) * eps\n\n    x_next = xt + dt * gT + dt.sqrt() * gW\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/vdm/#bionemo.moco.interpolants.continuous_time.continuous.vdm.VDM.step_ode","title":"<code>step_ode(model_out, t, xt, dt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration of ODE.</p> <p>See section B page 36 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf. and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the data. Defaults to None.</p> <code>None</code> <code>center</code> <code>bool</code> <p>Whether to center the data. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Float</code> <p>The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/vdm.py</code> <pre><code>def step_ode(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n) -&gt; Tensor:\n    \"\"\"Do one step integration of ODE.\n\n    See section B page 36 https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf.\n    and https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L730\n\n    Args:\n        model_out (Tensor): The output of the model.\n        xt (Tensor): The current data point.\n        t (Tensor): The current time step.\n        dt (Tensor): The time step increment.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n    log_snr = self.noise_schedule.calculate_log_snr(t, device=t.device)\n    alpha, sigma = self.noise_schedule.log_snr_to_alphas_sigmas(log_snr)\n    # Schedule coeffiecients\n    beta = self.noise_schedule.calculate_beta(t)\n    inverse_temperature = 1 / temperature\n    # Temperature coefficients\n    lambda_t = (\n        inverse_temperature * (sigma.pow(2) + alpha.pow(2)) / (inverse_temperature * sigma.pow(2) + alpha.pow(2))\n    )\n\n    score = self.score(x_hat, xt, t)\n    beta = pad_like(beta, model_out)\n    lambda_t = pad_like(lambda_t, model_out)\n\n    gT = (-1 / 2) * beta * (xt + lambda_t * score)\n\n    x_next = xt + gT * dt\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/augmentation_types/","title":"Augmentation types","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/augmentation_types/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.augmentation_types.AugmentationType","title":"<code>AugmentationType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An enumeration representing the type ofOptimal Transport that can be used in Continuous Flow Matching.</p> <ul> <li>EXACT_OT: Standard mini batch optimal transport defined in  https://arxiv.org/pdf/2302.00482.</li> <li>EQUIVARIANT_OT: Adding roto/translation optimization to mini batch OT see https://arxiv.org/pdf/2306.15030  https://arxiv.org/pdf/2312.07168 4.2.</li> <li>KABSCH: Simple Kabsch alignment between each data and noise point, No permuation # https://arxiv.org/pdf/2410.22388 Sec 3.2</li> </ul> <p>These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/augmentation_types.py</code> <pre><code>class AugmentationType(Enum):\n    \"\"\"An enumeration representing the type ofOptimal Transport that can be used in Continuous Flow Matching.\n\n    - **EXACT_OT**: Standard mini batch optimal transport defined in  https://arxiv.org/pdf/2302.00482.\n    - **EQUIVARIANT_OT**: Adding roto/translation optimization to mini batch OT see https://arxiv.org/pdf/2306.15030  https://arxiv.org/pdf/2312.07168 4.2.\n    - **KABSCH**: Simple Kabsch alignment between each data and noise point, No permuation # https://arxiv.org/pdf/2410.22388 Sec 3.2\n\n    These prediction types can be used to train neural networks for specific tasks, such as denoising, image synthesis, or time-series forecasting.\n    \"\"\"\n\n    EXACT_OT = \"exact_ot\"\n    EQUIVARIANT_OT = \"equivariant_ot\"\n    KABSCH = \"kabsch\"\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/","title":"Equivariant ot sampler","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler","title":"<code>EquivariantOTSampler</code>","text":"<p>Sampler for Mini-batch Optimal Transport Plan with cost calculated after Kabsch alignment.</p> <p>EquivariantOTSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean cost after Kabsch alignment) with different implementations of the plan calculation.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>class EquivariantOTSampler:\n    \"\"\"Sampler for Mini-batch Optimal Transport Plan with cost calculated after Kabsch alignment.\n\n    EquivariantOTSampler implements sampling coordinates according to an OT plan\n    (wrt squared Euclidean cost after Kabsch alignment) with different implementations of the plan calculation.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        method: str = \"exact\",\n        device: Union[str, torch.device] = \"cpu\",\n        num_threads: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the OTSampler class.\n\n        Args:\n            method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n            device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n        Raises:\n            ValueError: If the OT solver is not documented.\n            NotImplementedError: If the OT solver is not implemented.\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n        elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n            raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.device = device\n\n    def to_device(self, device: str):\n        \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n        Args:\n            device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n        Note:\n            This method is used to transfer the internal state of the OTSampler to a different device.\n            It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n        \"\"\"\n        self.device = device\n        for attr_name in dir(self):\n            if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n                setattr(self, attr_name, getattr(self, attr_name).to(device))\n        return self\n\n    def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n            batch_size (int): The batch size of the minibatch.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n        \"\"\"\n        if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n            raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = torch.multinomial(p, batch_size, replacement=replace)\n        return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n\n    def kabsch_align(self, target: Tensor, noise: Tensor) -&gt; Tensor:\n        \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n        Args:\n            target (Tensor): shape (N, *dim), data from source minibatch.\n            noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n        Returns:\n            R (Tensor): shape (*dim, *dim), the rotation matrix.\n        \"\"\"\n        dimension = target.shape[-1]\n        noise_centered = noise - noise.mean(dim=0)\n        target_centered = target - target.mean(dim=0)\n\n        # Compute the covariance matrix\n        covariance_matix = target_centered.T @ noise_centered\n\n        # Compute the SVD of the covariance matrix\n        U, S, Vt = torch.linalg.svd(covariance_matix)\n        d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n        d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n        R = Vt.T @ torch.diag(d_mat) @ U.T\n        return R\n\n    def _calculate_cost_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Compute the cost matrix between a source and a target minibatch.\n\n        The distance between noise and data is calculated after aligning them using Kabsch algorithm.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            M: shape (bs, bs), the cost matrix between noise and data in minibatch.\n            Rs: shape (bs, bs, *dim, *dim), the rotation matrix between noise and data in minibatch.\n        \"\"\"\n        if x0.shape[0] != x1.shape[0]:\n            raise ValueError(\"Shape mismatch: x0.shape = {}, x1.shape = {}\".format(x0.shape, x1.shape))\n        batchsize, maxlen, dimension = x0.shape[0], x0.shape[1], x0.shape[-1]\n        M = torch.zeros(batchsize, batchsize, device=x0.device)\n        Rs = torch.zeros(batchsize, batchsize, dimension, dimension, device=x0.device)\n        for i in range(batchsize):\n            for j in range(batchsize):\n                if mask is not None:\n                    x0i_mask = mask[i].bool()\n                else:\n                    x0i_mask = torch.ones(maxlen, device=x0.device).bool()\n                x0_masked, x1_masked = x0[i][x0i_mask], x1[j][x0i_mask]\n                # Rotate the data to align with the noise\n                R = self.kabsch_align(x1_masked, x0_masked)\n                x1_aligned = x1_masked @ R.T\n                # Here the cost only considered the rotational RMSD, not the translational RMSD\n                cost = torch.dist(x0_masked - x0_masked.mean(dim=0), x1_aligned - x1_aligned.mean(dim=0), p=2)\n                M[i, j] = cost\n                Rs[i, j] = R.T\n\n        return M, Rs\n\n    def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n            Rs (Tensor): shape (bs, bs, *dim, *dim), the rotation matrix between noise and data in minibatch.\n        \"\"\"\n        # Compute the cost matrix\n        M, Rs = self._calculate_cost_matrix(x0, x1, mask)\n\n        # Set uniform weights for all samples in a minibatch\n        a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n        # Compute the OT matrix using POT package\n        p = self.ot_fn(a, b, M)\n\n        # Handle Exceptions\n        if not torch.all(torch.isfinite(p)):\n            raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n        if torch.abs(p.sum()) &lt; 1e-8:\n            warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n            p = torch.ones_like(p) / p.numel()\n\n        return p, Rs\n\n    def apply_augmentation(\n        self,\n        x0: Tensor,\n        x1: Tensor,\n        mask: Optional[Tensor] = None,\n        replace: Bool = False,\n        sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n    ) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n        r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n        Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n            sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n        \"\"\"\n        # Calculate the optimal transport\n        pi, Rs = self.get_ot_matrix(x0, x1, mask)\n\n        # Sample (x0, x1) mapping indices from the OT matrix\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n\n        if not replace and (sort == \"noise\" or sort == \"x0\"):\n            sort_idx = torch.argsort(i)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n                raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n        elif not replace and (sort == \"data\" or sort == \"x1\"):\n            sort_idx = torch.argsort(j)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n                raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n\n        # Get the corresponding rotation matrices\n        rotations = Rs[i, j, :, :]\n        noise = x0[i]\n        # Align the data samples using the rotation matrices\n        x1_aligned = torch.bmm(x1[j], rotations)\n        # Returns the true data that has been permuated and rotated. Translations are done either in preprocessing or after the fact.\n        data = x1_aligned\n\n        if mask is not None:\n            if mask.device != x0.device:\n                mask = mask.to(x0.device)\n            mask = mask[i]\n        # Output the permuted samples in the minibatch\n        return noise, data, mask\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.__init__","title":"<code>__init__(method='exact', device='cpu', num_threads=1)</code>","text":"<p>Initialize the OTSampler class.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).</p> <code>'exact'</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>num_threads</code> <code>Union[int, str]</code> <p>Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the OT solver is not documented.</p> <code>NotImplementedError</code> <p>If the OT solver is not implemented.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def __init__(\n    self,\n    method: str = \"exact\",\n    device: Union[str, torch.device] = \"cpu\",\n    num_threads: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the OTSampler class.\n\n    Args:\n        method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n        device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n    Raises:\n        ValueError: If the OT solver is not documented.\n        NotImplementedError: If the OT solver is not implemented.\n    \"\"\"\n    # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n    # M is a cost matrix\n    if method == \"exact\":\n        self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n    elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n        raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    self.device = device\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler._calculate_cost_matrix","title":"<code>_calculate_cost_matrix(x0, x1, mask=None)</code>","text":"<p>Compute the cost matrix between a source and a target minibatch.</p> <p>The distance between noise and data is calculated after aligning them using Kabsch algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>M</code> <code>Tensor</code> <p>shape (bs, bs), the cost matrix between noise and data in minibatch.</p> <code>Rs</code> <code>Tensor</code> <p>shape (bs, bs, dim, dim), the rotation matrix between noise and data in minibatch.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def _calculate_cost_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Compute the cost matrix between a source and a target minibatch.\n\n    The distance between noise and data is calculated after aligning them using Kabsch algorithm.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        M: shape (bs, bs), the cost matrix between noise and data in minibatch.\n        Rs: shape (bs, bs, *dim, *dim), the rotation matrix between noise and data in minibatch.\n    \"\"\"\n    if x0.shape[0] != x1.shape[0]:\n        raise ValueError(\"Shape mismatch: x0.shape = {}, x1.shape = {}\".format(x0.shape, x1.shape))\n    batchsize, maxlen, dimension = x0.shape[0], x0.shape[1], x0.shape[-1]\n    M = torch.zeros(batchsize, batchsize, device=x0.device)\n    Rs = torch.zeros(batchsize, batchsize, dimension, dimension, device=x0.device)\n    for i in range(batchsize):\n        for j in range(batchsize):\n            if mask is not None:\n                x0i_mask = mask[i].bool()\n            else:\n                x0i_mask = torch.ones(maxlen, device=x0.device).bool()\n            x0_masked, x1_masked = x0[i][x0i_mask], x1[j][x0i_mask]\n            # Rotate the data to align with the noise\n            R = self.kabsch_align(x1_masked, x0_masked)\n            x1_aligned = x1_masked @ R.T\n            # Here the cost only considered the rotational RMSD, not the translational RMSD\n            cost = torch.dist(x0_masked - x0_masked.mean(dim=0), x1_aligned - x1_aligned.mean(dim=0), p=2)\n            M[i, j] = cost\n            Rs[i, j] = R.T\n\n    return M, Rs\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, replace=False, sort='x0')</code>","text":"<p>Sample indices for noise and data in minibatch according to OT plan.</p> <p>Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <code>sort</code> <code>str</code> <p>Optional Literal string to sort either x1 or x0 based on the input.</p> <code>'x0'</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor, Optional[Tensor]]</code> <p>tuple of 2 tensors, represents the noise and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def apply_augmentation(\n    self,\n    x0: Tensor,\n    x1: Tensor,\n    mask: Optional[Tensor] = None,\n    replace: Bool = False,\n    sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n    r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n    Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n    minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n        sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n    \"\"\"\n    # Calculate the optimal transport\n    pi, Rs = self.get_ot_matrix(x0, x1, mask)\n\n    # Sample (x0, x1) mapping indices from the OT matrix\n    i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n\n    if not replace and (sort == \"noise\" or sort == \"x0\"):\n        sort_idx = torch.argsort(i)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n            raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n    elif not replace and (sort == \"data\" or sort == \"x1\"):\n        sort_idx = torch.argsort(j)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n            raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n\n    # Get the corresponding rotation matrices\n    rotations = Rs[i, j, :, :]\n    noise = x0[i]\n    # Align the data samples using the rotation matrices\n    x1_aligned = torch.bmm(x1[j], rotations)\n    # Returns the true data that has been permuated and rotated. Translations are done either in preprocessing or after the fact.\n    data = x1_aligned\n\n    if mask is not None:\n        if mask.device != x0.device:\n            mask = mask.to(x0.device)\n        mask = mask[i]\n    # Output the permuted samples in the minibatch\n    return noise, data, mask\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.get_ot_matrix","title":"<code>get_ot_matrix(x0, x1, mask=None)</code>","text":"<p>Compute the OT matrix between a source and a target minibatch.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>p</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> <code>Rs</code> <code>Tensor</code> <p>shape (bs, bs, dim, dim), the rotation matrix between noise and data in minibatch.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n        Rs (Tensor): shape (bs, bs, *dim, *dim), the rotation matrix between noise and data in minibatch.\n    \"\"\"\n    # Compute the cost matrix\n    M, Rs = self._calculate_cost_matrix(x0, x1, mask)\n\n    # Set uniform weights for all samples in a minibatch\n    a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n    # Compute the OT matrix using POT package\n    p = self.ot_fn(a, b, M)\n\n    # Handle Exceptions\n    if not torch.all(torch.isfinite(p)):\n        raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n    if torch.abs(p.sum()) &lt; 1e-8:\n        warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n        p = torch.ones_like(p) / p.numel()\n\n    return p, Rs\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.kabsch_align","title":"<code>kabsch_align(target, noise)</code>","text":"<p>Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>shape (N, *dim), data from source minibatch.</p> required <code>noise</code> <code>Tensor</code> <p>shape (N, *dim), noise from source minibatch.</p> required <p>Returns:</p> Name Type Description <code>R</code> <code>Tensor</code> <p>shape (dim, dim), the rotation matrix.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def kabsch_align(self, target: Tensor, noise: Tensor) -&gt; Tensor:\n    \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n    Args:\n        target (Tensor): shape (N, *dim), data from source minibatch.\n        noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n    Returns:\n        R (Tensor): shape (*dim, *dim), the rotation matrix.\n    \"\"\"\n    dimension = target.shape[-1]\n    noise_centered = noise - noise.mean(dim=0)\n    target_centered = target - target.mean(dim=0)\n\n    # Compute the covariance matrix\n    covariance_matix = target_centered.T @ noise_centered\n\n    # Compute the SVD of the covariance matrix\n    U, S, Vt = torch.linalg.svd(covariance_matix)\n    d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n    d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n    R = Vt.T @ torch.diag(d_mat) @ U.T\n    return R\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.sample_map","title":"<code>sample_map(pi, batch_size, replace=False)</code>","text":"<p>Draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>pi</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> required <code>batch_size</code> <code>int</code> <p>The batch size of the minibatch.</p> required <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor]</code> <p>tuple of 2 tensors, represents the indices of noise and data samples from pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n        batch_size (int): The batch size of the minibatch.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n    \"\"\"\n    if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n        raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n    p = pi.flatten()\n    p = p / p.sum()\n    choices = torch.multinomial(p, batch_size, replacement=replace)\n    return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.equivariant_ot_sampler.EquivariantOTSampler.to_device","title":"<code>to_device(device)</code>","text":"<p>Moves all internal tensors to the specified device and updates the <code>self.device</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").</p> required Note <p>This method is used to transfer the internal state of the OTSampler to a different device. It updates the <code>self.device</code> attribute to reflect the new device and moves all internal tensors to the specified device.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/equivariant_ot_sampler.py</code> <pre><code>def to_device(self, device: str):\n    \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n    Args:\n        device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n    Note:\n        This method is used to transfer the internal state of the OTSampler to a different device.\n        It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n    \"\"\"\n    self.device = device\n    for attr_name in dir(self):\n        if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n            setattr(self, attr_name, getattr(self, attr_name).to(device))\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/","title":"Kabsch augmentation","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation","title":"<code>KabschAugmentation</code>","text":"<p>Point-wise Kabsch alignment.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>class KabschAugmentation:\n    \"\"\"Point-wise Kabsch alignment.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the KabschAugmentation instance.\n\n        Notes:\n            - This implementation assumes no required initialization arguments.\n            - You can add instance variables (e.g., `self.variable_name`) as needed.\n        \"\"\"\n        pass  # No operations are performed when initializing with no args\n\n    def kabsch_align(self, target: Tensor, noise: Tensor):\n        \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n        Args:\n            target (Tensor): shape (N, *dim), data from source minibatch.\n            noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n        Returns:\n            R (Tensor): shape (*dim, *dim), the rotation matrix.\n            Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n        \"\"\"\n        dimension = target.shape[-1]\n        noise_translation = noise.mean(dim=0)\n        noise_centered = noise - noise_translation\n        target_centered = target - target.mean(dim=0)\n\n        # Compute the covariance matrix\n        covariance_matix = target_centered.T @ noise_centered\n\n        # Compute the SVD of the covariance matrix\n        U, S, Vt = torch.linalg.svd(covariance_matix)\n        d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n        d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n        R = Vt.T @ torch.diag(d_mat) @ U.T\n\n        target_aligned = target_centered @ R.T + noise_translation\n\n        return R, target_aligned\n\n    def batch_kabsch_align(self, target: Tensor, noise: Tensor):\n        \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n        Args:\n            target (Tensor): shape (B, N, *dim), data from source minibatch.\n            noise (Tensor): shape (B, N, *dim), noise from source minibatch.\n\n        Returns:\n            R (Tensor): shape (*dim, *dim), the rotation matrix.\n            Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n        \"\"\"\n        # Corrected Batched Kabsch Alignment\n        batch_size, _, dimension = target.shape\n\n        # Center the target and noise tensors along the middle dimension (N) for each batch item\n        noise_translation = noise.mean(dim=1, keepdim=True)\n        noise_centered = noise - noise_translation\n        target_centered = target - target.mean(dim=1, keepdim=True)\n\n        # Compute the covariance matrix for each batch item\n        covariance_matrix = torch.matmul(target_centered.transpose(1, 2), noise_centered)\n\n        # Compute the SVD of the covariance matrix for each batch item\n        U, S, Vt = torch.linalg.svd(covariance_matrix)\n\n        # Adjust for proper rotation (determinant=1) for each batch item\n        d = torch.sign(torch.linalg.det(Vt @ U.transpose(-1, -2)))  # Keep as tensor for batch operations\n        d_mat = torch.diag_embed(\n            torch.cat(\n                [torch.ones(batch_size, dimension - 1, device=Vt.device, dtype=Vt.dtype), d.unsqueeze(-1)], dim=-1\n            )\n        )\n\n        R_batch = torch.matmul(torch.matmul(Vt.transpose(-1, -2), d_mat), U.transpose(-1, -2))\n\n        target_aligned = target_centered @ R_batch.transpose(-1, -2) + noise_translation\n        return R_batch, target_aligned\n\n    def apply_augmentation(\n        self,\n        x0: Tensor,\n        x1: Tensor,\n        mask: Optional[Tensor] = None,\n        align_noise_to_data=True,\n    ) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n        Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n            align_noise_to_data (bool): Direction of alignment default is True meaning it augments Noise to reduce error to Data.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n        \"\"\"\n        if x1.ndim &gt; 2:\n            align_func = self.batch_kabsch_align\n        else:\n            align_func = self.kabsch_align\n        if mask is not None:\n            mask = pad_like(mask, x1)\n            x1 = x1 * mask\n            x0 = x0 * mask\n        if align_noise_to_data:\n            # Compute the rotation matrix R that aligns x0 to x1\n            R, aligned_x0 = align_func(x0, x1)\n            noise = aligned_x0\n            data = x1\n        else:\n            # Compute the rotation matrix R that aligns x1 to x0\n            R, aligned_x1 = align_func(x1, x0)\n            noise = x0\n            data = aligned_x1\n        if mask is not None:\n            noise = noise * mask\n            data = data * mask\n        # Output the permuted samples in the minibatch\n        return noise, data\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the KabschAugmentation instance.</p> Notes <ul> <li>This implementation assumes no required initialization arguments.</li> <li>You can add instance variables (e.g., <code>self.variable_name</code>) as needed.</li> </ul> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the KabschAugmentation instance.\n\n    Notes:\n        - This implementation assumes no required initialization arguments.\n        - You can add instance variables (e.g., `self.variable_name`) as needed.\n    \"\"\"\n    pass  # No operations are performed when initializing with no args\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, align_noise_to_data=True)</code>","text":"<p>Sample indices for noise and data in minibatch according to OT plan.</p> <p>Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> required <code>align_noise_to_data</code> <code>bool</code> <p>Direction of alignment default is True meaning it augments Noise to reduce error to Data.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor]</code> <p>tuple of 2 tensors, represents the noise and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def apply_augmentation(\n    self,\n    x0: Tensor,\n    x1: Tensor,\n    mask: Optional[Tensor] = None,\n    align_noise_to_data=True,\n) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n    Compute the OT plan $\\pi$ (wrt squared Euclidean cost after Kabsch alignment) between a source and a target\n    minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n        align_noise_to_data (bool): Direction of alignment default is True meaning it augments Noise to reduce error to Data.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the noise and data samples following OT plan pi.\n    \"\"\"\n    if x1.ndim &gt; 2:\n        align_func = self.batch_kabsch_align\n    else:\n        align_func = self.kabsch_align\n    if mask is not None:\n        mask = pad_like(mask, x1)\n        x1 = x1 * mask\n        x0 = x0 * mask\n    if align_noise_to_data:\n        # Compute the rotation matrix R that aligns x0 to x1\n        R, aligned_x0 = align_func(x0, x1)\n        noise = aligned_x0\n        data = x1\n    else:\n        # Compute the rotation matrix R that aligns x1 to x0\n        R, aligned_x1 = align_func(x1, x0)\n        noise = x0\n        data = aligned_x1\n    if mask is not None:\n        noise = noise * mask\n        data = data * mask\n    # Output the permuted samples in the minibatch\n    return noise, data\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.batch_kabsch_align","title":"<code>batch_kabsch_align(target, noise)</code>","text":"<p>Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>shape (B, N, *dim), data from source minibatch.</p> required <code>noise</code> <code>Tensor</code> <p>shape (B, N, *dim), noise from source minibatch.</p> required <p>Returns:</p> Name Type Description <code>R</code> <code>Tensor</code> <p>shape (dim, dim), the rotation matrix.</p> <p>Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def batch_kabsch_align(self, target: Tensor, noise: Tensor):\n    \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n    Args:\n        target (Tensor): shape (B, N, *dim), data from source minibatch.\n        noise (Tensor): shape (B, N, *dim), noise from source minibatch.\n\n    Returns:\n        R (Tensor): shape (*dim, *dim), the rotation matrix.\n        Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n    \"\"\"\n    # Corrected Batched Kabsch Alignment\n    batch_size, _, dimension = target.shape\n\n    # Center the target and noise tensors along the middle dimension (N) for each batch item\n    noise_translation = noise.mean(dim=1, keepdim=True)\n    noise_centered = noise - noise_translation\n    target_centered = target - target.mean(dim=1, keepdim=True)\n\n    # Compute the covariance matrix for each batch item\n    covariance_matrix = torch.matmul(target_centered.transpose(1, 2), noise_centered)\n\n    # Compute the SVD of the covariance matrix for each batch item\n    U, S, Vt = torch.linalg.svd(covariance_matrix)\n\n    # Adjust for proper rotation (determinant=1) for each batch item\n    d = torch.sign(torch.linalg.det(Vt @ U.transpose(-1, -2)))  # Keep as tensor for batch operations\n    d_mat = torch.diag_embed(\n        torch.cat(\n            [torch.ones(batch_size, dimension - 1, device=Vt.device, dtype=Vt.dtype), d.unsqueeze(-1)], dim=-1\n        )\n    )\n\n    R_batch = torch.matmul(torch.matmul(Vt.transpose(-1, -2), d_mat), U.transpose(-1, -2))\n\n    target_aligned = target_centered @ R_batch.transpose(-1, -2) + noise_translation\n    return R_batch, target_aligned\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.kabsch_augmentation.KabschAugmentation.kabsch_align","title":"<code>kabsch_align(target, noise)</code>","text":"<p>Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>shape (N, *dim), data from source minibatch.</p> required <code>noise</code> <code>Tensor</code> <p>shape (N, *dim), noise from source minibatch.</p> required <p>Returns:</p> Name Type Description <code>R</code> <code>Tensor</code> <p>shape (dim, dim), the rotation matrix.</p> <p>Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/kabsch_augmentation.py</code> <pre><code>def kabsch_align(self, target: Tensor, noise: Tensor):\n    \"\"\"Find the Rotation matrix (R) such that RMSD is minimized between target @ R.T and noise.\n\n    Args:\n        target (Tensor): shape (N, *dim), data from source minibatch.\n        noise (Tensor): shape (N, *dim), noise from source minibatch.\n\n    Returns:\n        R (Tensor): shape (*dim, *dim), the rotation matrix.\n        Aliged Target (Tensor): target tensor rotated and shifted to reduced RMSD with noise\n    \"\"\"\n    dimension = target.shape[-1]\n    noise_translation = noise.mean(dim=0)\n    noise_centered = noise - noise_translation\n    target_centered = target - target.mean(dim=0)\n\n    # Compute the covariance matrix\n    covariance_matix = target_centered.T @ noise_centered\n\n    # Compute the SVD of the covariance matrix\n    U, S, Vt = torch.linalg.svd(covariance_matix)\n    d = torch.sign(torch.linalg.det(Vt.T @ U.T)).item()\n    d_mat = torch.tensor([1] * (dimension - 1) + [d], device=Vt.device, dtype=Vt.dtype)\n    R = Vt.T @ torch.diag(d_mat) @ U.T\n\n    target_aligned = target_centered @ R.T + noise_translation\n\n    return R, target_aligned\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/","title":"Ot sampler","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler","title":"<code>OTSampler</code>","text":"<p>Sampler for Exact Mini-batch Optimal Transport Plan.</p> <p>OTSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean cost) with different implementations of the plan calculation. Code is adapted from https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>class OTSampler:\n    \"\"\"Sampler for Exact Mini-batch Optimal Transport Plan.\n\n    OTSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean cost)\n    with different implementations of the plan calculation. Code is adapted from https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py\n\n    \"\"\"\n\n    def __init__(\n        self,\n        method: str = \"exact\",\n        device: Union[str, torch.device] = \"cpu\",\n        num_threads: int = 1,\n    ) -&gt; None:\n        \"\"\"Initialize the OTSampler class.\n\n        Args:\n            method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n            device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n        Raises:\n            ValueError: If the OT solver is not documented.\n            NotImplementedError: If the OT solver is not implemented.\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n        elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n            raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.device = device\n\n    def to_device(self, device: str):\n        \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n        Args:\n            device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n        Note:\n            This method is used to transfer the internal state of the OTSampler to a different device.\n            It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n        \"\"\"\n        self.device = device\n        for attr_name in dir(self):\n            if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n                setattr(self, attr_name, getattr(self, attr_name).to(device))\n        return self\n\n    def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n        r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n            batch_size (int): The batch size of the minibatch.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n        Returns:\n            Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n        \"\"\"\n        if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n            raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = torch.multinomial(p, batch_size, replacement=replace)\n        return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n\n    def _calculate_cost_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n        \"\"\"Compute the cost matrix between a source and a target minibatch.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            Tensor: shape (bs, bs), the cost matrix between noise and data in minibatch.\n        \"\"\"\n        if mask is None:\n            # Flatten the input tensors\n            x0, x1 = x0.reshape(x0.shape[0], -1), x1.reshape(x1.shape[0], -1)\n\n            # Compute the cost matrix. For exact OT, we use squared Euclidean distance.\n            M = torch.cdist(x0, x1) ** 2\n        else:\n            # Initialize the cost matrix\n            M = torch.zeros((x0.shape[0], x1.shape[0]))\n            # For each x0 sample, apply its mask to all x1 samples and calculate the cost\n            for i in range(x0.shape[0]):\n                x0i_mask = mask[i].unsqueeze(-1)\n                masked_x1 = x1 * x0i_mask\n                masked_x0 = x0[i] * x0i_mask\n                cost = torch.cdist(masked_x0.reshape(1, -1), masked_x1.reshape(x1.shape[0], -1)) ** 2\n                M[i] = cost\n        return M\n\n    def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n        \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n        Returns:\n            p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n\n        \"\"\"\n        # Compute the cost matrix\n        M = self._calculate_cost_matrix(x0, x1, mask)\n        # Set uniform weights for all samples in a minibatch\n        a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n        p = self.ot_fn(a, b, M)\n        # Handle exceptions\n        if not torch.all(torch.isfinite(p)):\n            raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n        if torch.abs(p.sum()) &lt; 1e-8:\n            warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n            p = torch.ones_like(p) / p.numel()\n\n        return p\n\n    def apply_augmentation(\n        self,\n        x0: Tensor,\n        x1: Tensor,\n        mask: Optional[Tensor] = None,\n        replace: Bool = False,\n        sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n    ) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n        r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n        Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n        Args:\n            x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n            x1 (Tensor): shape (bs, *dim), data from source minibatch.\n            mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n            replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n            sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n        Returns:\n            Tuple: tuple of 2 tensors or 3 tensors if mask is used, represents the noise (plus mask) and data samples following OT plan pi.\n        \"\"\"\n        if replace and sort is not None:\n            raise ValueError(\"Cannot sample with replacement and sort\")\n        # Calculate the optimal transport\n        pi = self.get_ot_matrix(x0, x1, mask)\n\n        # Sample (x0, x1) mapping indices from the OT matrix\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        if not replace and (sort == \"noise\" or sort == \"x0\"):\n            sort_idx = torch.argsort(i)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n                raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n            noise = x0\n            data = x1[j]\n        elif not replace and (sort == \"data\" or sort == \"x1\"):\n            sort_idx = torch.argsort(j)\n            i = i[sort_idx]\n            j = j[sort_idx]\n\n            if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n                raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n            noise = x0[i]\n            data = x1\n        else:\n            noise = x0[i]\n            data = x1[j]\n\n        # Output the permuted samples in the minibatch\n        if mask is not None:\n            if mask.device != x0.device:\n                mask = mask.to(x0.device)\n            mask = mask[i]\n        return noise, data, mask\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.__init__","title":"<code>__init__(method='exact', device='cpu', num_threads=1)</code>","text":"<p>Initialize the OTSampler class.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).</p> <code>'exact'</code> <code>device</code> <code>Union[str, device]</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>num_threads</code> <code>Union[int, str]</code> <p>Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the OT solver is not documented.</p> <code>NotImplementedError</code> <p>If the OT solver is not implemented.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def __init__(\n    self,\n    method: str = \"exact\",\n    device: Union[str, torch.device] = \"cpu\",\n    num_threads: int = 1,\n) -&gt; None:\n    \"\"\"Initialize the OTSampler class.\n\n    Args:\n        method (str): Choose which optimal transport solver you would like to use. Currently only support exact OT solvers (pot.emd).\n        device (Union[str, torch.device], optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        num_threads (Union[int, str], optional): Number of threads to use for OT solver. If \"max\", uses the maximum number of threads. Default is 1.\n\n    Raises:\n        ValueError: If the OT solver is not documented.\n        NotImplementedError: If the OT solver is not implemented.\n    \"\"\"\n    # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n    # M is a cost matrix\n    if method == \"exact\":\n        self.ot_fn: Callable[..., torch.Tensor] = partial(pot.emd, numThreads=num_threads)  # type: ignore\n    elif method in {\"sinkhorn\", \"unbalanced\", \"partial\"}:\n        raise NotImplementedError(\"OT solver other than 'exact' is not implemented.\")\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    self.device = device\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler._calculate_cost_matrix","title":"<code>_calculate_cost_matrix(x0, x1, mask=None)</code>","text":"<p>Compute the cost matrix between a source and a target minibatch.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>shape (bs, bs), the cost matrix between noise and data in minibatch.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def _calculate_cost_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Compute the cost matrix between a source and a target minibatch.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        Tensor: shape (bs, bs), the cost matrix between noise and data in minibatch.\n    \"\"\"\n    if mask is None:\n        # Flatten the input tensors\n        x0, x1 = x0.reshape(x0.shape[0], -1), x1.reshape(x1.shape[0], -1)\n\n        # Compute the cost matrix. For exact OT, we use squared Euclidean distance.\n        M = torch.cdist(x0, x1) ** 2\n    else:\n        # Initialize the cost matrix\n        M = torch.zeros((x0.shape[0], x1.shape[0]))\n        # For each x0 sample, apply its mask to all x1 samples and calculate the cost\n        for i in range(x0.shape[0]):\n            x0i_mask = mask[i].unsqueeze(-1)\n            masked_x1 = x1 * x0i_mask\n            masked_x0 = x0[i] * x0i_mask\n            cost = torch.cdist(masked_x0.reshape(1, -1), masked_x1.reshape(x1.shape[0], -1)) ** 2\n            M[i] = cost\n    return M\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.apply_augmentation","title":"<code>apply_augmentation(x0, x1, mask=None, replace=False, sort='x0')</code>","text":"<p>Sample indices for noise and data in minibatch according to OT plan.</p> <p>Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <code>sort</code> <code>str</code> <p>Optional Literal string to sort either x1 or x0 based on the input.</p> <code>'x0'</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor, Optional[Tensor]]</code> <p>tuple of 2 tensors or 3 tensors if mask is used, represents the noise (plus mask) and data samples following OT plan pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def apply_augmentation(\n    self,\n    x0: Tensor,\n    x1: Tensor,\n    mask: Optional[Tensor] = None,\n    replace: Bool = False,\n    sort: Optional[Literal[\"noise\", \"x0\", \"data\", \"x1\"]] = \"x0\",\n) -&gt; Tuple[Tensor, Tensor, Optional[Tensor]]:\n    r\"\"\"Sample indices for noise and data in minibatch according to OT plan.\n\n    Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n    minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n        sort (str): Optional Literal string to sort either x1 or x0 based on the input.\n\n    Returns:\n        Tuple: tuple of 2 tensors or 3 tensors if mask is used, represents the noise (plus mask) and data samples following OT plan pi.\n    \"\"\"\n    if replace and sort is not None:\n        raise ValueError(\"Cannot sample with replacement and sort\")\n    # Calculate the optimal transport\n    pi = self.get_ot_matrix(x0, x1, mask)\n\n    # Sample (x0, x1) mapping indices from the OT matrix\n    i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n    if not replace and (sort == \"noise\" or sort == \"x0\"):\n        sort_idx = torch.argsort(i)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (i == torch.arange(x0.shape[0], device=i.device)).all():\n            raise ValueError(\"x0_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n        noise = x0\n        data = x1[j]\n    elif not replace and (sort == \"data\" or sort == \"x1\"):\n        sort_idx = torch.argsort(j)\n        i = i[sort_idx]\n        j = j[sort_idx]\n\n        if not (j == torch.arange(x1.shape[0], device=j.device)).all():\n            raise ValueError(\"x1_idx should be a tensor from 0 to size - 1 when sort is 'noise' or 'x0\")\n        noise = x0[i]\n        data = x1\n    else:\n        noise = x0[i]\n        data = x1[j]\n\n    # Output the permuted samples in the minibatch\n    if mask is not None:\n        if mask.device != x0.device:\n            mask = mask.to(x0.device)\n        mask = mask[i]\n    return noise, data, mask\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.get_ot_matrix","title":"<code>get_ot_matrix(x0, x1, mask=None)</code>","text":"<p>Compute the OT matrix between a source and a target minibatch.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor</code> <p>shape (bs, *dim), noise from source minibatch.</p> required <code>x1</code> <code>Tensor</code> <p>shape (bs, *dim), data from source minibatch.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>p</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def get_ot_matrix(self, x0: Tensor, x1: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Compute the OT matrix between a source and a target minibatch.\n\n    Args:\n        x0 (Tensor): shape (bs, *dim), noise from source minibatch.\n        x1 (Tensor): shape (bs, *dim), data from source minibatch.\n        mask (Optional[Tensor], optional): mask to apply to the output, shape (batchsize, nodes), if not provided no mask is applied. Defaults to None.\n\n    Returns:\n        p (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n\n    \"\"\"\n    # Compute the cost matrix\n    M = self._calculate_cost_matrix(x0, x1, mask)\n    # Set uniform weights for all samples in a minibatch\n    a, b = pot.unif(x0.shape[0], type_as=M), pot.unif(x1.shape[0], type_as=M)\n\n    p = self.ot_fn(a, b, M)\n    # Handle exceptions\n    if not torch.all(torch.isfinite(p)):\n        raise ValueError(\"OT plan map is not finite, cost mean, max: {}, {}\".format(M.mean(), M.max()))\n    if torch.abs(p.sum()) &lt; 1e-8:\n        warnings.warn(\"Numerical errors in OT matrix, reverting to uniform plan.\")\n        p = torch.ones_like(p) / p.numel()\n\n    return p\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.sample_map","title":"<code>sample_map(pi, batch_size, replace=False)</code>","text":"<p>Draw source and target samples from pi $(x,z) \\sim \\pi$.</p> <p>Parameters:</p> Name Type Description Default <code>pi</code> <code>Tensor</code> <p>shape (bs, bs), the OT matrix between noise and data in minibatch.</p> required <code>batch_size</code> <code>int</code> <p>The batch size of the minibatch.</p> required <code>replace</code> <code>bool</code> <p>sampling w/ or w/o replacement from the OT plan, default to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Tensor, Tensor]</code> <p>tuple of 2 tensors, represents the indices of noise and data samples from pi.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def sample_map(self, pi: Tensor, batch_size: int, replace: Bool = False) -&gt; Tuple[Tensor, Tensor]:\n    r\"\"\"Draw source and target samples from pi $(x,z) \\sim \\pi$.\n\n    Args:\n        pi (Tensor): shape (bs, bs), the OT matrix between noise and data in minibatch.\n        batch_size (int): The batch size of the minibatch.\n        replace (bool): sampling w/ or w/o replacement from the OT plan, default to False.\n\n    Returns:\n        Tuple: tuple of 2 tensors, represents the indices of noise and data samples from pi.\n    \"\"\"\n    if pi.shape[0] != batch_size or pi.shape[1] != batch_size:\n        raise ValueError(\"Shape mismatch: pi.shape = {}, batch_size = {}\".format(pi.shape, batch_size))\n    p = pi.flatten()\n    p = p / p.sum()\n    choices = torch.multinomial(p, batch_size, replacement=replace)\n    return torch.div(choices, pi.shape[1], rounding_mode=\"floor\"), choices % pi.shape[1]\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler/#bionemo.moco.interpolants.continuous_time.continuous.data_augmentation.ot_sampler.OTSampler.to_device","title":"<code>to_device(device)</code>","text":"<p>Moves all internal tensors to the specified device and updates the <code>self.device</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").</p> required Note <p>This method is used to transfer the internal state of the OTSampler to a different device. It updates the <code>self.device</code> attribute to reflect the new device and moves all internal tensors to the specified device.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/continuous/data_augmentation/ot_sampler.py</code> <pre><code>def to_device(self, device: str):\n    \"\"\"Moves all internal tensors to the specified device and updates the `self.device` attribute.\n\n    Args:\n        device (str): The device to move the tensors to (e.g. \"cpu\", \"cuda:0\").\n\n    Note:\n        This method is used to transfer the internal state of the OTSampler to a different device.\n        It updates the `self.device` attribute to reflect the new device and moves all internal tensors to the specified device.\n    \"\"\"\n    self.device = device\n    for attr_name in dir(self):\n        if attr_name.startswith(\"_\") and isinstance(getattr(self, attr_name), torch.Tensor):\n            setattr(self, attr_name, getattr(self, attr_name).to(device))\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/","title":"Discrete flow matching","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher","title":"<code>DiscreteFlowMatcher</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Discrete Flow Model (DFM) interpolant.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>class DiscreteFlowMatcher(Interpolant):\n    \"\"\"A Discrete Flow Model (DFM) interpolant.\"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: DiscretePriorDistribution,\n        device: str = \"cpu\",\n        eps: Float = 1e-5,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initialize the DFM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The time distribution for the diffusion process.\n            prior_distribution (DiscretePriorDistribution): The prior distribution for the discrete masked tokens.\n            device (str, optional): The device to use for computations. Defaults to \"cpu\".\n            eps: small Float to prevent dividing by zero.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        self.num_classes = prior_distribution.num_classes\n        self.eps = eps\n        self.use_mask = isinstance(self.prior_distribution, DiscreteMaskedPrior)\n        if self.use_mask:\n            self.mask_index = prior_distribution.mask_dim  # type: ignore\n        self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n            noise: tensor noise ids\n        \"\"\"\n        if data.dtype == torch.float and data.ndim &gt; 2:\n            x1 = data.argmax(-1)\n        else:\n            x1 = data\n        x0 = noise\n        t = pad_like(t, x1)\n        threshold = torch.rand_like(x1.float())\n        xt = torch.where((threshold &lt; 1 - t), x0, x1)\n        return xt\n\n    def loss(\n        self,\n        logits: Tensor,\n        target: Tensor,\n        time: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        use_weight: Bool = False,\n    ):\n        \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n        The loss is calculated between the batch x node x class logits and the target batch x node.\n        If using a masked prior please pass in the correct mask to calculate loss values on only masked states.\n        i.e. mask = data_mask * is_masked_state which is calculated with self.prior_dist.is_masked(xt))\n\n        If `use_weight` is True, the loss is weighted by 1/(1-t) defined in equation 24 in Appndix C. of https://arxiv.org/pdf/2402.04997\n\n        Args:\n            logits (Tensor): The predicted output from the model, with shape batch x node x class.\n            target (Tensor): The target output for the model prediction, with shape batch x node.\n            time (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            use_weight (bool, optional): Whether to use the DFM time weight for the loss. Defaults to True.\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        assert target.ndim + 1 == logits.ndim\n        loss = self._loss_function(logits.transpose(-1, 1), target.long())\n        if mask is not None:\n            loss = loss * mask\n            num_non_masked_elements = torch.sum(mask, dim=-1)\n            num_non_masked_elements[num_non_masked_elements == 0] = (\n                1.0  #! prevents divide by zero since if the row is all zero the sum of loss = 0\n            )\n            loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n        else:\n            loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n        if use_weight:\n            if time is None:\n                raise ValueError(\"Time is required to compute the DFM liklehood weighting of 1/(1-t + self.eps)\")\n            loss = loss * 1 / (1 - time + self.eps)\n        return loss\n\n    def step(\n        self,\n        logits: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor | float,\n        temperature: Float = 1.0,\n        stochasticity: Float = 1.0,\n    ) -&gt; Tensor:\n        \"\"\"Perform a single step of DFM euler updates.\n\n        Args:\n            logits (Tensor): The input logits.\n            t (Tensor): The current time step.\n            xt (Tensor): The current state.\n            dt (Tensor | float): The time step increment.\n            temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n            stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n        Returns:\n            Tensor: The updated state.\n        \"\"\"\n        x_1_pred_logits = logits\n        S = x_1_pred_logits.shape[-1]\n        t = pad_like(t, logits)\n        if isinstance(dt, float):\n            dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n        dt = pad_like(dt, logits)  # type: ignore\n\n        if self.use_mask:\n            if self.mask_index &gt;= S:\n                raise ValueError(\n                    \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n                )\n\n            mask_one_hot = torch.zeros((S,), device=self.device)\n            mask_one_hot[self.mask_index] = 1.0\n            x_1_pred_logits[..., self.mask_index] = -1.0e9\n\n            x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n\n            xt_is_mask = (xt == self.mask_index).unsqueeze(-1).float()  # b x n x 1\n            step_prob = (\n                dt * x_1_pred_prob * ((1 + stochasticity * t) / (1 - t)) * xt_is_mask\n                + dt\n                * (1 - xt_is_mask)\n                * mask_one_hot.view(1, 1, -1)\n                * stochasticity\n                * (\n                    t + dt &lt; 1\n                ).float()  # No remasking if on final step. NOTE should probably use step_argmax or step_sample instead\n            )  # (b, n, S)\n            step_prob = self._regularize_step_probs(step_prob, xt)\n        else:\n            x_1_pred_prob = torch.nn.functional.softmax(x_1_pred_logits / temperature, dim=-1)  # (b, n, S)\n\n            pt_x1_eq_xt_prob = torch.gather(x_1_pred_prob, dim=-1, index=xt.long().unsqueeze(-1))  # (b, n, 1)\n\n            step_prob = (\n                dt * x_1_pred_prob * ((1 + stochasticity + stochasticity * (S - 1) * t) / (1 - t))\n                + dt * pt_x1_eq_xt_prob * stochasticity\n            )\n            step_prob = self._regularize_step_probs(step_prob, xt)\n\n        x_next = torch.multinomial(step_prob.view(-1, S), num_samples=1, generator=self.rng_generator).view(xt.shape)\n        return x_next\n\n    def _regularize_step_probs(self, step_prob: Tensor, xt: Tensor) -&gt; Tensor:\n        \"\"\"Regularize the step probabilities to ensure that the probability of the current state xt is set to the remaining probability mass after clipping and scattering.\n\n        Args:\n            step_prob (Tensor): The input step probabilities with shape (batch, node, class).\n            xt (Tensor): The current state with shape (batch, node).\n\n        Returns:\n            Tensor: The regularized step probabilities with shape (batch, node, class).\n        \"\"\"\n        device = step_prob.device\n        # Clamp the step probabilities to ensure they are within the valid range [0.0, 1.0]\n        step_prob = torch.clamp(step_prob, min=0.0, max=1.0)\n        # Set the probability of the current state xt to 0\n        step_prob.scatter_(\n            dim=-1,\n            index=xt.unsqueeze(-1),\n            src=torch.zeros((*xt.shape, 1), dtype=torch.float, device=device),\n        )\n        # Set the probability of the current state xt to the remaining probability mass\n        step_prob.scatter_(\n            dim=-1,\n            index=xt[..., None],\n            src=1 - torch.sum(step_prob, dim=-1, keepdim=True),\n        )\n        step_prob = torch.clamp(step_prob, min=0.0, max=1.0)\n        # Clamp the step probabilities again to ensure they are within the valid range [0.0, 1.0]\n        return step_prob\n\n    def step_purity(\n        self,\n        logits: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        dt: Tensor | float,\n        temperature: Float = 1.0,\n        stochasticity: Float = 1.0,\n    ) -&gt; Tensor:\n        \"\"\"Perform a single step of purity sampling.\n\n        https://github.com/jasonkyuyim/multiflow/blob/6278899970523bad29953047e7a42b32a41dc813/multiflow/data/interpolant.py#L346\n        Here's a high-level overview of what the function does:\n        TODO: check if the -1e9 and 1e-9 are small enough or using torch.inf would be better\n\n        1. Preprocessing:\n            Checks if dt is a float and converts it to a tensor if necessary.\n            Pads t and dt to match the shape of xt.\n            Checks if the mask_index is valid (i.e., within the range of possible discrete values).\n        2. Masking:\n            Sets the logits corresponding to the mask_index to a low value (-1e9) to effectively mask out those values.\n            Computes the softmax probabilities of the logits.\n            Sets the probability of the mask_index to a small value (1e-9) to avoid numerical issues.\n        3.Purity sampling:\n            Computes the maximum log probabilities of the softmax distribution.\n            Computes the indices of the top-number_to_unmask samples with the highest log probabilities.\n            Uses these indices to sample new values from the original distribution.\n        4. Unmasking and updating:\n            Creates a mask to select the top-number_to_unmask samples.\n            Uses this mask to update the current state xt with the new samples.\n        5. Re-masking:\n            Generates a new mask to randomly re-mask some of the updated samples.\n            Applies this mask to the updated state xt.\n\n        Args:\n            logits (Tensor): The input logits.\n            t (Tensor): The current time step.\n            xt (Tensor): The current state.\n            dt (Tensor): The time step increment.\n            temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n            stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n        Returns:\n            Tensor: The updated state.\n        \"\"\"\n        if logits.ndim &gt; 3:\n            raise ValueError(\"Purity Sampling is only implmented for logits shape batch x sequence x state space.\")\n        if isinstance(dt, float):\n            dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n        x_1_pred_logits = logits\n        B, N, S = x_1_pred_logits.shape\n\n        if not self.use_mask:\n            raise ValueError(\"Purity Sampling only works with a DiscreteMaskPrior\")\n\n        if self.mask_index &gt;= S:\n            raise ValueError(\n                \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n            )\n        x_1_pred_logits[..., self.mask_index] = -1.0e9\n        x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n        x_1_pred_prob[..., self.mask_index] = 1e-9\n        max_logprob = torch.max(torch.log(x_1_pred_prob), dim=-1)[0]  # (b, n)\n        max_logprob = max_logprob - (xt != self.mask_index).float() * 1e9\n        sorted_max_logprobs_idcs = torch.argsort(max_logprob, dim=-1, descending=True)  # (b, n)\n        unmask_probs = (dt * (1 + stochasticity * t) / (1 - t)).clamp(max=1)\n        # For M mask tokens we have p chance to unmask so we try for each one and see how many to do\n        number_to_unmask = torch.binomial(\n            count=torch.count_nonzero(xt == self.mask_index, dim=-1).float(), prob=unmask_probs\n        )\n        unmasked_samples = torch.multinomial(x_1_pred_prob.view(-1, S), num_samples=1).view(xt.shape)\n\n        # Taken from MultiFlow\n        # Vectorized version of:\n        # for b in range(B):\n        #     for d in range(D):\n        #         if d &lt; number_to_unmask[b]:\n        #             aatypes_t[b, d] = unmasked_samples[b, sorted_max_logprobs_idcs[b, d]]\n\n        D_grid = torch.arange(N, device=self.device).view(1, -1).repeat(B, 1)\n        mask1 = (D_grid &lt; number_to_unmask.view(-1, 1)).float()\n        initial_val_max_logprob_idcs = sorted_max_logprobs_idcs[:, 0].view(-1, 1).repeat(1, N)\n        masked_sorted_max_logprobs_idcs = (\n            mask1 * sorted_max_logprobs_idcs + (1 - mask1) * initial_val_max_logprob_idcs\n        ).long()\n        mask2 = torch.zeros((B, N), dtype=torch.long, device=self.device)\n        mask2.scatter_(\n            dim=1,\n            index=masked_sorted_max_logprobs_idcs,\n            src=torch.ones((B, N), dtype=torch.long, device=self.device),\n        )\n        unmask_zero_row = (number_to_unmask == 0).view(-1, 1).repeat(1, N).long()\n        mask2 = mask2 * (1 - unmask_zero_row)\n        x_next = xt * (1 - mask2) + unmasked_samples * mask2\n\n        # re-mask\n        u = torch.rand((B, N), device=self.device, generator=self.rng_generator)\n        dt = pad_like(dt, u)  # type: ignore\n        re_mask_mask = (u &lt; dt * stochasticity).long()\n        x_next = x_next * (1 - re_mask_mask) + self.mask_index * re_mask_mask\n\n        return x_next\n\n    def step_argmax(self, model_out: Tensor):\n        \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n        Args:\n            model_out (Tensor): The output of the model.\n\n        \"\"\"\n        if self.use_mask:\n            model_out[..., self.mask_index] = -1.0e9\n        return model_out.argmax(dim=-1)\n\n    def step_simple_sample(self, model_out: Tensor, temperature: float = 1.0, num_samples: int = 1):\n        \"\"\"Samples from the model output logits. Leads to more diversity than step_argmax.\n\n        Args:\n            model_out (Tensor): The output of the model.\n            temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n            num_samples (int): Number of samples to return\n\n        \"\"\"\n        if self.use_mask:\n            model_out[..., self.mask_index] = -1.0e9\n        samples = torch.multinomial(\n            torch.nn.functional.softmax(model_out / temperature, dim=-1).view(-1, self.num_classes),\n            num_samples=num_samples,\n            generator=self.rng_generator,\n        )  # batch * seq_len x num_samples\n        if num_samples == 1:\n            samples = samples.view(*model_out.shape[:-1])\n            # batch x seq_len\n        else:\n            samples = samples.view((*model_out.shape[:-1], num_samples))\n            # batch x seq_len x num_samples\n        return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.__init__","title":"<code>__init__(time_distribution, prior_distribution, device='cpu', eps=1e-05, rng_generator=None)</code>","text":"<p>Initialize the DFM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The time distribution for the diffusion process.</p> required <code>prior_distribution</code> <code>DiscretePriorDistribution</code> <p>The prior distribution for the discrete masked tokens.</p> required <code>device</code> <code>str</code> <p>The device to use for computations. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>eps</code> <code>Float</code> <p>small Float to prevent dividing by zero.</p> <code>1e-05</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: DiscretePriorDistribution,\n    device: str = \"cpu\",\n    eps: Float = 1e-5,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initialize the DFM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The time distribution for the diffusion process.\n        prior_distribution (DiscretePriorDistribution): The prior distribution for the discrete masked tokens.\n        device (str, optional): The device to use for computations. Defaults to \"cpu\".\n        eps: small Float to prevent dividing by zero.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    self.num_classes = prior_distribution.num_classes\n    self.eps = eps\n    self.use_mask = isinstance(self.prior_distribution, DiscreteMaskedPrior)\n    if self.use_mask:\n        self.mask_index = prior_distribution.mask_dim  # type: ignore\n    self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher._regularize_step_probs","title":"<code>_regularize_step_probs(step_prob, xt)</code>","text":"<p>Regularize the step probabilities to ensure that the probability of the current state xt is set to the remaining probability mass after clipping and scattering.</p> <p>Parameters:</p> Name Type Description Default <code>step_prob</code> <code>Tensor</code> <p>The input step probabilities with shape (batch, node, class).</p> required <code>xt</code> <code>Tensor</code> <p>The current state with shape (batch, node).</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The regularized step probabilities with shape (batch, node, class).</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def _regularize_step_probs(self, step_prob: Tensor, xt: Tensor) -&gt; Tensor:\n    \"\"\"Regularize the step probabilities to ensure that the probability of the current state xt is set to the remaining probability mass after clipping and scattering.\n\n    Args:\n        step_prob (Tensor): The input step probabilities with shape (batch, node, class).\n        xt (Tensor): The current state with shape (batch, node).\n\n    Returns:\n        Tensor: The regularized step probabilities with shape (batch, node, class).\n    \"\"\"\n    device = step_prob.device\n    # Clamp the step probabilities to ensure they are within the valid range [0.0, 1.0]\n    step_prob = torch.clamp(step_prob, min=0.0, max=1.0)\n    # Set the probability of the current state xt to 0\n    step_prob.scatter_(\n        dim=-1,\n        index=xt.unsqueeze(-1),\n        src=torch.zeros((*xt.shape, 1), dtype=torch.float, device=device),\n    )\n    # Set the probability of the current state xt to the remaining probability mass\n    step_prob.scatter_(\n        dim=-1,\n        index=xt[..., None],\n        src=1 - torch.sum(step_prob, dim=-1, keepdim=True),\n    )\n    step_prob = torch.clamp(step_prob, min=0.0, max=1.0)\n    # Clamp the step probabilities again to ensure they are within the valid range [0.0, 1.0]\n    return step_prob\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>tensor noise ids</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n        noise: tensor noise ids\n    \"\"\"\n    if data.dtype == torch.float and data.ndim &gt; 2:\n        x1 = data.argmax(-1)\n    else:\n        x1 = data\n    x0 = noise\n    t = pad_like(t, x1)\n    threshold = torch.rand_like(x1.float())\n    xt = torch.where((threshold &lt; 1 - t), x0, x1)\n    return xt\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.loss","title":"<code>loss(logits, target, time=None, mask=None, use_weight=False)</code>","text":"<p>Calculate the cross-entropy loss between the model prediction and the target output.</p> <p>The loss is calculated between the batch x node x class logits and the target batch x node. If using a masked prior please pass in the correct mask to calculate loss values on only masked states. i.e. mask = data_mask * is_masked_state which is calculated with self.prior_dist.is_masked(xt))</p> <p>If <code>use_weight</code> is True, the loss is weighted by 1/(1-t) defined in equation 24 in Appndix C. of https://arxiv.org/pdf/2402.04997</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted output from the model, with shape batch x node x class.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction, with shape batch x node.</p> required <code>time</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>use_weight</code> <code>bool</code> <p>Whether to use the DFM time weight for the loss. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def loss(\n    self,\n    logits: Tensor,\n    target: Tensor,\n    time: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    use_weight: Bool = False,\n):\n    \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n    The loss is calculated between the batch x node x class logits and the target batch x node.\n    If using a masked prior please pass in the correct mask to calculate loss values on only masked states.\n    i.e. mask = data_mask * is_masked_state which is calculated with self.prior_dist.is_masked(xt))\n\n    If `use_weight` is True, the loss is weighted by 1/(1-t) defined in equation 24 in Appndix C. of https://arxiv.org/pdf/2402.04997\n\n    Args:\n        logits (Tensor): The predicted output from the model, with shape batch x node x class.\n        target (Tensor): The target output for the model prediction, with shape batch x node.\n        time (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        use_weight (bool, optional): Whether to use the DFM time weight for the loss. Defaults to True.\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    assert target.ndim + 1 == logits.ndim\n    loss = self._loss_function(logits.transpose(-1, 1), target.long())\n    if mask is not None:\n        loss = loss * mask\n        num_non_masked_elements = torch.sum(mask, dim=-1)\n        num_non_masked_elements[num_non_masked_elements == 0] = (\n            1.0  #! prevents divide by zero since if the row is all zero the sum of loss = 0\n        )\n        loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n    else:\n        loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n    if use_weight:\n        if time is None:\n            raise ValueError(\"Time is required to compute the DFM liklehood weighting of 1/(1-t + self.eps)\")\n        loss = loss * 1 / (1 - time + self.eps)\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step","title":"<code>step(logits, t, xt, dt, temperature=1.0, stochasticity=1.0)</code>","text":"<p>Perform a single step of DFM euler updates.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The input logits.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current state.</p> required <code>dt</code> <code>Tensor | float</code> <p>The time step increment.</p> required <code>temperature</code> <code>Float</code> <p>The temperature for the softmax calculation. Defaults to 1.0.</p> <code>1.0</code> <code>stochasticity</code> <code>Float</code> <p>The stochasticity value for the step calculation. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The updated state.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step(\n    self,\n    logits: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor | float,\n    temperature: Float = 1.0,\n    stochasticity: Float = 1.0,\n) -&gt; Tensor:\n    \"\"\"Perform a single step of DFM euler updates.\n\n    Args:\n        logits (Tensor): The input logits.\n        t (Tensor): The current time step.\n        xt (Tensor): The current state.\n        dt (Tensor | float): The time step increment.\n        temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n        stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n    Returns:\n        Tensor: The updated state.\n    \"\"\"\n    x_1_pred_logits = logits\n    S = x_1_pred_logits.shape[-1]\n    t = pad_like(t, logits)\n    if isinstance(dt, float):\n        dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n    dt = pad_like(dt, logits)  # type: ignore\n\n    if self.use_mask:\n        if self.mask_index &gt;= S:\n            raise ValueError(\n                \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n            )\n\n        mask_one_hot = torch.zeros((S,), device=self.device)\n        mask_one_hot[self.mask_index] = 1.0\n        x_1_pred_logits[..., self.mask_index] = -1.0e9\n\n        x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n\n        xt_is_mask = (xt == self.mask_index).unsqueeze(-1).float()  # b x n x 1\n        step_prob = (\n            dt * x_1_pred_prob * ((1 + stochasticity * t) / (1 - t)) * xt_is_mask\n            + dt\n            * (1 - xt_is_mask)\n            * mask_one_hot.view(1, 1, -1)\n            * stochasticity\n            * (\n                t + dt &lt; 1\n            ).float()  # No remasking if on final step. NOTE should probably use step_argmax or step_sample instead\n        )  # (b, n, S)\n        step_prob = self._regularize_step_probs(step_prob, xt)\n    else:\n        x_1_pred_prob = torch.nn.functional.softmax(x_1_pred_logits / temperature, dim=-1)  # (b, n, S)\n\n        pt_x1_eq_xt_prob = torch.gather(x_1_pred_prob, dim=-1, index=xt.long().unsqueeze(-1))  # (b, n, 1)\n\n        step_prob = (\n            dt * x_1_pred_prob * ((1 + stochasticity + stochasticity * (S - 1) * t) / (1 - t))\n            + dt * pt_x1_eq_xt_prob * stochasticity\n        )\n        step_prob = self._regularize_step_probs(step_prob, xt)\n\n    x_next = torch.multinomial(step_prob.view(-1, S), num_samples=1, generator=self.rng_generator).view(xt.shape)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step_argmax","title":"<code>step_argmax(model_out)</code>","text":"<p>Returns the index of the maximum value in the last dimension of the model output.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step_argmax(self, model_out: Tensor):\n    \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n    Args:\n        model_out (Tensor): The output of the model.\n\n    \"\"\"\n    if self.use_mask:\n        model_out[..., self.mask_index] = -1.0e9\n    return model_out.argmax(dim=-1)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step_purity","title":"<code>step_purity(logits, t, xt, dt, temperature=1.0, stochasticity=1.0)</code>","text":"<p>Perform a single step of purity sampling.</p> <p>https://github.com/jasonkyuyim/multiflow/blob/6278899970523bad29953047e7a42b32a41dc813/multiflow/data/interpolant.py#L346 Here's a high-level overview of what the function does: TODO: check if the -1e9 and 1e-9 are small enough or using torch.inf would be better</p> <ol> <li>Preprocessing:     Checks if dt is a float and converts it to a tensor if necessary.     Pads t and dt to match the shape of xt.     Checks if the mask_index is valid (i.e., within the range of possible discrete values).</li> <li>Masking:     Sets the logits corresponding to the mask_index to a low value (-1e9) to effectively mask out those values.     Computes the softmax probabilities of the logits.     Sets the probability of the mask_index to a small value (1e-9) to avoid numerical issues. 3.Purity sampling:     Computes the maximum log probabilities of the softmax distribution.     Computes the indices of the top-number_to_unmask samples with the highest log probabilities.     Uses these indices to sample new values from the original distribution.</li> <li>Unmasking and updating:     Creates a mask to select the top-number_to_unmask samples.     Uses this mask to update the current state xt with the new samples.</li> <li>Re-masking:     Generates a new mask to randomly re-mask some of the updated samples.     Applies this mask to the updated state xt.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The input logits.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current state.</p> required <code>dt</code> <code>Tensor</code> <p>The time step increment.</p> required <code>temperature</code> <code>Float</code> <p>The temperature for the softmax calculation. Defaults to 1.0.</p> <code>1.0</code> <code>stochasticity</code> <code>Float</code> <p>The stochasticity value for the step calculation. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The updated state.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step_purity(\n    self,\n    logits: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    dt: Tensor | float,\n    temperature: Float = 1.0,\n    stochasticity: Float = 1.0,\n) -&gt; Tensor:\n    \"\"\"Perform a single step of purity sampling.\n\n    https://github.com/jasonkyuyim/multiflow/blob/6278899970523bad29953047e7a42b32a41dc813/multiflow/data/interpolant.py#L346\n    Here's a high-level overview of what the function does:\n    TODO: check if the -1e9 and 1e-9 are small enough or using torch.inf would be better\n\n    1. Preprocessing:\n        Checks if dt is a float and converts it to a tensor if necessary.\n        Pads t and dt to match the shape of xt.\n        Checks if the mask_index is valid (i.e., within the range of possible discrete values).\n    2. Masking:\n        Sets the logits corresponding to the mask_index to a low value (-1e9) to effectively mask out those values.\n        Computes the softmax probabilities of the logits.\n        Sets the probability of the mask_index to a small value (1e-9) to avoid numerical issues.\n    3.Purity sampling:\n        Computes the maximum log probabilities of the softmax distribution.\n        Computes the indices of the top-number_to_unmask samples with the highest log probabilities.\n        Uses these indices to sample new values from the original distribution.\n    4. Unmasking and updating:\n        Creates a mask to select the top-number_to_unmask samples.\n        Uses this mask to update the current state xt with the new samples.\n    5. Re-masking:\n        Generates a new mask to randomly re-mask some of the updated samples.\n        Applies this mask to the updated state xt.\n\n    Args:\n        logits (Tensor): The input logits.\n        t (Tensor): The current time step.\n        xt (Tensor): The current state.\n        dt (Tensor): The time step increment.\n        temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n        stochasticity (Float, optional): The stochasticity value for the step calculation. Defaults to 1.0.\n\n    Returns:\n        Tensor: The updated state.\n    \"\"\"\n    if logits.ndim &gt; 3:\n        raise ValueError(\"Purity Sampling is only implmented for logits shape batch x sequence x state space.\")\n    if isinstance(dt, float):\n        dt = torch.Tensor([dt] * t.shape[0]).to(self.device)\n    x_1_pred_logits = logits\n    B, N, S = x_1_pred_logits.shape\n\n    if not self.use_mask:\n        raise ValueError(\"Purity Sampling only works with a DiscreteMaskPrior\")\n\n    if self.mask_index &gt;= S:\n        raise ValueError(\n            \"If using a non inclusive DiscreteMaskedPrior initialization, please pad the logits input with DiscreteMaskedPrior.pad_sample(logits)\"\n        )\n    x_1_pred_logits[..., self.mask_index] = -1.0e9\n    x_1_pred_prob = F.softmax(x_1_pred_logits / temperature, dim=-1)\n    x_1_pred_prob[..., self.mask_index] = 1e-9\n    max_logprob = torch.max(torch.log(x_1_pred_prob), dim=-1)[0]  # (b, n)\n    max_logprob = max_logprob - (xt != self.mask_index).float() * 1e9\n    sorted_max_logprobs_idcs = torch.argsort(max_logprob, dim=-1, descending=True)  # (b, n)\n    unmask_probs = (dt * (1 + stochasticity * t) / (1 - t)).clamp(max=1)\n    # For M mask tokens we have p chance to unmask so we try for each one and see how many to do\n    number_to_unmask = torch.binomial(\n        count=torch.count_nonzero(xt == self.mask_index, dim=-1).float(), prob=unmask_probs\n    )\n    unmasked_samples = torch.multinomial(x_1_pred_prob.view(-1, S), num_samples=1).view(xt.shape)\n\n    # Taken from MultiFlow\n    # Vectorized version of:\n    # for b in range(B):\n    #     for d in range(D):\n    #         if d &lt; number_to_unmask[b]:\n    #             aatypes_t[b, d] = unmasked_samples[b, sorted_max_logprobs_idcs[b, d]]\n\n    D_grid = torch.arange(N, device=self.device).view(1, -1).repeat(B, 1)\n    mask1 = (D_grid &lt; number_to_unmask.view(-1, 1)).float()\n    initial_val_max_logprob_idcs = sorted_max_logprobs_idcs[:, 0].view(-1, 1).repeat(1, N)\n    masked_sorted_max_logprobs_idcs = (\n        mask1 * sorted_max_logprobs_idcs + (1 - mask1) * initial_val_max_logprob_idcs\n    ).long()\n    mask2 = torch.zeros((B, N), dtype=torch.long, device=self.device)\n    mask2.scatter_(\n        dim=1,\n        index=masked_sorted_max_logprobs_idcs,\n        src=torch.ones((B, N), dtype=torch.long, device=self.device),\n    )\n    unmask_zero_row = (number_to_unmask == 0).view(-1, 1).repeat(1, N).long()\n    mask2 = mask2 * (1 - unmask_zero_row)\n    x_next = xt * (1 - mask2) + unmasked_samples * mask2\n\n    # re-mask\n    u = torch.rand((B, N), device=self.device, generator=self.rng_generator)\n    dt = pad_like(dt, u)  # type: ignore\n    re_mask_mask = (u &lt; dt * stochasticity).long()\n    x_next = x_next * (1 - re_mask_mask) + self.mask_index * re_mask_mask\n\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching/#bionemo.moco.interpolants.continuous_time.discrete.discrete_flow_matching.DiscreteFlowMatcher.step_simple_sample","title":"<code>step_simple_sample(model_out, temperature=1.0, num_samples=1)</code>","text":"<p>Samples from the model output logits. Leads to more diversity than step_argmax.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <code>temperature</code> <code>Float</code> <p>The temperature for the softmax calculation. Defaults to 1.0.</p> <code>1.0</code> <code>num_samples</code> <code>int</code> <p>Number of samples to return</p> <code>1</code> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/discrete_flow_matching.py</code> <pre><code>def step_simple_sample(self, model_out: Tensor, temperature: float = 1.0, num_samples: int = 1):\n    \"\"\"Samples from the model output logits. Leads to more diversity than step_argmax.\n\n    Args:\n        model_out (Tensor): The output of the model.\n        temperature (Float, optional): The temperature for the softmax calculation. Defaults to 1.0.\n        num_samples (int): Number of samples to return\n\n    \"\"\"\n    if self.use_mask:\n        model_out[..., self.mask_index] = -1.0e9\n    samples = torch.multinomial(\n        torch.nn.functional.softmax(model_out / temperature, dim=-1).view(-1, self.num_classes),\n        num_samples=num_samples,\n        generator=self.rng_generator,\n    )  # batch * seq_len x num_samples\n    if num_samples == 1:\n        samples = samples.view(*model_out.shape[:-1])\n        # batch x seq_len\n    else:\n        samples = samples.view((*model_out.shape[:-1], num_samples))\n        # batch x seq_len x num_samples\n    return samples\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/","title":"Mdlm","text":""},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM","title":"<code>MDLM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Masked discrete Diffusion Language Model (MDLM) interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.discrete.mask import DiscreteMaskedPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.discrete.mdlm import MDLM\n&gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearTimeSchedule\n\n\nmdlm = MDLM(\n    time_distribution = UniformTimeDistribution(discrete_time = False,...),\n    prior_distribution = DiscreteMaskedPrior(...),\n    noise_schedule = CosineExpNoiseTransform(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = mdlm.sample_time(batch_size)\n    xt = mdlm.interpolate(data, time)\n\n    logits = model(xt, time)\n    loss = mdlm.loss(logits, data, xt, time)\n    loss.backward()\n\n# Generation\nx_pred = mdlm.sample_prior(data.shape)\nschedule = LinearTimeSchedule(...)\ninference_time = schedule.generate_schedule()\ndts = schedue.discreteize()\nfor t, dt in zip(inference_time, dts):\n    time = torch.full((batch_size,), t)\n    logits = model(x_pred, time)\n    x_pred = mdlm.step(logits, time, x_pred, dt)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>class MDLM(Interpolant):\n    \"\"\"A Masked discrete Diffusion Language Model (MDLM) interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.discrete.mask import DiscreteMaskedPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.continuous_time.discrete.mdlm import MDLM\n    &gt;&gt;&gt; from bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import LinearTimeSchedule\n\n\n    mdlm = MDLM(\n        time_distribution = UniformTimeDistribution(discrete_time = False,...),\n        prior_distribution = DiscreteMaskedPrior(...),\n        noise_schedule = CosineExpNoiseTransform(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = mdlm.sample_time(batch_size)\n        xt = mdlm.interpolate(data, time)\n\n        logits = model(xt, time)\n        loss = mdlm.loss(logits, data, xt, time)\n        loss.backward()\n\n    # Generation\n    x_pred = mdlm.sample_prior(data.shape)\n    schedule = LinearTimeSchedule(...)\n    inference_time = schedule.generate_schedule()\n    dts = schedue.discreteize()\n    for t, dt in zip(inference_time, dts):\n        time = torch.full((batch_size,), t)\n        logits = model(x_pred, time)\n        x_pred = mdlm.step(logits, time, x_pred, dt)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: DiscreteMaskedPrior,\n        noise_schedule: ContinuousExpNoiseTransform,\n        device: str = \"cpu\",\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initialize the Masked Discrete Language Model (MDLM) interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution governing the time variable in the diffusion process.\n            prior_distribution (DiscreteMaskedPrior): The prior distribution over the discrete token space, including masked tokens.\n            noise_schedule (ContinuousExpNoiseTransform): The noise schedule defining the noise intensity as a function of time.\n            device (str, optional): The device to use for computations. Defaults to \"cpu\".\n            rng_generator (Optional[torch.Generator], optional): The random number generator for reproducibility. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        if not isinstance(prior_distribution, DiscreteMaskedPrior):\n            raise ValueError(\"DiscreteMaskedPrior required for MDLM\")\n        if not isinstance(noise_schedule, ContinuousExpNoiseTransform):\n            raise ValueError(\"ContinuousExpNoiseTransform required for MDLM\")\n        self.noise_schedule = noise_schedule\n        self.num_classes = prior_distribution.num_classes\n        self.mask_index = prior_distribution.mask_dim\n        # Gumbel used for confidence sampling. Note rng_generator not compatible with torch.Distribution.\n        # self.gumbel_dist = torch.distributions.Gumbel(torch.tensor(0.0), torch.tensor(1.0))\n\n    def interpolate(self, data: Tensor, t: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n        \"\"\"\n        if data.dtype == torch.float and data.ndim &gt; 2:\n            x0 = data.argmax(-1)\n        else:\n            x0 = data\n        sigma = self.noise_schedule.calculate_sigma(t, data.device)\n        alpha = self.noise_schedule.sigma_to_alpha(sigma)\n        p_mask = 1 - alpha\n        p_mask = pad_like(p_mask, x0)\n        mask_indices = torch.rand(*x0.shape, device=x0.device, generator=self.rng_generator) &lt; p_mask\n        xt = torch.where(mask_indices, self.mask_index, x0)\n        return xt\n\n    def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n        \"\"\"Apply the forward process to the data at time t.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n\n        Returns:\n            Tensor: x(t) after applying the forward process\n        \"\"\"\n        return self.interpolate(data, t)\n\n    def loss(\n        self,\n        logits: Tensor,\n        target: Tensor,\n        xt: Tensor,\n        time: Tensor,\n        mask: Optional[Tensor] = None,\n        use_weight=True,\n    ):\n        \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n        The loss is calculated between the batch x node x class logits and the target batch x node,\n        considering the current state of the discrete sequence `xt` at time `time`.\n\n        If `use_weight` is True, the loss is weighted by the reduced form of the MDLM time weight for continuous NELBO,\n        as specified in equation 11 of https://arxiv.org/pdf/2406.07524. This weight is proportional to the derivative\n        of the noise schedule with respect to time, and is used to emphasize the importance of accurate predictions at\n        certain times in the diffusion process.\n\n        Args:\n            logits (Tensor): The predicted output from the model, with shape batch x node x class.\n            target (Tensor): The target output for the model prediction, with shape batch x node.\n            xt (Tensor): The current state of the discrete sequence, with shape batch x node.\n            time (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            use_weight (bool, optional): Whether to use the MDLM time weight for the loss. Defaults to True.\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        logprobs = self._subs_parameterization(logits, xt)\n        log_p_theta = torch.gather(input=logprobs, dim=-1, index=target[..., None]).squeeze(-1)\n\n        sigma = self.noise_schedule.calculate_sigma(time, target.device)\n        dsigma = self.noise_schedule.d_dt_sigma(time, target.device)  # type: ignore\n        loss = -log_p_theta\n        if use_weight:\n            loss = loss * (dsigma / torch.expm1(sigma))[:, None]\n\n        if mask is not None:\n            loss = loss * mask\n            num_non_masked_elements = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n        else:\n            loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n        return loss\n\n    def _subs_parameterization(self, logits: Tensor, xt: Tensor) -&gt; Tensor:\n        \"\"\"Apply subsititution parameterization to the logits.\n\n        This function enforces that the model can never predict a mask token by lowering the mask logits.\n        Then, for all unmasked tokens, it copies over from xt to enable carry over unmasked.\n        Once a token is unmasked, it stays the same.\n        See Sec. 3.2.3 https://arxiv.org/pdf/2406.07524.\n\n        Note that recent work has shown that allowing the model to rethink\n        carry over unmasking is beneficial https://arxiv.org/abs/2410.06264.\n\n        Args:\n            logits (Tensor): The logits tensor with shape batch x node x class.\n            xt (Tensor): The tensor of unmasked tokens with shape batch x node.\n\n        Returns:\n            Tensor: The modified logits tensor with substitution parameterization applied.\n        \"\"\"\n        logits[..., self.mask_index] += -1000000.0  # clean input is never masked\n        logprobs = logits - torch.logsumexp(logits, dim=-1, keepdim=True)  # normalize\n        unmasked_indices = xt != self.mask_index\n        logprobs[unmasked_indices] = -1000000.0\n        logprobs[unmasked_indices, xt[unmasked_indices]] = 0  # Unmasked token remains unchanged\n        return logprobs\n\n    def step(self, logits: Tensor, t: Tensor, xt: Tensor, dt: Tensor, temperature: float = 1.0) -&gt; Tensor:\n        \"\"\"Perform a single step of MDLM DDPM step.\n\n        Parameters:\n        logits (Tensor): The input logits.\n        t (Tensor): The current time step.\n        xt (Tensor): The current state.\n        dt (Tensor): The time step increment.\n        temperature (float): Softmax temperature defaults to 1.0.\n\n        Returns:\n        Tensor: The updated state.\n        \"\"\"\n        sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n        sigma_s = self.noise_schedule.calculate_sigma(t - dt, logits.device)\n        alpha_t = torch.exp(-sigma_t)\n        alpha_s = torch.exp(-sigma_s)\n        p_mask_s = 1 - alpha_s\n        alpha_t = pad_like(alpha_t, logits)\n        alpha_s = pad_like(alpha_s, logits)\n        p_mask_s = pad_like(p_mask_s, logits)\n        # Apply subs parameterization\n        log_p_x0 = self._subs_parameterization(logits, xt) / temperature\n        if p_mask_s.ndim != log_p_x0.ndim:\n            raise ValueError(f\"Dimension Mistmatch {p_mask_s.shape} {log_p_x0.shape}\")\n        # Equation 7 from MDLM\n        prob_s_given_t = log_p_x0.exp() * (\n            alpha_s - alpha_t\n        )  # righthand side (alpha_s - alpha_t)*x = (1 - alpha_t - (1 - alpha_s)) * x\n        prob_s_given_t[..., self.mask_index] = p_mask_s[..., 0]  # lefthand side (1 - alpha_s)*M\n        sampled_x = self._sample_categorical(prob_s_given_t)\n        carry_over_unmask = (xt != self.mask_index).to(xt.dtype)\n        return carry_over_unmask * xt + (1 - carry_over_unmask) * sampled_x\n\n    def _sample_categorical(self, categorical_probs: Tensor) -&gt; Tensor:\n        \"\"\"Sample from a categorical distribution using the Gumbel trick.\n\n        Args:\n            categorical_probs (Tensor): The probabilities of each category, shape batch x node x class.\n\n        Returns:\n            Tensor: The sampled category indices, shape batch x node.\n        \"\"\"\n        gumbel_norm = (\n            1e-10\n            - (\n                torch.rand(*categorical_probs.shape, device=categorical_probs.device, generator=self.rng_generator)\n                + 1e-10\n            ).log()\n        )\n        scaled_proability = categorical_probs / gumbel_norm\n        return scaled_proability.argmax(dim=-1)\n\n    def get_num_steps_confidence(self, xt: Tensor, num_tokens_unmask: int = 1):\n        \"\"\"Calculate the maximum number of steps with confidence.\n\n        This method computes the maximum count of occurrences where the input tensor `xt` matches the `mask_index`\n        along the last dimension (-1). The result is returned as a single float value.\n\n        Args:\n            xt (Tensor): Input tensor to evaluate against the mask index.\n            num_tokens_unmask (int): number of tokens to unamsk at each step.\n\n        Returns:\n            float: The maximum number of steps with confidence (i.e., matching the mask index).\n        \"\"\"\n        nsteps = (xt == self.mask_index).sum(-1).max().item()\n        if num_tokens_unmask == 1:\n            return int(nsteps)\n        else:\n            return int(max(math.ceil(nsteps // num_tokens_unmask), 1))\n\n    def step_confidence(\n        self,\n        logits: Tensor,\n        xt: Tensor,\n        curr_step: int,\n        num_steps: int,\n        logit_temperature: float = 1.0,\n        randomness: float = 1.0,\n        confidence_temperature: float = 1.0,\n        num_tokens_unmask: int = 1,\n    ) -&gt; Tensor:\n        \"\"\"Update the input sequence xt by sampling from the predicted logits and adding Gumbel noise.\n\n        Method taken from GenMol Lee et al. https://arxiv.org/abs/2501.06158\n\n        Args:\n            logits: Predicted logits\n            xt: Input sequence\n            curr_step: Current step\n            num_steps: Total number of steps\n            logit_temperature: Temperature for softmax over logits\n            randomness: Scale for Gumbel noise\n            confidence_temperature: Temperature for Gumbel confidence\n            num_tokens_unmask: number of tokens to unmask each step\n\n        Returns:\n            Updated input sequence xt unmasking num_tokens_unmask token each step.\n        \"\"\"\n        if xt.ndim &gt; 3:\n            raise NotImplementedError(\n                \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n            )\n        if curr_step &lt; 0 or num_steps &lt; 1 or num_tokens_unmask &lt; 1:\n            raise ValueError(\"Invalid input values for curr_step, num_steps, or num_tokens_unmask.\")\n        xt = xt.clone()\n        log_p_x0 = self._subs_parameterization(logits, xt)\n        # sample the code from the softmax prediction\n        probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n        preds = torch.distributions.Categorical(probs=probs).sample()\n\n        confidence = probs.gather(-1, preds.unsqueeze(-1)).squeeze(-1)\n        # add Gumbel noise decreasing over the sampling process\n        ratio = curr_step / (num_steps - 1)\n        # Using manual definition of 0,1 Gumbel to pass in generator, manually specifying the device is faster than transfer\n        gumbel_sample = -torch.log(\n            -torch.log(torch.rand(xt.shape, device=logits.device, generator=self.rng_generator))\n        )\n        # gumbel_sample = self.gumbel_dist.sample(xt.shape).to(logits.device)\n        gumbel_noise = gumbel_sample * randomness * (1 - ratio)  # type: ignore\n        confidence = (\n            (torch.log(confidence) + gumbel_noise) / confidence_temperature\n        )  # stems from tau of https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n\n        # do not predict on already predicted tokens\n        mask = xt == self.mask_index\n        confidence[~mask] = -torch.inf\n\n        # choose the predicted token with the highest confidence\n        confidence_threshold, idx_mask = torch.topk(confidence, k=num_tokens_unmask, dim=-1)\n        confidence_threshold = confidence_threshold[:, -1].unsqueeze(-1)\n\n        # replace the chosen tokens\n        to_replace = confidence &gt;= confidence_threshold\n        to_replace = (mask.float() * to_replace.float()).bool()\n        xt[to_replace] = preds[to_replace]\n        return xt\n\n    def step_argmax(self, model_out: Tensor):\n        \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n        Args:\n            model_out (Tensor): The output of the model.\n\n        Returns:\n            Tensor: The index of the maximum value in the last dimension of the model output.\n        \"\"\"\n        return model_out.argmax(dim=-1)\n\n    def calculate_score(self, logits, x, t):\n        \"\"\"Returns score of the given sample x at time t with the corresponding model output logits.\n\n        Args:\n            logits (Tensor): The output of the model.\n            x (Tensor): The current data point.\n            t (Tensor): The current time.\n\n        Returns:\n            Tensor: The score defined in Appendix C.3 Equation 76 of MDLM.\n        \"\"\"\n        sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n        log_ratio = -torch.log(\n            torch.expm1(sigma_t)\n        )  # log ( exp(-sigma) / (1 - exp(-sigma))) = log(1/ (exp(sigma) - 1))\n\n        # Create masked and unmasked log scores\n        masked_log_score = logits + pad_like(log_ratio, logits)  # xt is masked and prediction is not\n        masked_log_score[..., self.mask_index] = 0  # xt and prediction are mask\n\n        unmasked_log_score = torch.full_like(logits, -1000000.0)\n        unmasked_log_score.scatter_(-1, x[..., None], 0)  # place zeros where current predictions are\n        unmasked_log_score[..., self.mask_index] = -pad_like(log_ratio, logits[..., 0])\n\n        # Combine masked and unmasked log scores\n        masked_indices = (x == self.mask_index).to(logits.dtype)[..., None]\n        log_score = masked_log_score * masked_indices + unmasked_log_score * (1 - masked_indices)\n\n        return log_score.exp()\n\n    def step_self_path_planning(\n        self,\n        logits: Tensor,\n        xt: Tensor,\n        t: Tensor,\n        curr_step: int,\n        num_steps: int,\n        logit_temperature: float = 1.0,\n        randomness: float = 1.0,\n        confidence_temperature: float = 1.0,\n        score_type: Literal[\"confidence\", \"random\"] = \"confidence\",\n        fix_mask: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Self Path Planning (P2) Sampling from Peng et al. https://arxiv.org/html/2502.03540v1.\n\n        Args:\n            logits (Tensor): Predicted logits for sampling.\n            xt (Tensor): Input sequence to be updated.\n            t (Tensor): Time tensor (e.g., time steps or temporal info).\n            curr_step (int): Current iteration in the planning process.\n            num_steps (int): Total number of planning steps.\n            logit_temperature (float): Temperature for logits (default: 1.0).\n            randomness (float): Introduced randomness level (default: 1.0).\n            confidence_temperature (float): Temperature for confidence scoring (default: 1.0).\n            score_type (Literal[\"confidence\", \"random\"]): Sampling score type (default: \"confidence\").\n            fix_mask (Optional[Tensor]): inital mask where True when not a mask tokens (default: None).\n\n        Returns:\n            Tensor: Updated input sequence xt after iterative unmasking.\n        \"\"\"\n        if xt.ndim &gt; 3:\n            raise NotImplementedError(\n                \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n            )\n        if curr_step &lt; 0 or num_steps &lt; 1:\n            raise ValueError(\"Invalid input values for curr_step, num_steps.\")\n        xt = xt.clone()\n        if fix_mask is None:\n            fix_mask = torch.zeros_like(xt).bool()  #! if any sequenes are fixed from the start of trajectory\n        last_mask = xt == self.mask_index\n        unmask_candidates = (\n            last_mask &amp; ~fix_mask\n        )  #! I want to consider tokens to un mask that are currently masked and not fixed. This fixes a typo in pseudo code\n        x1_pred, logp = self.stochastic_sample_from_categorical(\n            logits, temperature=logit_temperature, noise_scale=confidence_temperature\n        )\n        if curr_step == num_steps - 1:\n            xt[last_mask] = x1_pred[last_mask]\n        else:\n            if score_type == \"confidence\":\n                score = logp\n            elif score_type == \"random\":\n                score = torch.rand_like(logp).log()\n\n            score = score.masked_fill(fix_mask.squeeze(-1), float(\"inf\"))\n            score[unmask_candidates.squeeze(-1)] *= randomness\n            num_to_mask = torch.clamp(\n                ((~fix_mask).sum(dim=1, keepdim=True).float() * t.unsqueeze(-1)).long(), max=xt.shape[-1] - 1\n            )  #! here is is t since diffusion time is 1 to 0. Clamp is to set it to 0 N-1 since topk uses it as indices\n            mask = self.topk_lowest_masking(score, num_to_mask)\n            xt[mask] = self.mask_index\n            mask_to_x1 = last_mask &amp; ~mask\n            xt[mask_to_x1] = x1_pred[mask_to_x1]\n        return xt\n\n    def topk_lowest_masking(self, scores: Tensor, cutoff_len: Tensor):\n        \"\"\"Generates a mask for the lowest scoring elements up to a specified cutoff length.\n\n        Args:\n            scores (Tensor): Input scores tensor with shape (... , num_elements)\n            cutoff_len (Tensor): Number of lowest-scoring elements to mask (per batch element)\n\n        Returns:\n            Tensor: Boolean mask tensor with same shape as `scores`, where `True` indicates\n                    the corresponding element is among the `cutoff_len` lowest scores.\n\n        Example:\n            &gt;&gt;&gt; scores = torch.tensor([[0.9, 0.8, 0.1, 0.05], [0.7, 0.4, 0.3, 0.2]])\n            &gt;&gt;&gt; cutoff_len = 2\n            &gt;&gt;&gt; mask = topk_lowest_masking(scores, cutoff_len)\n            &gt;&gt;&gt; print(mask)\n            tensor([[False, False, True, True],\n                    [False, True, True, False]])\n        \"\"\"\n        sorted_scores, _ = scores.sort(dim=-1)\n        threshold = sorted_scores.gather(dim=-1, index=cutoff_len)\n        return scores &lt; threshold\n\n    def stochastic_sample_from_categorical(self, logits: Tensor, temperature: float = 1.0, noise_scale: float = 1.0):\n        \"\"\"Stochastically samples from a categorical distribution defined by input logits, with optional temperature and noise scaling for diverse sampling.\n\n        Args:\n            logits (Tensor): Input logits tensor with shape (... , num_categories)\n            temperature (float, optional): Softmax temperature. Higher values produce more uniform samples. Defaults to 1.0.\n            noise_scale (float, optional): Scale for Gumbel noise. Higher values produce more diverse samples. Defaults to 1.0.\n\n        Returns:\n            tuple:\n                - **tokens** (LongTensor): Sampling result (category indices) with shape (... , )\n                - **scores** (Tensor): Corresponding log-softmax scores for the sampled tokens, with shape (... , )\n        \"\"\"\n        if temperature &gt; 0:\n            gumbel = -torch.log(\n                -torch.log(torch.rand(logits.shape, device=logits.device, generator=self.rng_generator) + 1e-8) + 1e-8\n            )  #! avoid device transfers\n            logits = logits / temperature + noise_scale * gumbel\n        scores, tokens = logits.log_softmax(dim=-1).max(dim=-1)\n        return tokens, scores\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, device='cpu', rng_generator=None)</code>","text":"<p>Initialize the Masked Discrete Language Model (MDLM) interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution governing the time variable in the diffusion process.</p> required <code>prior_distribution</code> <code>DiscreteMaskedPrior</code> <p>The prior distribution over the discrete token space, including masked tokens.</p> required <code>noise_schedule</code> <code>ContinuousExpNoiseTransform</code> <p>The noise schedule defining the noise intensity as a function of time.</p> required <code>device</code> <code>str</code> <p>The device to use for computations. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>The random number generator for reproducibility. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: DiscreteMaskedPrior,\n    noise_schedule: ContinuousExpNoiseTransform,\n    device: str = \"cpu\",\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initialize the Masked Discrete Language Model (MDLM) interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution governing the time variable in the diffusion process.\n        prior_distribution (DiscreteMaskedPrior): The prior distribution over the discrete token space, including masked tokens.\n        noise_schedule (ContinuousExpNoiseTransform): The noise schedule defining the noise intensity as a function of time.\n        device (str, optional): The device to use for computations. Defaults to \"cpu\".\n        rng_generator (Optional[torch.Generator], optional): The random number generator for reproducibility. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    if not isinstance(prior_distribution, DiscreteMaskedPrior):\n        raise ValueError(\"DiscreteMaskedPrior required for MDLM\")\n    if not isinstance(noise_schedule, ContinuousExpNoiseTransform):\n        raise ValueError(\"ContinuousExpNoiseTransform required for MDLM\")\n    self.noise_schedule = noise_schedule\n    self.num_classes = prior_distribution.num_classes\n    self.mask_index = prior_distribution.mask_dim\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM._sample_categorical","title":"<code>_sample_categorical(categorical_probs)</code>","text":"<p>Sample from a categorical distribution using the Gumbel trick.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_probs</code> <code>Tensor</code> <p>The probabilities of each category, shape batch x node x class.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The sampled category indices, shape batch x node.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def _sample_categorical(self, categorical_probs: Tensor) -&gt; Tensor:\n    \"\"\"Sample from a categorical distribution using the Gumbel trick.\n\n    Args:\n        categorical_probs (Tensor): The probabilities of each category, shape batch x node x class.\n\n    Returns:\n        Tensor: The sampled category indices, shape batch x node.\n    \"\"\"\n    gumbel_norm = (\n        1e-10\n        - (\n            torch.rand(*categorical_probs.shape, device=categorical_probs.device, generator=self.rng_generator)\n            + 1e-10\n        ).log()\n    )\n    scaled_proability = categorical_probs / gumbel_norm\n    return scaled_proability.argmax(dim=-1)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM._subs_parameterization","title":"<code>_subs_parameterization(logits, xt)</code>","text":"<p>Apply subsititution parameterization to the logits.</p> <p>This function enforces that the model can never predict a mask token by lowering the mask logits. Then, for all unmasked tokens, it copies over from xt to enable carry over unmasked. Once a token is unmasked, it stays the same. See Sec. 3.2.3 https://arxiv.org/pdf/2406.07524.</p> <p>Note that recent work has shown that allowing the model to rethink carry over unmasking is beneficial https://arxiv.org/abs/2410.06264.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The logits tensor with shape batch x node x class.</p> required <code>xt</code> <code>Tensor</code> <p>The tensor of unmasked tokens with shape batch x node.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The modified logits tensor with substitution parameterization applied.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def _subs_parameterization(self, logits: Tensor, xt: Tensor) -&gt; Tensor:\n    \"\"\"Apply subsititution parameterization to the logits.\n\n    This function enforces that the model can never predict a mask token by lowering the mask logits.\n    Then, for all unmasked tokens, it copies over from xt to enable carry over unmasked.\n    Once a token is unmasked, it stays the same.\n    See Sec. 3.2.3 https://arxiv.org/pdf/2406.07524.\n\n    Note that recent work has shown that allowing the model to rethink\n    carry over unmasking is beneficial https://arxiv.org/abs/2410.06264.\n\n    Args:\n        logits (Tensor): The logits tensor with shape batch x node x class.\n        xt (Tensor): The tensor of unmasked tokens with shape batch x node.\n\n    Returns:\n        Tensor: The modified logits tensor with substitution parameterization applied.\n    \"\"\"\n    logits[..., self.mask_index] += -1000000.0  # clean input is never masked\n    logprobs = logits - torch.logsumexp(logits, dim=-1, keepdim=True)  # normalize\n    unmasked_indices = xt != self.mask_index\n    logprobs[unmasked_indices] = -1000000.0\n    logprobs[unmasked_indices, xt[unmasked_indices]] = 0  # Unmasked token remains unchanged\n    return logprobs\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.calculate_score","title":"<code>calculate_score(logits, x, t)</code>","text":"<p>Returns score of the given sample x at time t with the corresponding model output logits.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The output of the model.</p> required <code>x</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The current time.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The score defined in Appendix C.3 Equation 76 of MDLM.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def calculate_score(self, logits, x, t):\n    \"\"\"Returns score of the given sample x at time t with the corresponding model output logits.\n\n    Args:\n        logits (Tensor): The output of the model.\n        x (Tensor): The current data point.\n        t (Tensor): The current time.\n\n    Returns:\n        Tensor: The score defined in Appendix C.3 Equation 76 of MDLM.\n    \"\"\"\n    sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n    log_ratio = -torch.log(\n        torch.expm1(sigma_t)\n    )  # log ( exp(-sigma) / (1 - exp(-sigma))) = log(1/ (exp(sigma) - 1))\n\n    # Create masked and unmasked log scores\n    masked_log_score = logits + pad_like(log_ratio, logits)  # xt is masked and prediction is not\n    masked_log_score[..., self.mask_index] = 0  # xt and prediction are mask\n\n    unmasked_log_score = torch.full_like(logits, -1000000.0)\n    unmasked_log_score.scatter_(-1, x[..., None], 0)  # place zeros where current predictions are\n    unmasked_log_score[..., self.mask_index] = -pad_like(log_ratio, logits[..., 0])\n\n    # Combine masked and unmasked log scores\n    masked_indices = (x == self.mask_index).to(logits.dtype)[..., None]\n    log_score = masked_log_score * masked_indices + unmasked_log_score * (1 - masked_indices)\n\n    return log_score.exp()\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.forward_process","title":"<code>forward_process(data, t)</code>","text":"<p>Apply the forward process to the data at time t.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>x(t) after applying the forward process</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n    \"\"\"Apply the forward process to the data at time t.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n\n    Returns:\n        Tensor: x(t) after applying the forward process\n    \"\"\"\n    return self.interpolate(data, t)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.get_num_steps_confidence","title":"<code>get_num_steps_confidence(xt, num_tokens_unmask=1)</code>","text":"<p>Calculate the maximum number of steps with confidence.</p> <p>This method computes the maximum count of occurrences where the input tensor <code>xt</code> matches the <code>mask_index</code> along the last dimension (-1). The result is returned as a single float value.</p> <p>Parameters:</p> Name Type Description Default <code>xt</code> <code>Tensor</code> <p>Input tensor to evaluate against the mask index.</p> required <code>num_tokens_unmask</code> <code>int</code> <p>number of tokens to unamsk at each step.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The maximum number of steps with confidence (i.e., matching the mask index).</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def get_num_steps_confidence(self, xt: Tensor, num_tokens_unmask: int = 1):\n    \"\"\"Calculate the maximum number of steps with confidence.\n\n    This method computes the maximum count of occurrences where the input tensor `xt` matches the `mask_index`\n    along the last dimension (-1). The result is returned as a single float value.\n\n    Args:\n        xt (Tensor): Input tensor to evaluate against the mask index.\n        num_tokens_unmask (int): number of tokens to unamsk at each step.\n\n    Returns:\n        float: The maximum number of steps with confidence (i.e., matching the mask index).\n    \"\"\"\n    nsteps = (xt == self.mask_index).sum(-1).max().item()\n    if num_tokens_unmask == 1:\n        return int(nsteps)\n    else:\n        return int(max(math.ceil(nsteps // num_tokens_unmask), 1))\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.interpolate","title":"<code>interpolate(data, t)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n    \"\"\"\n    if data.dtype == torch.float and data.ndim &gt; 2:\n        x0 = data.argmax(-1)\n    else:\n        x0 = data\n    sigma = self.noise_schedule.calculate_sigma(t, data.device)\n    alpha = self.noise_schedule.sigma_to_alpha(sigma)\n    p_mask = 1 - alpha\n    p_mask = pad_like(p_mask, x0)\n    mask_indices = torch.rand(*x0.shape, device=x0.device, generator=self.rng_generator) &lt; p_mask\n    xt = torch.where(mask_indices, self.mask_index, x0)\n    return xt\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.loss","title":"<code>loss(logits, target, xt, time, mask=None, use_weight=True)</code>","text":"<p>Calculate the cross-entropy loss between the model prediction and the target output.</p> <p>The loss is calculated between the batch x node x class logits and the target batch x node, considering the current state of the discrete sequence <code>xt</code> at time <code>time</code>.</p> <p>If <code>use_weight</code> is True, the loss is weighted by the reduced form of the MDLM time weight for continuous NELBO, as specified in equation 11 of https://arxiv.org/pdf/2406.07524. This weight is proportional to the derivative of the noise schedule with respect to time, and is used to emphasize the importance of accurate predictions at certain times in the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted output from the model, with shape batch x node x class.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction, with shape batch x node.</p> required <code>xt</code> <code>Tensor</code> <p>The current state of the discrete sequence, with shape batch x node.</p> required <code>time</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>use_weight</code> <code>bool</code> <p>Whether to use the MDLM time weight for the loss. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def loss(\n    self,\n    logits: Tensor,\n    target: Tensor,\n    xt: Tensor,\n    time: Tensor,\n    mask: Optional[Tensor] = None,\n    use_weight=True,\n):\n    \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n    The loss is calculated between the batch x node x class logits and the target batch x node,\n    considering the current state of the discrete sequence `xt` at time `time`.\n\n    If `use_weight` is True, the loss is weighted by the reduced form of the MDLM time weight for continuous NELBO,\n    as specified in equation 11 of https://arxiv.org/pdf/2406.07524. This weight is proportional to the derivative\n    of the noise schedule with respect to time, and is used to emphasize the importance of accurate predictions at\n    certain times in the diffusion process.\n\n    Args:\n        logits (Tensor): The predicted output from the model, with shape batch x node x class.\n        target (Tensor): The target output for the model prediction, with shape batch x node.\n        xt (Tensor): The current state of the discrete sequence, with shape batch x node.\n        time (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        use_weight (bool, optional): Whether to use the MDLM time weight for the loss. Defaults to True.\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    logprobs = self._subs_parameterization(logits, xt)\n    log_p_theta = torch.gather(input=logprobs, dim=-1, index=target[..., None]).squeeze(-1)\n\n    sigma = self.noise_schedule.calculate_sigma(time, target.device)\n    dsigma = self.noise_schedule.d_dt_sigma(time, target.device)  # type: ignore\n    loss = -log_p_theta\n    if use_weight:\n        loss = loss * (dsigma / torch.expm1(sigma))[:, None]\n\n    if mask is not None:\n        loss = loss * mask\n        num_non_masked_elements = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n    else:\n        loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step","title":"<code>step(logits, t, xt, dt, temperature=1.0)</code>","text":"<p>Perform a single step of MDLM DDPM step.</p> <p>Parameters: logits (Tensor): The input logits. t (Tensor): The current time step. xt (Tensor): The current state. dt (Tensor): The time step increment. temperature (float): Softmax temperature defaults to 1.0.</p> <p>Returns: Tensor: The updated state.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step(self, logits: Tensor, t: Tensor, xt: Tensor, dt: Tensor, temperature: float = 1.0) -&gt; Tensor:\n    \"\"\"Perform a single step of MDLM DDPM step.\n\n    Parameters:\n    logits (Tensor): The input logits.\n    t (Tensor): The current time step.\n    xt (Tensor): The current state.\n    dt (Tensor): The time step increment.\n    temperature (float): Softmax temperature defaults to 1.0.\n\n    Returns:\n    Tensor: The updated state.\n    \"\"\"\n    sigma_t = self.noise_schedule.calculate_sigma(t, logits.device)\n    sigma_s = self.noise_schedule.calculate_sigma(t - dt, logits.device)\n    alpha_t = torch.exp(-sigma_t)\n    alpha_s = torch.exp(-sigma_s)\n    p_mask_s = 1 - alpha_s\n    alpha_t = pad_like(alpha_t, logits)\n    alpha_s = pad_like(alpha_s, logits)\n    p_mask_s = pad_like(p_mask_s, logits)\n    # Apply subs parameterization\n    log_p_x0 = self._subs_parameterization(logits, xt) / temperature\n    if p_mask_s.ndim != log_p_x0.ndim:\n        raise ValueError(f\"Dimension Mistmatch {p_mask_s.shape} {log_p_x0.shape}\")\n    # Equation 7 from MDLM\n    prob_s_given_t = log_p_x0.exp() * (\n        alpha_s - alpha_t\n    )  # righthand side (alpha_s - alpha_t)*x = (1 - alpha_t - (1 - alpha_s)) * x\n    prob_s_given_t[..., self.mask_index] = p_mask_s[..., 0]  # lefthand side (1 - alpha_s)*M\n    sampled_x = self._sample_categorical(prob_s_given_t)\n    carry_over_unmask = (xt != self.mask_index).to(xt.dtype)\n    return carry_over_unmask * xt + (1 - carry_over_unmask) * sampled_x\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_argmax","title":"<code>step_argmax(model_out)</code>","text":"<p>Returns the index of the maximum value in the last dimension of the model output.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The index of the maximum value in the last dimension of the model output.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_argmax(self, model_out: Tensor):\n    \"\"\"Returns the index of the maximum value in the last dimension of the model output.\n\n    Args:\n        model_out (Tensor): The output of the model.\n\n    Returns:\n        Tensor: The index of the maximum value in the last dimension of the model output.\n    \"\"\"\n    return model_out.argmax(dim=-1)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_confidence","title":"<code>step_confidence(logits, xt, curr_step, num_steps, logit_temperature=1.0, randomness=1.0, confidence_temperature=1.0, num_tokens_unmask=1)</code>","text":"<p>Update the input sequence xt by sampling from the predicted logits and adding Gumbel noise.</p> <p>Method taken from GenMol Lee et al. https://arxiv.org/abs/2501.06158</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Predicted logits</p> required <code>xt</code> <code>Tensor</code> <p>Input sequence</p> required <code>curr_step</code> <code>int</code> <p>Current step</p> required <code>num_steps</code> <code>int</code> <p>Total number of steps</p> required <code>logit_temperature</code> <code>float</code> <p>Temperature for softmax over logits</p> <code>1.0</code> <code>randomness</code> <code>float</code> <p>Scale for Gumbel noise</p> <code>1.0</code> <code>confidence_temperature</code> <code>float</code> <p>Temperature for Gumbel confidence</p> <code>1.0</code> <code>num_tokens_unmask</code> <code>int</code> <p>number of tokens to unmask each step</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Updated input sequence xt unmasking num_tokens_unmask token each step.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_confidence(\n    self,\n    logits: Tensor,\n    xt: Tensor,\n    curr_step: int,\n    num_steps: int,\n    logit_temperature: float = 1.0,\n    randomness: float = 1.0,\n    confidence_temperature: float = 1.0,\n    num_tokens_unmask: int = 1,\n) -&gt; Tensor:\n    \"\"\"Update the input sequence xt by sampling from the predicted logits and adding Gumbel noise.\n\n    Method taken from GenMol Lee et al. https://arxiv.org/abs/2501.06158\n\n    Args:\n        logits: Predicted logits\n        xt: Input sequence\n        curr_step: Current step\n        num_steps: Total number of steps\n        logit_temperature: Temperature for softmax over logits\n        randomness: Scale for Gumbel noise\n        confidence_temperature: Temperature for Gumbel confidence\n        num_tokens_unmask: number of tokens to unmask each step\n\n    Returns:\n        Updated input sequence xt unmasking num_tokens_unmask token each step.\n    \"\"\"\n    if xt.ndim &gt; 3:\n        raise NotImplementedError(\n            \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n        )\n    if curr_step &lt; 0 or num_steps &lt; 1 or num_tokens_unmask &lt; 1:\n        raise ValueError(\"Invalid input values for curr_step, num_steps, or num_tokens_unmask.\")\n    xt = xt.clone()\n    log_p_x0 = self._subs_parameterization(logits, xt)\n    # sample the code from the softmax prediction\n    probs = torch.softmax(log_p_x0 / logit_temperature, dim=-1)\n    preds = torch.distributions.Categorical(probs=probs).sample()\n\n    confidence = probs.gather(-1, preds.unsqueeze(-1)).squeeze(-1)\n    # add Gumbel noise decreasing over the sampling process\n    ratio = curr_step / (num_steps - 1)\n    # Using manual definition of 0,1 Gumbel to pass in generator, manually specifying the device is faster than transfer\n    gumbel_sample = -torch.log(\n        -torch.log(torch.rand(xt.shape, device=logits.device, generator=self.rng_generator))\n    )\n    # gumbel_sample = self.gumbel_dist.sample(xt.shape).to(logits.device)\n    gumbel_noise = gumbel_sample * randomness * (1 - ratio)  # type: ignore\n    confidence = (\n        (torch.log(confidence) + gumbel_noise) / confidence_temperature\n    )  # stems from tau of https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n\n    # do not predict on already predicted tokens\n    mask = xt == self.mask_index\n    confidence[~mask] = -torch.inf\n\n    # choose the predicted token with the highest confidence\n    confidence_threshold, idx_mask = torch.topk(confidence, k=num_tokens_unmask, dim=-1)\n    confidence_threshold = confidence_threshold[:, -1].unsqueeze(-1)\n\n    # replace the chosen tokens\n    to_replace = confidence &gt;= confidence_threshold\n    to_replace = (mask.float() * to_replace.float()).bool()\n    xt[to_replace] = preds[to_replace]\n    return xt\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.step_self_path_planning","title":"<code>step_self_path_planning(logits, xt, t, curr_step, num_steps, logit_temperature=1.0, randomness=1.0, confidence_temperature=1.0, score_type='confidence', fix_mask=None)</code>","text":"<p>Self Path Planning (P2) Sampling from Peng et al. https://arxiv.org/html/2502.03540v1.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Predicted logits for sampling.</p> required <code>xt</code> <code>Tensor</code> <p>Input sequence to be updated.</p> required <code>t</code> <code>Tensor</code> <p>Time tensor (e.g., time steps or temporal info).</p> required <code>curr_step</code> <code>int</code> <p>Current iteration in the planning process.</p> required <code>num_steps</code> <code>int</code> <p>Total number of planning steps.</p> required <code>logit_temperature</code> <code>float</code> <p>Temperature for logits (default: 1.0).</p> <code>1.0</code> <code>randomness</code> <code>float</code> <p>Introduced randomness level (default: 1.0).</p> <code>1.0</code> <code>confidence_temperature</code> <code>float</code> <p>Temperature for confidence scoring (default: 1.0).</p> <code>1.0</code> <code>score_type</code> <code>Literal['confidence', 'random']</code> <p>Sampling score type (default: \"confidence\").</p> <code>'confidence'</code> <code>fix_mask</code> <code>Optional[Tensor]</code> <p>inital mask where True when not a mask tokens (default: None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Updated input sequence xt after iterative unmasking.</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def step_self_path_planning(\n    self,\n    logits: Tensor,\n    xt: Tensor,\n    t: Tensor,\n    curr_step: int,\n    num_steps: int,\n    logit_temperature: float = 1.0,\n    randomness: float = 1.0,\n    confidence_temperature: float = 1.0,\n    score_type: Literal[\"confidence\", \"random\"] = \"confidence\",\n    fix_mask: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Self Path Planning (P2) Sampling from Peng et al. https://arxiv.org/html/2502.03540v1.\n\n    Args:\n        logits (Tensor): Predicted logits for sampling.\n        xt (Tensor): Input sequence to be updated.\n        t (Tensor): Time tensor (e.g., time steps or temporal info).\n        curr_step (int): Current iteration in the planning process.\n        num_steps (int): Total number of planning steps.\n        logit_temperature (float): Temperature for logits (default: 1.0).\n        randomness (float): Introduced randomness level (default: 1.0).\n        confidence_temperature (float): Temperature for confidence scoring (default: 1.0).\n        score_type (Literal[\"confidence\", \"random\"]): Sampling score type (default: \"confidence\").\n        fix_mask (Optional[Tensor]): inital mask where True when not a mask tokens (default: None).\n\n    Returns:\n        Tensor: Updated input sequence xt after iterative unmasking.\n    \"\"\"\n    if xt.ndim &gt; 3:\n        raise NotImplementedError(\n            \"step_confidence is implemented for Batch x Sequence x State Space shaped tensors.\"\n        )\n    if curr_step &lt; 0 or num_steps &lt; 1:\n        raise ValueError(\"Invalid input values for curr_step, num_steps.\")\n    xt = xt.clone()\n    if fix_mask is None:\n        fix_mask = torch.zeros_like(xt).bool()  #! if any sequenes are fixed from the start of trajectory\n    last_mask = xt == self.mask_index\n    unmask_candidates = (\n        last_mask &amp; ~fix_mask\n    )  #! I want to consider tokens to un mask that are currently masked and not fixed. This fixes a typo in pseudo code\n    x1_pred, logp = self.stochastic_sample_from_categorical(\n        logits, temperature=logit_temperature, noise_scale=confidence_temperature\n    )\n    if curr_step == num_steps - 1:\n        xt[last_mask] = x1_pred[last_mask]\n    else:\n        if score_type == \"confidence\":\n            score = logp\n        elif score_type == \"random\":\n            score = torch.rand_like(logp).log()\n\n        score = score.masked_fill(fix_mask.squeeze(-1), float(\"inf\"))\n        score[unmask_candidates.squeeze(-1)] *= randomness\n        num_to_mask = torch.clamp(\n            ((~fix_mask).sum(dim=1, keepdim=True).float() * t.unsqueeze(-1)).long(), max=xt.shape[-1] - 1\n        )  #! here is is t since diffusion time is 1 to 0. Clamp is to set it to 0 N-1 since topk uses it as indices\n        mask = self.topk_lowest_masking(score, num_to_mask)\n        xt[mask] = self.mask_index\n        mask_to_x1 = last_mask &amp; ~mask\n        xt[mask_to_x1] = x1_pred[mask_to_x1]\n    return xt\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.stochastic_sample_from_categorical","title":"<code>stochastic_sample_from_categorical(logits, temperature=1.0, noise_scale=1.0)</code>","text":"<p>Stochastically samples from a categorical distribution defined by input logits, with optional temperature and noise scaling for diverse sampling.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Input logits tensor with shape (... , num_categories)</p> required <code>temperature</code> <code>float</code> <p>Softmax temperature. Higher values produce more uniform samples. Defaults to 1.0.</p> <code>1.0</code> <code>noise_scale</code> <code>float</code> <p>Scale for Gumbel noise. Higher values produce more diverse samples. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>tokens (LongTensor): Sampling result (category indices) with shape (... , )</li> <li>scores (Tensor): Corresponding log-softmax scores for the sampled tokens, with shape (... , )</li> </ul> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def stochastic_sample_from_categorical(self, logits: Tensor, temperature: float = 1.0, noise_scale: float = 1.0):\n    \"\"\"Stochastically samples from a categorical distribution defined by input logits, with optional temperature and noise scaling for diverse sampling.\n\n    Args:\n        logits (Tensor): Input logits tensor with shape (... , num_categories)\n        temperature (float, optional): Softmax temperature. Higher values produce more uniform samples. Defaults to 1.0.\n        noise_scale (float, optional): Scale for Gumbel noise. Higher values produce more diverse samples. Defaults to 1.0.\n\n    Returns:\n        tuple:\n            - **tokens** (LongTensor): Sampling result (category indices) with shape (... , )\n            - **scores** (Tensor): Corresponding log-softmax scores for the sampled tokens, with shape (... , )\n    \"\"\"\n    if temperature &gt; 0:\n        gumbel = -torch.log(\n            -torch.log(torch.rand(logits.shape, device=logits.device, generator=self.rng_generator) + 1e-8) + 1e-8\n        )  #! avoid device transfers\n        logits = logits / temperature + noise_scale * gumbel\n    scores, tokens = logits.log_softmax(dim=-1).max(dim=-1)\n    return tokens, scores\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/continuous_time/discrete/mdlm/#bionemo.moco.interpolants.continuous_time.discrete.mdlm.MDLM.topk_lowest_masking","title":"<code>topk_lowest_masking(scores, cutoff_len)</code>","text":"<p>Generates a mask for the lowest scoring elements up to a specified cutoff length.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>Input scores tensor with shape (... , num_elements)</p> required <code>cutoff_len</code> <code>Tensor</code> <p>Number of lowest-scoring elements to mask (per batch element)</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>Boolean mask tensor with same shape as <code>scores</code>, where <code>True</code> indicates     the corresponding element is among the <code>cutoff_len</code> lowest scores.</p> Example <p>scores = torch.tensor([[0.9, 0.8, 0.1, 0.05], [0.7, 0.4, 0.3, 0.2]]) cutoff_len = 2 mask = topk_lowest_masking(scores, cutoff_len) print(mask) tensor([[False, False, True, True],         [False, True, True, False]])</p> Source code in <code>bionemo/moco/interpolants/continuous_time/discrete/mdlm.py</code> <pre><code>def topk_lowest_masking(self, scores: Tensor, cutoff_len: Tensor):\n    \"\"\"Generates a mask for the lowest scoring elements up to a specified cutoff length.\n\n    Args:\n        scores (Tensor): Input scores tensor with shape (... , num_elements)\n        cutoff_len (Tensor): Number of lowest-scoring elements to mask (per batch element)\n\n    Returns:\n        Tensor: Boolean mask tensor with same shape as `scores`, where `True` indicates\n                the corresponding element is among the `cutoff_len` lowest scores.\n\n    Example:\n        &gt;&gt;&gt; scores = torch.tensor([[0.9, 0.8, 0.1, 0.05], [0.7, 0.4, 0.3, 0.2]])\n        &gt;&gt;&gt; cutoff_len = 2\n        &gt;&gt;&gt; mask = topk_lowest_masking(scores, cutoff_len)\n        &gt;&gt;&gt; print(mask)\n        tensor([[False, False, True, True],\n                [False, True, True, False]])\n    \"\"\"\n    sorted_scores, _ = scores.sort(dim=-1)\n    threshold = sorted_scores.gather(dim=-1, index=cutoff_len)\n    return scores &lt; threshold\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/utils/#bionemo.moco.interpolants.discrete_time.utils.safe_index","title":"<code>safe_index(tensor, index, device)</code>","text":"<p>Safely indexes a tensor using a given index and returns the result on a specified device.</p> <p>Note can implement forcing with  return tensor[index.to(tensor.device)].to(device) but has costly migration.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to be indexed.</p> required <code>index</code> <code>Tensor</code> <p>The index to use for indexing the tensor.</p> required <code>device</code> <code>device</code> <p>The device on which the result should be returned.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The indexed tensor on the specified device.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tensor, index are not all on the same device.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/utils.py</code> <pre><code>def safe_index(tensor: Tensor, index: Tensor, device: Optional[torch.device]):\n    \"\"\"Safely indexes a tensor using a given index and returns the result on a specified device.\n\n    Note can implement forcing with  return tensor[index.to(tensor.device)].to(device) but has costly migration.\n\n    Args:\n        tensor (Tensor): The tensor to be indexed.\n        index (Tensor): The index to use for indexing the tensor.\n        device (torch.device): The device on which the result should be returned.\n\n    Returns:\n        Tensor: The indexed tensor on the specified device.\n\n    Raises:\n        ValueError: If tensor, index are not all on the same device.\n    \"\"\"\n    if not (tensor.device == index.device):\n        raise ValueError(\n            f\"Tensor, index, and device must all be on the same device. \"\n            f\"Got tensor.device={tensor.device}, index.device={index.device}, and device={device}.\"\n        )\n\n    return tensor[index].to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/","title":"Ddpm","text":""},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM","title":"<code>DDPM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Denoising Diffusion Probabilistic Model (DDPM) interpolant.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n&gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n&gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM\n&gt;&gt;&gt; from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\n&gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\n\n\nddpm = DDPM(\n    time_distribution = UniformTimeDistribution(discrete_time = True,...),\n    prior_distribution = GaussianPrior(...),\n    noise_schedule = DiscreteCosineNoiseSchedule(...),\n    )\nmodel = Model(...)\n\n# Training\nfor epoch in range(1000):\n    data = data_loader.get(...)\n    time = ddpm.sample_time(batch_size)\n    noise = ddpm.sample_prior(data.shape)\n    xt = ddpm.interpolate(data, noise, time)\n\n    x_pred = model(xt, time)\n    loss = ddpm.loss(x_pred, data, time)\n    loss.backward()\n\n# Generation\nx_pred = ddpm.sample_prior(data.shape)\nfor t in DiscreteLinearTimeSchedule(...).generate_schedule():\n    time = torch.full((batch_size,), t)\n    x_hat = model(x_pred, time)\n    x_pred = ddpm.step(x_hat, time, x_pred)\nreturn x_pred\n</code></pre></p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>class DDPM(Interpolant):\n    \"\"\"A Denoising Diffusion Probabilistic Model (DDPM) interpolant.\n\n     -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\n    &gt;&gt;&gt; from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\n    &gt;&gt;&gt; from bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM\n    &gt;&gt;&gt; from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\n    &gt;&gt;&gt; from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\n\n\n    ddpm = DDPM(\n        time_distribution = UniformTimeDistribution(discrete_time = True,...),\n        prior_distribution = GaussianPrior(...),\n        noise_schedule = DiscreteCosineNoiseSchedule(...),\n        )\n    model = Model(...)\n\n    # Training\n    for epoch in range(1000):\n        data = data_loader.get(...)\n        time = ddpm.sample_time(batch_size)\n        noise = ddpm.sample_prior(data.shape)\n        xt = ddpm.interpolate(data, noise, time)\n\n        x_pred = model(xt, time)\n        loss = ddpm.loss(x_pred, data, time)\n        loss.backward()\n\n    # Generation\n    x_pred = ddpm.sample_prior(data.shape)\n    for t in DiscreteLinearTimeSchedule(...).generate_schedule():\n        time = torch.full((batch_size,), t)\n        x_hat = model(x_pred, time)\n        x_pred = ddpm.step(x_hat, time, x_pred)\n    return x_pred\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: PriorDistribution,\n        noise_schedule: DiscreteNoiseSchedule,\n        prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n        device: Union[str, torch.device] = \"cpu\",\n        last_time_idx: int = 0,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the DDPM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n            prediction_type (PredictionType): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n            device (str): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            last_time_idx (int, optional): The last time index for discrete time. Set to 0 if discrete time is T-1, ..., 0 or 1 if T, ..., 1. Defaults to 0.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        super().__init__(time_distribution, prior_distribution, device, rng_generator)\n        if not isinstance(prior_distribution, GaussianPrior):\n            warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n        self.noise_schedule = noise_schedule\n        self._initialize_schedules(device)\n        self.prediction_type = string_to_enum(prediction_type, PredictionType)\n        self._loss_function = nn.MSELoss(reduction=\"none\")\n        self.last_time_idx = last_time_idx\n\n    def _initialize_schedules(self, device: Union[str, torch.device] = \"cpu\"):\n        \"\"\"Sets up the Denoising Diffusion Probabilistic Model (DDPM) equations.\n\n        This method initializes the schedules for the forward and reverse processes of the DDPM. It calculates the\n        alphas, betas, and log variances required for the diffusion process.\n\n        Specifically, it computes:\n\n        * `alpha_bar`: the cumulative product of `alpha_t`\n        * `alpha_bar_prev`: the previous cumulative product of `alpha_t`\n        * `posterior_variance`: the variance of the posterior distribution\n        * `posterior_mean_c0_coef` and `posterior_mean_ct_coef`: the coefficients for the posterior mean\n        * `log_var`: the log variance of the posterior distribution\n\n        These values are then used to set up the forward and reverse schedules for the DDPM.\n        Specifically this is equation (6) (7) from https://arxiv.org/pdf/2006.11239\n        \"\"\"\n        if self.noise_schedule is None:\n            raise ValueError(\"noise_schedule cannot be None for DDPM\")\n        alphas = self.noise_schedule.generate_schedule(device=device)\n        betas = 1 - alphas\n        log_alpha = torch.log(alphas)\n        log_alpha_bar = torch.cumsum(log_alpha, dim=0)\n        alpha_bar = alphas_cumprod = torch.exp(log_alpha_bar)\n        alpha_bar_prev = alphas_cumprod_prev = torch.nn.functional.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n        posterior_variance = betas * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar)\n        posterior_mean_c0_coef = betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alpha_bar)\n        posterior_mean_ct_coef = (1.0 - alpha_bar_prev) * torch.sqrt(alphas) / (1.0 - alpha_bar)\n        # log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        posterior_logvar = torch.log(\n            torch.nn.functional.pad(posterior_variance[:-1], (1, 0), value=posterior_variance[0].item())\n        )\n        self._forward_data_schedule = torch.sqrt(alpha_bar)\n        self._forward_noise_schedule = torch.sqrt(1 - alpha_bar)\n        self._reverse_data_schedule = posterior_mean_c0_coef\n        self._reverse_noise_schedule = posterior_mean_ct_coef\n        self._log_var = posterior_logvar\n        self._alpha_bar = alpha_bar\n        self._alpha_bar_prev = alpha_bar_prev\n        self._betas = betas\n        self._posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n\n    @property\n    def forward_data_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the forward data schedule.\"\"\"\n        return self._forward_data_schedule\n\n    @property\n    def forward_noise_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the forward noise schedule.\"\"\"\n        return self._forward_noise_schedule\n\n    @property\n    def reverse_data_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the reverse data schedule.\"\"\"\n        return self._reverse_data_schedule\n\n    @property\n    def reverse_noise_schedule(self) -&gt; torch.Tensor:\n        \"\"\"Returns the reverse noise schedule.\"\"\"\n        return self._reverse_noise_schedule\n\n    @property\n    def log_var(self) -&gt; torch.Tensor:\n        \"\"\"Returns the log variance.\"\"\"\n        return self._log_var\n\n    @property\n    def alpha_bar(self) -&gt; torch.Tensor:\n        \"\"\"Returns the alpha bar values.\"\"\"\n        return self._alpha_bar\n\n    @property\n    def alpha_bar_prev(self) -&gt; torch.Tensor:\n        \"\"\"Returns the previous alpha bar values.\"\"\"\n        return self._alpha_bar_prev\n\n    def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor): noise from prior()\n        \"\"\"\n        psi = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n        omega = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n        psi = pad_like(psi, data)\n        omega = pad_like(omega, data)\n        x_t = data * psi + noise * omega\n        return x_t\n\n    def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n        \"\"\"Get x(t) with given time t from noise and data.\n\n        Args:\n            data (Tensor): target\n            t (Tensor): time\n            noise (Tensor, optional): noise from prior(). Defaults to None.\n        \"\"\"\n        if noise is None:\n            noise = self.sample_prior(data.shape)\n        return self.interpolate(data, t, noise)\n\n    def process_data_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n        \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n        This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n        Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n        The conversion formulas are as follows:\n        - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n        - For \"data\" prediction type: `pred_data = model_output`\n        - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n        Args:\n            model_output (Tensor): The output of the model.\n            sample (Tensor): The input sample.\n            t (Tensor): The time step.\n\n        Returns:\n            The data prediction based on the prediction type.\n\n        Raises:\n            ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n        \"\"\"\n        data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n        noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_data = (sample - noise_scale * model_output) / data_scale\n        elif self.prediction_type == PredictionType.DATA:\n            pred_data = model_output\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n                f\" PredictionType.VELOCITY for DDPM.\"\n            )\n        return pred_data\n\n    def process_noise_prediction(self, model_output, sample, t):\n        \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n        Args:\n            model_output: The output of the model.\n            sample: The input sample.\n            t: The time step.\n\n        Returns:\n            The input as noise if the prediction type is \"noise\".\n\n        Raises:\n            ValueError: If the prediction type is not \"noise\".\n        \"\"\"\n        data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n        noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n        data_scale = pad_like(data_scale, model_output)\n        noise_scale = pad_like(noise_scale, model_output)\n        if self.prediction_type == PredictionType.NOISE:\n            pred_noise = model_output\n        elif self.prediction_type == PredictionType.DATA:\n            pred_noise = (sample - data_scale * model_output) / noise_scale\n        elif self.prediction_type == PredictionType.VELOCITY:\n            pred_data = data_scale * sample - noise_scale * model_output\n            pred_noise = (sample - data_scale * pred_data) / noise_scale\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n                \" `v_prediction`  for DDPM.\"\n            )\n        return pred_noise\n\n    def calculate_velocity(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n        \"\"\"Calculate the velocity term given the data, time step, and noise.\n\n        Args:\n            data (Tensor): The input data.\n            t (Tensor): The current time step.\n            noise (Tensor): The noise term.\n\n        Returns:\n            Tensor: The calculated velocity term.\n        \"\"\"\n        data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n        noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n        data_scale = pad_like(data_scale, data)\n        noise_scale = pad_like(noise_scale, data)\n        v = data_scale * noise - noise_scale * data\n        return v\n\n    @torch.no_grad()\n    def step(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ):\n        \"\"\"Do one step integration.\n\n        Args:\n        model_out (Tensor): The output of the model.\n        t (Tensor): The current time step.\n        xt (Tensor): The current data point.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n        Note:\n        The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.\n\n        Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n        For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        x_hat = self.process_data_prediction(model_out, xt, t)\n        psi_r = safe_index(self._reverse_data_schedule, t - self.last_time_idx, x_hat.device)\n        omega_r = safe_index(self._reverse_noise_schedule, t - self.last_time_idx, x_hat.device)\n        log_var = safe_index(self._log_var, t - self.last_time_idx, x_hat.device)  # self._log_var[t.long()]\n        nonzero_mask = (t &gt; self.last_time_idx).float()\n        psi_r = pad_like(psi_r, x_hat)\n        omega_r = pad_like(omega_r, x_hat)\n        log_var = pad_like(log_var, x_hat)\n        nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n        mean = psi_r * x_hat + omega_r * xt\n        eps = torch.randn_like(mean).to(model_out.device)\n\n        x_next = mean + nonzero_mask * (0.5 * log_var).exp() * eps * temperature\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def step_noise(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        center: Bool = False,\n        temperature: Float = 1.0,\n    ):\n        \"\"\"Do one step integration.\n\n        Args:\n        model_out (Tensor): The output of the model.\n        t (Tensor): The current time step.\n        xt (Tensor): The current data point.\n        mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n        center (bool, optional): Whether to center the data. Defaults to False.\n        temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n        Note:\n        The temperature parameter controls the level of randomness in the sampling process.\n        A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2)\n        result in less random and more deterministic samples. This can be useful for tasks\n        that require more control over the generation process.\n\n        Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n        For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        eps_hat = self.process_noise_prediction(model_out, xt, t)\n        beta_t = safe_index(self._betas, t - self.last_time_idx, model_out.device)\n        recip_sqrt_alpha_t = torch.sqrt(1 / (1 - beta_t))\n        eps_factor = (\n            safe_index(self._betas, t - self.last_time_idx, model_out.device)\n            / (1 - safe_index(self._alpha_bar, t - self.last_time_idx, model_out.device)).sqrt()\n        )\n        var = safe_index(self._posterior_variance, t - self.last_time_idx, model_out.device)  # self._log_var[t.long()]\n\n        nonzero_mask = (t &gt; self.last_time_idx).float()\n        nonzero_mask = pad_like(nonzero_mask, model_out)\n        eps_factor = pad_like(eps_factor, xt)\n        recip_sqrt_alpha_t = pad_like(recip_sqrt_alpha_t, xt)\n        var = pad_like(var, xt)\n\n        x_next = (\n            recip_sqrt_alpha_t * (xt - eps_factor * eps_hat)\n            + nonzero_mask * var.sqrt() * torch.randn_like(eps_hat).to(model_out.device) * temperature\n        )\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n        \"\"\"Converts the data prediction to the estimated score function.\n\n        Args:\n            x_hat (Tensor): The predicted data point.\n            xt (Tensor): The current data point.\n            t (Tensor): The time step.\n\n        Returns:\n            The estimated score function.\n        \"\"\"\n        alpha = safe_index(self._forward_data_schedule, t - self.last_time_idx, x_hat.device)\n        beta = safe_index(self._forward_noise_schedule, t - self.last_time_idx, x_hat.device)\n        alpha = pad_like(alpha, x_hat)\n        beta = pad_like(beta, x_hat)\n        score = alpha * x_hat - xt\n        score = score / (beta * beta)\n        return score\n\n    def step_ddim(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        eta: Float = 0.0,\n        center: Bool = False,\n    ):\n        \"\"\"Do one step of DDIM sampling.\n\n        Args:\n            model_out (Tensor): output of the model\n            t (Tensor): current time step\n            xt (Tensor): current data point\n            mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n            eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n            center (Bool, optional): whether to center the data point. Defaults to False.\n        \"\"\"\n        if mask is not None:\n            model_out = model_out * mask.unsqueeze(-1)\n        data_pred = self.process_data_prediction(model_out, xt, t)\n        noise_pred = self.process_noise_prediction(model_out, xt, t)\n        eps = torch.randn_like(data_pred).to(model_out.device)\n        sigma = (\n            eta\n            * torch.sqrt((1 - self._alpha_bar_prev) / (1 - self._alpha_bar))\n            * torch.sqrt(1 - self._alpha_bar / self._alpha_bar_prev)\n        )\n        sigma_t = safe_index(sigma, t - self.last_time_idx, model_out.device)\n        psi_r = safe_index(torch.sqrt(self._alpha_bar_prev), t - self.last_time_idx, model_out.device)\n        omega_r = safe_index(torch.sqrt(1 - self._alpha_bar_prev - sigma**2), t - self.last_time_idx, model_out.device)\n        sigma_t = pad_like(sigma_t, model_out)\n        psi_r = pad_like(psi_r, model_out)\n        omega_r = pad_like(omega_r, model_out)\n        mean = data_pred * psi_r + omega_r * noise_pred\n        x_next = mean + sigma_t * eps\n        x_next = self.clean_mask_center(x_next, mask, center)\n        return x_next\n\n    def set_loss_weight_fn(self, fn):\n        \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n        Args:\n            fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n        \"\"\"\n        self.loss_weight = fn\n\n    def loss_weight(self, raw_loss: Tensor, t: Optional[Tensor], weight_type: str) -&gt; Tensor:\n        \"\"\"Calculates the weight for the loss based on the given weight type.\n\n        These data_to_noise loss weights is derived in Equation (9) of https://arxiv.org/pdf/2202.00512.\n\n        Args:\n            raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n            t (Tensor): The time step.\n            weight_type (str): The type of weight to use. Can be \"ones\" or \"data_to_noise\" or \"noise_to_data\".\n\n        Returns:\n            Tensor: The weight for the loss.\n\n        Raises:\n            ValueError: If the weight type is not recognized.\n        \"\"\"\n        if weight_type == \"ones\":\n            schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n        elif weight_type == \"data_to_noise\":\n            if t is None:\n                raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n            schedule = (safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n                safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n            )\n            schedule = pad_like(schedule, raw_loss)\n        elif weight_type == \"noise_to_data\":\n            if t is None:\n                raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n            schedule = (safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n                safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n            )\n            schedule = pad_like(schedule, raw_loss)\n        else:\n            raise ValueError(\"Invalid loss weight keyword\")\n        return schedule\n\n    def loss(\n        self,\n        model_pred: Tensor,\n        target: Tensor,\n        t: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        weight_type: Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"] = \"ones\",\n    ):\n        \"\"\"Calculate the loss given the model prediction, data sample, and time.\n\n        The default weight_type is \"ones\" meaning no change / multiplying by all ones.\n        data_to_noise is available to scale the data MSE loss into the appropriate loss that is theoretically equivalent\n        to noise prediction. noise_to_data is provided for a similar reason for completeness.\n\n        Args:\n            model_pred (Tensor): The predicted output from the model.\n            target (Tensor): The target output for the model prediction.\n            t (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            weight_type (Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"]): The type of weight to use for the loss. Defaults to \"ones\".\n\n        Returns:\n            Tensor: The calculated loss batch tensor.\n        \"\"\"\n        raw_loss = self._loss_function(model_pred, target)\n        if weight_type != \"ones\":\n            update_weight = self.loss_weight(raw_loss, t, weight_type)\n            loss = raw_loss * update_weight\n        else:\n            loss = raw_loss\n        if mask is not None:\n            loss = loss * mask.unsqueeze(-1)\n            n_elem = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n        else:\n            loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n        return loss\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.alpha_bar","title":"<code>alpha_bar</code>  <code>property</code>","text":"<p>Returns the alpha bar values.</p>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.alpha_bar_prev","title":"<code>alpha_bar_prev</code>  <code>property</code>","text":"<p>Returns the previous alpha bar values.</p>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.forward_data_schedule","title":"<code>forward_data_schedule</code>  <code>property</code>","text":"<p>Returns the forward data schedule.</p>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.forward_noise_schedule","title":"<code>forward_noise_schedule</code>  <code>property</code>","text":"<p>Returns the forward noise schedule.</p>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.log_var","title":"<code>log_var</code>  <code>property</code>","text":"<p>Returns the log variance.</p>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.reverse_data_schedule","title":"<code>reverse_data_schedule</code>  <code>property</code>","text":"<p>Returns the reverse data schedule.</p>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.reverse_noise_schedule","title":"<code>reverse_noise_schedule</code>  <code>property</code>","text":"<p>Returns the reverse noise schedule.</p>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, prediction_type=PredictionType.DATA, device='cpu', last_time_idx=0, rng_generator=None)</code>","text":"<p>Initializes the DDPM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>noise_schedule</code> <code>DiscreteNoiseSchedule</code> <p>The schedule of noise, defining the amount of noise added at each time step.</p> required <code>prediction_type</code> <code>PredictionType</code> <p>The type of prediction, either \"data\" or another type. Defaults to \"data\".</p> <code>DATA</code> <code>device</code> <code>str</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>last_time_idx</code> <code>int</code> <p>The last time index for discrete time. Set to 0 if discrete time is T-1, ..., 0 or 1 if T, ..., 1. Defaults to 0.</p> <code>0</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: PriorDistribution,\n    noise_schedule: DiscreteNoiseSchedule,\n    prediction_type: Union[PredictionType, str] = PredictionType.DATA,\n    device: Union[str, torch.device] = \"cpu\",\n    last_time_idx: int = 0,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the DDPM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n        prediction_type (PredictionType): The type of prediction, either \"data\" or another type. Defaults to \"data\".\n        device (str): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        last_time_idx (int, optional): The last time index for discrete time. Set to 0 if discrete time is T-1, ..., 0 or 1 if T, ..., 1. Defaults to 0.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    super().__init__(time_distribution, prior_distribution, device, rng_generator)\n    if not isinstance(prior_distribution, GaussianPrior):\n        warnings.warn(\"Prior distribution is not a GaussianPrior, unexpected behavior may occur\")\n    self.noise_schedule = noise_schedule\n    self._initialize_schedules(device)\n    self.prediction_type = string_to_enum(prediction_type, PredictionType)\n    self._loss_function = nn.MSELoss(reduction=\"none\")\n    self.last_time_idx = last_time_idx\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM._initialize_schedules","title":"<code>_initialize_schedules(device='cpu')</code>","text":"<p>Sets up the Denoising Diffusion Probabilistic Model (DDPM) equations.</p> <p>This method initializes the schedules for the forward and reverse processes of the DDPM. It calculates the alphas, betas, and log variances required for the diffusion process.</p> <p>Specifically, it computes:</p> <ul> <li><code>alpha_bar</code>: the cumulative product of <code>alpha_t</code></li> <li><code>alpha_bar_prev</code>: the previous cumulative product of <code>alpha_t</code></li> <li><code>posterior_variance</code>: the variance of the posterior distribution</li> <li><code>posterior_mean_c0_coef</code> and <code>posterior_mean_ct_coef</code>: the coefficients for the posterior mean</li> <li><code>log_var</code>: the log variance of the posterior distribution</li> </ul> <p>These values are then used to set up the forward and reverse schedules for the DDPM. Specifically this is equation (6) (7) from https://arxiv.org/pdf/2006.11239</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def _initialize_schedules(self, device: Union[str, torch.device] = \"cpu\"):\n    \"\"\"Sets up the Denoising Diffusion Probabilistic Model (DDPM) equations.\n\n    This method initializes the schedules for the forward and reverse processes of the DDPM. It calculates the\n    alphas, betas, and log variances required for the diffusion process.\n\n    Specifically, it computes:\n\n    * `alpha_bar`: the cumulative product of `alpha_t`\n    * `alpha_bar_prev`: the previous cumulative product of `alpha_t`\n    * `posterior_variance`: the variance of the posterior distribution\n    * `posterior_mean_c0_coef` and `posterior_mean_ct_coef`: the coefficients for the posterior mean\n    * `log_var`: the log variance of the posterior distribution\n\n    These values are then used to set up the forward and reverse schedules for the DDPM.\n    Specifically this is equation (6) (7) from https://arxiv.org/pdf/2006.11239\n    \"\"\"\n    if self.noise_schedule is None:\n        raise ValueError(\"noise_schedule cannot be None for DDPM\")\n    alphas = self.noise_schedule.generate_schedule(device=device)\n    betas = 1 - alphas\n    log_alpha = torch.log(alphas)\n    log_alpha_bar = torch.cumsum(log_alpha, dim=0)\n    alpha_bar = alphas_cumprod = torch.exp(log_alpha_bar)\n    alpha_bar_prev = alphas_cumprod_prev = torch.nn.functional.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n    posterior_variance = betas * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar)\n    posterior_mean_c0_coef = betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alpha_bar)\n    posterior_mean_ct_coef = (1.0 - alpha_bar_prev) * torch.sqrt(alphas) / (1.0 - alpha_bar)\n    # log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n    posterior_logvar = torch.log(\n        torch.nn.functional.pad(posterior_variance[:-1], (1, 0), value=posterior_variance[0].item())\n    )\n    self._forward_data_schedule = torch.sqrt(alpha_bar)\n    self._forward_noise_schedule = torch.sqrt(1 - alpha_bar)\n    self._reverse_data_schedule = posterior_mean_c0_coef\n    self._reverse_noise_schedule = posterior_mean_ct_coef\n    self._log_var = posterior_logvar\n    self._alpha_bar = alpha_bar\n    self._alpha_bar_prev = alpha_bar_prev\n    self._betas = betas\n    self._posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.calculate_velocity","title":"<code>calculate_velocity(data, t, noise)</code>","text":"<p>Calculate the velocity term given the data, time step, and noise.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>noise</code> <code>Tensor</code> <p>The noise term.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The calculated velocity term.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def calculate_velocity(self, data: Tensor, t: Tensor, noise: Tensor) -&gt; Tensor:\n    \"\"\"Calculate the velocity term given the data, time step, and noise.\n\n    Args:\n        data (Tensor): The input data.\n        t (Tensor): The current time step.\n        noise (Tensor): The noise term.\n\n    Returns:\n        Tensor: The calculated velocity term.\n    \"\"\"\n    data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n    noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n    data_scale = pad_like(data_scale, data)\n    noise_scale = pad_like(noise_scale, data)\n    v = data_scale * noise - noise_scale * data\n    return v\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.forward_process","title":"<code>forward_process(data, t, noise=None)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior(). Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor, noise: Optional[Tensor] = None):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor, optional): noise from prior(). Defaults to None.\n    \"\"\"\n    if noise is None:\n        noise = self.sample_prior(data.shape)\n    return self.interpolate(data, t, noise)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.interpolate","title":"<code>interpolate(data, t, noise)</code>","text":"<p>Get x(t) with given time t from noise and data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <code>noise</code> <code>Tensor</code> <p>noise from prior()</p> required Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor, noise: Tensor):\n    \"\"\"Get x(t) with given time t from noise and data.\n\n    Args:\n        data (Tensor): target\n        t (Tensor): time\n        noise (Tensor): noise from prior()\n    \"\"\"\n    psi = safe_index(self._forward_data_schedule, t - self.last_time_idx, data.device)\n    omega = safe_index(self._forward_noise_schedule, t - self.last_time_idx, data.device)\n    psi = pad_like(psi, data)\n    omega = pad_like(omega, data)\n    x_t = data * psi + noise * omega\n    return x_t\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.loss","title":"<code>loss(model_pred, target, t=None, mask=None, weight_type='ones')</code>","text":"<p>Calculate the loss given the model prediction, data sample, and time.</p> <p>The default weight_type is \"ones\" meaning no change / multiplying by all ones. data_to_noise is available to scale the data MSE loss into the appropriate loss that is theoretically equivalent to noise prediction. noise_to_data is provided for a similar reason for completeness.</p> <p>Parameters:</p> Name Type Description Default <code>model_pred</code> <code>Tensor</code> <p>The predicted output from the model.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction.</p> required <code>t</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> <code>None</code> <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>weight_type</code> <code>Literal['ones', 'data_to_noise', 'noise_to_data']</code> <p>The type of weight to use for the loss. Defaults to \"ones\".</p> <code>'ones'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss batch tensor.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def loss(\n    self,\n    model_pred: Tensor,\n    target: Tensor,\n    t: Optional[Tensor] = None,\n    mask: Optional[Tensor] = None,\n    weight_type: Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"] = \"ones\",\n):\n    \"\"\"Calculate the loss given the model prediction, data sample, and time.\n\n    The default weight_type is \"ones\" meaning no change / multiplying by all ones.\n    data_to_noise is available to scale the data MSE loss into the appropriate loss that is theoretically equivalent\n    to noise prediction. noise_to_data is provided for a similar reason for completeness.\n\n    Args:\n        model_pred (Tensor): The predicted output from the model.\n        target (Tensor): The target output for the model prediction.\n        t (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        weight_type (Literal[\"ones\", \"data_to_noise\", \"noise_to_data\"]): The type of weight to use for the loss. Defaults to \"ones\".\n\n    Returns:\n        Tensor: The calculated loss batch tensor.\n    \"\"\"\n    raw_loss = self._loss_function(model_pred, target)\n    if weight_type != \"ones\":\n        update_weight = self.loss_weight(raw_loss, t, weight_type)\n        loss = raw_loss * update_weight\n    else:\n        loss = raw_loss\n    if mask is not None:\n        loss = loss * mask.unsqueeze(-1)\n        n_elem = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / n_elem\n    else:\n        loss = torch.sum(loss, dim=tuple(range(1, raw_loss.ndim))) / model_pred.size(1)\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.loss_weight","title":"<code>loss_weight(raw_loss, t, weight_type)</code>","text":"<p>Calculates the weight for the loss based on the given weight type.</p> <p>These data_to_noise loss weights is derived in Equation (9) of https://arxiv.org/pdf/2202.00512.</p> <p>Parameters:</p> Name Type Description Default <code>raw_loss</code> <code>Tensor</code> <p>The raw loss calculated from the model prediction and target.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <code>weight_type</code> <code>str</code> <p>The type of weight to use. Can be \"ones\" or \"data_to_noise\" or \"noise_to_data\".</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The weight for the loss.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the weight type is not recognized.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def loss_weight(self, raw_loss: Tensor, t: Optional[Tensor], weight_type: str) -&gt; Tensor:\n    \"\"\"Calculates the weight for the loss based on the given weight type.\n\n    These data_to_noise loss weights is derived in Equation (9) of https://arxiv.org/pdf/2202.00512.\n\n    Args:\n        raw_loss (Tensor): The raw loss calculated from the model prediction and target.\n        t (Tensor): The time step.\n        weight_type (str): The type of weight to use. Can be \"ones\" or \"data_to_noise\" or \"noise_to_data\".\n\n    Returns:\n        Tensor: The weight for the loss.\n\n    Raises:\n        ValueError: If the weight type is not recognized.\n    \"\"\"\n    if weight_type == \"ones\":\n        schedule = torch.ones_like(raw_loss).to(raw_loss.device)\n    elif weight_type == \"data_to_noise\":\n        if t is None:\n            raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n        schedule = (safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n            safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n        )\n        schedule = pad_like(schedule, raw_loss)\n    elif weight_type == \"noise_to_data\":\n        if t is None:\n            raise ValueError(\"Time cannot be None when using the data_to_noise loss weight\")\n        schedule = (safe_index(self._forward_noise_schedule, t - self.last_time_idx, raw_loss.device) ** 2) / (\n            safe_index(self._forward_data_schedule, t - self.last_time_idx, raw_loss.device) ** 2\n        )\n        schedule = pad_like(schedule, raw_loss)\n    else:\n        raise ValueError(\"Invalid loss weight keyword\")\n    return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.process_data_prediction","title":"<code>process_data_prediction(model_output, sample, t)</code>","text":"<p>Converts the model output to a data prediction based on the prediction type.</p> <p>This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512. Given the model output and the sample, we convert the output to a data prediction based on the prediction type. The conversion formulas are as follows: - For \"noise\" prediction type: <code>pred_data = (sample - noise_scale * model_output) / data_scale</code> - For \"data\" prediction type: <code>pred_data = model_output</code> - For \"v_prediction\" prediction type: <code>pred_data = data_scale * sample - noise_scale * model_output</code></p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Tensor</code> <p>The output of the model.</p> required <code>sample</code> <code>Tensor</code> <p>The input sample.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The data prediction based on the prediction type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def process_data_prediction(self, model_output: Tensor, sample: Tensor, t: Tensor):\n    \"\"\"Converts the model output to a data prediction based on the prediction type.\n\n    This conversion stems from the Progressive Distillation for Fast Sampling of Diffusion Models https://arxiv.org/pdf/2202.00512.\n    Given the model output and the sample, we convert the output to a data prediction based on the prediction type.\n    The conversion formulas are as follows:\n    - For \"noise\" prediction type: `pred_data = (sample - noise_scale * model_output) / data_scale`\n    - For \"data\" prediction type: `pred_data = model_output`\n    - For \"v_prediction\" prediction type: `pred_data = data_scale * sample - noise_scale * model_output`\n\n    Args:\n        model_output (Tensor): The output of the model.\n        sample (Tensor): The input sample.\n        t (Tensor): The time step.\n\n    Returns:\n        The data prediction based on the prediction type.\n\n    Raises:\n        ValueError: If the prediction type is not one of \"noise\", \"data\", or \"v_prediction\".\n    \"\"\"\n    data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n    noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_data = (sample - noise_scale * model_output) / data_scale\n    elif self.prediction_type == PredictionType.DATA:\n        pred_data = model_output\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of PredictionType.NOISE, PredictionType.DATA or\"\n            f\" PredictionType.VELOCITY for DDPM.\"\n        )\n    return pred_data\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.process_noise_prediction","title":"<code>process_noise_prediction(model_output, sample, t)</code>","text":"<p>Do the same as process_data_prediction but take the model output and convert to nosie.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <p>The output of the model.</p> required <code>sample</code> <p>The input sample.</p> required <code>t</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The input as noise if the prediction type is \"noise\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prediction type is not \"noise\".</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def process_noise_prediction(self, model_output, sample, t):\n    \"\"\"Do the same as process_data_prediction but take the model output and convert to nosie.\n\n    Args:\n        model_output: The output of the model.\n        sample: The input sample.\n        t: The time step.\n\n    Returns:\n        The input as noise if the prediction type is \"noise\".\n\n    Raises:\n        ValueError: If the prediction type is not \"noise\".\n    \"\"\"\n    data_scale = safe_index(self._forward_data_schedule, t - self.last_time_idx, model_output.device)\n    noise_scale = safe_index(self._forward_noise_schedule, t - self.last_time_idx, model_output.device)\n    data_scale = pad_like(data_scale, model_output)\n    noise_scale = pad_like(noise_scale, model_output)\n    if self.prediction_type == PredictionType.NOISE:\n        pred_noise = model_output\n    elif self.prediction_type == PredictionType.DATA:\n        pred_noise = (sample - data_scale * model_output) / noise_scale\n    elif self.prediction_type == PredictionType.VELOCITY:\n        pred_data = data_scale * sample - noise_scale * model_output\n        pred_noise = (sample - data_scale * pred_data) / noise_scale\n    else:\n        raise ValueError(\n            f\"prediction_type given as {self.prediction_type} must be one of `noise`, `data` or\"\n            \" `v_prediction`  for DDPM.\"\n        )\n    return pred_noise\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.score","title":"<code>score(x_hat, xt, t)</code>","text":"<p>Converts the data prediction to the estimated score function.</p> <p>Parameters:</p> Name Type Description Default <code>x_hat</code> <code>Tensor</code> <p>The predicted data point.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>t</code> <code>Tensor</code> <p>The time step.</p> required <p>Returns:</p> Type Description <p>The estimated score function.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def score(self, x_hat: Tensor, xt: Tensor, t: Tensor):\n    \"\"\"Converts the data prediction to the estimated score function.\n\n    Args:\n        x_hat (Tensor): The predicted data point.\n        xt (Tensor): The current data point.\n        t (Tensor): The time step.\n\n    Returns:\n        The estimated score function.\n    \"\"\"\n    alpha = safe_index(self._forward_data_schedule, t - self.last_time_idx, x_hat.device)\n    beta = safe_index(self._forward_noise_schedule, t - self.last_time_idx, x_hat.device)\n    alpha = pad_like(alpha, x_hat)\n    beta = pad_like(beta, x_hat)\n    score = alpha * x_hat - xt\n    score = score / (beta * beta)\n    return score\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.set_loss_weight_fn","title":"<code>set_loss_weight_fn(fn)</code>","text":"<p>Sets the loss_weight attribute of the instance to the given function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <p>The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.</p> required Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def set_loss_weight_fn(self, fn):\n    \"\"\"Sets the loss_weight attribute of the instance to the given function.\n\n    Args:\n        fn: The function to set as the loss_weight attribute. This function should take three arguments: raw_loss, t, and weight_type.\n    \"\"\"\n    self.loss_weight = fn\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.step","title":"<code>step(model_out, t, xt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration.</p> <p>Args: model_out (Tensor): The output of the model. t (Tensor): The current time step. xt (Tensor): The current data point. mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None. center (bool, optional): Whether to center the data. Defaults to False. temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <p>Note: The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.</p> <p>Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask. For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>@torch.no_grad()\ndef step(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n):\n    \"\"\"Do one step integration.\n\n    Args:\n    model_out (Tensor): The output of the model.\n    t (Tensor): The current time step.\n    xt (Tensor): The current data point.\n    mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n    center (bool, optional): Whether to center the data. Defaults to False.\n    temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n    Note:\n    The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.\n\n    Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n    For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    x_hat = self.process_data_prediction(model_out, xt, t)\n    psi_r = safe_index(self._reverse_data_schedule, t - self.last_time_idx, x_hat.device)\n    omega_r = safe_index(self._reverse_noise_schedule, t - self.last_time_idx, x_hat.device)\n    log_var = safe_index(self._log_var, t - self.last_time_idx, x_hat.device)  # self._log_var[t.long()]\n    nonzero_mask = (t &gt; self.last_time_idx).float()\n    psi_r = pad_like(psi_r, x_hat)\n    omega_r = pad_like(omega_r, x_hat)\n    log_var = pad_like(log_var, x_hat)\n    nonzero_mask = pad_like(nonzero_mask, x_hat)\n\n    mean = psi_r * x_hat + omega_r * xt\n    eps = torch.randn_like(mean).to(model_out.device)\n\n    x_next = mean + nonzero_mask * (0.5 * log_var).exp() * eps * temperature\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.step_ddim","title":"<code>step_ddim(model_out, t, xt, mask=None, eta=0.0, center=False)</code>","text":"<p>Do one step of DDIM sampling.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>output of the model</p> required <code>t</code> <code>Tensor</code> <p>current time step</p> required <code>xt</code> <code>Tensor</code> <p>current data point</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>mask for the data point. Defaults to None.</p> <code>None</code> <code>eta</code> <code>Float</code> <p>DDIM sampling parameter. Defaults to 0.0.</p> <code>0.0</code> <code>center</code> <code>Bool</code> <p>whether to center the data point. Defaults to False.</p> <code>False</code> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def step_ddim(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    eta: Float = 0.0,\n    center: Bool = False,\n):\n    \"\"\"Do one step of DDIM sampling.\n\n    Args:\n        model_out (Tensor): output of the model\n        t (Tensor): current time step\n        xt (Tensor): current data point\n        mask (Optional[Tensor], optional): mask for the data point. Defaults to None.\n        eta (Float, optional): DDIM sampling parameter. Defaults to 0.0.\n        center (Bool, optional): whether to center the data point. Defaults to False.\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    data_pred = self.process_data_prediction(model_out, xt, t)\n    noise_pred = self.process_noise_prediction(model_out, xt, t)\n    eps = torch.randn_like(data_pred).to(model_out.device)\n    sigma = (\n        eta\n        * torch.sqrt((1 - self._alpha_bar_prev) / (1 - self._alpha_bar))\n        * torch.sqrt(1 - self._alpha_bar / self._alpha_bar_prev)\n    )\n    sigma_t = safe_index(sigma, t - self.last_time_idx, model_out.device)\n    psi_r = safe_index(torch.sqrt(self._alpha_bar_prev), t - self.last_time_idx, model_out.device)\n    omega_r = safe_index(torch.sqrt(1 - self._alpha_bar_prev - sigma**2), t - self.last_time_idx, model_out.device)\n    sigma_t = pad_like(sigma_t, model_out)\n    psi_r = pad_like(psi_r, model_out)\n    omega_r = pad_like(omega_r, model_out)\n    mean = data_pred * psi_r + omega_r * noise_pred\n    x_next = mean + sigma_t * eps\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/continuous/ddpm/#bionemo.moco.interpolants.discrete_time.continuous.ddpm.DDPM.step_noise","title":"<code>step_noise(model_out, t, xt, mask=None, center=False, temperature=1.0)</code>","text":"<p>Do one step integration.</p> <p>Args: model_out (Tensor): The output of the model. t (Tensor): The current time step. xt (Tensor): The current data point. mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None. center (bool, optional): Whether to center the data. Defaults to False. temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.</p> <p>Note: The temperature parameter controls the level of randomness in the sampling process. A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2) result in less random and more deterministic samples. This can be useful for tasks that require more control over the generation process.</p> <p>Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask. For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0</p> Source code in <code>bionemo/moco/interpolants/discrete_time/continuous/ddpm.py</code> <pre><code>def step_noise(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    center: Bool = False,\n    temperature: Float = 1.0,\n):\n    \"\"\"Do one step integration.\n\n    Args:\n    model_out (Tensor): The output of the model.\n    t (Tensor): The current time step.\n    xt (Tensor): The current data point.\n    mask (Optional[Tensor], optional): An optional mask to apply to the data. Defaults to None.\n    center (bool, optional): Whether to center the data. Defaults to False.\n    temperature (Float, optional): The temperature parameter for low temperature sampling. Defaults to 1.0.\n\n    Note:\n    The temperature parameter controls the level of randomness in the sampling process.\n    A temperature of 1.0 corresponds to standard diffusion sampling, while lower temperatures (e.g. 0.5, 0.2)\n    result in less random and more deterministic samples. This can be useful for tasks\n    that require more control over the generation process.\n\n    Note for discrete time we sample from [T-1, ..., 1, 0] for T steps so we sample t = 0 hence the mask.\n    For continuous time we start from [1, 1 -dt, ..., dt] for T steps where s = t - 1 when t = 0 i.e dt is then 0\n\n    \"\"\"\n    if mask is not None:\n        model_out = model_out * mask.unsqueeze(-1)\n    eps_hat = self.process_noise_prediction(model_out, xt, t)\n    beta_t = safe_index(self._betas, t - self.last_time_idx, model_out.device)\n    recip_sqrt_alpha_t = torch.sqrt(1 / (1 - beta_t))\n    eps_factor = (\n        safe_index(self._betas, t - self.last_time_idx, model_out.device)\n        / (1 - safe_index(self._alpha_bar, t - self.last_time_idx, model_out.device)).sqrt()\n    )\n    var = safe_index(self._posterior_variance, t - self.last_time_idx, model_out.device)  # self._log_var[t.long()]\n\n    nonzero_mask = (t &gt; self.last_time_idx).float()\n    nonzero_mask = pad_like(nonzero_mask, model_out)\n    eps_factor = pad_like(eps_factor, xt)\n    recip_sqrt_alpha_t = pad_like(recip_sqrt_alpha_t, xt)\n    var = pad_like(var, xt)\n\n    x_next = (\n        recip_sqrt_alpha_t * (xt - eps_factor * eps_hat)\n        + nonzero_mask * var.sqrt() * torch.randn_like(eps_hat).to(model_out.device) * temperature\n    )\n    x_next = self.clean_mask_center(x_next, mask, center)\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/","title":"D3pm","text":""},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM","title":"<code>D3PM</code>","text":"<p>               Bases: <code>Interpolant</code></p> <p>A Discrete Denoising Diffusion Probabilistic Model (D3PM) interpolant.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>class D3PM(Interpolant):\n    \"\"\"A Discrete Denoising Diffusion Probabilistic Model (D3PM) interpolant.\"\"\"\n\n    def __init__(\n        self,\n        time_distribution: TimeDistribution,\n        prior_distribution: DiscretePriorDistribution,\n        noise_schedule: DiscreteNoiseSchedule,\n        device: str = \"cpu\",\n        last_time_idx: int = 0,\n        rng_generator: Optional[torch.Generator] = None,\n    ):\n        \"\"\"Initializes the D3PM interpolant.\n\n        Args:\n            time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n            prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n            noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n            device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n            last_time_idx (int, optional): The last time index to consider in the interpolation process. Defaults to 0.\n            rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n        \"\"\"\n        # We initialize with CPU due to numerical precision issues on A100 that are not observed on A6000\n        super().__init__(time_distribution, prior_distribution, \"cpu\", rng_generator)\n        self.noise_schedule = noise_schedule\n        self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n        self.timesteps = noise_schedule.nsteps\n        self.num_classes = prior_distribution.num_classes\n        self.terminal_distribution = prior_distribution.prior_dist.to(self.device)\n        self._initialize_schedules(self.device)\n        self.last_time_idx = last_time_idx\n        self.to_device(device)\n\n    def _get_Qt(self, alphas: Tensor) -&gt; Tensor:\n        \"\"\"Calculate the transition matrix Qt based on the terminal distribution.\n\n        The transition matrix Qt represents the probabilities of transitioning from one state to another at a given time step.\n        It is calculated based on the terminal distribution, which can be either uniform, a mask, or a custom distribution.\n        See Appendix A.2 D3PM https://arxiv.org/pdf/2107.03006 which shows what happens for various prior distributions.\n\n        The terminal distribution can be:\n        - Uniform: a uniform distribution over all states.\n        - Mask: a mask where the last dimension is 1 and the rest are 0.\n        - Custom: a custom distribution provided by the user.\n\n        Args:\n            alphas (Tensor): A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.\n\n        Returns:\n            Tensor: The transition matrix Qt.\n        \"\"\"\n        QT = []\n        for alpha_t in alphas:\n            stay_prob = torch.eye(len(self.terminal_distribution), device=self.device) * alpha_t\n            diffuse_prob = (1.0 - alpha_t) * (\n                torch.ones(1, len(self.terminal_distribution), device=self.device)\n                * (self.terminal_distribution.unsqueeze(0))\n            )\n            QT.append(stay_prob + diffuse_prob)\n        return torch.stack(QT, dim=0)\n\n    def _calculate_transition_matrix(self, alphas: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Calculates the rate transition matrix `Qt`, its cumulative variant `Qt_bar`, and the cumulative variant of the previous time step `Qt_bar_prev`.\n\n        Args:\n            alphas (Tensor): A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.\n\n        Returns:\n            Tuple[Tensor, Tensor, Tensor]: A tuple containing the rate transition matrix `Qt`, its cumulative variant `Qt_bar`, and the cumulative variant of the previous time step `Qt_bar_prev`.\n        \"\"\"\n        Qt = self._get_Qt(alphas)\n        Qt_prev = torch.eye(self.num_classes, device=self.device)\n        Qt_bar = []\n        for i in range(len(alphas)):\n            Qtb = Qt_prev @ Qt[i]\n            if torch.any((Qtb.sum(-1) - 1.0).abs() &gt; 1e-4):\n                raise ValueError(f\"Invalid Distribution for Qt_bar at step {i}\")\n            Qt_bar.append(Qtb)\n            Qt_prev = Qtb\n        Qt_bar = torch.stack(Qt_bar)\n        Qt_bar_prev = Qt_bar[:-1]\n        Qt_prev_pad = torch.eye(self.num_classes, device=self.device)\n        Qt_bar_prev = torch.concat([Qt_prev_pad.unsqueeze(0), Qt_bar_prev], dim=0)\n        return Qt, Qt_bar, Qt_bar_prev\n\n    def _initialize_schedules(self, device):\n        \"\"\"Initializes the transition matrices for the discrete diffusion process.\n\n        This method computes the rate transition matrix `Qt` and its cumulative variants `Qt_bar` and `Qt_prev_bar`\n        based on the provided noise schedule.\n\n        Note:\n            `Qt` represents the rate transition matrix, where `Qt[t]` is the transition matrix at time step `t`.\n            `Qt_bar` and `Qt_prev_bar` are the cumulative variants of `Qt`, where `Qt_bar[t]` represents the cumulative\n            transition matrix from time step `0` to `t`, and `Qt_prev_bar[t]` represents the cumulative transition matrix\n            from time step `0` to `t-1`.\n\n        Args:\n            device (str): The device on which to compute the transition matrices.\n        \"\"\"\n        if self.noise_schedule is None:\n            raise ValueError(\"noise_schedule cannot be None for D3PM\")\n        alphas = self.noise_schedule.generate_schedule(device=device)\n        log_alpha = torch.log(alphas)\n        log_alpha_bar = torch.cumsum(log_alpha, dim=0)\n        self._alpha_bar = torch.exp(log_alpha_bar)\n        #! Note to users that the tranditional cosine schedule is a very quick convergence of alpha. Pay close attention to the scheduler here\n        Qt, Qt_bar, Qt_prev_bar = self._calculate_transition_matrix(alphas)\n        self._Qt = Qt[-self.timesteps :]\n        self._Qt_transposed = self._Qt.transpose(1, 2)\n        self._Qt_bar = Qt_bar[-self.timesteps :]\n        self._Qt_prev_bar = Qt_prev_bar[-self.timesteps :]\n\n    def interpolate(self, data: Tensor, t: Tensor):\n        \"\"\"Interpolate using discrete interpolation method.\n\n        This method implements Equation 2 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which\n        calculates the interpolated discrete state `xt` at time `t` given the input data and noise\n        via q(xt|x0) = Cat(xt; p = x0*Qt_bar).\n\n        Args:\n            data (Tensor): The input data to be interpolated.\n            t (Tensor): The time step at which to interpolate.\n\n        Returns:\n            Tensor: The interpolated discrete state `xt` at time `t`.\n        \"\"\"\n        if not _is_one_hot(data, self.num_classes):\n            x1_hot = F.one_hot(data, self.num_classes)\n        else:\n            x1_hot = data\n        ford = safe_index(self._Qt_bar, t - self.last_time_idx, data.device)\n        if x1_hot.ndim &gt; 3:  # einsum precision issues on A100 not A6000 for 2D inputs\n            ford_prep = ford\n            for _ in range(x1_hot.ndim - 2):\n                ford_prep = ford_prep.unsqueeze(1)\n            probs = (x1_hot.float().unsqueeze(-2) * ford_prep).sum(dim=(-2))\n        else:\n            probs = torch.einsum(\"b...j, bji -&gt; b...i\", [x1_hot.float(), ford])\n        if torch.any((probs.sum(-1) - 1.0).abs() &gt; 1e-4):\n            raise ValueError(\n                f\"**INVALID BEHAVIOR** Probability Distribution does not sum to 1.0 for time {t}. \"\n                f\"**INVESTIGATE YOUR DEVICE PRECISION**: This error has been triggered before on A100 by initializing the Qt terms on gpu. \"\n                f\"Normalized to ensure validity. Original sums: {probs.sum(-1)}\",\n            )\n        xt = self._sample_categorical(torch.log(probs) + 1.0e-6)\n        return xt\n\n    def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n        \"\"\"Apply the forward process to the data at time t.\n\n        Args:\n            data (Tensor): target discrete ids\n            t (Tensor): time\n\n        Returns:\n            Tensor: x(t) after applying the forward process\n        \"\"\"\n        return self.interpolate(data, t)\n\n    def _sample_categorical(self, logits, mask: Optional[Tensor] = None, temperature: Float = 1.0) -&gt; Tensor:\n        \"\"\"Sample a categorical distribution using the Gumbel-Softmax trick.\n\n        This method samples a categorical distribution from the given logits,\n        optionally applying a mask and using a specified temperature.\n\n        Args:\n            logits (Tensor): The logits of the categorical distribution.\n            mask (Optional[Tensor], optional): An optional mask to apply to the noise added to logits. Defaults to None.\n            temperature (float, optional): The temperature to use for the Gumbel-Softmax trick. Defaults to 1.0.\n\n        Returns:\n            Tensor: A sample from the categorical distribution.\n        \"\"\"\n        noise = torch.rand_like(logits)\n        noise = torch.clip(noise, 1.0e-6, 1.0)\n        gumbel_noise = -torch.log(-torch.log(noise))\n        if mask is not None:\n            sample = torch.argmax((logits / temperature) + gumbel_noise * mask, dim=-1)\n        else:\n            sample = torch.argmax((logits / temperature) + gumbel_noise, dim=-1)\n        return sample\n\n    def _q_posterior_logits(\n        self, model_out: Tensor, t: Tensor, xt: Tensor, model_out_is_logits: bool = True\n    ) -&gt; Tensor:\n        \"\"\"Calculate the q-posterior logits using the predicted x0 and the current state xt at time t.\n\n        This method implements Equation 3 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which calculates the q-posterior\n        distribution over the previous state x0 given the current state xt and the model output.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step.\n            t (Tensor): The current time step.\n            xt (Tensor): The current discrete state at time t.\n            model_out_is_logits (bool, optional): A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.\n\n        Returns:\n            Tensor: The q-posterior logits.\n        \"\"\"\n        if not model_out_is_logits:  # model_out.dtype == torch.int64 or model_out.dtype == torch.int32:\n            # Convert model output to logits if it's a categorical distribution\n            x0_logits = torch.log(torch.nn.functional.one_hot(model_out, self.num_classes).float() + 1.0e-6)\n        else:\n            # Otherwise, assume model output is already logits\n            x0_logits = model_out.clone()\n\n        # Calculate xt_guess: the predicted probability of xt given x0 and t\n        xt_guess = torch.einsum(\n            \"b...j, bji -&gt; b...i\",\n            [\n                torch.nn.functional.one_hot(xt, self.num_classes).float(),\n                safe_index(self._Qt_transposed, t - self.last_time_idx, model_out.device),\n            ],\n        )\n\n        # Calculate softmaxed x0_logits\n        softmaxed = torch.softmax(x0_logits, dim=-1)  # bs, ..., num_classes\n\n        # Calculate x0_guess: the predicted probability of x0 given xt and t-1\n        x0_guess = torch.einsum(\n            \"b...c,bcd-&gt;b...d\",\n            softmaxed,\n            safe_index(self._Qt_prev_bar, t - self.last_time_idx, model_out.device),\n        )\n\n        # Calculate q-posterior logits\n        out = torch.log(xt_guess + 1.0e-6) + torch.log(x0_guess + 1.0e-6)\n        t_broadcast = t.reshape((t.shape[0], *[1] * (xt.dim())))\n        q_posterior_logits = torch.where(t_broadcast == self.last_time_idx, x0_logits, out)\n        return q_posterior_logits\n\n    def step(\n        self,\n        model_out: Tensor,\n        t: Tensor,\n        xt: Tensor,\n        mask: Optional[Tensor] = None,\n        temperature: Float = 1.0,\n        model_out_is_logits: bool = True,\n    ):\n        \"\"\"Perform a single step in the discrete interpolant method, transitioning from the current discrete state `xt` at time `t` to the next state.\n\n        This step involves:\n\n        1. Computing the predicted q-posterior logits using the model output `model_out` and the current state `xt` at time `t`.\n        2. Sampling the next state from the predicted q-posterior distribution using the Gumbel-Softmax trick.\n\n        Args:\n            model_out (Tensor): The output of the model at the current time step, which is used to compute the predicted q-posterior logits.\n            t (Tensor): The current time step, which is used to index into the transition matrices and compute the predicted q-posterior logits.\n            xt (Tensor): The current discrete state at time `t`, which is used to compute the predicted q-posterior logits and sample the next state.\n            mask (Optional[Tensor], optional): An optional mask to apply to the next state, which can be used to mask out certain tokens or regions. Defaults to None.\n            temperature (Float, optional): The temperature to use for the Gumbel-Softmax trick, which controls the randomness of the sampling process. Defaults to 1.0.\n            model_out_is_logits (bool, optional): A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.\n\n        Returns:\n            Tensor: The next discrete state at time `t-1`.\n        \"\"\"\n        pred_q_posterior_logits = self._q_posterior_logits(model_out, t, xt, model_out_is_logits)\n        nonzero_mask = (t != self.last_time_idx).to(xt.dtype).reshape(xt.shape[0], *([1] * (len(xt.shape))))\n        x_next = self._sample_categorical(pred_q_posterior_logits, nonzero_mask, temperature=temperature)\n        # # Apply mask if provided\n        if mask is not None:\n            x_next = x_next * mask\n        return x_next\n\n    def loss(\n        self,\n        logits: Tensor,\n        target: Tensor,\n        xt: Tensor,\n        time: Tensor,\n        mask: Optional[Tensor] = None,\n        vb_scale: Float = 0.0,\n    ):\n        \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n        The loss is calculated between the batch x node x class logits and the target batch x node. If a mask is provided, the loss is\n        calculated only for the non-masked elements. Additionally, if vb_scale is greater than 0, the variational lower bound loss is\n        calculated and added to the total loss.\n\n        Args:\n            logits (Tensor): The predicted output from the model, with shape batch x node x class.\n            target (Tensor): The target output for the model prediction, with shape batch x node.\n            xt (Tensor): The current data point.\n            time (Tensor): The time at which the loss is calculated.\n            mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n            vb_scale (Float, optional): The scale factor for the variational lower bound loss. Defaults to 0.0.\n\n        Returns:\n            Tensor: The calculated loss tensor. If aggregate is True, the loss and variational lower bound loss are aggregated and\n            returned as a single tensor. Otherwise, the loss and variational lower bound loss are returned as separate tensors.\n        \"\"\"\n        assert target.ndim + 1 == logits.ndim\n        loss = self._loss_function(logits.transpose(-1, 1), target.long())\n        if mask is not None:\n            loss = loss * mask\n            num_non_masked_elements = torch.sum(mask, dim=-1)\n            loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n        else:\n            loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n        if vb_scale &gt; 0:\n            target = F.one_hot(target, num_classes=self.num_classes).float()\n            true_q_posterior_logits = self._q_posterior_logits(target, time, xt)\n            pred_q_posterior_logits = self._q_posterior_logits(logits, time, xt)\n            vb_loss = self._variational_lower_bound(true_q_posterior_logits, pred_q_posterior_logits)\n            vb_loss = vb_scale * vb_loss\n        else:\n            vb_loss = 0\n        if vb_scale &gt; 0:\n            loss += vb_loss\n        return loss\n\n    def _variational_lower_bound(self, dist1: Tensor, dist2: Tensor) -&gt; Tensor:\n        \"\"\"Calculate the variational lower bound (VLB) between two distributions.\n\n        The VLB measures the difference between the true and approximate posterior distributions.\n        It is used to regularize the model and encourage it to produce more accurate predictions.\n\n        Args:\n            dist1 (Tensor): The true posterior distribution.\n            dist2 (Tensor): The approximate posterior distribution.\n\n        Returns:\n            Tensor: The variational lower bound loss.\n        \"\"\"\n        # Flatten dist1 and dist2 to simplify calculations\n        dist1 = dist1.flatten(start_dim=0, end_dim=-2)\n        dist2 = dist2.flatten(start_dim=0, end_dim=-2)\n\n        # Calculate the VLB\n        out = torch.softmax(dist1 + 1.0e-6, dim=-1) * (\n            torch.log_softmax(dist1 + 1.0e-6, dim=-1) - torch.log_softmax(dist2 + 1.0e-6, dim=-1)\n        )\n        # Return the mean of the VLB across all elements\n        return out.sum(dim=-1).mean()\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.__init__","title":"<code>__init__(time_distribution, prior_distribution, noise_schedule, device='cpu', last_time_idx=0, rng_generator=None)</code>","text":"<p>Initializes the D3PM interpolant.</p> <p>Parameters:</p> Name Type Description Default <code>time_distribution</code> <code>TimeDistribution</code> <p>The distribution of time steps, used to sample time points for the diffusion process.</p> required <code>prior_distribution</code> <code>PriorDistribution</code> <p>The prior distribution of the variable, used as the starting point for the diffusion process.</p> required <code>noise_schedule</code> <code>DiscreteNoiseSchedule</code> <p>The schedule of noise, defining the amount of noise added at each time step.</p> required <code>device</code> <code>str</code> <p>The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".</p> <code>'cpu'</code> <code>last_time_idx</code> <code>int</code> <p>The last time index to consider in the interpolation process. Defaults to 0.</p> <code>0</code> <code>rng_generator</code> <code>Optional[Generator]</code> <p>An optional :class:<code>torch.Generator</code> for reproducible sampling. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def __init__(\n    self,\n    time_distribution: TimeDistribution,\n    prior_distribution: DiscretePriorDistribution,\n    noise_schedule: DiscreteNoiseSchedule,\n    device: str = \"cpu\",\n    last_time_idx: int = 0,\n    rng_generator: Optional[torch.Generator] = None,\n):\n    \"\"\"Initializes the D3PM interpolant.\n\n    Args:\n        time_distribution (TimeDistribution): The distribution of time steps, used to sample time points for the diffusion process.\n        prior_distribution (PriorDistribution): The prior distribution of the variable, used as the starting point for the diffusion process.\n        noise_schedule (DiscreteNoiseSchedule): The schedule of noise, defining the amount of noise added at each time step.\n        device (str, optional): The device on which to run the interpolant, either \"cpu\" or a CUDA device (e.g. \"cuda:0\"). Defaults to \"cpu\".\n        last_time_idx (int, optional): The last time index to consider in the interpolation process. Defaults to 0.\n        rng_generator: An optional :class:`torch.Generator` for reproducible sampling. Defaults to None.\n    \"\"\"\n    # We initialize with CPU due to numerical precision issues on A100 that are not observed on A6000\n    super().__init__(time_distribution, prior_distribution, \"cpu\", rng_generator)\n    self.noise_schedule = noise_schedule\n    self._loss_function = nn.CrossEntropyLoss(reduction=\"none\")\n    self.timesteps = noise_schedule.nsteps\n    self.num_classes = prior_distribution.num_classes\n    self.terminal_distribution = prior_distribution.prior_dist.to(self.device)\n    self._initialize_schedules(self.device)\n    self.last_time_idx = last_time_idx\n    self.to_device(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM._calculate_transition_matrix","title":"<code>_calculate_transition_matrix(alphas)</code>","text":"<p>Calculates the rate transition matrix <code>Qt</code>, its cumulative variant <code>Qt_bar</code>, and the cumulative variant of the previous time step <code>Qt_bar_prev</code>.</p> <p>Parameters:</p> Name Type Description Default <code>alphas</code> <code>Tensor</code> <p>A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[Tensor, Tensor, Tensor]: A tuple containing the rate transition matrix <code>Qt</code>, its cumulative variant <code>Qt_bar</code>, and the cumulative variant of the previous time step <code>Qt_bar_prev</code>.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def _calculate_transition_matrix(self, alphas: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Calculates the rate transition matrix `Qt`, its cumulative variant `Qt_bar`, and the cumulative variant of the previous time step `Qt_bar_prev`.\n\n    Args:\n        alphas (Tensor): A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.\n\n    Returns:\n        Tuple[Tensor, Tensor, Tensor]: A tuple containing the rate transition matrix `Qt`, its cumulative variant `Qt_bar`, and the cumulative variant of the previous time step `Qt_bar_prev`.\n    \"\"\"\n    Qt = self._get_Qt(alphas)\n    Qt_prev = torch.eye(self.num_classes, device=self.device)\n    Qt_bar = []\n    for i in range(len(alphas)):\n        Qtb = Qt_prev @ Qt[i]\n        if torch.any((Qtb.sum(-1) - 1.0).abs() &gt; 1e-4):\n            raise ValueError(f\"Invalid Distribution for Qt_bar at step {i}\")\n        Qt_bar.append(Qtb)\n        Qt_prev = Qtb\n    Qt_bar = torch.stack(Qt_bar)\n    Qt_bar_prev = Qt_bar[:-1]\n    Qt_prev_pad = torch.eye(self.num_classes, device=self.device)\n    Qt_bar_prev = torch.concat([Qt_prev_pad.unsqueeze(0), Qt_bar_prev], dim=0)\n    return Qt, Qt_bar, Qt_bar_prev\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM._get_Qt","title":"<code>_get_Qt(alphas)</code>","text":"<p>Calculate the transition matrix Qt based on the terminal distribution.</p> <p>The transition matrix Qt represents the probabilities of transitioning from one state to another at a given time step. It is calculated based on the terminal distribution, which can be either uniform, a mask, or a custom distribution. See Appendix A.2 D3PM https://arxiv.org/pdf/2107.03006 which shows what happens for various prior distributions.</p> <p>The terminal distribution can be: - Uniform: a uniform distribution over all states. - Mask: a mask where the last dimension is 1 and the rest are 0. - Custom: a custom distribution provided by the user.</p> <p>Parameters:</p> Name Type Description Default <code>alphas</code> <code>Tensor</code> <p>A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The transition matrix Qt.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def _get_Qt(self, alphas: Tensor) -&gt; Tensor:\n    \"\"\"Calculate the transition matrix Qt based on the terminal distribution.\n\n    The transition matrix Qt represents the probabilities of transitioning from one state to another at a given time step.\n    It is calculated based on the terminal distribution, which can be either uniform, a mask, or a custom distribution.\n    See Appendix A.2 D3PM https://arxiv.org/pdf/2107.03006 which shows what happens for various prior distributions.\n\n    The terminal distribution can be:\n    - Uniform: a uniform distribution over all states.\n    - Mask: a mask where the last dimension is 1 and the rest are 0.\n    - Custom: a custom distribution provided by the user.\n\n    Args:\n        alphas (Tensor): A tensor of probabilities, where each alpha represents the probability of staying in a state at a given time step.\n\n    Returns:\n        Tensor: The transition matrix Qt.\n    \"\"\"\n    QT = []\n    for alpha_t in alphas:\n        stay_prob = torch.eye(len(self.terminal_distribution), device=self.device) * alpha_t\n        diffuse_prob = (1.0 - alpha_t) * (\n            torch.ones(1, len(self.terminal_distribution), device=self.device)\n            * (self.terminal_distribution.unsqueeze(0))\n        )\n        QT.append(stay_prob + diffuse_prob)\n    return torch.stack(QT, dim=0)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM._initialize_schedules","title":"<code>_initialize_schedules(device)</code>","text":"<p>Initializes the transition matrices for the discrete diffusion process.</p> <p>This method computes the rate transition matrix <code>Qt</code> and its cumulative variants <code>Qt_bar</code> and <code>Qt_prev_bar</code> based on the provided noise schedule.</p> Note <p><code>Qt</code> represents the rate transition matrix, where <code>Qt[t]</code> is the transition matrix at time step <code>t</code>. <code>Qt_bar</code> and <code>Qt_prev_bar</code> are the cumulative variants of <code>Qt</code>, where <code>Qt_bar[t]</code> represents the cumulative transition matrix from time step <code>0</code> to <code>t</code>, and <code>Qt_prev_bar[t]</code> represents the cumulative transition matrix from time step <code>0</code> to <code>t-1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device on which to compute the transition matrices.</p> required Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def _initialize_schedules(self, device):\n    \"\"\"Initializes the transition matrices for the discrete diffusion process.\n\n    This method computes the rate transition matrix `Qt` and its cumulative variants `Qt_bar` and `Qt_prev_bar`\n    based on the provided noise schedule.\n\n    Note:\n        `Qt` represents the rate transition matrix, where `Qt[t]` is the transition matrix at time step `t`.\n        `Qt_bar` and `Qt_prev_bar` are the cumulative variants of `Qt`, where `Qt_bar[t]` represents the cumulative\n        transition matrix from time step `0` to `t`, and `Qt_prev_bar[t]` represents the cumulative transition matrix\n        from time step `0` to `t-1`.\n\n    Args:\n        device (str): The device on which to compute the transition matrices.\n    \"\"\"\n    if self.noise_schedule is None:\n        raise ValueError(\"noise_schedule cannot be None for D3PM\")\n    alphas = self.noise_schedule.generate_schedule(device=device)\n    log_alpha = torch.log(alphas)\n    log_alpha_bar = torch.cumsum(log_alpha, dim=0)\n    self._alpha_bar = torch.exp(log_alpha_bar)\n    #! Note to users that the tranditional cosine schedule is a very quick convergence of alpha. Pay close attention to the scheduler here\n    Qt, Qt_bar, Qt_prev_bar = self._calculate_transition_matrix(alphas)\n    self._Qt = Qt[-self.timesteps :]\n    self._Qt_transposed = self._Qt.transpose(1, 2)\n    self._Qt_bar = Qt_bar[-self.timesteps :]\n    self._Qt_prev_bar = Qt_prev_bar[-self.timesteps :]\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM._q_posterior_logits","title":"<code>_q_posterior_logits(model_out, t, xt, model_out_is_logits=True)</code>","text":"<p>Calculate the q-posterior logits using the predicted x0 and the current state xt at time t.</p> <p>This method implements Equation 3 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which calculates the q-posterior distribution over the previous state x0 given the current state xt and the model output.</p> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model at the current time step.</p> required <code>t</code> <code>Tensor</code> <p>The current time step.</p> required <code>xt</code> <code>Tensor</code> <p>The current discrete state at time t.</p> required <code>model_out_is_logits</code> <code>bool</code> <p>A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The q-posterior logits.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def _q_posterior_logits(\n    self, model_out: Tensor, t: Tensor, xt: Tensor, model_out_is_logits: bool = True\n) -&gt; Tensor:\n    \"\"\"Calculate the q-posterior logits using the predicted x0 and the current state xt at time t.\n\n    This method implements Equation 3 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which calculates the q-posterior\n    distribution over the previous state x0 given the current state xt and the model output.\n\n    Args:\n        model_out (Tensor): The output of the model at the current time step.\n        t (Tensor): The current time step.\n        xt (Tensor): The current discrete state at time t.\n        model_out_is_logits (bool, optional): A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.\n\n    Returns:\n        Tensor: The q-posterior logits.\n    \"\"\"\n    if not model_out_is_logits:  # model_out.dtype == torch.int64 or model_out.dtype == torch.int32:\n        # Convert model output to logits if it's a categorical distribution\n        x0_logits = torch.log(torch.nn.functional.one_hot(model_out, self.num_classes).float() + 1.0e-6)\n    else:\n        # Otherwise, assume model output is already logits\n        x0_logits = model_out.clone()\n\n    # Calculate xt_guess: the predicted probability of xt given x0 and t\n    xt_guess = torch.einsum(\n        \"b...j, bji -&gt; b...i\",\n        [\n            torch.nn.functional.one_hot(xt, self.num_classes).float(),\n            safe_index(self._Qt_transposed, t - self.last_time_idx, model_out.device),\n        ],\n    )\n\n    # Calculate softmaxed x0_logits\n    softmaxed = torch.softmax(x0_logits, dim=-1)  # bs, ..., num_classes\n\n    # Calculate x0_guess: the predicted probability of x0 given xt and t-1\n    x0_guess = torch.einsum(\n        \"b...c,bcd-&gt;b...d\",\n        softmaxed,\n        safe_index(self._Qt_prev_bar, t - self.last_time_idx, model_out.device),\n    )\n\n    # Calculate q-posterior logits\n    out = torch.log(xt_guess + 1.0e-6) + torch.log(x0_guess + 1.0e-6)\n    t_broadcast = t.reshape((t.shape[0], *[1] * (xt.dim())))\n    q_posterior_logits = torch.where(t_broadcast == self.last_time_idx, x0_logits, out)\n    return q_posterior_logits\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM._sample_categorical","title":"<code>_sample_categorical(logits, mask=None, temperature=1.0)</code>","text":"<p>Sample a categorical distribution using the Gumbel-Softmax trick.</p> <p>This method samples a categorical distribution from the given logits, optionally applying a mask and using a specified temperature.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The logits of the categorical distribution.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the noise added to logits. Defaults to None.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to use for the Gumbel-Softmax trick. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A sample from the categorical distribution.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def _sample_categorical(self, logits, mask: Optional[Tensor] = None, temperature: Float = 1.0) -&gt; Tensor:\n    \"\"\"Sample a categorical distribution using the Gumbel-Softmax trick.\n\n    This method samples a categorical distribution from the given logits,\n    optionally applying a mask and using a specified temperature.\n\n    Args:\n        logits (Tensor): The logits of the categorical distribution.\n        mask (Optional[Tensor], optional): An optional mask to apply to the noise added to logits. Defaults to None.\n        temperature (float, optional): The temperature to use for the Gumbel-Softmax trick. Defaults to 1.0.\n\n    Returns:\n        Tensor: A sample from the categorical distribution.\n    \"\"\"\n    noise = torch.rand_like(logits)\n    noise = torch.clip(noise, 1.0e-6, 1.0)\n    gumbel_noise = -torch.log(-torch.log(noise))\n    if mask is not None:\n        sample = torch.argmax((logits / temperature) + gumbel_noise * mask, dim=-1)\n    else:\n        sample = torch.argmax((logits / temperature) + gumbel_noise, dim=-1)\n    return sample\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM._variational_lower_bound","title":"<code>_variational_lower_bound(dist1, dist2)</code>","text":"<p>Calculate the variational lower bound (VLB) between two distributions.</p> <p>The VLB measures the difference between the true and approximate posterior distributions. It is used to regularize the model and encourage it to produce more accurate predictions.</p> <p>Parameters:</p> Name Type Description Default <code>dist1</code> <code>Tensor</code> <p>The true posterior distribution.</p> required <code>dist2</code> <code>Tensor</code> <p>The approximate posterior distribution.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The variational lower bound loss.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def _variational_lower_bound(self, dist1: Tensor, dist2: Tensor) -&gt; Tensor:\n    \"\"\"Calculate the variational lower bound (VLB) between two distributions.\n\n    The VLB measures the difference between the true and approximate posterior distributions.\n    It is used to regularize the model and encourage it to produce more accurate predictions.\n\n    Args:\n        dist1 (Tensor): The true posterior distribution.\n        dist2 (Tensor): The approximate posterior distribution.\n\n    Returns:\n        Tensor: The variational lower bound loss.\n    \"\"\"\n    # Flatten dist1 and dist2 to simplify calculations\n    dist1 = dist1.flatten(start_dim=0, end_dim=-2)\n    dist2 = dist2.flatten(start_dim=0, end_dim=-2)\n\n    # Calculate the VLB\n    out = torch.softmax(dist1 + 1.0e-6, dim=-1) * (\n        torch.log_softmax(dist1 + 1.0e-6, dim=-1) - torch.log_softmax(dist2 + 1.0e-6, dim=-1)\n    )\n    # Return the mean of the VLB across all elements\n    return out.sum(dim=-1).mean()\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.forward_process","title":"<code>forward_process(data, t)</code>","text":"<p>Apply the forward process to the data at time t.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>target discrete ids</p> required <code>t</code> <code>Tensor</code> <p>time</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>x(t) after applying the forward process</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def forward_process(self, data: Tensor, t: Tensor) -&gt; Tensor:\n    \"\"\"Apply the forward process to the data at time t.\n\n    Args:\n        data (Tensor): target discrete ids\n        t (Tensor): time\n\n    Returns:\n        Tensor: x(t) after applying the forward process\n    \"\"\"\n    return self.interpolate(data, t)\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.interpolate","title":"<code>interpolate(data, t)</code>","text":"<p>Interpolate using discrete interpolation method.</p> <p>This method implements Equation 2 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which calculates the interpolated discrete state <code>xt</code> at time <code>t</code> given the input data and noise via q(xt|x0) = Cat(xt; p = x0*Qt_bar).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input data to be interpolated.</p> required <code>t</code> <code>Tensor</code> <p>The time step at which to interpolate.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The interpolated discrete state <code>xt</code> at time <code>t</code>.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def interpolate(self, data: Tensor, t: Tensor):\n    \"\"\"Interpolate using discrete interpolation method.\n\n    This method implements Equation 2 from the D3PM paper (https://arxiv.org/pdf/2107.03006), which\n    calculates the interpolated discrete state `xt` at time `t` given the input data and noise\n    via q(xt|x0) = Cat(xt; p = x0*Qt_bar).\n\n    Args:\n        data (Tensor): The input data to be interpolated.\n        t (Tensor): The time step at which to interpolate.\n\n    Returns:\n        Tensor: The interpolated discrete state `xt` at time `t`.\n    \"\"\"\n    if not _is_one_hot(data, self.num_classes):\n        x1_hot = F.one_hot(data, self.num_classes)\n    else:\n        x1_hot = data\n    ford = safe_index(self._Qt_bar, t - self.last_time_idx, data.device)\n    if x1_hot.ndim &gt; 3:  # einsum precision issues on A100 not A6000 for 2D inputs\n        ford_prep = ford\n        for _ in range(x1_hot.ndim - 2):\n            ford_prep = ford_prep.unsqueeze(1)\n        probs = (x1_hot.float().unsqueeze(-2) * ford_prep).sum(dim=(-2))\n    else:\n        probs = torch.einsum(\"b...j, bji -&gt; b...i\", [x1_hot.float(), ford])\n    if torch.any((probs.sum(-1) - 1.0).abs() &gt; 1e-4):\n        raise ValueError(\n            f\"**INVALID BEHAVIOR** Probability Distribution does not sum to 1.0 for time {t}. \"\n            f\"**INVESTIGATE YOUR DEVICE PRECISION**: This error has been triggered before on A100 by initializing the Qt terms on gpu. \"\n            f\"Normalized to ensure validity. Original sums: {probs.sum(-1)}\",\n        )\n    xt = self._sample_categorical(torch.log(probs) + 1.0e-6)\n    return xt\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.loss","title":"<code>loss(logits, target, xt, time, mask=None, vb_scale=0.0)</code>","text":"<p>Calculate the cross-entropy loss between the model prediction and the target output.</p> <p>The loss is calculated between the batch x node x class logits and the target batch x node. If a mask is provided, the loss is calculated only for the non-masked elements. Additionally, if vb_scale is greater than 0, the variational lower bound loss is calculated and added to the total loss.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted output from the model, with shape batch x node x class.</p> required <code>target</code> <code>Tensor</code> <p>The target output for the model prediction, with shape batch x node.</p> required <code>xt</code> <code>Tensor</code> <p>The current data point.</p> required <code>time</code> <code>Tensor</code> <p>The time at which the loss is calculated.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>The mask for the data point. Defaults to None.</p> <code>None</code> <code>vb_scale</code> <code>Float</code> <p>The scale factor for the variational lower bound loss. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The calculated loss tensor. If aggregate is True, the loss and variational lower bound loss are aggregated and</p> <p>returned as a single tensor. Otherwise, the loss and variational lower bound loss are returned as separate tensors.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def loss(\n    self,\n    logits: Tensor,\n    target: Tensor,\n    xt: Tensor,\n    time: Tensor,\n    mask: Optional[Tensor] = None,\n    vb_scale: Float = 0.0,\n):\n    \"\"\"Calculate the cross-entropy loss between the model prediction and the target output.\n\n    The loss is calculated between the batch x node x class logits and the target batch x node. If a mask is provided, the loss is\n    calculated only for the non-masked elements. Additionally, if vb_scale is greater than 0, the variational lower bound loss is\n    calculated and added to the total loss.\n\n    Args:\n        logits (Tensor): The predicted output from the model, with shape batch x node x class.\n        target (Tensor): The target output for the model prediction, with shape batch x node.\n        xt (Tensor): The current data point.\n        time (Tensor): The time at which the loss is calculated.\n        mask (Optional[Tensor], optional): The mask for the data point. Defaults to None.\n        vb_scale (Float, optional): The scale factor for the variational lower bound loss. Defaults to 0.0.\n\n    Returns:\n        Tensor: The calculated loss tensor. If aggregate is True, the loss and variational lower bound loss are aggregated and\n        returned as a single tensor. Otherwise, the loss and variational lower bound loss are returned as separate tensors.\n    \"\"\"\n    assert target.ndim + 1 == logits.ndim\n    loss = self._loss_function(logits.transpose(-1, 1), target.long())\n    if mask is not None:\n        loss = loss * mask\n        num_non_masked_elements = torch.sum(mask, dim=-1)\n        loss = torch.sum(loss, dim=(-1)) / num_non_masked_elements\n    else:\n        loss = torch.sum(loss, dim=(-1)) / logits.size(1)\n    if vb_scale &gt; 0:\n        target = F.one_hot(target, num_classes=self.num_classes).float()\n        true_q_posterior_logits = self._q_posterior_logits(target, time, xt)\n        pred_q_posterior_logits = self._q_posterior_logits(logits, time, xt)\n        vb_loss = self._variational_lower_bound(true_q_posterior_logits, pred_q_posterior_logits)\n        vb_loss = vb_scale * vb_loss\n    else:\n        vb_loss = 0\n    if vb_scale &gt; 0:\n        loss += vb_loss\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm.D3PM.step","title":"<code>step(model_out, t, xt, mask=None, temperature=1.0, model_out_is_logits=True)</code>","text":"<p>Perform a single step in the discrete interpolant method, transitioning from the current discrete state <code>xt</code> at time <code>t</code> to the next state.</p> <p>This step involves:</p> <ol> <li>Computing the predicted q-posterior logits using the model output <code>model_out</code> and the current state <code>xt</code> at time <code>t</code>.</li> <li>Sampling the next state from the predicted q-posterior distribution using the Gumbel-Softmax trick.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>model_out</code> <code>Tensor</code> <p>The output of the model at the current time step, which is used to compute the predicted q-posterior logits.</p> required <code>t</code> <code>Tensor</code> <p>The current time step, which is used to index into the transition matrices and compute the predicted q-posterior logits.</p> required <code>xt</code> <code>Tensor</code> <p>The current discrete state at time <code>t</code>, which is used to compute the predicted q-posterior logits and sample the next state.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>An optional mask to apply to the next state, which can be used to mask out certain tokens or regions. Defaults to None.</p> <code>None</code> <code>temperature</code> <code>Float</code> <p>The temperature to use for the Gumbel-Softmax trick, which controls the randomness of the sampling process. Defaults to 1.0.</p> <code>1.0</code> <code>model_out_is_logits</code> <code>bool</code> <p>A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The next discrete state at time <code>t-1</code>.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def step(\n    self,\n    model_out: Tensor,\n    t: Tensor,\n    xt: Tensor,\n    mask: Optional[Tensor] = None,\n    temperature: Float = 1.0,\n    model_out_is_logits: bool = True,\n):\n    \"\"\"Perform a single step in the discrete interpolant method, transitioning from the current discrete state `xt` at time `t` to the next state.\n\n    This step involves:\n\n    1. Computing the predicted q-posterior logits using the model output `model_out` and the current state `xt` at time `t`.\n    2. Sampling the next state from the predicted q-posterior distribution using the Gumbel-Softmax trick.\n\n    Args:\n        model_out (Tensor): The output of the model at the current time step, which is used to compute the predicted q-posterior logits.\n        t (Tensor): The current time step, which is used to index into the transition matrices and compute the predicted q-posterior logits.\n        xt (Tensor): The current discrete state at time `t`, which is used to compute the predicted q-posterior logits and sample the next state.\n        mask (Optional[Tensor], optional): An optional mask to apply to the next state, which can be used to mask out certain tokens or regions. Defaults to None.\n        temperature (Float, optional): The temperature to use for the Gumbel-Softmax trick, which controls the randomness of the sampling process. Defaults to 1.0.\n        model_out_is_logits (bool, optional): A flag indicating whether the model output is already in logits form. If True, the output is assumed to be logits; otherwise, it is converted to logits. Defaults to True.\n\n    Returns:\n        Tensor: The next discrete state at time `t-1`.\n    \"\"\"\n    pred_q_posterior_logits = self._q_posterior_logits(model_out, t, xt, model_out_is_logits)\n    nonzero_mask = (t != self.last_time_idx).to(xt.dtype).reshape(xt.shape[0], *([1] * (len(xt.shape))))\n    x_next = self._sample_categorical(pred_q_posterior_logits, nonzero_mask, temperature=temperature)\n    # # Apply mask if provided\n    if mask is not None:\n        x_next = x_next * mask\n    return x_next\n</code></pre>"},{"location":"API_reference/bionemo/moco/interpolants/discrete_time/discrete/d3pm/#bionemo.moco.interpolants.discrete_time.discrete.d3pm._is_one_hot","title":"<code>_is_one_hot(data, num_classes)</code>","text":"<p>Check if data is one-hot encoded.</p> <p>Parameters: - data (Tensor): Input data to check. - num_classes (int): Expected number of classes for one-hot encoding.</p> <p>Returns: - bool: True if data is one-hot encoded, False otherwise.</p> Source code in <code>bionemo/moco/interpolants/discrete_time/discrete/d3pm.py</code> <pre><code>def _is_one_hot(data, num_classes):\n    \"\"\"Check if data is one-hot encoded.\n\n    Parameters:\n    - data (Tensor): Input data to check.\n    - num_classes (int): Expected number of classes for one-hot encoding.\n\n    Returns:\n    - bool: True if data is one-hot encoded, False otherwise.\n    \"\"\"\n    if len(data.shape) &lt; 2 or data.shape[-1] != num_classes:\n        return False  # Not one-hot if last dim doesn't match num_classes or less than 2D\n\n    # Check if all vectors are one-hot\n    return (data.sum(dim=-1) == 1).all() and (data.flatten().shape[0] / num_classes) % 1 == 0\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/","title":"Inference time schedules","text":""},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.ContinuousInferenceSchedule","title":"<code>ContinuousInferenceSchedule</code>","text":"<p>               Bases: <code>InferenceSchedule</code></p> <p>A base class for continuous time inference schedules.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class ContinuousInferenceSchedule(InferenceSchedule):\n    \"\"\"A base class for continuous time inference schedules.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the ContinuousInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        \"\"\"\n        super().__init__(nsteps, min_t, padding, dilation, direction, device)\n        self.inclusive_end = inclusive_end\n\n    def discretize(\n        self,\n        nsteps: Optional[int] = None,\n        schedule: Optional[Tensor] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Discretize the time schedule into a list of time deltas.\n\n        Args:\n            nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n            schedule (Optional[Tensor]): Time scheudle if None will generate it with generate_schedule.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time deltas.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if schedule is None:\n            schedule = self.generate_schedule(nsteps, device=device)\n        if self.direction == TimeDirection.UNIFIED:\n            schedule = torch.cat((schedule, torch.ones((1,), device=schedule.device)))\n            dt = schedule[1:] - schedule[:-1]\n        else:\n            schedule = torch.cat((schedule, torch.zeros((1,), device=schedule.device)))\n            dt = -1 * (schedule[1:] - schedule[:-1])\n        return dt\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.ContinuousInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the ContinuousInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the ContinuousInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    \"\"\"\n    super().__init__(nsteps, min_t, padding, dilation, direction, device)\n    self.inclusive_end = inclusive_end\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.ContinuousInferenceSchedule.discretize","title":"<code>discretize(nsteps=None, schedule=None, device=None)</code>","text":"<p>Discretize the time schedule into a list of time deltas.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optioanl[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>schedule</code> <code>Optional[Tensor]</code> <p>Time scheudle if None will generate it with generate_schedule.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time deltas.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def discretize(\n    self,\n    nsteps: Optional[int] = None,\n    schedule: Optional[Tensor] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Discretize the time schedule into a list of time deltas.\n\n    Args:\n        nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n        schedule (Optional[Tensor]): Time scheudle if None will generate it with generate_schedule.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time deltas.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if schedule is None:\n        schedule = self.generate_schedule(nsteps, device=device)\n    if self.direction == TimeDirection.UNIFIED:\n        schedule = torch.cat((schedule, torch.ones((1,), device=schedule.device)))\n        dt = schedule[1:] - schedule[:-1]\n    else:\n        schedule = torch.cat((schedule, torch.zeros((1,), device=schedule.device)))\n        dt = -1 * (schedule[1:] - schedule[:-1])\n    return dt\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteInferenceSchedule","title":"<code>DiscreteInferenceSchedule</code>","text":"<p>               Bases: <code>InferenceSchedule</code></p> <p>A base class for discrete time inference schedules.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class DiscreteInferenceSchedule(InferenceSchedule):\n    \"\"\"A base class for discrete time inference schedules.\"\"\"\n\n    def discretize(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Discretize the time schedule into a list of time deltas.\n\n        Args:\n            nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time deltas.\n        \"\"\"\n        if self.padding &gt; 0 or self.dilation &gt; 0:\n            raise NotImplementedError(\"discreteize is not implemented for discrete schedules with padding or dilation\")\n        if device is None:\n            device = self.device\n        return torch.full(\n            (nsteps if nsteps is not None else self.nsteps,),\n            1 / (nsteps if nsteps is not None else self.nsteps),\n            device=device,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteInferenceSchedule.discretize","title":"<code>discretize(nsteps=None, device=None)</code>","text":"<p>Discretize the time schedule into a list of time deltas.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optioanl[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time deltas.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def discretize(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Discretize the time schedule into a list of time deltas.\n\n    Args:\n        nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time deltas.\n    \"\"\"\n    if self.padding &gt; 0 or self.dilation &gt; 0:\n        raise NotImplementedError(\"discreteize is not implemented for discrete schedules with padding or dilation\")\n    if device is None:\n        device = self.device\n    return torch.full(\n        (nsteps if nsteps is not None else self.nsteps,),\n        1 / (nsteps if nsteps is not None else self.nsteps),\n        device=device,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteLinearInferenceSchedule","title":"<code>DiscreteLinearInferenceSchedule</code>","text":"<p>               Bases: <code>DiscreteInferenceSchedule</code></p> <p>A linear time schedule for discrete time inference.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class DiscreteLinearInferenceSchedule(DiscreteInferenceSchedule):\n    \"\"\"A linear time schedule for discrete time inference.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the DiscreteLinearInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, min_t, padding, dilation, direction, device)\n\n    def generate_schedule(\n        self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n    ) -&gt; Tensor:\n        \"\"\"Generate the linear time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time steps.\n            Tensor: A tensor of time steps.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / self.dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n        schedule = torch.arange(nsteps).to(device=device)\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = schedule.flip(0)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, schedule[-1] * torch.ones(self.padding, device=device)))\n        return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteLinearInferenceSchedule.__init__","title":"<code>__init__(nsteps, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the DiscreteLinearInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the DiscreteLinearInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, min_t, padding, dilation, direction, device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.DiscreteLinearInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the linear time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n) -&gt; Tensor:\n    \"\"\"Generate the linear time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time steps.\n        Tensor: A tensor of time steps.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / self.dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n    schedule = torch.arange(nsteps).to(device=device)\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = schedule.flip(0)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, schedule[-1] * torch.ones(self.padding, device=device)))\n    return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule","title":"<code>InferenceSchedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for inference time schedules.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class InferenceSchedule(ABC):\n    \"\"\"A base class for inference time schedules.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the InferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        \"\"\"\n        self.nsteps = nsteps\n        self.min_t = min_t\n        self.padding = padding\n        self.dilation = dilation\n        self.direction = string_to_enum(direction, TimeDirection)\n        self.device = device\n\n    @abstractmethod\n    def generate_schedule(\n        self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n    ) -&gt; Tensor:\n        \"\"\"Generate the time schedule as a tensor.\n\n        Args:\n            nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        pass\n\n    def pad_time(\n        self, n_samples: int, scalar_time: Float, device: Optional[Union[str, torch.device]] = None\n    ) -&gt; Tensor:\n        \"\"\"Creates a tensor of shape (n_samples,) filled with a scalar time value.\n\n        Args:\n            n_samples (int): The desired dimension of the output tensor.\n            scalar_time (Float): The scalar time value to fill the tensor with.\n            device (Optional[Union[str, torch.device]], optional):\n                The device to place the tensor on. Defaults to None, which uses the default device.\n\n        Returns:\n            Tensor: A tensor of shape (n_samples,) filled with the scalar time value.\n        \"\"\"\n        return torch.full((n_samples,), fill_value=scalar_time).to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule.__init__","title":"<code>__init__(nsteps, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the InferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the InferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    \"\"\"\n    self.nsteps = nsteps\n    self.min_t = min_t\n    self.padding = padding\n    self.dilation = dilation\n    self.direction = string_to_enum(direction, TimeDirection)\n    self.device = device\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>  <code>abstractmethod</code>","text":"<p>Generate the time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optioanl[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>@abstractmethod\ndef generate_schedule(\n    self, nsteps: Optional[int] = None, device: Optional[Union[str, torch.device]] = None\n) -&gt; Tensor:\n    \"\"\"Generate the time schedule as a tensor.\n\n    Args:\n        nsteps (Optioanl[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.InferenceSchedule.pad_time","title":"<code>pad_time(n_samples, scalar_time, device=None)</code>","text":"<p>Creates a tensor of shape (n_samples,) filled with a scalar time value.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The desired dimension of the output tensor.</p> required <code>scalar_time</code> <code>Float</code> <p>The scalar time value to fill the tensor with.</p> required <code>device</code> <code>Optional[Union[str, device]]</code> <p>The device to place the tensor on. Defaults to None, which uses the default device.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of shape (n_samples,) filled with the scalar time value.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def pad_time(\n    self, n_samples: int, scalar_time: Float, device: Optional[Union[str, torch.device]] = None\n) -&gt; Tensor:\n    \"\"\"Creates a tensor of shape (n_samples,) filled with a scalar time value.\n\n    Args:\n        n_samples (int): The desired dimension of the output tensor.\n        scalar_time (Float): The scalar time value to fill the tensor with.\n        device (Optional[Union[str, torch.device]], optional):\n            The device to place the tensor on. Defaults to None, which uses the default device.\n\n    Returns:\n        Tensor: A tensor of shape (n_samples,) filled with the scalar time value.\n    \"\"\"\n    return torch.full((n_samples,), fill_value=scalar_time).to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LinearInferenceSchedule","title":"<code>LinearInferenceSchedule</code>","text":"<p>               Bases: <code>ContinuousInferenceSchedule</code></p> <p>A linear time schedule for continuous time inference.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class LinearInferenceSchedule(ContinuousInferenceSchedule):\n    \"\"\"A linear time schedule for continuous time inference.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the LinearInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the linear time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor of time steps.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n        if not self.inclusive_end:\n            schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device)\n            schedule = schedule[:-1]\n        else:\n            schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device)\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = 1 - schedule\n        return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LinearInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the LinearInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the LinearInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at 1.0-1/nsteps (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LinearInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the linear time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the linear time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor of time steps.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n    if not self.inclusive_end:\n        schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device)\n        schedule = schedule[:-1]\n    else:\n        schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device)\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = 1 - schedule\n    return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LogInferenceSchedule","title":"<code>LogInferenceSchedule</code>","text":"<p>               Bases: <code>ContinuousInferenceSchedule</code></p> <p>A log time schedule for inference, where time steps are generated by taking the logarithm of a uniform schedule.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class LogInferenceSchedule(ContinuousInferenceSchedule):\n    \"\"\"A log time schedule for inference, where time steps are generated by taking the logarithm of a uniform schedule.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        exponent: Float = -2.0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the LogInferenceSchedule.\n\n        Returns a log space time schedule.\n\n        Which for 100 steps with default parameters is:\n            tensor([0.0000, 0.0455, 0.0889, 0.1303, 0.1699, 0.2077, 0.2439, 0.2783, 0.3113,\n                    0.3427, 0.3728, 0.4015, 0.4288, 0.4550, 0.4800, 0.5039, 0.5266, 0.5484,\n                    0.5692, 0.5890, 0.6080, 0.6261, 0.6434, 0.6599, 0.6756, 0.6907, 0.7051,\n                    0.7188, 0.7319, 0.7444, 0.7564, 0.7678, 0.7787, 0.7891, 0.7991, 0.8086,\n                    0.8176, 0.8263, 0.8346, 0.8425, 0.8500, 0.8572, 0.8641, 0.8707, 0.8769,\n                    0.8829, 0.8887, 0.8941, 0.8993, 0.9043, 0.9091, 0.9136, 0.9180, 0.9221,\n                    0.9261, 0.9299, 0.9335, 0.9369, 0.9402, 0.9434, 0.9464, 0.9492, 0.9520,\n                    0.9546, 0.9571, 0.9595, 0.9618, 0.9639, 0.9660, 0.9680, 0.9699, 0.9717,\n                    0.9734, 0.9751, 0.9767, 0.9782, 0.9796, 0.9810, 0.9823, 0.9835, 0.9847,\n                    0.9859, 0.9870, 0.9880, 0.9890, 0.9899, 0.9909, 0.9917, 0.9925, 0.9933,\n                    0.9941, 0.9948, 0.9955, 0.9962, 0.9968, 0.9974, 0.9980, 0.9985, 0.9990,\n                    0.9995])\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            exponent (Float): log space exponent parameter defaults to -2.0. The lower number the more aggressive the acceleration of 0 to 0.9 will be thus having more steps from 0.9 to 1.0.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n        if exponent is None:\n            raise ValueError(\"exponent cannot be None for the log schedule\")\n        if exponent &gt;= 0:\n            raise ValueError(f\"exponent input must be &lt;0, got {exponent}\")\n        self.exponent = exponent\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the log time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / self.dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n\n        if not self.inclusive_end:\n            t = 1.0 - torch.logspace(self.exponent, 0, nsteps + 1).flip(0).to(device=device)\n            t = t - torch.min(t)\n            schedule = t / torch.max(t)\n            schedule = schedule[:-1]\n        else:\n            t = 1.0 - torch.logspace(self.exponent, 0, nsteps).flip(0).to(device=device)\n            t = t - torch.min(t)\n            schedule = t / torch.max(t)\n\n        if self.min_t &gt; 0:\n            schedule = torch.clamp(schedule, min=self.min_t)\n\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = 1 - schedule\n        return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LogInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, exponent=-2.0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the LogInferenceSchedule.</p> <p>Returns a log space time schedule.</p> Which for 100 steps with default parameters is <p>tensor([0.0000, 0.0455, 0.0889, 0.1303, 0.1699, 0.2077, 0.2439, 0.2783, 0.3113,         0.3427, 0.3728, 0.4015, 0.4288, 0.4550, 0.4800, 0.5039, 0.5266, 0.5484,         0.5692, 0.5890, 0.6080, 0.6261, 0.6434, 0.6599, 0.6756, 0.6907, 0.7051,         0.7188, 0.7319, 0.7444, 0.7564, 0.7678, 0.7787, 0.7891, 0.7991, 0.8086,         0.8176, 0.8263, 0.8346, 0.8425, 0.8500, 0.8572, 0.8641, 0.8707, 0.8769,         0.8829, 0.8887, 0.8941, 0.8993, 0.9043, 0.9091, 0.9136, 0.9180, 0.9221,         0.9261, 0.9299, 0.9335, 0.9369, 0.9402, 0.9434, 0.9464, 0.9492, 0.9520,         0.9546, 0.9571, 0.9595, 0.9618, 0.9639, 0.9660, 0.9680, 0.9699, 0.9717,         0.9734, 0.9751, 0.9767, 0.9782, 0.9796, 0.9810, 0.9823, 0.9835, 0.9847,         0.9859, 0.9870, 0.9880, 0.9890, 0.9899, 0.9909, 0.9917, 0.9925, 0.9933,         0.9941, 0.9948, 0.9955, 0.9962, 0.9968, 0.9974, 0.9980, 0.9985, 0.9990,         0.9995])</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>exponent</code> <code>Float</code> <p>log space exponent parameter defaults to -2.0. The lower number the more aggressive the acceleration of 0 to 0.9 will be thus having more steps from 0.9 to 1.0.</p> <code>-2.0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    exponent: Float = -2.0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the LogInferenceSchedule.\n\n    Returns a log space time schedule.\n\n    Which for 100 steps with default parameters is:\n        tensor([0.0000, 0.0455, 0.0889, 0.1303, 0.1699, 0.2077, 0.2439, 0.2783, 0.3113,\n                0.3427, 0.3728, 0.4015, 0.4288, 0.4550, 0.4800, 0.5039, 0.5266, 0.5484,\n                0.5692, 0.5890, 0.6080, 0.6261, 0.6434, 0.6599, 0.6756, 0.6907, 0.7051,\n                0.7188, 0.7319, 0.7444, 0.7564, 0.7678, 0.7787, 0.7891, 0.7991, 0.8086,\n                0.8176, 0.8263, 0.8346, 0.8425, 0.8500, 0.8572, 0.8641, 0.8707, 0.8769,\n                0.8829, 0.8887, 0.8941, 0.8993, 0.9043, 0.9091, 0.9136, 0.9180, 0.9221,\n                0.9261, 0.9299, 0.9335, 0.9369, 0.9402, 0.9434, 0.9464, 0.9492, 0.9520,\n                0.9546, 0.9571, 0.9595, 0.9618, 0.9639, 0.9660, 0.9680, 0.9699, 0.9717,\n                0.9734, 0.9751, 0.9767, 0.9782, 0.9796, 0.9810, 0.9823, 0.9835, 0.9847,\n                0.9859, 0.9870, 0.9880, 0.9890, 0.9899, 0.9909, 0.9917, 0.9925, 0.9933,\n                0.9941, 0.9948, 0.9955, 0.9962, 0.9968, 0.9974, 0.9980, 0.9985, 0.9990,\n                0.9995])\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        exponent (Float): log space exponent parameter defaults to -2.0. The lower number the more aggressive the acceleration of 0 to 0.9 will be thus having more steps from 0.9 to 1.0.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n    if exponent is None:\n        raise ValueError(\"exponent cannot be None for the log schedule\")\n    if exponent &gt;= 0:\n        raise ValueError(f\"exponent input must be &lt;0, got {exponent}\")\n    self.exponent = exponent\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.LogInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the log time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the log time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / self.dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n\n    if not self.inclusive_end:\n        t = 1.0 - torch.logspace(self.exponent, 0, nsteps + 1).flip(0).to(device=device)\n        t = t - torch.min(t)\n        schedule = t / torch.max(t)\n        schedule = schedule[:-1]\n    else:\n        t = 1.0 - torch.logspace(self.exponent, 0, nsteps).flip(0).to(device=device)\n        t = t - torch.min(t)\n        schedule = t / torch.max(t)\n\n    if self.min_t &gt; 0:\n        schedule = torch.clamp(schedule, min=self.min_t)\n\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = 1 - schedule\n    return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.PowerInferenceSchedule","title":"<code>PowerInferenceSchedule</code>","text":"<p>               Bases: <code>ContinuousInferenceSchedule</code></p> <p>A power time schedule for inference, where time steps are generated by raising a uniform schedule to a specified power.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>class PowerInferenceSchedule(ContinuousInferenceSchedule):\n    \"\"\"A power time schedule for inference, where time steps are generated by raising a uniform schedule to a specified power.\"\"\"\n\n    def __init__(\n        self,\n        nsteps: int,\n        inclusive_end: bool = False,\n        min_t: Float = 0,\n        padding: Float = 0,\n        dilation: Float = 0,\n        exponent: Float = 1.0,\n        direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n        device: Union[str, torch.device] = \"cpu\",\n    ):\n        \"\"\"Initialize the PowerInferenceSchedule.\n\n        Args:\n            nsteps (int): Number of time steps.\n            inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n            min_t (Float): minimum time value defaults to 0.\n            padding (Float): padding time value defaults to 0.\n            dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n            exponent (Float): Power parameter defaults to 1.0.\n            direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n        self.exponent = exponent\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Optional[Union[str, torch.device]] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the power time schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n\n        Returns:\n            Tensor: A tensor of time steps.\n            Tensor: A tensor of time steps.\n        \"\"\"\n        if device is None:\n            device = self.device\n        if nsteps is None:\n            nsteps = self.nsteps\n        nsteps -= self.padding\n        dilation = self.dilation + 1\n        if dilation &gt; 1:\n            if nsteps % dilation != 0:\n                raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n            nsteps = int(nsteps / dilation)\n        if nsteps is None:\n            raise ValueError(\"nsteps cannot be None\")\n        if not self.inclusive_end:\n            schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device) ** self.exponent\n            schedule = schedule[:-1]\n        else:\n            schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device) ** self.exponent\n        if dilation &gt; 1:\n            schedule = schedule.repeat_interleave(dilation)\n        if self.padding &gt; 0:\n            schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n        if self.direction == TimeDirection.DIFFUSION:\n            schedule = 1 - schedule\n        return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.PowerInferenceSchedule.__init__","title":"<code>__init__(nsteps, inclusive_end=False, min_t=0, padding=0, dilation=0, exponent=1.0, direction=TimeDirection.UNIFIED, device='cpu')</code>","text":"<p>Initialize the PowerInferenceSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of time steps.</p> required <code>inclusive_end</code> <code>bool</code> <p>If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).</p> <code>False</code> <code>min_t</code> <code>Float</code> <p>minimum time value defaults to 0.</p> <code>0</code> <code>padding</code> <code>Float</code> <p>padding time value defaults to 0.</p> <code>0</code> <code>dilation</code> <code>Float</code> <p>dilation time value defaults to 0 ie the number of replicates.</p> <code>0</code> <code>exponent</code> <code>Float</code> <p>Power parameter defaults to 1.0.</p> <code>1.0</code> <code>direction</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>UNIFIED</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def __init__(\n    self,\n    nsteps: int,\n    inclusive_end: bool = False,\n    min_t: Float = 0,\n    padding: Float = 0,\n    dilation: Float = 0,\n    exponent: Float = 1.0,\n    direction: Union[TimeDirection, str] = TimeDirection.UNIFIED,\n    device: Union[str, torch.device] = \"cpu\",\n):\n    \"\"\"Initialize the PowerInferenceSchedule.\n\n    Args:\n        nsteps (int): Number of time steps.\n        inclusive_end (bool): If True, include the end value (1.0) in the schedule otherwise ends at &lt;1.0 (default is False).\n        min_t (Float): minimum time value defaults to 0.\n        padding (Float): padding time value defaults to 0.\n        dilation (Float): dilation time value defaults to 0 ie the number of replicates.\n        exponent (Float): Power parameter defaults to 1.0.\n        direction (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    super().__init__(nsteps, inclusive_end, min_t, padding, dilation, direction, device)\n    self.exponent = exponent\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/inference_time_schedules/#bionemo.moco.schedules.inference_time_schedules.PowerInferenceSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device=None)</code>","text":"<p>Generate the power time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> <code>Tensor</code> <code>Tensor</code> <p>A tensor of time steps.</p> Source code in <code>bionemo/moco/schedules/inference_time_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the power time schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n\n\n    Returns:\n        Tensor: A tensor of time steps.\n        Tensor: A tensor of time steps.\n    \"\"\"\n    if device is None:\n        device = self.device\n    if nsteps is None:\n        nsteps = self.nsteps\n    nsteps -= self.padding\n    dilation = self.dilation + 1\n    if dilation &gt; 1:\n        if nsteps % dilation != 0:\n            raise ValueError(f\"nsteps ({nsteps}) is not divisible by dilation + 1 ({dilation})\")\n        nsteps = int(nsteps / dilation)\n    if nsteps is None:\n        raise ValueError(\"nsteps cannot be None\")\n    if not self.inclusive_end:\n        schedule = torch.linspace(self.min_t, 1, nsteps + 1).to(device=device) ** self.exponent\n        schedule = schedule[:-1]\n    else:\n        schedule = torch.linspace(self.min_t, 1, nsteps).to(device=device) ** self.exponent\n    if dilation &gt; 1:\n        schedule = schedule.repeat_interleave(dilation)\n    if self.padding &gt; 0:\n        schedule = torch.cat((schedule, torch.ones(self.padding, device=device)))\n    if self.direction == TimeDirection.DIFFUSION:\n        schedule = 1 - schedule\n    return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/moco/schedules/utils/#bionemo.moco.schedules.utils.TimeDirection","title":"<code>TimeDirection</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for the direction of the noise schedule.</p> Source code in <code>bionemo/moco/schedules/utils.py</code> <pre><code>class TimeDirection(Enum):\n    \"\"\"Enum for the direction of the noise schedule.\"\"\"\n\n    UNIFIED = \"unified\"  # Noise(0) --&gt; Data(1)\n    DIFFUSION = \"diffusion\"  # Noise(1) --&gt; Data(0)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/","title":"Continuous noise transforms","text":""},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform","title":"<code>ContinuousExpNoiseTransform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for continuous schedules.</p> <p>alpha = exp(- sigma) where 1 - alpha controls the masking fraction.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>class ContinuousExpNoiseTransform(ABC):\n    \"\"\"A base class for continuous schedules.\n\n    alpha = exp(- sigma) where 1 - alpha controls the masking fraction.\n    \"\"\"\n\n    def __init__(self, direction: TimeDirection):\n        \"\"\"Initialize the DiscreteNoiseSchedule.\n\n        Args:\n            direction : TimeDirection, required this defines in which direction the scheduler was built\n        \"\"\"\n        self.direction = string_to_enum(direction, TimeDirection)\n\n    def calculate_sigma(\n        self,\n        t: Tensor,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Calculate the sigma for the given time steps.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n            synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n\n        Raises:\n            ValueError: If the input time steps exceed the maximum allowed value of 1.\n        \"\"\"\n        if t.max() &gt; 1:\n            raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n        if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n            t = 1 - t\n        return self._calculate_sigma(t, device)\n\n    @abstractmethod\n    def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the -log of the clean data value for the given time steps.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n        \"\"\"\n        pass\n\n    def sigma_to_alpha(self, sigma: Tensor) -&gt; Tensor:\n        \"\"\"Converts sigma to alpha values by alpha = exp(- sigma).\n\n        Args:\n            sigma (Tensor): The input sigma tensor.\n\n        Returns:\n            Tensor: A tensor containing the alpha values.\n        \"\"\"\n        return torch.exp(-1 * sigma)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform.__init__","title":"<code>__init__(direction)</code>","text":"<p>Initialize the DiscreteNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <p>TimeDirection, required this defines in which direction the scheduler was built</p> required Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def __init__(self, direction: TimeDirection):\n    \"\"\"Initialize the DiscreteNoiseSchedule.\n\n    Args:\n        direction : TimeDirection, required this defines in which direction the scheduler was built\n    \"\"\"\n    self.direction = string_to_enum(direction, TimeDirection)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform._calculate_sigma","title":"<code>_calculate_sigma(t, device='cpu')</code>  <code>abstractmethod</code>","text":"<p>Calculate the -log of the clean data value for the given time steps.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the sigma values for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>@abstractmethod\ndef _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Calculate the -log of the clean data value for the given time steps.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the sigma values for the given time steps.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform.calculate_sigma","title":"<code>calculate_sigma(t, device='cpu', synchronize=None)</code>","text":"<p>Calculate the sigma for the given time steps.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps, with values ranging from 0 to 1.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>synchronize</code> <code>optional[TimeDirection]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the sigma values for the given time steps.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input time steps exceed the maximum allowed value of 1.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def calculate_sigma(\n    self,\n    t: Tensor,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Calculate the sigma for the given time steps.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n        synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n    Returns:\n        Tensor: A tensor representing the sigma values for the given time steps.\n\n    Raises:\n        ValueError: If the input time steps exceed the maximum allowed value of 1.\n    \"\"\"\n    if t.max() &gt; 1:\n        raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n    if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n        t = 1 - t\n    return self._calculate_sigma(t, device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.ContinuousExpNoiseTransform.sigma_to_alpha","title":"<code>sigma_to_alpha(sigma)</code>","text":"<p>Converts sigma to alpha values by alpha = exp(- sigma).</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>Tensor</code> <p>The input sigma tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor containing the alpha values.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def sigma_to_alpha(self, sigma: Tensor) -&gt; Tensor:\n    \"\"\"Converts sigma to alpha values by alpha = exp(- sigma).\n\n    Args:\n        sigma (Tensor): The input sigma tensor.\n\n    Returns:\n        Tensor: A tensor containing the alpha values.\n    \"\"\"\n    return torch.exp(-1 * sigma)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.CosineExpNoiseTransform","title":"<code>CosineExpNoiseTransform</code>","text":"<p>               Bases: <code>ContinuousExpNoiseTransform</code></p> <p>A cosine Exponential noise schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>class CosineExpNoiseTransform(ContinuousExpNoiseTransform):\n    \"\"\"A cosine Exponential noise schedule.\"\"\"\n\n    def __init__(self, eps: Float = 1.0e-3):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            eps (Float): small number to prevent numerical issues.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.eps = eps\n\n    def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate negative log of data interpolant fraction.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n        \"\"\"\n        cos = torch.cos(t * torch.pi / 2).to(device)\n        return -torch.log(self.eps + (1 - self.eps) * cos)\n\n    def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Compute the derivative of sigma with respect to time.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the derivative of sigma with respect to time.\n\n        Notes:\n            The derivative of sigma as a function of time is given by:\n\n            d/dt sigma(t) = d/dt (-log(cos(t * pi / 2) + eps))\n\n            Using the chain rule, we get:\n\n            d/dt sigma(t) = (-1 / (cos(t * pi / 2) + eps)) * (-sin(t * pi / 2) * pi / 2)\n\n            This is the derivative that is computed and returned by this method.\n        \"\"\"\n        cos = (1 - self.eps) * torch.cos(t * torch.pi / 2)\n        sin = (1 - self.eps) * torch.sin(t * torch.pi / 2)\n        scale = torch.pi / 2\n        derivative = scale * sin / (cos + self.eps)\n        return derivative.to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.CosineExpNoiseTransform.__init__","title":"<code>__init__(eps=0.001)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>Float</code> <p>small number to prevent numerical issues.</p> <code>0.001</code> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def __init__(self, eps: Float = 1.0e-3):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        eps (Float): small number to prevent numerical issues.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.eps = eps\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.CosineExpNoiseTransform._calculate_sigma","title":"<code>_calculate_sigma(t, device='cpu')</code>","text":"<p>Calculate negative log of data interpolant fraction.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the sigma values for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Calculate negative log of data interpolant fraction.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the sigma values for the given time steps.\n    \"\"\"\n    cos = torch.cos(t * torch.pi / 2).to(device)\n    return -torch.log(self.eps + (1 - self.eps) * cos)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.CosineExpNoiseTransform.d_dt_sigma","title":"<code>d_dt_sigma(t, device='cpu')</code>","text":"<p>Compute the derivative of sigma with respect to time.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the derivative of sigma with respect to time.</p> Notes <p>The derivative of sigma as a function of time is given by:</p> <p>d/dt sigma(t) = d/dt (-log(cos(t * pi / 2) + eps))</p> <p>Using the chain rule, we get:</p> <p>d/dt sigma(t) = (-1 / (cos(t * pi / 2) + eps)) * (-sin(t * pi / 2) * pi / 2)</p> <p>This is the derivative that is computed and returned by this method.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Compute the derivative of sigma with respect to time.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the derivative of sigma with respect to time.\n\n    Notes:\n        The derivative of sigma as a function of time is given by:\n\n        d/dt sigma(t) = d/dt (-log(cos(t * pi / 2) + eps))\n\n        Using the chain rule, we get:\n\n        d/dt sigma(t) = (-1 / (cos(t * pi / 2) + eps)) * (-sin(t * pi / 2) * pi / 2)\n\n        This is the derivative that is computed and returned by this method.\n    \"\"\"\n    cos = (1 - self.eps) * torch.cos(t * torch.pi / 2)\n    sin = (1 - self.eps) * torch.sin(t * torch.pi / 2)\n    scale = torch.pi / 2\n    derivative = scale * sin / (cos + self.eps)\n    return derivative.to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.LogLinearExpNoiseTransform","title":"<code>LogLinearExpNoiseTransform</code>","text":"<p>               Bases: <code>ContinuousExpNoiseTransform</code></p> <p>A log linear exponential schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>class LogLinearExpNoiseTransform(ContinuousExpNoiseTransform):\n    \"\"\"A log linear exponential schedule.\"\"\"\n\n    def __init__(self, eps: Float = 1.0e-3):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            eps (Float): small value to prevent numerical issues.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.eps = eps\n\n    def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate negative log of data interpolant fraction.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the sigma values for the given time steps.\n        \"\"\"\n        return -torch.log1p(-(1 - self.eps) * t).to(device)\n\n    def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Compute the derivative of sigma with respect to time.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the derivative of sigma with respect to time.\n        \"\"\"\n        derivative = (1 - self.eps) / (1 - (1 - self.eps) * t)\n        return derivative.to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.LogLinearExpNoiseTransform.__init__","title":"<code>__init__(eps=0.001)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>Float</code> <p>small value to prevent numerical issues.</p> <code>0.001</code> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def __init__(self, eps: Float = 1.0e-3):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        eps (Float): small value to prevent numerical issues.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.eps = eps\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.LogLinearExpNoiseTransform._calculate_sigma","title":"<code>_calculate_sigma(t, device='cpu')</code>","text":"<p>Calculate negative log of data interpolant fraction.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the sigma values for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def _calculate_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Calculate negative log of data interpolant fraction.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the sigma values for the given time steps.\n    \"\"\"\n    return -torch.log1p(-(1 - self.eps) * t).to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_noise_transforms/#bionemo.moco.schedules.noise.continuous_noise_transforms.LogLinearExpNoiseTransform.d_dt_sigma","title":"<code>d_dt_sigma(t, device='cpu')</code>","text":"<p>Compute the derivative of sigma with respect to time.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the derivative of sigma with respect to time.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_noise_transforms.py</code> <pre><code>def d_dt_sigma(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Compute the derivative of sigma with respect to time.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the derivative of sigma with respect to time.\n    \"\"\"\n    derivative = (1 - self.eps) / (1 - (1 - self.eps) * t)\n    return derivative.to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/","title":"Continuous snr transforms","text":""},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform","title":"<code>ContinuousSNRTransform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for continuous SNR schedules.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class ContinuousSNRTransform(ABC):\n    \"\"\"A base class for continuous SNR schedules.\"\"\"\n\n    def __init__(self, direction: TimeDirection):\n        \"\"\"Initialize the DiscreteNoiseSchedule.\n\n        Args:\n            direction (TimeDirection): required this defines in which direction the scheduler was built\n        \"\"\"\n        self.direction = string_to_enum(direction, TimeDirection)\n\n    def calculate_log_snr(\n        self,\n        t: Tensor,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Public wrapper to generate the time schedule as a tensor.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n            synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n        Returns:\n            Tensor: A tensor representing the log signal-to-noise (SNR) ratio for the given time steps.\n        \"\"\"\n        if t.max() &gt; 1:\n            raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n        if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n            t = 1 - t\n        return self._calculate_log_snr(t, device)\n\n    @abstractmethod\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the log signal-to-noise (SNR) ratio.\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the log SNR values for the given time steps.\n        \"\"\"\n        pass\n\n    def log_snr_to_alphas_sigmas(self, log_snr: Tensor) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"Converts log signal-to-noise ratio (SNR) to alpha and sigma values.\n\n        Args:\n            log_snr (Tensor): The input log SNR tensor.\n\n        Returns:\n            tuple[Tensor, Tensor]: A tuple containing the squared root of alpha and sigma values.\n        \"\"\"\n        squared_alpha = log_snr.sigmoid()\n        squared_sigma = (-log_snr).sigmoid()\n        return squared_alpha.sqrt(), squared_sigma.sqrt()\n\n    def derivative(self, t: Tensor, func: Callable) -&gt; Tensor:\n        \"\"\"Compute derivative of a function, it supports bached single variable inputs.\n\n        Args:\n            t (Tensor): time variable at which derivatives are taken\n            func (Callable): function for derivative calculation\n\n        Returns:\n            Tensor: derivative that is detached from the computational graph\n        \"\"\"\n        with torch.enable_grad():\n            t.requires_grad_(True)\n            derivative = torch.autograd.grad(func(t).sum(), t, create_graph=False)[0].detach()\n            t.requires_grad_(False)\n        return derivative\n\n    def calculate_general_sde_terms(self, t):\n        \"\"\"Compute the general SDE terms for a given time step t.\n\n        Args:\n            t (Tensor): The input tensor representing the time step.\n\n        Returns:\n            tuple[Tensor, Tensor]: A tuple containing the drift term f_t and the diffusion term g_t_2.\n\n        Notes:\n            This method computes the drift and diffusion terms of the general SDE, which can be used to simulate the stochastic process.\n            The drift term represents the deterministic part of the process, while the diffusion term represents the stochastic part.\n        \"\"\"\n        t = t.clone()\n        t.requires_grad_(True)\n\n        # Compute log SNR\n        log_snr = self.calculate_log_snr(t, device=t.device)\n\n        # Alpha^2 and Sigma^2\n        alpha_squared = torch.sigmoid(log_snr)\n        sigma_squared = torch.sigmoid(-log_snr)\n\n        # Log Alpha\n        log_alpha = 0.5 * torch.log(alpha_squared)\n\n        # Compute derivatives\n        log_alpha_deriv = torch.autograd.grad(log_alpha.sum(), t, create_graph=False)[0].detach()\n        sigma_squared_deriv = torch.autograd.grad(sigma_squared.sum(), t, create_graph=False)[0].detach()\n\n        # Compute drift and diffusion terms\n        f_t = log_alpha_deriv  # Drift term\n        g_t_2 = sigma_squared_deriv - 2 * log_alpha_deriv * sigma_squared  # Diffusion term\n\n        return f_t, g_t_2\n\n    def calculate_beta(self, t):\n        r\"\"\"Compute the drift coefficient for the OU process of the form $dx = -\\frac{1}{2} \\beta(t) x dt + sqrt(beta(t)) dw_t$.\n\n        beta = d/dt log(alpha**2) = 2 * 1/alpha * d/dt(alpha)\n\n        Args:\n            t (Union[float, Tensor]): t in [0, 1]\n\n        Returns:\n            Tensor: beta(t)\n        \"\"\"\n        t = t.clone()\n        t.requires_grad_(True)\n        log_snr = self.calculate_log_snr(t, device=t.device)\n        alpha = self.calculate_alpha_log_snr(log_snr).detach()\n        alpha_deriv_t = self.derivative(t, self.calculate_alpha_t).detach()\n        beta = 2.0 * alpha_deriv_t / alpha\n        # Chroma has a negative here but when removing the negative we get f = d/dt log (alpha**2) and the step_ode function works as expected\n        return beta\n\n    def calculate_alpha_log_snr(self, log_snr: Tensor) -&gt; Tensor:\n        \"\"\"Compute alpha values based on the log SNR.\n\n        Args:\n            log_snr (Tensor): The input tensor representing the log signal-to-noise ratio.\n\n        Returns:\n            Tensor: A tensor representing the alpha values for the given log SNR.\n\n        Notes:\n            This method computes alpha values as the square root of the sigmoid of the log SNR.\n        \"\"\"\n        return torch.sigmoid(log_snr).sqrt()\n\n    def calculate_alpha_t(self, t: Tensor) -&gt; Tensor:\n        \"\"\"Compute alpha values based on the log SNR schedule.\n\n        Parameters:\n            t (Tensor): The input tensor representing the time steps.\n\n        Returns:\n            Tensor: A tensor representing the alpha values for the given time steps.\n\n        Notes:\n            This method computes alpha values as the square root of the sigmoid of the log SNR.\n        \"\"\"\n        log_snr = self.calculate_log_snr(t, device=t.device)\n        alpha = torch.sigmoid(log_snr).sqrt()\n        return alpha\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.__init__","title":"<code>__init__(direction)</code>","text":"<p>Initialize the DiscreteNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>TimeDirection</code> <p>required this defines in which direction the scheduler was built</p> required Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, direction: TimeDirection):\n    \"\"\"Initialize the DiscreteNoiseSchedule.\n\n    Args:\n        direction (TimeDirection): required this defines in which direction the scheduler was built\n    \"\"\"\n    self.direction = string_to_enum(direction, TimeDirection)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform._calculate_log_snr","title":"<code>_calculate_log_snr(t, device='cpu')</code>  <code>abstractmethod</code>","text":"<p>Generate the log signal-to-noise (SNR) ratio.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the log SNR values for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>@abstractmethod\ndef _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Generate the log signal-to-noise (SNR) ratio.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the log SNR values for the given time steps.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_alpha_log_snr","title":"<code>calculate_alpha_log_snr(log_snr)</code>","text":"<p>Compute alpha values based on the log SNR.</p> <p>Parameters:</p> Name Type Description Default <code>log_snr</code> <code>Tensor</code> <p>The input tensor representing the log signal-to-noise ratio.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the alpha values for the given log SNR.</p> Notes <p>This method computes alpha values as the square root of the sigmoid of the log SNR.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_alpha_log_snr(self, log_snr: Tensor) -&gt; Tensor:\n    \"\"\"Compute alpha values based on the log SNR.\n\n    Args:\n        log_snr (Tensor): The input tensor representing the log signal-to-noise ratio.\n\n    Returns:\n        Tensor: A tensor representing the alpha values for the given log SNR.\n\n    Notes:\n        This method computes alpha values as the square root of the sigmoid of the log SNR.\n    \"\"\"\n    return torch.sigmoid(log_snr).sqrt()\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_alpha_t","title":"<code>calculate_alpha_t(t)</code>","text":"<p>Compute alpha values based on the log SNR schedule.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the alpha values for the given time steps.</p> Notes <p>This method computes alpha values as the square root of the sigmoid of the log SNR.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_alpha_t(self, t: Tensor) -&gt; Tensor:\n    \"\"\"Compute alpha values based on the log SNR schedule.\n\n    Parameters:\n        t (Tensor): The input tensor representing the time steps.\n\n    Returns:\n        Tensor: A tensor representing the alpha values for the given time steps.\n\n    Notes:\n        This method computes alpha values as the square root of the sigmoid of the log SNR.\n    \"\"\"\n    log_snr = self.calculate_log_snr(t, device=t.device)\n    alpha = torch.sigmoid(log_snr).sqrt()\n    return alpha\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_beta","title":"<code>calculate_beta(t)</code>","text":"<p>Compute the drift coefficient for the OU process of the form $dx = -\\frac{1}{2} \\beta(t) x dt + sqrt(beta(t)) dw_t$.</p> <p>beta = d/dt log(alpha**2) = 2 * 1/alpha * d/dt(alpha)</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[float, Tensor]</code> <p>t in [0, 1]</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>beta(t)</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_beta(self, t):\n    r\"\"\"Compute the drift coefficient for the OU process of the form $dx = -\\frac{1}{2} \\beta(t) x dt + sqrt(beta(t)) dw_t$.\n\n    beta = d/dt log(alpha**2) = 2 * 1/alpha * d/dt(alpha)\n\n    Args:\n        t (Union[float, Tensor]): t in [0, 1]\n\n    Returns:\n        Tensor: beta(t)\n    \"\"\"\n    t = t.clone()\n    t.requires_grad_(True)\n    log_snr = self.calculate_log_snr(t, device=t.device)\n    alpha = self.calculate_alpha_log_snr(log_snr).detach()\n    alpha_deriv_t = self.derivative(t, self.calculate_alpha_t).detach()\n    beta = 2.0 * alpha_deriv_t / alpha\n    # Chroma has a negative here but when removing the negative we get f = d/dt log (alpha**2) and the step_ode function works as expected\n    return beta\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_general_sde_terms","title":"<code>calculate_general_sde_terms(t)</code>","text":"<p>Compute the general SDE terms for a given time step t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time step.</p> required <p>Returns:</p> Type Description <p>tuple[Tensor, Tensor]: A tuple containing the drift term f_t and the diffusion term g_t_2.</p> Notes <p>This method computes the drift and diffusion terms of the general SDE, which can be used to simulate the stochastic process. The drift term represents the deterministic part of the process, while the diffusion term represents the stochastic part.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_general_sde_terms(self, t):\n    \"\"\"Compute the general SDE terms for a given time step t.\n\n    Args:\n        t (Tensor): The input tensor representing the time step.\n\n    Returns:\n        tuple[Tensor, Tensor]: A tuple containing the drift term f_t and the diffusion term g_t_2.\n\n    Notes:\n        This method computes the drift and diffusion terms of the general SDE, which can be used to simulate the stochastic process.\n        The drift term represents the deterministic part of the process, while the diffusion term represents the stochastic part.\n    \"\"\"\n    t = t.clone()\n    t.requires_grad_(True)\n\n    # Compute log SNR\n    log_snr = self.calculate_log_snr(t, device=t.device)\n\n    # Alpha^2 and Sigma^2\n    alpha_squared = torch.sigmoid(log_snr)\n    sigma_squared = torch.sigmoid(-log_snr)\n\n    # Log Alpha\n    log_alpha = 0.5 * torch.log(alpha_squared)\n\n    # Compute derivatives\n    log_alpha_deriv = torch.autograd.grad(log_alpha.sum(), t, create_graph=False)[0].detach()\n    sigma_squared_deriv = torch.autograd.grad(sigma_squared.sum(), t, create_graph=False)[0].detach()\n\n    # Compute drift and diffusion terms\n    f_t = log_alpha_deriv  # Drift term\n    g_t_2 = sigma_squared_deriv - 2 * log_alpha_deriv * sigma_squared  # Diffusion term\n\n    return f_t, g_t_2\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.calculate_log_snr","title":"<code>calculate_log_snr(t, device='cpu', synchronize=None)</code>","text":"<p>Public wrapper to generate the time schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps, with values ranging from 0 to 1.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>synchronize</code> <code>optional[TimeDirection]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the log signal-to-noise (SNR) ratio for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def calculate_log_snr(\n    self,\n    t: Tensor,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Public wrapper to generate the time schedule as a tensor.\n\n    Args:\n        t (Tensor): The input tensor representing the time steps, with values ranging from 0 to 1.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n        synchronize (optional[TimeDirection]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one. Defaults to None.\n\n    Returns:\n        Tensor: A tensor representing the log signal-to-noise (SNR) ratio for the given time steps.\n    \"\"\"\n    if t.max() &gt; 1:\n        raise ValueError(f\"Invalid value: max continuous time is 1, but got {t.max().item()}\")\n\n    if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n        t = 1 - t\n    return self._calculate_log_snr(t, device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.derivative","title":"<code>derivative(t, func)</code>","text":"<p>Compute derivative of a function, it supports bached single variable inputs.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>time variable at which derivatives are taken</p> required <code>func</code> <code>Callable</code> <p>function for derivative calculation</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>derivative that is detached from the computational graph</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def derivative(self, t: Tensor, func: Callable) -&gt; Tensor:\n    \"\"\"Compute derivative of a function, it supports bached single variable inputs.\n\n    Args:\n        t (Tensor): time variable at which derivatives are taken\n        func (Callable): function for derivative calculation\n\n    Returns:\n        Tensor: derivative that is detached from the computational graph\n    \"\"\"\n    with torch.enable_grad():\n        t.requires_grad_(True)\n        derivative = torch.autograd.grad(func(t).sum(), t, create_graph=False)[0].detach()\n        t.requires_grad_(False)\n    return derivative\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.ContinuousSNRTransform.log_snr_to_alphas_sigmas","title":"<code>log_snr_to_alphas_sigmas(log_snr)</code>","text":"<p>Converts log signal-to-noise ratio (SNR) to alpha and sigma values.</p> <p>Parameters:</p> Name Type Description Default <code>log_snr</code> <code>Tensor</code> <p>The input log SNR tensor.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>tuple[Tensor, Tensor]: A tuple containing the squared root of alpha and sigma values.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def log_snr_to_alphas_sigmas(self, log_snr: Tensor) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Converts log signal-to-noise ratio (SNR) to alpha and sigma values.\n\n    Args:\n        log_snr (Tensor): The input log SNR tensor.\n\n    Returns:\n        tuple[Tensor, Tensor]: A tuple containing the squared root of alpha and sigma values.\n    \"\"\"\n    squared_alpha = log_snr.sigmoid()\n    squared_sigma = (-log_snr).sigmoid()\n    return squared_alpha.sqrt(), squared_sigma.sqrt()\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.CosineSNRTransform","title":"<code>CosineSNRTransform</code>","text":"<p>               Bases: <code>ContinuousSNRTransform</code></p> <p>A cosine SNR schedule.</p> <p>Parameters:</p> Name Type Description Default <code>nu</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule exponent (default is 1.0).</p> <code>1.0</code> <code>s</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule shift (default is 0.008).</p> <code>0.008</code> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class CosineSNRTransform(ContinuousSNRTransform):\n    \"\"\"A cosine SNR schedule.\n\n    Args:\n        nu (Optional[Float]): Hyperparameter for the cosine schedule exponent (default is 1.0).\n        s (Optional[Float]): Hyperparameter for the cosine schedule shift (default is 0.008).\n    \"\"\"\n\n    def __init__(self, nu: Float = 1.0, s: Float = 0.008):\n        \"\"\"Initialize the CosineNoiseSchedule.\"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.nu = nu\n        self.s = s\n\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n        The SNR is the equivalent to alpha_bar**2 / (1 - alpha_bar**2) from DDPM.\n        This method computes the log SNR as described in the paper \"Improved Denoising Diffusion Probabilistic Models\" (https://arxiv.org/pdf/2107.00630).\n        Note 1 / (1 + exp(- log_snr)) returns this cosine**2 for alpha_bar**2\n        See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (str): Device to place the schedule on (default is \"cpu\").\n\n        Returns:\n            Tensor: A tensor representing the log SNR for the given time steps.\n        \"\"\"\n        return -log((torch.cos((t**self.nu + self.s) / (1 + self.s) * math.pi * 0.5) ** -2) - 1, eps=1e-5).to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.CosineSNRTransform.__init__","title":"<code>__init__(nu=1.0, s=0.008)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, nu: Float = 1.0, s: Float = 0.008):\n    \"\"\"Initialize the CosineNoiseSchedule.\"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.nu = nu\n    self.s = s\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.CosineSNRTransform._calculate_log_snr","title":"<code>_calculate_log_snr(t, device='cpu')</code>","text":"<p>Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.</p> <p>The SNR is the equivalent to alpha_bar2 / (1 - alpha_bar2) from DDPM. This method computes the log SNR as described in the paper \"Improved Denoising Diffusion Probabilistic Models\" (https://arxiv.org/pdf/2107.00630). Note 1 / (1 + exp(- log_snr)) returns this cosine2 for alpha_bar2 See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>str</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the log SNR for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n    The SNR is the equivalent to alpha_bar**2 / (1 - alpha_bar**2) from DDPM.\n    This method computes the log SNR as described in the paper \"Improved Denoising Diffusion Probabilistic Models\" (https://arxiv.org/pdf/2107.00630).\n    Note 1 / (1 + exp(- log_snr)) returns this cosine**2 for alpha_bar**2\n    See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (str): Device to place the schedule on (default is \"cpu\").\n\n    Returns:\n        Tensor: A tensor representing the log SNR for the given time steps.\n    \"\"\"\n    return -log((torch.cos((t**self.nu + self.s) / (1 + self.s) * math.pi * 0.5) ** -2) - 1, eps=1e-5).to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearLogInterpolatedSNRTransform","title":"<code>LinearLogInterpolatedSNRTransform</code>","text":"<p>               Bases: <code>ContinuousSNRTransform</code></p> <p>A Linear Log space interpolated SNR schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class LinearLogInterpolatedSNRTransform(ContinuousSNRTransform):\n    \"\"\"A Linear Log space interpolated SNR schedule.\"\"\"\n\n    def __init__(self, min_value: Float = -7.0, max_value=13.5):\n        \"\"\"Initialize the Linear log space interpolated SNR Schedule from Chroma.\n\n        Args:\n            min_value (Float): The min log SNR value.\n            max_value (Float): the max log SNR value.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.min_value = min_value\n        self.max_value = max_value\n\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n        See https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L316C23-L316C50\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the log SNR for the given time steps.\n        \"\"\"\n        log_snr = (1 - t) * self.max_value + t * self.min_value\n        return log_snr.to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearLogInterpolatedSNRTransform.__init__","title":"<code>__init__(min_value=-7.0, max_value=13.5)</code>","text":"<p>Initialize the Linear log space interpolated SNR Schedule from Chroma.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>Float</code> <p>The min log SNR value.</p> <code>-7.0</code> <code>max_value</code> <code>Float</code> <p>the max log SNR value.</p> <code>13.5</code> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, min_value: Float = -7.0, max_value=13.5):\n    \"\"\"Initialize the Linear log space interpolated SNR Schedule from Chroma.\n\n    Args:\n        min_value (Float): The min log SNR value.\n        max_value (Float): the max log SNR value.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.min_value = min_value\n    self.max_value = max_value\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearLogInterpolatedSNRTransform._calculate_log_snr","title":"<code>_calculate_log_snr(t, device='cpu')</code>","text":"<p>Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.</p> <p>See https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L316C23-L316C50</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the log SNR for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n    See https://github.com/generatebio/chroma/blob/929407c605013613941803c6113adefdccaad679/chroma/layers/structure/diffusion.py#L316C23-L316C50\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the log SNR for the given time steps.\n    \"\"\"\n    log_snr = (1 - t) * self.max_value + t * self.min_value\n    return log_snr.to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearSNRTransform","title":"<code>LinearSNRTransform</code>","text":"<p>               Bases: <code>ContinuousSNRTransform</code></p> <p>A Linear SNR schedule.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>class LinearSNRTransform(ContinuousSNRTransform):\n    \"\"\"A Linear SNR schedule.\"\"\"\n\n    def __init__(self, min_value: Float = 1.0e-4):\n        \"\"\"Initialize the Linear SNR Transform.\n\n        Args:\n            min_value (Float): min vaue of SNR defaults to 1.e-4.\n        \"\"\"\n        self.direction = TimeDirection.DIFFUSION\n        self.min_value = min_value\n\n    def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n        The SNR is the equivalent to alpha_bar**2 / (1 - alpha_bar**2) from DDPM.\n        See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py\n\n        Args:\n            t (Tensor): The input tensor representing the time steps.\n            device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n        Returns:\n            Tensor: A tensor representing the log SNR for the given time steps.\n        \"\"\"\n        # This is equivalanet to the interpolated one from -10 to 9.2\n        return -log(torch.expm1(self.min_value + 10 * (t**2))).to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearSNRTransform.__init__","title":"<code>__init__(min_value=0.0001)</code>","text":"<p>Initialize the Linear SNR Transform.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>Float</code> <p>min vaue of SNR defaults to 1.e-4.</p> <code>0.0001</code> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def __init__(self, min_value: Float = 1.0e-4):\n    \"\"\"Initialize the Linear SNR Transform.\n\n    Args:\n        min_value (Float): min vaue of SNR defaults to 1.e-4.\n    \"\"\"\n    self.direction = TimeDirection.DIFFUSION\n    self.min_value = min_value\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.LinearSNRTransform._calculate_log_snr","title":"<code>_calculate_log_snr(t, device='cpu')</code>","text":"<p>Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.</p> <p>The SNR is the equivalent to alpha_bar2 / (1 - alpha_bar2) from DDPM. See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor representing the time steps.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to place the schedule on. Defaults to \"cpu\".</p> <code>'cpu'</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the log SNR for the given time steps.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def _calculate_log_snr(self, t: Tensor, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Calculate the log signal-to-noise ratio (SNR) for the cosine noise schedule i.e. -gamma.\n\n    The SNR is the equivalent to alpha_bar**2 / (1 - alpha_bar**2) from DDPM.\n    See  https://openreview.net/attachment?id=2LdBqxc1Yv&amp;name=supplementary_material and https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/continuous_time_gaussian_diffusion.py\n\n    Args:\n        t (Tensor): The input tensor representing the time steps.\n        device (Optional[str]): The device to place the schedule on. Defaults to \"cpu\".\n\n    Returns:\n        Tensor: A tensor representing the log SNR for the given time steps.\n    \"\"\"\n    # This is equivalanet to the interpolated one from -10 to 9.2\n    return -log(torch.expm1(self.min_value + 10 * (t**2))).to(device)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/continuous_snr_transforms/#bionemo.moco.schedules.noise.continuous_snr_transforms.log","title":"<code>log(t, eps=1e-20)</code>","text":"<p>Compute the natural logarithm of a tensor, clamping values to avoid numerical instability.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>The input tensor.</p> required <code>eps</code> <code>float</code> <p>The minimum value to clamp the input tensor (default is 1e-20).</p> <code>1e-20</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The natural logarithm of the input tensor.</p> Source code in <code>bionemo/moco/schedules/noise/continuous_snr_transforms.py</code> <pre><code>def log(t, eps=1e-20):\n    \"\"\"Compute the natural logarithm of a tensor, clamping values to avoid numerical instability.\n\n    Args:\n        t (Tensor): The input tensor.\n        eps (float, optional): The minimum value to clamp the input tensor (default is 1e-20).\n\n    Returns:\n        Tensor: The natural logarithm of the input tensor.\n    \"\"\"\n    return torch.log(t.clamp(min=eps))\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/","title":"Discrete noise schedules","text":""},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteCosineNoiseSchedule","title":"<code>DiscreteCosineNoiseSchedule</code>","text":"<p>               Bases: <code>DiscreteNoiseSchedule</code></p> <p>A cosine discrete noise schedule.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>class DiscreteCosineNoiseSchedule(DiscreteNoiseSchedule):\n    \"\"\"A cosine discrete noise schedule.\"\"\"\n\n    def __init__(self, nsteps: int, nu: Float = 1.0, s: Float = 0.008):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            nsteps (int): Number of discrete steps.\n            nu (Optional[Float]): Hyperparameter for the cosine schedule exponent (default is 1.0).\n            s (Optional[Float]): Hyperparameter for the cosine schedule shift (default is 0.008).\n        \"\"\"\n        super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n        self.nu = nu\n        self.s = s\n\n    def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the cosine noise schedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        if nsteps is None:\n            nsteps = self.nsteps\n        steps = (\n            nsteps + 1\n        )  #! matches OpenAI code https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py#L62\n        x = torch.linspace(0, nsteps, steps, device=device)\n        alphas_cumprod = torch.cos(((x / nsteps) ** self.nu + self.s) / (1 + self.s) * torch.pi * 0.5) ** 2\n        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n        betas = torch.clip(betas, 0.001, 0.999)\n        return 1 - betas\n\n    def _clip_noise_schedule(self, alphas2: Tensor, clip_value: Float = 0.001) -&gt; Tensor:\n        \"\"\"For a noise schedule given by alpha^2, this clips alpha_t / alpha_t-1. This may help improve stability during sampling.\n\n        Args:\n            alphas2 (Tensor): The noise schedule given by alpha^2.\n            clip_value (Optional[Float]): The minimum value for alpha_t / alpha_t-1 (default is 0.001).\n\n        Returns:\n            Tensor: The clipped noise schedule.\n        \"\"\"\n        alphas2 = torch.cat([torch.ones(1, device=alphas2.device), alphas2], dim=0)\n\n        alphas_step = alphas2[1:] / alphas2[:-1]\n\n        alphas_step = torch.clamp(alphas_step, min=clip_value, max=1.0)\n        alphas2 = torch.cumprod(alphas_step, dim=0)\n\n        return alphas2\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteCosineNoiseSchedule.__init__","title":"<code>__init__(nsteps, nu=1.0, s=0.008)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>Number of discrete steps.</p> required <code>nu</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule exponent (default is 1.0).</p> <code>1.0</code> <code>s</code> <code>Optional[Float]</code> <p>Hyperparameter for the cosine schedule shift (default is 0.008).</p> <code>0.008</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def __init__(self, nsteps: int, nu: Float = 1.0, s: Float = 0.008):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        nsteps (int): Number of discrete steps.\n        nu (Optional[Float]): Hyperparameter for the cosine schedule exponent (default is 1.0).\n        s (Optional[Float]): Hyperparameter for the cosine schedule shift (default is 0.008).\n    \"\"\"\n    super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n    self.nu = nu\n    self.s = s\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteCosineNoiseSchedule._clip_noise_schedule","title":"<code>_clip_noise_schedule(alphas2, clip_value=0.001)</code>","text":"<p>For a noise schedule given by alpha^2, this clips alpha_t / alpha_t-1. This may help improve stability during sampling.</p> <p>Parameters:</p> Name Type Description Default <code>alphas2</code> <code>Tensor</code> <p>The noise schedule given by alpha^2.</p> required <code>clip_value</code> <code>Optional[Float]</code> <p>The minimum value for alpha_t / alpha_t-1 (default is 0.001).</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The clipped noise schedule.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def _clip_noise_schedule(self, alphas2: Tensor, clip_value: Float = 0.001) -&gt; Tensor:\n    \"\"\"For a noise schedule given by alpha^2, this clips alpha_t / alpha_t-1. This may help improve stability during sampling.\n\n    Args:\n        alphas2 (Tensor): The noise schedule given by alpha^2.\n        clip_value (Optional[Float]): The minimum value for alpha_t / alpha_t-1 (default is 0.001).\n\n    Returns:\n        Tensor: The clipped noise schedule.\n    \"\"\"\n    alphas2 = torch.cat([torch.ones(1, device=alphas2.device), alphas2], dim=0)\n\n    alphas_step = alphas2[1:] / alphas2[:-1]\n\n    alphas_step = torch.clamp(alphas_step, min=clip_value, max=1.0)\n    alphas2 = torch.cumprod(alphas_step, dim=0)\n\n    return alphas2\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteCosineNoiseSchedule._generate_schedule","title":"<code>_generate_schedule(nsteps=None, device='cpu')</code>","text":"<p>Generate the cosine noise schedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Generate the cosine noise schedule.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    if nsteps is None:\n        nsteps = self.nsteps\n    steps = (\n        nsteps + 1\n    )  #! matches OpenAI code https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py#L62\n    x = torch.linspace(0, nsteps, steps, device=device)\n    alphas_cumprod = torch.cos(((x / nsteps) ** self.nu + self.s) / (1 + self.s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    betas = torch.clip(betas, 0.001, 0.999)\n    return 1 - betas\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteLinearNoiseSchedule","title":"<code>DiscreteLinearNoiseSchedule</code>","text":"<p>               Bases: <code>DiscreteNoiseSchedule</code></p> <p>A linear discrete noise schedule.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>class DiscreteLinearNoiseSchedule(DiscreteNoiseSchedule):\n    \"\"\"A linear discrete noise schedule.\"\"\"\n\n    def __init__(self, nsteps: int, beta_start: Float = 1e-4, beta_end: Float = 0.02):\n        \"\"\"Initialize the CosineNoiseSchedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            beta_start (Optional[int]): starting beta value. Defaults to 1e-4.\n            beta_end (Optional[int]): end beta value. Defaults to 0.02.\n        \"\"\"\n        super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n\n    def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the cosine noise schedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        if nsteps is None:\n            nsteps = self.nsteps\n        betas = torch.linspace(self.beta_start, self.beta_end, nsteps, dtype=torch.float32, device=device)\n        return 1 - betas\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteLinearNoiseSchedule.__init__","title":"<code>__init__(nsteps, beta_start=0.0001, beta_end=0.02)</code>","text":"<p>Initialize the CosineNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> required <code>beta_start</code> <code>Optional[int]</code> <p>starting beta value. Defaults to 1e-4.</p> <code>0.0001</code> <code>beta_end</code> <code>Optional[int]</code> <p>end beta value. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def __init__(self, nsteps: int, beta_start: Float = 1e-4, beta_end: Float = 0.02):\n    \"\"\"Initialize the CosineNoiseSchedule.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        beta_start (Optional[int]): starting beta value. Defaults to 1e-4.\n        beta_end (Optional[int]): end beta value. Defaults to 0.02.\n    \"\"\"\n    super().__init__(nsteps=nsteps, direction=TimeDirection.DIFFUSION)\n    self.beta_start = beta_start\n    self.beta_end = beta_end\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteLinearNoiseSchedule._generate_schedule","title":"<code>_generate_schedule(nsteps=None, device='cpu')</code>","text":"<p>Generate the cosine noise schedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Generate the cosine noise schedule.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    if nsteps is None:\n        nsteps = self.nsteps\n    betas = torch.linspace(self.beta_start, self.beta_end, nsteps, dtype=torch.float32, device=device)\n    return 1 - betas\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule","title":"<code>DiscreteNoiseSchedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for discrete noise schedules.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>class DiscreteNoiseSchedule(ABC):\n    \"\"\"A base class for discrete noise schedules.\"\"\"\n\n    def __init__(self, nsteps: int, direction: TimeDirection):\n        \"\"\"Initialize the DiscreteNoiseSchedule.\n\n        Args:\n           nsteps (int): number of discrete steps.\n           direction (TimeDirection): required this defines in which direction the scheduler was built\n        \"\"\"\n        self.nsteps = nsteps\n        self.direction = string_to_enum(direction, TimeDirection)\n\n    def generate_schedule(\n        self,\n        nsteps: Optional[int] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Generate the noise schedule as a tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n            synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one (default is None).\n        \"\"\"\n        schedule = self._generate_schedule(nsteps, device)\n        if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n            return torch.flip(schedule, dims=[0])\n        else:\n            return schedule\n\n    @abstractmethod\n    def _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n        \"\"\"Generate the noise schedule tensor.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        \"\"\"\n        pass\n\n    def calculate_derivative(\n        self,\n        nsteps: Optional[int] = None,\n        device: Union[str, torch.device] = \"cpu\",\n        synchronize: Optional[TimeDirection] = None,\n    ) -&gt; Tensor:\n        \"\"\"Calculate the time derivative of the schedule.\n\n        Args:\n            nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n            device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n            synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n                this parameter allows to flip the direction to match the specified one (default is None).\n\n        Returns:\n            Tensor: A tensor representing the time derivative of the schedule.\n\n        Raises:\n            NotImplementedError: If the derivative calculation is not implemented for this schedule.\n        \"\"\"\n        raise NotImplementedError(\"Derivative calculation is not implemented for this schedule.\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule.__init__","title":"<code>__init__(nsteps, direction)</code>","text":"<p>Initialize the DiscreteNoiseSchedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>int</code> <p>number of discrete steps.</p> required <code>direction</code> <code>TimeDirection</code> <p>required this defines in which direction the scheduler was built</p> required Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def __init__(self, nsteps: int, direction: TimeDirection):\n    \"\"\"Initialize the DiscreteNoiseSchedule.\n\n    Args:\n       nsteps (int): number of discrete steps.\n       direction (TimeDirection): required this defines in which direction the scheduler was built\n    \"\"\"\n    self.nsteps = nsteps\n    self.direction = string_to_enum(direction, TimeDirection)\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule._generate_schedule","title":"<code>_generate_schedule(nsteps=None, device='cpu')</code>  <code>abstractmethod</code>","text":"<p>Generate the noise schedule tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>@abstractmethod\ndef _generate_schedule(self, nsteps: Optional[int] = None, device: Union[str, torch.device] = \"cpu\") -&gt; Tensor:\n    \"\"\"Generate the noise schedule tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule.calculate_derivative","title":"<code>calculate_derivative(nsteps=None, device='cpu', synchronize=None)</code>","text":"<p>Calculate the time derivative of the schedule.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> <code>synchronize</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>A tensor representing the time derivative of the schedule.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the derivative calculation is not implemented for this schedule.</p> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def calculate_derivative(\n    self,\n    nsteps: Optional[int] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Calculate the time derivative of the schedule.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one (default is None).\n\n    Returns:\n        Tensor: A tensor representing the time derivative of the schedule.\n\n    Raises:\n        NotImplementedError: If the derivative calculation is not implemented for this schedule.\n    \"\"\"\n    raise NotImplementedError(\"Derivative calculation is not implemented for this schedule.\")\n</code></pre>"},{"location":"API_reference/bionemo/moco/schedules/noise/discrete_noise_schedules/#bionemo.moco.schedules.noise.discrete_noise_schedules.DiscreteNoiseSchedule.generate_schedule","title":"<code>generate_schedule(nsteps=None, device='cpu', synchronize=None)</code>","text":"<p>Generate the noise schedule as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>nsteps</code> <code>Optional[int]</code> <p>Number of time steps. If None, uses the value from initialization.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to place the schedule on (default is \"cpu\").</p> <code>'cpu'</code> <code>synchronize</code> <code>Optional[str]</code> <p>TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction, this parameter allows to flip the direction to match the specified one (default is None).</p> <code>None</code> Source code in <code>bionemo/moco/schedules/noise/discrete_noise_schedules.py</code> <pre><code>def generate_schedule(\n    self,\n    nsteps: Optional[int] = None,\n    device: Union[str, torch.device] = \"cpu\",\n    synchronize: Optional[TimeDirection] = None,\n) -&gt; Tensor:\n    \"\"\"Generate the noise schedule as a tensor.\n\n    Args:\n        nsteps (Optional[int]): Number of time steps. If None, uses the value from initialization.\n        device (Optional[str]): Device to place the schedule on (default is \"cpu\").\n        synchronize (Optional[str]): TimeDirection to synchronize the schedule with. If the schedule is defined with a different direction,\n            this parameter allows to flip the direction to match the specified one (default is None).\n    \"\"\"\n    schedule = self._generate_schedule(nsteps, device)\n    if synchronize and self.direction != string_to_enum(synchronize, TimeDirection):\n        return torch.flip(schedule, dims=[0])\n    else:\n        return schedule\n</code></pre>"},{"location":"API_reference/bionemo/moco/testing/parallel_test_utils/","title":"Parallel test utils","text":""},{"location":"API_reference/bionemo/moco/testing/parallel_test_utils/#bionemo.moco.testing.parallel_test_utils.clean_up_distributed","title":"<code>clean_up_distributed()</code>","text":"<p>Cleans up the distributed environment.</p> <p>Destroys the process group and empties the CUDA cache.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>bionemo/moco/testing/parallel_test_utils.py</code> <pre><code>def clean_up_distributed() -&gt; None:\n    \"\"\"Cleans up the distributed environment.\n\n    Destroys the process group and empties the CUDA cache.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    if dist.is_initialized():\n        dist.destroy_process_group()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"API_reference/bionemo/moco/testing/parallel_test_utils/#bionemo.moco.testing.parallel_test_utils.parallel_context","title":"<code>parallel_context(rank=0, world_size=1)</code>","text":"<p>Context manager for torch distributed testing.</p> <p>Sets up and cleans up the distributed environment, including the device mesh.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank of the process. Defaults to 0.</p> <code>0</code> <code>world_size</code> <code>int</code> <p>The world size of the distributed environment. Defaults to 1.</p> <code>1</code> <p>Yields:</p> Type Description <p>None</p> Source code in <code>bionemo/moco/testing/parallel_test_utils.py</code> <pre><code>@contextmanager\ndef parallel_context(\n    rank: int = 0,\n    world_size: int = 1,\n):\n    \"\"\"Context manager for torch distributed testing.\n\n    Sets up and cleans up the distributed environment, including the device mesh.\n\n    Args:\n        rank (int): The rank of the process. Defaults to 0.\n        world_size (int): The world size of the distributed environment. Defaults to 1.\n\n    Yields:\n        None\n    \"\"\"\n    with MonkeyPatch.context() as context:\n        clean_up_distributed()\n\n        # distributed and parallel state set up\n        if not os.environ.get(\"MASTER_ADDR\", None):\n            context.setenv(\"MASTER_ADDR\", DEFAULT_MASTER_ADDR)\n        if not os.environ.get(\"MASTER_PORT\", None):\n            context.setenv(\"MASTER_PORT\", DEFAULT_MASTER_PORT)\n        context.setenv(\"RANK\", str(rank))\n\n        dist.init_process_group(backend=\"nccl\", world_size=world_size)\n\n        yield\n\n        clean_up_distributed()\n</code></pre>"},{"location":"API_reference/bionemo/noodles/nvfaidx/","title":"Nvfaidx","text":""},{"location":"API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.NvFaidx","title":"<code>NvFaidx</code>","text":"<p>NvFaidx is a rest + pyo3 replacement for PyFaidx that provides a dictionary-like interface to reference genomes.</p> <p>This class is a collection of SequenceAccessors, organized by sequence-id in a dictionary like manner. SequenceAcecessors  are similar dict-like interfaces over actual sequence entries in the underlying index. Furthermore, utilities are provided  for parsing faidx files, building faidx files, and storing faidx files to disk.</p> <p>IMPORTANT by default all fasta files build an in-memory faidx object. This is due easy mistakes that may occur if a faidx file is constructed while using multi-processing (such as a default constructor that creates these files on the fly). However, methods exist to create these methods manually where a user has more control over multiprocessing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; index = NvFaidx(fasta_file, faidx_path=None, ignore_existing_fai=True)\n&gt;&gt;&gt; index['chr1'] # Returns a SequenceAccessor for chr1\n&gt;&gt;&gt; index['chr1'][0:10] # Returns the first 10 bases of chr1.\n&gt;&gt;&gt; faidx_filename = NvFaidx.create_faidx(fasta_file) # Creates a faidx to disk.\n&gt;&gt;&gt; index = NvFaidx(fasta_File, faidx_filename, ignore_existing_fai = True) # Uses a faidx from disk.\n</code></pre> <p>Motivation and more details:</p> <p>NvFaidx is built using Noodles as a backend for Fai objects, and memory maps for backing the underlying fasta. Using a backend of Memmaps provide the following benefits:     - The kernel implements this mechanism by using page faults     - Each read in a mmap'd file results in a page fault: there's nothing in memory to read!     - The kernel handles this page fault by going to the disk, reading the file in the specified offset + index,         then returning to the user process with what it just read, preventing penalties from context switching.</p> <p>Context: PyFaidx or any buffered read based index is not process safe, and therefore does not play nice with pytorch dataloaders. Due to the order of operations, the underlying file handle is shared between processes, when <code>seek()</code> is called to perform random lookups, this can cause unexpected behavior in the forked processes. Ref: https://github.com/mdshw5/pyfaidx/issues/211</p> For a good solution we need three things <p>1) Safe index creation, in multi-process or multi-node scenarios, this should be restricted to a single node     where all workers block until it is complete (not implemented above) 2) Index object instantion must be fast. 3) Read-only use of the index object must be both thread safe and process safe with python.</p> <p>See Also: bionemo.noodles.nvfaidx.SequenceAccessor</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>class NvFaidx:\n    \"\"\"NvFaidx is a rest + pyo3 replacement for PyFaidx that provides a dictionary-like interface to reference genomes.\n\n    This class is a collection of SequenceAccessors, organized by sequence-id in a dictionary like manner. SequenceAcecessors\n     are similar dict-like interfaces over actual sequence entries in the underlying index. Furthermore, utilities are provided\n     for parsing faidx files, building faidx files, and storing faidx files to disk.\n\n    **IMPORTANT** by default all fasta files build an in-memory faidx object. This is due easy mistakes that may occur\n    if a faidx file is constructed while using multi-processing (such as a default constructor that creates these files on the fly).\n    However, methods exist to create these methods manually where a user has more control over multiprocessing.\n\n    Examples:\n        &gt;&gt;&gt; index = NvFaidx(fasta_file, faidx_path=None, ignore_existing_fai=True)\n        &gt;&gt;&gt; index['chr1'] # Returns a SequenceAccessor for chr1\n        &gt;&gt;&gt; index['chr1'][0:10] # Returns the first 10 bases of chr1.\n        &gt;&gt;&gt; faidx_filename = NvFaidx.create_faidx(fasta_file) # Creates a faidx to disk.\n        &gt;&gt;&gt; index = NvFaidx(fasta_File, faidx_filename, ignore_existing_fai = True) # Uses a faidx from disk.\n\n\n    Motivation and more details:\n\n    NvFaidx is built using Noodles as a backend for Fai objects, and memory maps for backing the underlying fasta.\n    Using a backend of Memmaps provide the following benefits:\n        - The kernel implements this mechanism by using page faults\n        - Each read in a mmap'd file results in a page fault: there's nothing in memory to read!\n        - The kernel handles this page fault by going to the disk, reading the file in the specified offset + index,\n            then returning to the user process with what it just read, preventing penalties from context switching.\n\n    *Context*: PyFaidx or _any_ buffered read based index is not process safe, and therefore does not play nice with pytorch dataloaders.\n    Due to the order of operations, the underlying file handle is shared between processes, when `seek()` is called to perform random lookups,\n    this can cause unexpected behavior in the forked processes.\n    Ref: https://github.com/mdshw5/pyfaidx/issues/211\n\n    For a good solution we need three things:\n        1) Safe index creation, in multi-process or multi-node scenarios, this should be restricted to a single node\n            where all workers block until it is complete (not implemented above)\n        2) Index object instantion must be fast.\n        3) Read-only use of the index object must be both thread safe and process safe with python.\n\n    See Also: bionemo.noodles.nvfaidx.SequenceAccessor\n    \"\"\"\n\n    def __init__(\n        self,\n        fasta_path: str | Path,\n        faidx_path: Optional[str | Path] = None,\n        ignore_existing_fai: bool = True,\n        allow_duplicate_seqids: bool = False,\n    ):\n        \"\"\"Construct a dict-like object representing a memmapped, indexed FASTA file.\n\n        This is an indexed fasta reader. Consequences of this are that the FASTA file must be well formed, meaning\n        sequence-ids and line-lengths must conform to FASTA standards. Additionally, the order of returned seqid, sequence\n        pairs when iterating over the index is not guaranteed to be the same order as the underlying fasta file.\n\n        Args:\n            fasta_path (str): Path to the FASTA file.\n            faidx_path (str): Path to the FAI index file. If None, one will be created.\n            ignore_existing_fai (bool): If True, ignore any existing FAI file and create an in-memory index. Note that\n                this will also ignore `faidx_path`.\n            allow_duplicate_seqids (bool): If true, will produce index for invalid fastas which contain duplicate seqids.\n                In this scenario, indexing is performed by integer rather than strings.\n\n                Example with invalid seqids.\n                    &gt;chr1 dupes|not|allowd\n                    ATGATGATGATG\n                    &gt;chr1 whoops|there|is|dupe\n                    ATGATGATGATG\n                NvFaidx:\n                    {\n                        0 : SequenceAccessor(chr1 dupes|not|allowed),\n                        1 : SequenceAccessor(chr1 whoops|there|is|dupe)\n                    }\n\n        \"\"\"\n        if isinstance(fasta_path, Path):\n            fasta_path = str(fasta_path)\n        elif not isinstance(fasta_path, str):\n            raise TypeError(f\"fasta_path must be a `str` or `pathlib.Path`, got: {type(fasta_path)}\")\n\n        if isinstance(faidx_path, Path):\n            faidx_path = str(faidx_path)\n        elif not isinstance(faidx_path, str) and faidx_path is not None:\n            raise TypeError(f\"faidx_path must be a `str`, `pathlib.Path`, or None. got: {type(faidx_path)}\")\n\n        match (fasta_path, faidx_path, ignore_existing_fai):\n            case (_, _, True):\n                self.reader = PyIndexedMmapFastaReader(fasta_path, ignore_existing_fai=ignore_existing_fai)\n            case (_, faidx_path, _) if faidx_path is not None:\n                self.reader = PyIndexedMmapFastaReader.from_fasta_and_faidx(fasta_path, faidx_path)\n            # In this case, faidx path is None and ignore_existing is False, and it covers all other cases.\n            case (_, None, False):\n                # But the logic here doesnt make sense, ignore_existing is false, but it should only use if it if it exists.\n                self.reader = PyIndexedMmapFastaReader(fasta_path, False)\n            case _:\n                raise ValueError(\"unreachable condition.\")\n\n        self.records: Dict[str | int, PyFaidxRecord] = {record.name: record for record in self.reader.records()}\n        if len(self.records) != len(self.reader.records()):\n            if not allow_duplicate_seqids:\n                raise ValueError(\n                    \"Non-unique sequence-id detected in FASTA, this is invalid. Correct headers and try again or pass allow_duplicate_seqid'\"\n                )\n            else:\n                self.records: Dict[str | int, PyFaidxRecord] = dict(enumerate(self.reader.records()))\n\n    def __getitem__(self, seqid: str) -&gt; SequenceAccessor:  # noqa: D105\n        if seqid not in self.records:\n            raise KeyError(f\"Sequence '{seqid}' not found in index.\")\n\n        # Return a SequenceAccessor for slicing access\n        record_length = self.records[seqid].length\n        return SequenceAccessor(self.reader, seqid, record_length)\n\n    def __contains__(self, seqid: str) -&gt; bool:  # noqa: D105\n        return seqid in self.records\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        return len(self.records)\n\n    def keys(self) -&gt; set[str]:  # noqa: D102\n        return set(self.records.keys())\n\n    # These provide dict like iteration functionality\n    def __iter__(self):  # noqa: D105\n        return iter(self.keys())\n\n    def items(self):  # noqa: D102\n        for key in self.keys():\n            yield key, self[key][:]\n\n    def values(self):  # noqa: D102\n        for key in self.keys():\n            yield self[key][:]\n\n    @staticmethod\n    def create_faidx(fasta_filename: str | Path, force: bool = False) -&gt; str:\n        \"\"\"Create a FAI index for a FASTA file, the result is saved in the same location as `fasta_filename`, with a .fai extension.\n\n        Args:\n            fasta_filename (str): Path to the FASTA file to be indexed.\n            force (bool): Delete existing faidx file and create a new index file.\n        \"\"\"\n        if isinstance(fasta_filename, Path):\n            fasta_filename = str(fasta_filename)\n        return PyIndexedMmapFastaReader.create_faidx(fasta_filename, force)\n</code></pre>"},{"location":"API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.NvFaidx.__init__","title":"<code>__init__(fasta_path, faidx_path=None, ignore_existing_fai=True, allow_duplicate_seqids=False)</code>","text":"<p>Construct a dict-like object representing a memmapped, indexed FASTA file.</p> <p>This is an indexed fasta reader. Consequences of this are that the FASTA file must be well formed, meaning sequence-ids and line-lengths must conform to FASTA standards. Additionally, the order of returned seqid, sequence pairs when iterating over the index is not guaranteed to be the same order as the underlying fasta file.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_path</code> <code>str</code> <p>Path to the FASTA file.</p> required <code>faidx_path</code> <code>str</code> <p>Path to the FAI index file. If None, one will be created.</p> <code>None</code> <code>ignore_existing_fai</code> <code>bool</code> <p>If True, ignore any existing FAI file and create an in-memory index. Note that this will also ignore <code>faidx_path</code>.</p> <code>True</code> <code>allow_duplicate_seqids</code> <code>bool</code> <p>If true, will produce index for invalid fastas which contain duplicate seqids. In this scenario, indexing is performed by integer rather than strings.</p> <p>Example with invalid seqids.     &gt;chr1 dupes|not|allowd     ATGATGATGATG     &gt;chr1 whoops|there|is|dupe     ATGATGATGATG NvFaidx:     {         0 : SequenceAccessor(chr1 dupes|not|allowed),         1 : SequenceAccessor(chr1 whoops|there|is|dupe)     }</p> <code>False</code> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def __init__(\n    self,\n    fasta_path: str | Path,\n    faidx_path: Optional[str | Path] = None,\n    ignore_existing_fai: bool = True,\n    allow_duplicate_seqids: bool = False,\n):\n    \"\"\"Construct a dict-like object representing a memmapped, indexed FASTA file.\n\n    This is an indexed fasta reader. Consequences of this are that the FASTA file must be well formed, meaning\n    sequence-ids and line-lengths must conform to FASTA standards. Additionally, the order of returned seqid, sequence\n    pairs when iterating over the index is not guaranteed to be the same order as the underlying fasta file.\n\n    Args:\n        fasta_path (str): Path to the FASTA file.\n        faidx_path (str): Path to the FAI index file. If None, one will be created.\n        ignore_existing_fai (bool): If True, ignore any existing FAI file and create an in-memory index. Note that\n            this will also ignore `faidx_path`.\n        allow_duplicate_seqids (bool): If true, will produce index for invalid fastas which contain duplicate seqids.\n            In this scenario, indexing is performed by integer rather than strings.\n\n            Example with invalid seqids.\n                &gt;chr1 dupes|not|allowd\n                ATGATGATGATG\n                &gt;chr1 whoops|there|is|dupe\n                ATGATGATGATG\n            NvFaidx:\n                {\n                    0 : SequenceAccessor(chr1 dupes|not|allowed),\n                    1 : SequenceAccessor(chr1 whoops|there|is|dupe)\n                }\n\n    \"\"\"\n    if isinstance(fasta_path, Path):\n        fasta_path = str(fasta_path)\n    elif not isinstance(fasta_path, str):\n        raise TypeError(f\"fasta_path must be a `str` or `pathlib.Path`, got: {type(fasta_path)}\")\n\n    if isinstance(faidx_path, Path):\n        faidx_path = str(faidx_path)\n    elif not isinstance(faidx_path, str) and faidx_path is not None:\n        raise TypeError(f\"faidx_path must be a `str`, `pathlib.Path`, or None. got: {type(faidx_path)}\")\n\n    match (fasta_path, faidx_path, ignore_existing_fai):\n        case (_, _, True):\n            self.reader = PyIndexedMmapFastaReader(fasta_path, ignore_existing_fai=ignore_existing_fai)\n        case (_, faidx_path, _) if faidx_path is not None:\n            self.reader = PyIndexedMmapFastaReader.from_fasta_and_faidx(fasta_path, faidx_path)\n        # In this case, faidx path is None and ignore_existing is False, and it covers all other cases.\n        case (_, None, False):\n            # But the logic here doesnt make sense, ignore_existing is false, but it should only use if it if it exists.\n            self.reader = PyIndexedMmapFastaReader(fasta_path, False)\n        case _:\n            raise ValueError(\"unreachable condition.\")\n\n    self.records: Dict[str | int, PyFaidxRecord] = {record.name: record for record in self.reader.records()}\n    if len(self.records) != len(self.reader.records()):\n        if not allow_duplicate_seqids:\n            raise ValueError(\n                \"Non-unique sequence-id detected in FASTA, this is invalid. Correct headers and try again or pass allow_duplicate_seqid'\"\n            )\n        else:\n            self.records: Dict[str | int, PyFaidxRecord] = dict(enumerate(self.reader.records()))\n</code></pre>"},{"location":"API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.NvFaidx.create_faidx","title":"<code>create_faidx(fasta_filename, force=False)</code>  <code>staticmethod</code>","text":"<p>Create a FAI index for a FASTA file, the result is saved in the same location as <code>fasta_filename</code>, with a .fai extension.</p> <p>Parameters:</p> Name Type Description Default <code>fasta_filename</code> <code>str</code> <p>Path to the FASTA file to be indexed.</p> required <code>force</code> <code>bool</code> <p>Delete existing faidx file and create a new index file.</p> <code>False</code> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>@staticmethod\ndef create_faidx(fasta_filename: str | Path, force: bool = False) -&gt; str:\n    \"\"\"Create a FAI index for a FASTA file, the result is saved in the same location as `fasta_filename`, with a .fai extension.\n\n    Args:\n        fasta_filename (str): Path to the FASTA file to be indexed.\n        force (bool): Delete existing faidx file and create a new index file.\n    \"\"\"\n    if isinstance(fasta_filename, Path):\n        fasta_filename = str(fasta_filename)\n    return PyIndexedMmapFastaReader.create_faidx(fasta_filename, force)\n</code></pre>"},{"location":"API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor","title":"<code>SequenceAccessor</code>","text":"<p>SequenceAccessor provides a dictionary-like interface to a single sequence in an indexed FASTA file.</p> <p>This allows for random access to the sequence, either by index, or by slice.</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>class SequenceAccessor:\n    \"\"\"SequenceAccessor provides a dictionary-like interface to a single sequence in an indexed FASTA file.\n\n    This allows for random access to the sequence, either by index, or by slice.\n    \"\"\"\n\n    def __init__(self, reader: PyIndexedMmapFastaReader, seqid: str, length: int) -&gt; None:\n        \"\"\"Construct a SequenceAccessor object. Ultimately this is used as a convenience object with NvFaidx.\n\n        When querying the following are true:\n            - Negative indexing is supported, but it does not wrap. so query[-10000] for a sequence of length 1 will fail.\n            - out of bounds indexing is truncated: query[1:999999999] will return a string from position 1 to the terminus.\n            - reversed slices return the empty string: query[999:1] is the empty string.\n            - empty slice returns the full string: query[:] is the full string of the sequence.\n            - beginning of slice is beyond the range of the contig, the empty string is returned.\n\n        Additionally there are convenience methods that you may find useful in the class definition.\n\n        Args:\n            reader (PyIndexedMmapFastaReader): The indexed reader object that provides access to the underlying FASTA file.\n            seqid (str): The sequence identifier.\n            length (int): The length of the sequence.\n        \"\"\"\n        self.reader = reader\n        self.seqid = seqid\n        self.length = length\n\n    def __getitem__(self, key: int | slice) -&gt; str:  # noqa: D105\n        if isinstance(key, slice):\n            # Provide defaults for missing arguments in the slice.\n            start = key.start if key.start is not None else 0\n            stop = key.stop if key.stop is not None else self.length\n\n            # Handle negative cases, remember, you can be arbitrarily negative in a slice.\n            if start &lt; 0:\n                start += self.length\n            if stop &lt; 0:\n                stop += self.length\n\n            # Clamp normalized indices to valid range\n            start = max(0, min(self.length, start))\n            stop = max(0, min(self.length, stop))\n\n            # Bounds checking after normalization\n            if start &gt; stop:\n                return \"\"  # Return empty string for an empty slice\n\n            # Construct region string\n            region_str = f\"{self.seqid}:{start + 1}-{stop}\"  # +1 for 1-based indexing\n            return self.reader.read_sequence_mmap(region_str)\n\n        elif isinstance(key, int):\n            # Normalize single integer for negative indexing\n            if key &lt; 0:\n                key += self.length\n\n            # Bounds checking\n            if key &lt; 0 or key &gt;= self.length:\n                raise IndexError(f\"Position {key} is out of bounds for '{self.seqid}' with length {self.length}.\")\n\n            # Query single nucleotide by creating a 1-length region\n            region_str = f\"{self.seqid}:{key + 1}-{key + 1}\"  # +1 for 1-based indexing\n            return self.reader.read_sequence_mmap(region_str)\n\n        else:\n            raise TypeError(\"Index must be an integer or a slice.\")\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        return self.length\n\n    def sequence_id(self) -&gt; str:\n        \"\"\"Returns the sequenceid of this SequenceAccessor.\"\"\"\n        return self.seqid\n\n    def sequence(self) -&gt; str:\n        \"\"\"Returns the sequence associated with this SequenceAccessor as a string.\"\"\"\n        return self[:]\n</code></pre>"},{"location":"API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor.__init__","title":"<code>__init__(reader, seqid, length)</code>","text":"<p>Construct a SequenceAccessor object. Ultimately this is used as a convenience object with NvFaidx.</p> When querying the following are true <ul> <li>Negative indexing is supported, but it does not wrap. so query[-10000] for a sequence of length 1 will fail.</li> <li>out of bounds indexing is truncated: query[1:999999999] will return a string from position 1 to the terminus.</li> <li>reversed slices return the empty string: query[999:1] is the empty string.</li> <li>empty slice returns the full string: query[:] is the full string of the sequence.</li> <li>beginning of slice is beyond the range of the contig, the empty string is returned.</li> </ul> <p>Additionally there are convenience methods that you may find useful in the class definition.</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>PyIndexedMmapFastaReader</code> <p>The indexed reader object that provides access to the underlying FASTA file.</p> required <code>seqid</code> <code>str</code> <p>The sequence identifier.</p> required <code>length</code> <code>int</code> <p>The length of the sequence.</p> required Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def __init__(self, reader: PyIndexedMmapFastaReader, seqid: str, length: int) -&gt; None:\n    \"\"\"Construct a SequenceAccessor object. Ultimately this is used as a convenience object with NvFaidx.\n\n    When querying the following are true:\n        - Negative indexing is supported, but it does not wrap. so query[-10000] for a sequence of length 1 will fail.\n        - out of bounds indexing is truncated: query[1:999999999] will return a string from position 1 to the terminus.\n        - reversed slices return the empty string: query[999:1] is the empty string.\n        - empty slice returns the full string: query[:] is the full string of the sequence.\n        - beginning of slice is beyond the range of the contig, the empty string is returned.\n\n    Additionally there are convenience methods that you may find useful in the class definition.\n\n    Args:\n        reader (PyIndexedMmapFastaReader): The indexed reader object that provides access to the underlying FASTA file.\n        seqid (str): The sequence identifier.\n        length (int): The length of the sequence.\n    \"\"\"\n    self.reader = reader\n    self.seqid = seqid\n    self.length = length\n</code></pre>"},{"location":"API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor.sequence","title":"<code>sequence()</code>","text":"<p>Returns the sequence associated with this SequenceAccessor as a string.</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def sequence(self) -&gt; str:\n    \"\"\"Returns the sequence associated with this SequenceAccessor as a string.\"\"\"\n    return self[:]\n</code></pre>"},{"location":"API_reference/bionemo/noodles/nvfaidx/#bionemo.noodles.nvfaidx.SequenceAccessor.sequence_id","title":"<code>sequence_id()</code>","text":"<p>Returns the sequenceid of this SequenceAccessor.</p> Source code in <code>bionemo/noodles/nvfaidx.py</code> <pre><code>def sequence_id(self) -&gt; str:\n    \"\"\"Returns the sequenceid of this SequenceAccessor.\"\"\"\n    return self.seqid\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/","title":"Single cell row dataset","text":""},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset","title":"<code>SingleCellRowDataset</code>","text":"<p>               Bases: <code>SingleCellRowDatasetCore</code>, <code>Dataset</code></p> <p>One row in an ann dataframe (hdf5 file with a spare array format).</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>class SingleCellRowDataset(SingleCellRowDatasetCore, Dataset):\n    \"\"\"One row in an ann dataframe (hdf5 file with a spare array format).\"\"\"\n\n    @abstractmethod\n    def load(self, data_path: str) -&gt; None:\n        \"\"\"Loads the data from datapath.\n\n        Calls to __len__ and __getitem__ Must be valid after a call to\n        this method.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def save(self, data_path: str) -&gt; None:\n        \"\"\"Saves the class to an archive at datapath.\"\"\"\n        raise NotImplementedError()\n\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset.load","title":"<code>load(data_path)</code>  <code>abstractmethod</code>","text":"<p>Loads the data from datapath.</p> <p>Calls to len and getitem Must be valid after a call to this method.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef load(self, data_path: str) -&gt; None:\n    \"\"\"Loads the data from datapath.\n\n    Calls to __len__ and __getitem__ Must be valid after a call to\n    this method.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset.save","title":"<code>save(data_path)</code>  <code>abstractmethod</code>","text":"<p>Saves the class to an archive at datapath.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef save(self, data_path: str) -&gt; None:\n    \"\"\"Saves the class to an archive at datapath.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore","title":"<code>SingleCellRowDatasetCore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Implements the actual ann data-like interface.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>class SingleCellRowDatasetCore(ABC):\n    \"\"\"Implements the actual ann data-like interface.\"\"\"\n\n    @abstractmethod\n    def load_h5ad(self, h5ad_path: str) -&gt; None:\n        \"\"\"Loads an H5AD file and converts it into the backing representation.\n\n        Calls to __len__ and __getitem__ Must be valid after a call to\n        this method.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Return the number of non-zero values in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_of_values(self) -&gt; int:\n        \"\"\"Return the total number of values in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_of_rows(self) -&gt; int:\n        \"\"\"Return the number of rows in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Returns the shape of the object, which may be ragged.\n\n        A ragged dataset is where the number and dimension of features\n        can be different at every row.\n        \"\"\"\n        raise NotImplementedError()\n\n    def sparsity(self) -&gt; float:\n        \"\"\"Return the sparsity of the underlying data.\n\n        Sparsity is defined as the fraction of zero values in the data.\n        It is within the range [0, 1.0]. If there are no values, the\n        sparsity is defined as 0.0.\n        \"\"\"\n        total_values = self.number_of_values()\n        if total_values == 0:\n            return 0.0\n\n        nonzero_values = self.number_nonzero_values()\n        zero_values = total_values - nonzero_values\n        sparsity_value = zero_values / total_values\n        return sparsity_value\n\n    @abstractmethod\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.load_h5ad","title":"<code>load_h5ad(h5ad_path)</code>  <code>abstractmethod</code>","text":"<p>Loads an H5AD file and converts it into the backing representation.</p> <p>Calls to len and getitem Must be valid after a call to this method.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef load_h5ad(self, h5ad_path: str) -&gt; None:\n    \"\"\"Loads an H5AD file and converts it into the backing representation.\n\n    Calls to __len__ and __getitem__ Must be valid after a call to\n    this method.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_nonzero_values","title":"<code>number_nonzero_values()</code>  <code>abstractmethod</code>","text":"<p>Return the number of non-zero values in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_nonzero_values(self) -&gt; int:\n    \"\"\"Return the number of non-zero values in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_of_rows","title":"<code>number_of_rows()</code>  <code>abstractmethod</code>","text":"<p>Return the number of rows in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_of_rows(self) -&gt; int:\n    \"\"\"Return the number of rows in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_of_values","title":"<code>number_of_values()</code>  <code>abstractmethod</code>","text":"<p>Return the total number of values in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_of_values(self) -&gt; int:\n    \"\"\"Return the total number of values in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.shape","title":"<code>shape()</code>  <code>abstractmethod</code>","text":"<p>Returns the shape of the object, which may be ragged.</p> <p>A ragged dataset is where the number and dimension of features can be different at every row.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Returns the shape of the object, which may be ragged.\n\n    A ragged dataset is where the number and dimension of features\n    can be different at every row.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.sparsity","title":"<code>sparsity()</code>","text":"<p>Return the sparsity of the underlying data.</p> <p>Sparsity is defined as the fraction of zero values in the data. It is within the range [0, 1.0]. If there are no values, the sparsity is defined as 0.0.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>def sparsity(self) -&gt; float:\n    \"\"\"Return the sparsity of the underlying data.\n\n    Sparsity is defined as the fraction of zero values in the data.\n    It is within the range [0, 1.0]. If there are no values, the\n    sparsity is defined as 0.0.\n    \"\"\"\n    total_values = self.number_of_values()\n    if total_values == 0:\n        return 0.0\n\n    nonzero_values = self.number_nonzero_values()\n    zero_values = total_values - nonzero_values\n    sparsity_value = zero_values / total_values\n    return sparsity_value\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.version","title":"<code>version()</code>  <code>abstractmethod</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/","title":"Row feature index","text":""},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex","title":"<code>RowFeatureIndex</code>","text":"<p>Maintains a mapping between a row and its features.</p> <p>This is a ragged dataset, where the number and dimension of features can be different at every row.</p> <p>Attributes:</p> Name Type Description <code>_cumulative_sum_index</code> <code>array</code> <p>Pointer that deliniates which entries</p> <code>_feature_arr</code> <code>list[dict[str, ndarray]]</code> <p>list of feature dictionaries for each dataset</p> <code>_num_genes_per_row</code> <code>list[int]</code> <p>list that tracks the feature length (number of genes) for each dataset.</p> <code>_labels</code> <code>list[str]</code> <p>list of labels</p> <code>_version</code> <p>The version of the dataset</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>class RowFeatureIndex:\n    \"\"\"Maintains a mapping between a row and its features.\n\n    This is a ragged dataset, where the number and dimension of features\n    can be different at every row.\n\n    Attributes:\n        _cumulative_sum_index: Pointer that deliniates which entries\n        correspondto a given row. For examples if the array is [-1, 200, 201],\n        rows 0 to 199 correspond to _feature_arr[0] and 200 corresponds to\n        _feature_arr[1]\n        _feature_arr: list of feature dictionaries for each dataset\n        _num_genes_per_row: list that tracks the feature length (number of genes) for each dataset.\n        Extracting this information repeatedly from self._feature_arr would be cumbersome which is why we\n        add this attribute.\n        _labels: list of labels\n        _version: The version of the dataset\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Instantiates the index.\"\"\"\n        self._cumulative_sum_index: np.array = np.array([-1])\n        self._feature_arr: list[dict[str, np.ndarray]] = []\n        self._num_genes_per_row: list[int] = []\n        self._version = importlib.metadata.version(\"bionemo.scdl\")\n        self._labels: list[str] = []\n\n    def _get_dataset_id(self, row) -&gt; int:\n        \"\"\"Gets the dataset id for a specified row index.\n\n        Args:\n            row (int): The index of the row.\n\n        Returns:\n            An int representing the dataset id the row belongs to.\n        \"\"\"\n        # creates a mask for values where cumulative sum &gt; row\n        mask = ~(self._cumulative_sum_index &gt; row)\n        # Sum these to get the index of the first range &gt; row\n        # Subtract one to get the range containing row.\n        d_id = sum(mask) - 1\n        return d_id\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def __len__(self) -&gt; int:\n        \"\"\"The length is the number of rows or RowFeatureIndex length.\"\"\"\n        return len(self._feature_arr)\n\n    def append_features(self, n_obs: int, features: dict[str, np.ndarray], label: Optional[str] = None) -&gt; None:\n        \"\"\"Updates the index with the given features.\n\n        The dict is inserted into the feature array by adding a\n        new span to the row lookup index. Additionally, we update the number of genes for the newly added row.\n\n        Args:\n            n_obs (int): The number of times that these feature occur in the\n            class.\n            features (dict): Corresponding features.\n            label (str): Label for the features.\n        \"\"\"\n        if isinstance(features, pd.DataFrame):\n            raise TypeError(\"Expected a dictionary, but received a Pandas DataFrame.\")\n        csum = max(self._cumulative_sum_index[-1], 0)\n\n        # If the new feature array is identical to the last one, it is not appended. Instead, the last array accounts\n        # for the additional n_obs also.\n        if len(self._feature_arr) &gt; 0 and are_dicts_equal(self._feature_arr[-1], features):\n            self._cumulative_sum_index[-1] = csum + n_obs\n        else:\n            self._cumulative_sum_index = np.append(self._cumulative_sum_index, csum + n_obs)\n            self._feature_arr.append(features)\n            self._labels.append(label)\n            if len(features) == 0:\n                num_genes = 0\n            else:\n                num_genes = len(features[next(iter(features.keys()))])\n            self._num_genes_per_row.append(num_genes)\n\n    def lookup(self, row: int, select_features: Optional[list[str]] = None) -&gt; Tuple[list[np.ndarray], str]:\n        \"\"\"Find the features at a given row.\n\n        It is assumed that the row is\n        non-zero._cumulative_sum_index contains pointers to which rows correspond\n        to given dictionaries. To obtain a specific row, we determine where it is\n        located in _cumulative_sum_index and then look up that dictionary in\n        _feature_arr\n        Args:\n            row (int): The row in the feature index.\n            select_features (list[str]): a list of features to select\n        Returns\n            list[np.ndarray]: list of np arrays with the feature values in that row of the specified features\n            str: optional label for the row\n        Raises:\n            IndexError: An error occured due to input row being negative or it\n            exceeding the larger row of the rows in the index. It is also raised\n            if there are no entries in the index yet.\n        \"\"\"\n        if row &lt; 0:\n            raise IndexError(f\"Row index {row} is not valid. It must be non-negative.\")\n        if len(self._cumulative_sum_index) &lt; 2:\n            raise IndexError(\"There are no features to lookup.\")\n\n        if row &gt; self._cumulative_sum_index[-1]:\n            raise IndexError(\n                f\"Row index {row} is larger than number of rows in FeatureIndex ({self._cumulative_sum_index[-1]}).\"\n            )\n        d_id = self._get_dataset_id(row)\n\n        # Retrieve the features for the identified value.\n        features_dict = self._feature_arr[d_id]\n\n        # If specific features are to be selected, filter the features.\n        if select_features is not None:\n            features = []\n            for feature in select_features:\n                if feature not in features_dict:\n                    raise ValueError(f\"Provided feature column {feature} in select_features not present in dataset.\")\n                features.append(features_dict[feature])\n        else:\n            features = [features_dict[f] for f in features_dict]\n\n        # Return the features for the identified range.\n        return features, self._labels[d_id]\n\n    def number_vars_at_row(self, row: int) -&gt; int:\n        \"\"\"Return number of variables in a given row.\n\n        Args:\n            row (int): The row in the feature index.\n\n        Returns:\n            The length of the features at the row\n        \"\"\"\n        return self._num_genes_per_row[self._get_dataset_id(row)]\n\n    def column_dims(self) -&gt; list[int]:\n        \"\"\"Return the number of columns in all rows.\n\n        Args:\n            length of features at every row is returned.\n\n        Returns:\n            A list containing the lengths of the features in every row\n        \"\"\"\n        return self._num_genes_per_row\n\n    def number_of_values(self) -&gt; list[int]:\n        \"\"\"Get the total number of values in the array.\n\n        For each row, the number of genes is counted.\n\n        Returns:\n            A list containing the lengths of the features in every block of rows\n        \"\"\"\n        if len(self._feature_arr) == 0:\n            return [0]\n        rows = [\n            self._cumulative_sum_index[i] - max(self._cumulative_sum_index[i - 1], 0)\n            for i in range(1, len(self._cumulative_sum_index))\n        ]\n        vals = []\n        vals = [n_rows * self._num_genes_per_row[i] for i, n_rows in enumerate(rows)]\n        return vals\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the index\"\".\n\n        Returns:\n            An integer corresponding to the number or rows in the index\n        \"\"\"\n        return int(max(self._cumulative_sum_index[-1], 0))\n\n    def concat(self, other_row_index: RowFeatureIndex, fail_on_empty_index: bool = True) -&gt; RowFeatureIndex:\n        \"\"\"Concatenates the other FeatureIndex to this one.\n\n        Returns the new, updated index. Warning: modifies this index in-place.\n\n        Args:\n            other_row_index: another RowFeatureIndex\n            fail_on_empty_index: A boolean flag that sets whether to raise an\n            error if an empty row index is passed in.\n\n        Returns:\n            self, the RowIndexFeature after the concatenations.\n\n        Raises:\n            TypeError if other_row_index is not a RowFeatureIndex\n            ValueError if an empty RowFeatureIndex is passed and the function is\n            set to fail in this case.\n        \"\"\"\n        match other_row_index:\n            case self.__class__():\n                pass\n            case _:\n                raise TypeError(\"Error: trying to concatenate something that's not a RowFeatureIndex.\")\n\n        if fail_on_empty_index and not len(other_row_index._feature_arr) &gt; 0:\n            raise ValueError(\"Error: Cannot append empty FeatureIndex.\")\n        for i, feats in enumerate(list(other_row_index._feature_arr)):\n            c_span = other_row_index._cumulative_sum_index[i + 1]\n            label = other_row_index._labels[i]\n            self.append_features(c_span, feats, label)\n\n        return self\n\n    def save(self, datapath: str) -&gt; None:\n        \"\"\"Saves the RowFeatureIndex to a given path.\n\n        Args:\n            datapath: path to save the index\n        \"\"\"\n        Path(datapath).mkdir(parents=True, exist_ok=True)\n        num_digits = len(str(len(self._feature_arr)))\n        for index, feature_dict in enumerate(self._feature_arr):\n            table = pa.table({column: pa.array(values) for column, values in feature_dict.items()})\n            dataframe_str_index = f\"{index:0{num_digits}d}\"\n            pq.write_table(table, f\"{datapath}/dataframe_{dataframe_str_index}.parquet\")\n\n        np.save(Path(datapath) / \"cumulative_sum_index.npy\", self._cumulative_sum_index)\n        np.save(Path(datapath) / \"labels.npy\", self._labels)\n        np.save(Path(datapath) / \"version.npy\", np.array(self._version))\n\n    @staticmethod\n    def load(datapath: str) -&gt; RowFeatureIndex:\n        \"\"\"Loads the data from datapath.\n\n        Args:\n            datapath: the path to load from\n        Returns:\n            An instance of RowFeatureIndex\n        \"\"\"\n        new_row_feat_index = RowFeatureIndex()\n        parquet_data_paths = sorted(Path(datapath).rglob(\"*.parquet\"))\n        data_tables = [pq.read_table(csv_path) for csv_path in parquet_data_paths]\n        new_row_feat_index._feature_arr = [\n            {column: table[column].to_numpy() for column in table.column_names} for table in data_tables\n        ]\n        new_row_feat_index._num_genes_per_row = [\n            len(feats[next(iter(feats.keys()))]) for feats in new_row_feat_index._feature_arr\n        ]\n\n        new_row_feat_index._cumulative_sum_index = np.load(Path(datapath) / \"cumulative_sum_index.npy\")\n        new_row_feat_index._labels = np.load(Path(datapath) / \"labels.npy\", allow_pickle=True)\n        new_row_feat_index._version = np.load(Path(datapath) / \"version.npy\").item()\n        return new_row_feat_index\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.__init__","title":"<code>__init__()</code>","text":"<p>Instantiates the index.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Instantiates the index.\"\"\"\n    self._cumulative_sum_index: np.array = np.array([-1])\n    self._feature_arr: list[dict[str, np.ndarray]] = []\n    self._num_genes_per_row: list[int] = []\n    self._version = importlib.metadata.version(\"bionemo.scdl\")\n    self._labels: list[str] = []\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.__len__","title":"<code>__len__()</code>","text":"<p>The length is the number of rows or RowFeatureIndex length.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"The length is the number of rows or RowFeatureIndex length.\"\"\"\n    return len(self._feature_arr)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex._get_dataset_id","title":"<code>_get_dataset_id(row)</code>","text":"<p>Gets the dataset id for a specified row index.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>int</code> <p>The index of the row.</p> required <p>Returns:</p> Type Description <code>int</code> <p>An int representing the dataset id the row belongs to.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def _get_dataset_id(self, row) -&gt; int:\n    \"\"\"Gets the dataset id for a specified row index.\n\n    Args:\n        row (int): The index of the row.\n\n    Returns:\n        An int representing the dataset id the row belongs to.\n    \"\"\"\n    # creates a mask for values where cumulative sum &gt; row\n    mask = ~(self._cumulative_sum_index &gt; row)\n    # Sum these to get the index of the first range &gt; row\n    # Subtract one to get the range containing row.\n    d_id = sum(mask) - 1\n    return d_id\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.append_features","title":"<code>append_features(n_obs, features, label=None)</code>","text":"<p>Updates the index with the given features.</p> <p>The dict is inserted into the feature array by adding a new span to the row lookup index. Additionally, we update the number of genes for the newly added row.</p> <p>Parameters:</p> Name Type Description Default <code>n_obs</code> <code>int</code> <p>The number of times that these feature occur in the</p> required <code>features</code> <code>dict</code> <p>Corresponding features.</p> required <code>label</code> <code>str</code> <p>Label for the features.</p> <code>None</code> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def append_features(self, n_obs: int, features: dict[str, np.ndarray], label: Optional[str] = None) -&gt; None:\n    \"\"\"Updates the index with the given features.\n\n    The dict is inserted into the feature array by adding a\n    new span to the row lookup index. Additionally, we update the number of genes for the newly added row.\n\n    Args:\n        n_obs (int): The number of times that these feature occur in the\n        class.\n        features (dict): Corresponding features.\n        label (str): Label for the features.\n    \"\"\"\n    if isinstance(features, pd.DataFrame):\n        raise TypeError(\"Expected a dictionary, but received a Pandas DataFrame.\")\n    csum = max(self._cumulative_sum_index[-1], 0)\n\n    # If the new feature array is identical to the last one, it is not appended. Instead, the last array accounts\n    # for the additional n_obs also.\n    if len(self._feature_arr) &gt; 0 and are_dicts_equal(self._feature_arr[-1], features):\n        self._cumulative_sum_index[-1] = csum + n_obs\n    else:\n        self._cumulative_sum_index = np.append(self._cumulative_sum_index, csum + n_obs)\n        self._feature_arr.append(features)\n        self._labels.append(label)\n        if len(features) == 0:\n            num_genes = 0\n        else:\n            num_genes = len(features[next(iter(features.keys()))])\n        self._num_genes_per_row.append(num_genes)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.column_dims","title":"<code>column_dims()</code>","text":"<p>Return the number of columns in all rows.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>A list containing the lengths of the features in every row</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def column_dims(self) -&gt; list[int]:\n    \"\"\"Return the number of columns in all rows.\n\n    Args:\n        length of features at every row is returned.\n\n    Returns:\n        A list containing the lengths of the features in every row\n    \"\"\"\n    return self._num_genes_per_row\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.concat","title":"<code>concat(other_row_index, fail_on_empty_index=True)</code>","text":"<p>Concatenates the other FeatureIndex to this one.</p> <p>Returns the new, updated index. Warning: modifies this index in-place.</p> <p>Parameters:</p> Name Type Description Default <code>other_row_index</code> <code>RowFeatureIndex</code> <p>another RowFeatureIndex</p> required <code>fail_on_empty_index</code> <code>bool</code> <p>A boolean flag that sets whether to raise an</p> <code>True</code> <p>Returns:</p> Type Description <code>RowFeatureIndex</code> <p>self, the RowIndexFeature after the concatenations.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def concat(self, other_row_index: RowFeatureIndex, fail_on_empty_index: bool = True) -&gt; RowFeatureIndex:\n    \"\"\"Concatenates the other FeatureIndex to this one.\n\n    Returns the new, updated index. Warning: modifies this index in-place.\n\n    Args:\n        other_row_index: another RowFeatureIndex\n        fail_on_empty_index: A boolean flag that sets whether to raise an\n        error if an empty row index is passed in.\n\n    Returns:\n        self, the RowIndexFeature after the concatenations.\n\n    Raises:\n        TypeError if other_row_index is not a RowFeatureIndex\n        ValueError if an empty RowFeatureIndex is passed and the function is\n        set to fail in this case.\n    \"\"\"\n    match other_row_index:\n        case self.__class__():\n            pass\n        case _:\n            raise TypeError(\"Error: trying to concatenate something that's not a RowFeatureIndex.\")\n\n    if fail_on_empty_index and not len(other_row_index._feature_arr) &gt; 0:\n        raise ValueError(\"Error: Cannot append empty FeatureIndex.\")\n    for i, feats in enumerate(list(other_row_index._feature_arr)):\n        c_span = other_row_index._cumulative_sum_index[i + 1]\n        label = other_row_index._labels[i]\n        self.append_features(c_span, feats, label)\n\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.load","title":"<code>load(datapath)</code>  <code>staticmethod</code>","text":"<p>Loads the data from datapath.</p> <p>Parameters:</p> Name Type Description Default <code>datapath</code> <code>str</code> <p>the path to load from</p> required <p>Returns:     An instance of RowFeatureIndex</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>@staticmethod\ndef load(datapath: str) -&gt; RowFeatureIndex:\n    \"\"\"Loads the data from datapath.\n\n    Args:\n        datapath: the path to load from\n    Returns:\n        An instance of RowFeatureIndex\n    \"\"\"\n    new_row_feat_index = RowFeatureIndex()\n    parquet_data_paths = sorted(Path(datapath).rglob(\"*.parquet\"))\n    data_tables = [pq.read_table(csv_path) for csv_path in parquet_data_paths]\n    new_row_feat_index._feature_arr = [\n        {column: table[column].to_numpy() for column in table.column_names} for table in data_tables\n    ]\n    new_row_feat_index._num_genes_per_row = [\n        len(feats[next(iter(feats.keys()))]) for feats in new_row_feat_index._feature_arr\n    ]\n\n    new_row_feat_index._cumulative_sum_index = np.load(Path(datapath) / \"cumulative_sum_index.npy\")\n    new_row_feat_index._labels = np.load(Path(datapath) / \"labels.npy\", allow_pickle=True)\n    new_row_feat_index._version = np.load(Path(datapath) / \"version.npy\").item()\n    return new_row_feat_index\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.lookup","title":"<code>lookup(row, select_features=None)</code>","text":"<p>Find the features at a given row.</p> <p>It is assumed that the row is non-zero._cumulative_sum_index contains pointers to which rows correspond to given dictionaries. To obtain a specific row, we determine where it is located in _cumulative_sum_index and then look up that dictionary in _feature_arr Args:     row (int): The row in the feature index.     select_features (list[str]): a list of features to select Returns     list[np.ndarray]: list of np arrays with the feature values in that row of the specified features     str: optional label for the row Raises:     IndexError: An error occured due to input row being negative or it     exceeding the larger row of the rows in the index. It is also raised     if there are no entries in the index yet.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def lookup(self, row: int, select_features: Optional[list[str]] = None) -&gt; Tuple[list[np.ndarray], str]:\n    \"\"\"Find the features at a given row.\n\n    It is assumed that the row is\n    non-zero._cumulative_sum_index contains pointers to which rows correspond\n    to given dictionaries. To obtain a specific row, we determine where it is\n    located in _cumulative_sum_index and then look up that dictionary in\n    _feature_arr\n    Args:\n        row (int): The row in the feature index.\n        select_features (list[str]): a list of features to select\n    Returns\n        list[np.ndarray]: list of np arrays with the feature values in that row of the specified features\n        str: optional label for the row\n    Raises:\n        IndexError: An error occured due to input row being negative or it\n        exceeding the larger row of the rows in the index. It is also raised\n        if there are no entries in the index yet.\n    \"\"\"\n    if row &lt; 0:\n        raise IndexError(f\"Row index {row} is not valid. It must be non-negative.\")\n    if len(self._cumulative_sum_index) &lt; 2:\n        raise IndexError(\"There are no features to lookup.\")\n\n    if row &gt; self._cumulative_sum_index[-1]:\n        raise IndexError(\n            f\"Row index {row} is larger than number of rows in FeatureIndex ({self._cumulative_sum_index[-1]}).\"\n        )\n    d_id = self._get_dataset_id(row)\n\n    # Retrieve the features for the identified value.\n    features_dict = self._feature_arr[d_id]\n\n    # If specific features are to be selected, filter the features.\n    if select_features is not None:\n        features = []\n        for feature in select_features:\n            if feature not in features_dict:\n                raise ValueError(f\"Provided feature column {feature} in select_features not present in dataset.\")\n            features.append(features_dict[feature])\n    else:\n        features = [features_dict[f] for f in features_dict]\n\n    # Return the features for the identified range.\n    return features, self._labels[d_id]\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the index\"\".</p> <p>Returns:</p> Type Description <code>int</code> <p>An integer corresponding to the number or rows in the index</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the index\"\".\n\n    Returns:\n        An integer corresponding to the number or rows in the index\n    \"\"\"\n    return int(max(self._cumulative_sum_index[-1], 0))\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Get the total number of values in the array.</p> <p>For each row, the number of genes is counted.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>A list containing the lengths of the features in every block of rows</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_of_values(self) -&gt; list[int]:\n    \"\"\"Get the total number of values in the array.\n\n    For each row, the number of genes is counted.\n\n    Returns:\n        A list containing the lengths of the features in every block of rows\n    \"\"\"\n    if len(self._feature_arr) == 0:\n        return [0]\n    rows = [\n        self._cumulative_sum_index[i] - max(self._cumulative_sum_index[i - 1], 0)\n        for i in range(1, len(self._cumulative_sum_index))\n    ]\n    vals = []\n    vals = [n_rows * self._num_genes_per_row[i] for i, n_rows in enumerate(rows)]\n    return vals\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_vars_at_row","title":"<code>number_vars_at_row(row)</code>","text":"<p>Return number of variables in a given row.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>int</code> <p>The row in the feature index.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The length of the features at the row</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_vars_at_row(self, row: int) -&gt; int:\n    \"\"\"Return number of variables in a given row.\n\n    Args:\n        row (int): The row in the feature index.\n\n    Returns:\n        The length of the features at the row\n    \"\"\"\n    return self._num_genes_per_row[self._get_dataset_id(row)]\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.save","title":"<code>save(datapath)</code>","text":"<p>Saves the RowFeatureIndex to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>datapath</code> <code>str</code> <p>path to save the index</p> required Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def save(self, datapath: str) -&gt; None:\n    \"\"\"Saves the RowFeatureIndex to a given path.\n\n    Args:\n        datapath: path to save the index\n    \"\"\"\n    Path(datapath).mkdir(parents=True, exist_ok=True)\n    num_digits = len(str(len(self._feature_arr)))\n    for index, feature_dict in enumerate(self._feature_arr):\n        table = pa.table({column: pa.array(values) for column, values in feature_dict.items()})\n        dataframe_str_index = f\"{index:0{num_digits}d}\"\n        pq.write_table(table, f\"{datapath}/dataframe_{dataframe_str_index}.parquet\")\n\n    np.save(Path(datapath) / \"cumulative_sum_index.npy\", self._cumulative_sum_index)\n    np.save(Path(datapath) / \"labels.npy\", self._labels)\n    np.save(Path(datapath) / \"version.npy\", np.array(self._version))\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.are_dicts_equal","title":"<code>are_dicts_equal(dict1, dict2)</code>","text":"<p>Compare two dictionaries with string keys and numpy.ndarray values.</p> <p>Parameters:</p> Name Type Description Default <code>dict1</code> <code>dict[str, ndarray]</code> <p>The first dictionary to compare.</p> required <code>dict2</code> <code>dict[str, ndarray]</code> <p>The second dictionary to compare.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the dictionaries have the same keys and all corresponding   numpy arrays are equal; False otherwise.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def are_dicts_equal(dict1: dict[str, np.ndarray], dict2: dict[str, np.ndarray]) -&gt; bool:\n    \"\"\"Compare two dictionaries with string keys and numpy.ndarray values.\n\n    Args:\n        dict1 (dict[str, np.ndarray]): The first dictionary to compare.\n        dict2 (dict[str, np.ndarray]): The second dictionary to compare.\n\n    Returns:\n        bool: True if the dictionaries have the same keys and all corresponding\n              numpy arrays are equal; False otherwise.\n    \"\"\"\n    return dict1.keys() == dict2.keys() and all(np.array_equal(dict1[k], dict2[k]) for k in dict1)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/","title":"Single cell collection","text":""},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.FileNames","title":"<code>FileNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Names of files that are generated in SingleCellCollection.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>class FileNames(str, Enum):\n    \"\"\"Names of files that are generated in SingleCellCollection.\"\"\"\n\n    VERSION = \"version.json\"\n    METADATA = \"metadata.json\"\n    FEATURES = \"features\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection","title":"<code>SingleCellCollection</code>","text":"<p>               Bases: <code>SingleCellRowDatasetCore</code></p> <p>A collection of one or more SingleCellMemMapDatasets.</p> <p>SingleCellCollection support most of the functionality of the SingleCellDataSet API. An SingleCellCollection can be converted to a single SingleCellMemMapDataset. A SingleCellCollection enables the use of heterogeneous datasets, such as those composed of many AnnData files.</p> <p>Attributes:</p> Name Type Description <code>_version</code> <code>str</code> <p>The version of the dataset</p> <code>data_path</code> <code>str</code> <p>The directory where the colleection of datasets is stored.</p> <code>_feature_index</code> <code>RowFeatureIndex</code> <p>The corresponding RowFeatureIndex where features are</p> <code>fname_to_mmap</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>dictionary to hold each SingleCellMemMapDataset object.</p> <code>False</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>not ragged; all SingleCellMemMapDataset have same column dimemsion</p> <code>True</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>ragged; scmmap column dimemsions vary</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>class SingleCellCollection(SingleCellRowDatasetCore):\n    \"\"\"A collection of one or more SingleCellMemMapDatasets.\n\n    SingleCellCollection support most of the functionality of the\n    SingleCellDataSet API. An SingleCellCollection can be converted\n    to a single SingleCellMemMapDataset. A SingleCellCollection\n    enables the use of heterogeneous datasets, such as those composed of many\n    AnnData files.\n\n    Attributes:\n        _version: The version of the dataset\n        data_path: The directory where the colleection of datasets is stored.\n        _feature_index: The corresponding RowFeatureIndex where features are\n        stored.\n        fname_to_mmap:  dictionary to hold each SingleCellMemMapDataset object.\n        This maps from the path to the dataset.\n        ragged dataset is an dataset of arrays where the arrays have different\n        lengths\n        False: not ragged; all SingleCellMemMapDataset have same column dimemsion\n        True: ragged; scmmap column dimemsions vary\n    \"\"\"\n\n    def __init__(self, data_path: str) -&gt; None:\n        \"\"\"Instantiate the class.\n\n        Args:\n            data_path: Where the class will be stored.\n        \"\"\"\n        self.data_path: str = data_path\n        self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n        self.metadata: Dict[str, int] = {}\n        self._feature_index: RowFeatureIndex = RowFeatureIndex()\n        self.fname_to_mmap: Dict[str, SingleCellMemMapDataset] = {}\n\n        Path(self.data_path).mkdir(parents=True, exist_ok=True)\n\n        # Write the version\n        if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n            with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n                json.dump(self.version(), vfi)\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def load_h5ad(self, h5ad_path: str) -&gt; None:\n        \"\"\"Loads data from an existing AnnData archive.\n\n        This creates and saves a new backing data structure.\n        Then, the location and the data and the dataset are stored.\n\n        Args:\n            h5ad_path: the path to AnnData archive\n        \"\"\"\n        mmap_path = Path(self.data_path) / Path(h5ad_path).stem\n        self.fname_to_mmap[mmap_path] = _create_single_cell_memmap_dataset_from_h5ad(\n            h5ad_path=h5ad_path, base_directory_path=self.data_path\n        )\n        self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n\n    def load_h5ad_multi(self, directory_path: str, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n        \"\"\"Loads one or more AnnData files and adds them to the collection.\n\n        Args:\n            directory_path: The path to the directory with the AnnData files\n            max_workers: the maximal number of workers to use\n            use_processes: If True, use ProcessPoolExecutor; otherwise, use\n                ThreadPoolExecutor\n        Raises:\n            FileNotFoundError: If no h5ad files are found in the directory.\n            RuntimeError: If an error occurs in the loading of any of the h5ad files.\n        \"\"\"\n        directory_path = Path(directory_path)\n        ann_data_paths = sorted(directory_path.rglob(\"*.h5ad\"))\n        if len(ann_data_paths) == 0:\n            raise FileNotFoundError(f\"There a no h5ad files in {directory_path}.\")\n        mmap_paths = [Path(self.data_path) / Path(ann_datapath).stem for ann_datapath in ann_data_paths]\n        queue = AsyncWorkQueue(max_workers=max_workers, use_processes=use_processes)\n        for ann in ann_data_paths:\n            queue.submit_task(_create_single_cell_memmap_dataset_from_h5ad, ann, base_directory_path=self.data_path)\n        queue.wait()\n        mmaps = queue.get_task_results()\n\n        for result in mmaps:\n            if isinstance(result, Exception):\n                raise RuntimeError(f\"Error in processing file {ann}: {result}\") from result\n\n        for mmap_path, mmap in zip(mmap_paths, mmaps):\n            if isinstance(mmap, Exception):\n                raise RuntimeError(f\"Error in processing file {mmap_path}: {mmap}\") from mmap\n\n            self.fname_to_mmap[mmap_path] = mmap\n            self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Sum of the number of non zero entries in each dataset.\"\"\"\n        return sum([self.fname_to_mmap[mmap_path].number_nonzero_values() for mmap_path in self.fname_to_mmap])\n\n    def number_of_values(self) -&gt; int:\n        \"\"\"Sum of the number of values in each dataset.\"\"\"\n        return sum([self.fname_to_mmap[mmap_path].number_of_values() for mmap_path in self.fname_to_mmap])\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the dataset.\n\n        Returns:\n            The number of rows in the dataset\n        Raises:\n            ValueError if the length of the number of rows in the feature\n            index does not correspond to the number of stored rows.\n        \"\"\"\n        row_sum_from_datasets = sum(\n            [self.fname_to_mmap[mmap_path].number_of_rows() for mmap_path in self.fname_to_mmap]\n        )\n        if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != row_sum_from_datasets:\n            raise ValueError(\n                f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                             does not correspond to the number of rows in the datasets {row_sum_from_datasets}\"\"\"\n            )\n\n        return row_sum_from_datasets\n\n    def number_of_variables(self) -&gt; List[int]:\n        \"\"\"If ragged, returns a list of variable lengths.\n\n        If not ragged, returns a list with one entry. A ragged\n        collection is one where the datasets have different lengths.\n        \"\"\"\n        if len(self._feature_index) == 0:\n            return [0]\n        else:\n            num_vars = self._feature_index.column_dims()\n            return num_vars\n\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Get the shape of the dataset.\n\n        This is the number of entries by the the length of the feature index\n        corresponding to that variable.\n\n        Returns:\n            The total number of elements across dataset\n            A list containing the number of variables for each entry in the\n                RowFeatureIndex.\n        \"\"\"\n        return self.number_of_rows(), self.number_of_variables()\n\n    def flatten(\n        self,\n        output_path: str,\n        destroy_on_copy: bool = False,\n    ) -&gt; None:\n        \"\"\"Flattens the collection into a single SingleCellMemMapDataset.\n\n        Args:\n            output_path: location to store new dataset\n            destroy_on_copy: Whether to remove the current data_path\n        \"\"\"\n        output = SingleCellMemMapDataset(\n            output_path,\n            num_elements=self.number_of_rows(),\n            num_rows=self.number_nonzero_values(),\n            mode=Mode.CREATE_APPEND,\n        )\n\n        output.concat(list(self.fname_to_mmap.values()))\n\n        # Hit save!\n        output.save()\n\n        if destroy_on_copy:\n            shutil.rmtree(self.data_path)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.__init__","title":"<code>__init__(data_path)</code>","text":"<p>Instantiate the class.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Where the class will be stored.</p> required Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def __init__(self, data_path: str) -&gt; None:\n    \"\"\"Instantiate the class.\n\n    Args:\n        data_path: Where the class will be stored.\n    \"\"\"\n    self.data_path: str = data_path\n    self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n    self.metadata: Dict[str, int] = {}\n    self._feature_index: RowFeatureIndex = RowFeatureIndex()\n    self.fname_to_mmap: Dict[str, SingleCellMemMapDataset] = {}\n\n    Path(self.data_path).mkdir(parents=True, exist_ok=True)\n\n    # Write the version\n    if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n        with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n            json.dump(self.version(), vfi)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.flatten","title":"<code>flatten(output_path, destroy_on_copy=False)</code>","text":"<p>Flattens the collection into a single SingleCellMemMapDataset.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>location to store new dataset</p> required <code>destroy_on_copy</code> <code>bool</code> <p>Whether to remove the current data_path</p> <code>False</code> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def flatten(\n    self,\n    output_path: str,\n    destroy_on_copy: bool = False,\n) -&gt; None:\n    \"\"\"Flattens the collection into a single SingleCellMemMapDataset.\n\n    Args:\n        output_path: location to store new dataset\n        destroy_on_copy: Whether to remove the current data_path\n    \"\"\"\n    output = SingleCellMemMapDataset(\n        output_path,\n        num_elements=self.number_of_rows(),\n        num_rows=self.number_nonzero_values(),\n        mode=Mode.CREATE_APPEND,\n    )\n\n    output.concat(list(self.fname_to_mmap.values()))\n\n    # Hit save!\n    output.save()\n\n    if destroy_on_copy:\n        shutil.rmtree(self.data_path)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.load_h5ad","title":"<code>load_h5ad(h5ad_path)</code>","text":"<p>Loads data from an existing AnnData archive.</p> <p>This creates and saves a new backing data structure. Then, the location and the data and the dataset are stored.</p> <p>Parameters:</p> Name Type Description Default <code>h5ad_path</code> <code>str</code> <p>the path to AnnData archive</p> required Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def load_h5ad(self, h5ad_path: str) -&gt; None:\n    \"\"\"Loads data from an existing AnnData archive.\n\n    This creates and saves a new backing data structure.\n    Then, the location and the data and the dataset are stored.\n\n    Args:\n        h5ad_path: the path to AnnData archive\n    \"\"\"\n    mmap_path = Path(self.data_path) / Path(h5ad_path).stem\n    self.fname_to_mmap[mmap_path] = _create_single_cell_memmap_dataset_from_h5ad(\n        h5ad_path=h5ad_path, base_directory_path=self.data_path\n    )\n    self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.load_h5ad_multi","title":"<code>load_h5ad_multi(directory_path, max_workers=5, use_processes=False)</code>","text":"<p>Loads one or more AnnData files and adds them to the collection.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>The path to the directory with the AnnData files</p> required <code>max_workers</code> <code>int</code> <p>the maximal number of workers to use</p> <code>5</code> <code>use_processes</code> <code>bool</code> <p>If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor</p> <code>False</code> <p>Raises:     FileNotFoundError: If no h5ad files are found in the directory.     RuntimeError: If an error occurs in the loading of any of the h5ad files.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def load_h5ad_multi(self, directory_path: str, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n    \"\"\"Loads one or more AnnData files and adds them to the collection.\n\n    Args:\n        directory_path: The path to the directory with the AnnData files\n        max_workers: the maximal number of workers to use\n        use_processes: If True, use ProcessPoolExecutor; otherwise, use\n            ThreadPoolExecutor\n    Raises:\n        FileNotFoundError: If no h5ad files are found in the directory.\n        RuntimeError: If an error occurs in the loading of any of the h5ad files.\n    \"\"\"\n    directory_path = Path(directory_path)\n    ann_data_paths = sorted(directory_path.rglob(\"*.h5ad\"))\n    if len(ann_data_paths) == 0:\n        raise FileNotFoundError(f\"There a no h5ad files in {directory_path}.\")\n    mmap_paths = [Path(self.data_path) / Path(ann_datapath).stem for ann_datapath in ann_data_paths]\n    queue = AsyncWorkQueue(max_workers=max_workers, use_processes=use_processes)\n    for ann in ann_data_paths:\n        queue.submit_task(_create_single_cell_memmap_dataset_from_h5ad, ann, base_directory_path=self.data_path)\n    queue.wait()\n    mmaps = queue.get_task_results()\n\n    for result in mmaps:\n        if isinstance(result, Exception):\n            raise RuntimeError(f\"Error in processing file {ann}: {result}\") from result\n\n    for mmap_path, mmap in zip(mmap_paths, mmaps):\n        if isinstance(mmap, Exception):\n            raise RuntimeError(f\"Error in processing file {mmap_path}: {mmap}\") from mmap\n\n        self.fname_to_mmap[mmap_path] = mmap\n        self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_nonzero_values","title":"<code>number_nonzero_values()</code>","text":"<p>Sum of the number of non zero entries in each dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_nonzero_values(self) -&gt; int:\n    \"\"\"Sum of the number of non zero entries in each dataset.\"\"\"\n    return sum([self.fname_to_mmap[mmap_path].number_nonzero_values() for mmap_path in self.fname_to_mmap])\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the dataset</p> <p>Raises:     ValueError if the length of the number of rows in the feature     index does not correspond to the number of stored rows.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the dataset.\n\n    Returns:\n        The number of rows in the dataset\n    Raises:\n        ValueError if the length of the number of rows in the feature\n        index does not correspond to the number of stored rows.\n    \"\"\"\n    row_sum_from_datasets = sum(\n        [self.fname_to_mmap[mmap_path].number_of_rows() for mmap_path in self.fname_to_mmap]\n    )\n    if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != row_sum_from_datasets:\n        raise ValueError(\n            f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                         does not correspond to the number of rows in the datasets {row_sum_from_datasets}\"\"\"\n        )\n\n    return row_sum_from_datasets\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Sum of the number of values in each dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_values(self) -&gt; int:\n    \"\"\"Sum of the number of values in each dataset.\"\"\"\n    return sum([self.fname_to_mmap[mmap_path].number_of_values() for mmap_path in self.fname_to_mmap])\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_variables","title":"<code>number_of_variables()</code>","text":"<p>If ragged, returns a list of variable lengths.</p> <p>If not ragged, returns a list with one entry. A ragged collection is one where the datasets have different lengths.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_variables(self) -&gt; List[int]:\n    \"\"\"If ragged, returns a list of variable lengths.\n\n    If not ragged, returns a list with one entry. A ragged\n    collection is one where the datasets have different lengths.\n    \"\"\"\n    if len(self._feature_index) == 0:\n        return [0]\n    else:\n        num_vars = self._feature_index.column_dims()\n        return num_vars\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.shape","title":"<code>shape()</code>","text":"<p>Get the shape of the dataset.</p> <p>This is the number of entries by the the length of the feature index corresponding to that variable.</p> <p>Returns:</p> Type Description <code>int</code> <p>The total number of elements across dataset</p> <code>List[int]</code> <p>A list containing the number of variables for each entry in the RowFeatureIndex.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Get the shape of the dataset.\n\n    This is the number of entries by the the length of the feature index\n    corresponding to that variable.\n\n    Returns:\n        The total number of elements across dataset\n        A list containing the number of variables for each entry in the\n            RowFeatureIndex.\n    \"\"\"\n    return self.number_of_rows(), self.number_of_variables()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection._create_single_cell_memmap_dataset_from_h5ad","title":"<code>_create_single_cell_memmap_dataset_from_h5ad(h5ad_path, base_directory_path)</code>","text":"<p>The SingleCellMemMapDataset is loaded from h5ad_path.</p> <p>The data is stored in the base_data_path directory.</p> <p>Parameters:</p> Name Type Description Default <code>h5ad_path</code> <code>str</code> <p>the path to the dataset</p> required <code>base_directory_path</code> <code>str</code> <p>the base directory path where the dataset will be stored</p> required <p>Returns:     The created SingleCellMemMapDataset</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def _create_single_cell_memmap_dataset_from_h5ad(h5ad_path: str, base_directory_path: str) -&gt; SingleCellMemMapDataset:\n    \"\"\"The SingleCellMemMapDataset is loaded from h5ad_path.\n\n    The data is stored in the base_data_path directory.\n\n    Args:\n        h5ad_path: the path to the dataset\n        base_directory_path: the base directory path where the dataset will be stored\n    Returns:\n        The created SingleCellMemMapDataset\n    \"\"\"\n    fname = Path(h5ad_path).stem\n    obj = SingleCellMemMapDataset(data_path=Path(base_directory_path) / fname, h5ad_path=h5ad_path)\n    return obj\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/","title":"Single cell memmap dataset","text":""},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.FileNames","title":"<code>FileNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Names of files that are generated in SingleCellCollection.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class FileNames(str, Enum):\n    \"\"\"Names of files that are generated in SingleCellCollection.\"\"\"\n\n    DATA = \"data.npy\"\n    COLPTR = \"col_ptr.npy\"\n    ROWPTR = \"row_ptr.npy\"\n    METADATA = \"metadata.json\"\n    DTYPE = \"dtypes.json\"\n    FEATURES = \"features\"\n    VERSION = \"version.json\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.METADATA","title":"<code>METADATA</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Stored metadata.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class METADATA(str, Enum):\n    \"\"\"Stored metadata.\"\"\"\n\n    NUM_ROWS = \"num_rows\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid modes for the single cell memory mapped dataset.</p> <p>The write append mode is 'w+' while the read append mode is 'r+'.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class Mode(str, Enum):\n    \"\"\"Valid modes for the single cell memory mapped dataset.\n\n    The write append mode is 'w+' while the read append mode is 'r+'.\n    \"\"\"\n\n    CREATE_APPEND = \"w+\"\n    READ_APPEND = \"r+\"\n    READ = \"r\"\n    CREATE = \"w\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset","title":"<code>SingleCellMemMapDataset</code>","text":"<p>               Bases: <code>SingleCellRowDataset</code></p> <p>Represents one or more AnnData matrices.</p> <p>Data is stored in large, memory-mapped arrays that enables fast access of datasets larger than the available amount of RAM on a system. SCMMAP implements a consistent API defined in SingleCellRowDataset.</p> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Location of np.memmap files to be loaded from or that will be</p> <code>mode</code> <code>Mode</code> <p>Whether the dataset will be read in (r+) from np.memmap files or</p> <code>data</code> <code>Optional[ndarray]</code> <p>A numpy array of the data</p> <code>row_index</code> <code>Optional[ndarray]</code> <p>A numpy array of row pointers</p> <code>col_index</code> <code>Optional[ndarray]</code> <p>A numpy array of column values</p> <code>metadata</code> <code>Dict[str, int]</code> <p>Various metata about the dataset.</p> <code>_feature_index</code> <code>RowFeatureIndex</code> <p>The corresponding RowFeatureIndex where features are</p> <code>dtypes</code> <code>Dict[FileNames, str]</code> <p>A dictionary containing the datatypes of the data, row_index,</p> <code>_version</code> <code>str</code> <p>The version of the dataset</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class SingleCellMemMapDataset(SingleCellRowDataset):\n    \"\"\"Represents one or more AnnData matrices.\n\n    Data is stored in large, memory-mapped arrays that enables fast access of\n    datasets larger than the available amount of RAM on a system. SCMMAP\n    implements a consistent API defined in SingleCellRowDataset.\n\n    Attributes:\n        data_path: Location of np.memmap files to be loaded from or that will be\n        created.\n        mode: Whether the dataset will be read in (r+) from np.memmap files or\n        written to np.memmap files (w+).\n        data: A numpy array of the data\n        row_index: A numpy array of row pointers\n        col_index: A numpy array of column values\n        metadata: Various metata about the dataset.\n        _feature_index: The corresponding RowFeatureIndex where features are\n        stored\n        dtypes: A dictionary containing the datatypes of the data, row_index,\n        and col_index arrays.\n        _version: The version of the dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        h5ad_path: Optional[str] = None,\n        num_elements: Optional[int] = None,\n        num_rows: Optional[int] = None,\n        mode: Mode = Mode.READ_APPEND,\n        paginated_load_cutoff: int = 10_000,\n        load_block_row_size: int = 1_000_000,\n        feature_index_name=\"feature_id\",\n    ) -&gt; None:\n        \"\"\"Instantiate the class.\n\n        Args:\n            data_path: The location where the data np.memmap files are read from\n            or stored.\n            h5ad_path: Optional, the location of the h5_ad path.\n            num_elements: The total number of elements in the array.\n            num_rows: The number of rows in the data frame.\n            mode: Whether to read or write from the data_path.\n            paginated_load_cutoff: MB size on disk at which to load the h5ad structure with paginated load.\n            load_block_row_size: Number of rows to load into memory with paginated load\n            feature_index_name: The name of the features if the features are only stored in features_df.index.values\n        \"\"\"\n        self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n        self.data_path: str = data_path\n        self.mode: Mode = mode\n        self.paginated_load_cutoff = paginated_load_cutoff\n        self.load_block_row_size = load_block_row_size\n        self.feature_index_name = feature_index_name\n        # Backing arrays\n        self.data: Optional[np.ndarray] = None\n        self.row_index: Optional[np.ndarray] = None\n        self.row_index: Optional[np.ndarray] = None\n\n        # Metadata and attributes\n        self.metadata: Dict[str, int] = {}\n\n        # Stores the Feature Index, which tracks\n        # the original AnnData features (e.g., gene names)\n        # and allows us to store ragged arrays in our SCMMAP structure.\n        self._feature_index: RowFeatureIndex = RowFeatureIndex()\n\n        # Variables for int packing / reduced precision\n        self.dtypes: Dict[FileNames, str] = {\n            f\"{FileNames.DATA.value}\": \"float32\",\n            f\"{FileNames.COLPTR.value}\": \"uint32\",\n            f\"{FileNames.ROWPTR.value}\": \"uint64\",\n        }\n\n        if mode == Mode.CREATE_APPEND and os.path.exists(data_path):\n            raise FileExistsError(f\"Output directory already exists: {data_path}\")\n\n        if h5ad_path is not None and (data_path is not None and os.path.exists(data_path)):\n            raise FileExistsError(\n                \"Invalid input; both an existing SCMMAP and an h5ad file were passed. \"\n                \"Please pass either an existing SCMMAP or an h5ad file.\"\n            )\n\n        # If there is only a data path, and it exists already, load SCMMAP data.\n        elif data_path is not None and os.path.exists(data_path):\n            self.__init__obj()\n            self.load(data_path)\n\n        # If there is only an h5ad path, load the HDF5 data\n        elif h5ad_path is not None:\n            self.__init__obj()\n            self.load_h5ad(h5ad_path)\n        else:\n            match num_rows, num_elements:\n                case (int(), int()):\n                    self.__init__obj()\n                    self._init_arrs(num_elements=num_elements, num_rows=num_rows)\n                case _:\n                    raise ValueError(\n                        \"An np.memmap path, an h5ad path, or the number of elements and rows is required\" \"\"\n                    )\n\n    def __init__obj(self):\n        \"\"\"Initializes the datapath and writes the version.\"\"\"\n        os.makedirs(self.data_path, exist_ok=True)\n\n        # Write the version\n        if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n            with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n                json.dump(self.version(), vfi)\n\n    def _init_arrs(self, num_elements: int, num_rows: int) -&gt; None:\n        self.mode = Mode.CREATE_APPEND\n        data_arr, col_arr, row_arr = _create_compressed_sparse_row_memmaps(\n            num_elements=num_elements,\n            num_rows=num_rows,\n            memmap_dir_path=Path(self.data_path),\n            mode=self.mode,\n            dtypes=self.dtypes,\n        )\n        self.data = data_arr\n        self.col_index = col_arr\n        self.row_index = row_arr\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def get_row(\n        self,\n        index: int,\n        return_features: bool = False,\n        feature_vars: Optional[List[str]] = None,\n    ) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], List[np.ndarray]]:\n        \"\"\"Returns a given row in the dataset along with optional features.\n\n        Args:\n            index: The row to be returned. This is in the range of [0, num_rows)\n            return_features: boolean that indicates whether to return features\n            feature_vars: Optional, feature variables to extract\n        Return:\n            [Tuple[np.ndarray, np.ndarray]: data values and column pointes\n            List[np.ndarray]: optional, corresponding features.\n        \"\"\"\n        start = self.row_index[index]\n        end = self.row_index[index + 1]\n        values = self.data[start:end]\n        columns = self.col_index[start:end]\n        ret = (values, columns)\n        if return_features:\n            return ret, self._feature_index.lookup(index, select_features=feature_vars)[0]\n        else:\n            return ret, None\n\n    def get_row_padded(\n        self,\n        index: int,\n        return_features: bool = False,\n        feature_vars: Optional[List[str]] = None,\n    ) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n        \"\"\"Returns a padded version of a row in the dataset.\n\n        A padded version is one where the a sparse array representation is\n        converted to a conventional represenentation. Optionally, features are\n        returned.\n\n        Args:\n            index: The row to be returned\n            return_features: boolean that indicates whether to return features\n            feature_vars: Optional, feature variables to extract\n        Return:\n            np.ndarray: conventional row representation\n            List[np.ndarray]: optional, corresponding features.\n        \"\"\"\n        (row_values, row_column_pointer), features = self.get_row(index, return_features, feature_vars)\n        return (\n            _pad_sparse_array(row_values, row_column_pointer, self._feature_index.number_vars_at_row(index)),\n            features,\n        )\n\n    def get_row_column(self, index: int, column: int, impute_missing_zeros: bool = True) -&gt; Optional[float]:\n        \"\"\"Returns the value at a given index and the corresponding column.\n\n        Args:\n            index: The index to be returned\n            column: The column to be returned\n            impute_missing_zeros: boolean that indicates whether to set missing\n            data to 0\n        Return:\n            A float that is the value in the array or None.\n        \"\"\"\n        (row_values, row_column_pointer), _ = self.get_row(index)\n        if column is not None:\n            for col_index, col in enumerate(row_column_pointer):\n                if col == column:\n                    # return the value at this position\n                    return row_values[col_index]\n                elif col &gt; column:\n                    try:\n                        raise ValueError(f\"Column pointer {col} is larger than the column {column}.\")\n                    except ValueError:\n                        break\n            return 0.0 if impute_missing_zeros else None\n\n    def features(self) -&gt; Optional[RowFeatureIndex]:\n        \"\"\"Return the corresponding RowFeatureIndex.\"\"\"\n        return self._feature_index\n\n    def _load_mmap_file_if_exists(self, file_path, dtype):\n        if os.path.exists(file_path):\n            return np.memmap(file_path, dtype=dtype, mode=self.mode)\n        else:\n            raise FileNotFoundError(f\"The mmap file at {file_path} is missing\")\n\n    def load(self, stored_path: str) -&gt; None:\n        \"\"\"Loads the data at store_path that is an np.memmap format.\n\n        Args:\n            stored_path: directory with np.memmap files\n        Raises:\n            FileNotFoundError if the corresponding directory or files are not\n            found, or if the metadata file is not present.\n        \"\"\"\n        if not os.path.exists(stored_path):\n            raise FileNotFoundError(\n                f\"\"\"Error: the specified data path to the mmap files {stored_path} does not exist.\n                                    Specify an updated filepath or provide an h5ad path to the dataset. The data can\n                                    be loaded with SingleCellMemMapDataset.load_h5ad. Alternatively, the class can be instantiated\n                                    with  SingleCellMemMapDataset(&lt;path to data that will be created&gt;, h5ad_path=&lt;path to h5ad file&gt;\"\"\"\n            )\n        self.data_path = stored_path\n        self.mode = Mode.READ_APPEND\n\n        # Metadata is required, so we must check if it exists and fail if not.\n        if not os.path.exists(f\"{self.data_path}/{FileNames.METADATA.value}\"):\n            raise FileNotFoundError(\n                f\"Error: the metadata file {self.data_path}/{FileNames.METADATA.value} does not exist.\"\n            )\n\n        with open(f\"{self.data_path}/{FileNames.METADATA.value}\", Mode.READ_APPEND.value) as mfi:\n            self.metadata = json.load(mfi)\n\n        if os.path.exists(f\"{self.data_path}/{FileNames.FEATURES.value}\"):\n            self._feature_index = RowFeatureIndex.load(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n        if os.path.exists(f\"{self.data_path}/{FileNames.DTYPE.value}\"):\n            with open(f\"{self.data_path}/{FileNames.DTYPE.value}\") as dfi:\n                self.dtypes = json.load(dfi)\n\n        # mmap the existing arrays\n        self.data = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.DATA.value}\", self.dtypes[f\"{FileNames.DATA.value}\"]\n        )\n        self.row_index = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\", dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"]\n        )\n        self.col_index = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.COLPTR.value}\", dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"]\n        )\n\n    def _write_metadata(self) -&gt; None:\n        with open(f\"{self.data_path}/{FileNames.METADATA.value}\", f\"{Mode.CREATE.value}\") as mfi:\n            json.dump(self.metadata, mfi)\n\n    def regular_load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; Tuple[pd.DataFrame, int]:\n        \"\"\"Method for loading an h5ad file into memorySu and converting it to the SCDL format.\n\n        Args:\n            anndata_path: location of data to load\n        Raises:\n            NotImplementedError if the data is not in scipy.sparse.spmatrix format\n            ValueError it there is not count data\n        Returns:\n            pd.DataFrame: var variables for features\n            int: number of rows in the dataframe.\n\n        \"\"\"\n        adata = ad.read_h5ad(anndata_path)  # slow\n\n        if not isinstance(adata.X, scipy.sparse.spmatrix):\n            raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n        # Check if raw data is present\n        raw = getattr(adata, \"raw\", None)\n        count_data = None\n        if raw is not None:\n            # If it is, attempt to get the counts in the raw data.\n            count_data = getattr(raw, \"X\", None)\n\n        if count_data is None:\n            # No raw counts were present, resort to normalized\n            count_data = getattr(adata, \"X\")\n        if count_data is None:\n            raise ValueError(\"This file does not have count data\")\n\n        shape = count_data.shape\n        num_rows = shape[0]\n\n        num_elements_stored = count_data.nnz\n\n        self.dtypes[f\"{FileNames.DATA.value}\"] = count_data.dtype\n\n        # Create the arrays.\n        self._init_arrs(num_elements_stored, num_rows)\n        # Store data\n        self.data[0:num_elements_stored] = count_data.data\n\n        # Store the col idx array\n        self.col_index[0:num_elements_stored] = count_data.indices.astype(int)\n\n        # Store the row idx array\n        self.row_index[0 : num_rows + 1] = count_data.indptr.astype(int)\n\n        return adata.var, num_rows\n\n    def paginated_load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; Tuple[pd.DataFrame, int]:\n        \"\"\"Method for block loading a larger h5ad file and converting it to the SCDL format.\n\n        This should be used in the case when the entire anndata file cannot be loaded into memory.\n        The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk\n        is converted into numpy memory maps which are then concatenated together.\n\n        Raises:\n            NotImplementedError if the data is not loaded in the CSRDataset format.\n\n        Returns:\n            pd.DataFrame: var variables for features\n            int: number of rows in the dataframe.\n        \"\"\"\n        adata = ad.read_h5ad(anndata_path, backed=True)\n\n        if not isinstance(adata.X, ad.experimental.CSRDataset):\n            raise NotImplementedError(\"Non-sparse format cannot be loaded: {type(adata.X)}.\")\n        num_rows = adata.X.shape[0]\n\n        self.dtypes[f\"{FileNames.DATA.value}\"] = adata.X.dtype\n\n        # Read the row indices into a memory map.\n        mode = Mode.CREATE_APPEND\n        self.row_index = _create_row_memmaps(num_rows, Path(self.data_path), mode, self.dtypes)\n        self.row_index[:] = adata.X._indptr.astype(int)\n\n        # The data from each column and data chunk of the original anndata file is read in. This is saved into the final\n        # location of the memmap file. In this step, it is saved in the binary file format.\n        memmap_dir_path = Path(self.data_path)\n        with (\n            open(f\"{memmap_dir_path}/{FileNames.COLPTR.value}\", \"wb\") as col_file,\n            open(f\"{memmap_dir_path}/{FileNames.DATA.value}\", \"wb\") as data_file,\n        ):\n            n_elements = 0\n            for row_start in range(0, num_rows, self.load_block_row_size):\n                # Write each array's data to the file in binary format\n                col_block = adata.X[row_start : row_start + self.load_block_row_size].indices\n                col_file.write(col_block.tobytes())\n\n                data_block = adata.X[row_start : row_start + self.load_block_row_size].data\n                data_file.write(data_block.tobytes())\n\n                n_elements += len(data_block)\n\n        # The column and data files are re-opened as memory-mapped arrays with the final shape\n        mode = Mode.READ_APPEND\n        self.col_index = np.memmap(\n            f\"{memmap_dir_path}/{FileNames.COLPTR.value}\",\n            self.dtypes[f\"{FileNames.COLPTR.value}\"],\n            mode=mode,\n            shape=(n_elements,),\n        )\n        self.data = np.memmap(\n            f\"{memmap_dir_path}/{FileNames.DATA.value}\",\n            dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n            mode=mode,\n            shape=(n_elements,),\n        )\n        return adata.var, num_rows\n\n    def load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; None:\n        \"\"\"Loads an existing AnnData archive from disk.\n\n        This creates a new backing data structure which is saved.\n        Note: the storage utilized will roughly double. Currently, the data must\n        be in a scipy.sparse.spmatrix format.\n\n        Args:\n            anndata_path: location of data to load\n        Raises:\n            FileNotFoundError if the data path does not exist.\n            NotImplementedError if the data is not in scipy.sparse.spmatrix\n            format\n            ValueError it there is not count data\n        \"\"\"\n        if not os.path.exists(anndata_path):\n            raise FileNotFoundError(f\"Error: could not find h5ad path {anndata_path}\")\n        file_size_MB = os.path.getsize(anndata_path) / (1_024**2)\n\n        if file_size_MB &lt; self.paginated_load_cutoff:\n            features_df, num_rows = self.regular_load_h5ad(anndata_path)\n        else:\n            features_df, num_rows = self.paginated_load_h5ad(anndata_path)\n        if len(features_df.columns) &gt; 0:\n            features = {col: np.array(features_df[col].values) for col in features_df.columns}\n        elif len(features_df.index) &gt; 0:\n            features = {self.feature_index_name: features_df.index.values}\n        else:\n            features = {}\n        self._feature_index.append_features(n_obs=num_rows, features=features, label=anndata_path)\n        self.save()\n\n    def save(self, output_path: Optional[str] = None) -&gt; None:\n        \"\"\"Saves the class to a given output path.\n\n        Args:\n            output_path: The location to save - not yet implemented and should\n            be self.data_path\n\n        Raises:\n           NotImplementedError if output_path is not None.\n        \"\"\"\n        if f\"{METADATA.NUM_ROWS.value}\" not in self.metadata:\n            self.metadata[f\"{METADATA.NUM_ROWS.value}\"] = self.number_of_rows()\n\n        self._write_metadata()\n        # Write the feature index. This may not exist.\n        self._feature_index.save(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n        # Ensure the object is in a valid state. These are saved at creation!\n        for postfix in [\n            f\"{FileNames.VERSION.value}\",\n            f\"{FileNames.DATA.value}\",\n            f\"{FileNames.COLPTR.value}\",\n            f\"{FileNames.ROWPTR.value}\",\n            f\"{FileNames.FEATURES.value}\",\n        ]:\n            if not os.path.exists(f\"{self.data_path}/{postfix}\"):\n                raise FileNotFoundError(f\"This file should exist from object creation: {self.data_path}/{postfix}\")\n\n        self.data.flush()\n        self.row_index.flush()\n        self.col_index.flush()\n\n        if output_path is not None:\n            raise NotImplementedError(\"Saving to separate path is not yet implemented.\")\n\n        return True\n\n    def number_of_values(self) -&gt; int:\n        \"\"\"Get the total number of values in the array.\n\n        For each index, the length of the corresponding np.ndarray of features is counted.\n\n        Returns:\n            The sum of lengths of the features in every row\n        \"\"\"\n        return sum(self._feature_index.number_of_values())\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the dataset.\n\n        Returns:\n            The number of rows in the dataset\n        Raises:\n            ValueError if the length of the number of rows in the feature\n            index does not correspond to the number of stored rows.\n        \"\"\"\n        if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != self.row_index.size - 1:\n            raise ValueError(\n                f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                             does not correspond to the number of rows in the row_index {self.row_index.size - 1}\"\"\"\n            )\n        return self._feature_index.number_of_rows()\n\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Number of non zero entries in the dataset.\"\"\"\n        return self.data.size\n\n    def __len__(self):\n        \"\"\"Return the number of rows.\"\"\"\n        return self.number_of_rows()\n\n    def __getitem__(self, idx: int) -&gt; torch.Tensor:\n        \"\"\"Get the row values located and index idx.\"\"\"\n        return torch.from_numpy(np.stack(self.get_row(idx)[0]))\n\n    def number_of_variables(self) -&gt; List[int]:\n        \"\"\"Get the number of features in every entry in the dataset.\n\n        Returns:\n            A list containing the lengths of the features in every row\n        \"\"\"\n        feats = self._feature_index\n        if len(feats) == 0:\n            return [0]\n        num_vars = feats.column_dims()\n        return num_vars\n\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Get the shape of the dataset.\n\n        This is the number of entries by the the length of the feature index\n        corresponding to that variable.\n\n        Returns:\n            The number of elements in the dataset\n            A list containing the number of variables for each row.\n        \"\"\"\n        return self.number_of_rows(), self.number_of_variables()\n\n    def concat(\n        self,\n        other_dataset: Union[list[\"SingleCellMemMapDataset\"], \"SingleCellMemMapDataset\"],\n    ) -&gt; None:\n        \"\"\"Concatenates another SingleCellMemMapDataset to the existing one.\n\n        The data is stored in the same place as for the original data set. This\n        necessitates using _swap_memmap_array.\n\n        Args:\n            other_dataset: A SingleCellMemMapDataset or a list of\n            SingleCellMemMapDatasets\n\n        Raises:\n           ValueError if the other dataset(s) are not of the same version or\n           something of another type is passed in.\n        \"\"\"\n        # Verify the other dataset or datasets are of the same type.\n        match other_dataset:\n            case self.__class__():\n                other_dataset = [other_dataset]\n            case list():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Expecting either a {SingleCellMemMapDataset} or a list thereof. Actually got: {type(other_dataset)}\"\n                )\n\n        for dataset in other_dataset:\n            if self.version() != dataset.version():\n                raise ValueError(\n                    f\"\"\"Incompatable versions: input version: {dataset.version()},\n            this version:  {self.version}\"\"\"\n                )\n\n        # Set our mode:\n        self.mode: Mode = Mode.READ_APPEND\n\n        mmaps = []\n        mmaps.extend(other_dataset)\n        # Calculate the size of our new dataset arrays\n        total_num_elements = (self.number_nonzero_values() if self.number_of_rows() &gt; 0 else 0) + sum(\n            [m.number_nonzero_values() for m in mmaps]\n        )\n        total_num_rows = self.number_of_rows() + sum([m.number_of_rows() for m in mmaps])\n\n        # Create new arrays to store the data, colptr, and rowptr.\n        with tempfile.TemporaryDirectory(prefix=\"_tmp\", dir=self.data_path) as tmp:\n            data_arr, col_arr, row_arr = _create_compressed_sparse_row_memmaps(\n                num_elements=total_num_elements,\n                num_rows=total_num_rows,\n                memmap_dir_path=Path(tmp),\n                mode=Mode.CREATE_APPEND,\n                dtypes=self.dtypes,\n            )\n            # Copy the data from self and other into the new arrays.\n            cumulative_elements = 0\n            cumulative_rows = 0\n            if self.number_of_rows() &gt; 0:\n                data_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.data.data\n                col_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.col_index.data\n                row_arr[cumulative_rows : cumulative_rows + self.number_of_rows() + 1] = self.row_index.data\n                cumulative_elements += self.number_nonzero_values()\n                cumulative_rows += self.number_of_rows()\n            for mmap in mmaps:\n                # Fill the data array for the span of this scmmap\n                data_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.data.data\n                # fill the col array for the span of this scmmap\n                col_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.col_index.data\n                # Fill the row array for the span of this scmmap\n                row_arr[cumulative_rows : cumulative_rows + mmap.number_of_rows() + 1] = (\n                    mmap.row_index + int(cumulative_elements)\n                ).data\n\n                self._feature_index.concat(mmap._feature_index)\n                # Update counters\n                cumulative_elements += mmap.number_nonzero_values()\n                cumulative_rows += mmap.number_of_rows()\n            # The arrays are swapped to ensure that the data remains stored at self.data_path and\n            # not at a temporary filepath.\n            _swap_mmap_array(\n                data_arr,\n                f\"{tmp}/{FileNames.DATA.value}\",\n                self.data,\n                f\"{self.data_path}/{FileNames.DATA.value}\",\n                destroy_src=True,\n            )\n            _swap_mmap_array(\n                col_arr,\n                f\"{tmp}/{FileNames.COLPTR.value}\",\n                self.col_index,\n                f\"{self.data_path}/{FileNames.COLPTR.value}\",\n                destroy_src=True,\n            )\n            _swap_mmap_array(\n                row_arr,\n                f\"{tmp}/{FileNames.ROWPTR.value}\",\n                self.row_index,\n                f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n                destroy_src=True,\n            )\n            # Reopen the data, colptr, and rowptr arrays\n            self.data = np.memmap(\n                f\"{self.data_path}/{FileNames.DATA.value}\",\n                dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n                shape=(cumulative_elements,),\n                mode=Mode.READ_APPEND.value,\n            )\n            self.row_index = np.memmap(\n                f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n                dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"],\n                shape=(cumulative_rows + 1,),\n                mode=Mode.READ_APPEND.value,\n            )\n            self.col_index = np.memmap(\n                f\"{self.data_path}/{FileNames.COLPTR.value}\",\n                dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"],\n                shape=(cumulative_elements,),\n                mode=Mode.READ_APPEND.value,\n            )\n\n        self.save()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get the row values located and index idx.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; torch.Tensor:\n    \"\"\"Get the row values located and index idx.\"\"\"\n    return torch.from_numpy(np.stack(self.get_row(idx)[0]))\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__init__","title":"<code>__init__(data_path, h5ad_path=None, num_elements=None, num_rows=None, mode=Mode.READ_APPEND, paginated_load_cutoff=10000, load_block_row_size=1000000, feature_index_name='feature_id')</code>","text":"<p>Instantiate the class.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The location where the data np.memmap files are read from</p> required <code>h5ad_path</code> <code>Optional[str]</code> <p>Optional, the location of the h5_ad path.</p> <code>None</code> <code>num_elements</code> <code>Optional[int]</code> <p>The total number of elements in the array.</p> <code>None</code> <code>num_rows</code> <code>Optional[int]</code> <p>The number of rows in the data frame.</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>Whether to read or write from the data_path.</p> <code>READ_APPEND</code> <code>paginated_load_cutoff</code> <code>int</code> <p>MB size on disk at which to load the h5ad structure with paginated load.</p> <code>10000</code> <code>load_block_row_size</code> <code>int</code> <p>Number of rows to load into memory with paginated load</p> <code>1000000</code> <code>feature_index_name</code> <p>The name of the features if the features are only stored in features_df.index.values</p> <code>'feature_id'</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    h5ad_path: Optional[str] = None,\n    num_elements: Optional[int] = None,\n    num_rows: Optional[int] = None,\n    mode: Mode = Mode.READ_APPEND,\n    paginated_load_cutoff: int = 10_000,\n    load_block_row_size: int = 1_000_000,\n    feature_index_name=\"feature_id\",\n) -&gt; None:\n    \"\"\"Instantiate the class.\n\n    Args:\n        data_path: The location where the data np.memmap files are read from\n        or stored.\n        h5ad_path: Optional, the location of the h5_ad path.\n        num_elements: The total number of elements in the array.\n        num_rows: The number of rows in the data frame.\n        mode: Whether to read or write from the data_path.\n        paginated_load_cutoff: MB size on disk at which to load the h5ad structure with paginated load.\n        load_block_row_size: Number of rows to load into memory with paginated load\n        feature_index_name: The name of the features if the features are only stored in features_df.index.values\n    \"\"\"\n    self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n    self.data_path: str = data_path\n    self.mode: Mode = mode\n    self.paginated_load_cutoff = paginated_load_cutoff\n    self.load_block_row_size = load_block_row_size\n    self.feature_index_name = feature_index_name\n    # Backing arrays\n    self.data: Optional[np.ndarray] = None\n    self.row_index: Optional[np.ndarray] = None\n    self.row_index: Optional[np.ndarray] = None\n\n    # Metadata and attributes\n    self.metadata: Dict[str, int] = {}\n\n    # Stores the Feature Index, which tracks\n    # the original AnnData features (e.g., gene names)\n    # and allows us to store ragged arrays in our SCMMAP structure.\n    self._feature_index: RowFeatureIndex = RowFeatureIndex()\n\n    # Variables for int packing / reduced precision\n    self.dtypes: Dict[FileNames, str] = {\n        f\"{FileNames.DATA.value}\": \"float32\",\n        f\"{FileNames.COLPTR.value}\": \"uint32\",\n        f\"{FileNames.ROWPTR.value}\": \"uint64\",\n    }\n\n    if mode == Mode.CREATE_APPEND and os.path.exists(data_path):\n        raise FileExistsError(f\"Output directory already exists: {data_path}\")\n\n    if h5ad_path is not None and (data_path is not None and os.path.exists(data_path)):\n        raise FileExistsError(\n            \"Invalid input; both an existing SCMMAP and an h5ad file were passed. \"\n            \"Please pass either an existing SCMMAP or an h5ad file.\"\n        )\n\n    # If there is only a data path, and it exists already, load SCMMAP data.\n    elif data_path is not None and os.path.exists(data_path):\n        self.__init__obj()\n        self.load(data_path)\n\n    # If there is only an h5ad path, load the HDF5 data\n    elif h5ad_path is not None:\n        self.__init__obj()\n        self.load_h5ad(h5ad_path)\n    else:\n        match num_rows, num_elements:\n            case (int(), int()):\n                self.__init__obj()\n                self._init_arrs(num_elements=num_elements, num_rows=num_rows)\n            case _:\n                raise ValueError(\n                    \"An np.memmap path, an h5ad path, or the number of elements and rows is required\" \"\"\n                )\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__init__obj","title":"<code>__init__obj()</code>","text":"<p>Initializes the datapath and writes the version.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __init__obj(self):\n    \"\"\"Initializes the datapath and writes the version.\"\"\"\n    os.makedirs(self.data_path, exist_ok=True)\n\n    # Write the version\n    if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n        with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n            json.dump(self.version(), vfi)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of rows.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of rows.\"\"\"\n    return self.number_of_rows()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.concat","title":"<code>concat(other_dataset)</code>","text":"<p>Concatenates another SingleCellMemMapDataset to the existing one.</p> <p>The data is stored in the same place as for the original data set. This necessitates using _swap_memmap_array.</p> <p>Parameters:</p> Name Type Description Default <code>other_dataset</code> <code>Union[list[SingleCellMemMapDataset], SingleCellMemMapDataset]</code> <p>A SingleCellMemMapDataset or a list of</p> required Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def concat(\n    self,\n    other_dataset: Union[list[\"SingleCellMemMapDataset\"], \"SingleCellMemMapDataset\"],\n) -&gt; None:\n    \"\"\"Concatenates another SingleCellMemMapDataset to the existing one.\n\n    The data is stored in the same place as for the original data set. This\n    necessitates using _swap_memmap_array.\n\n    Args:\n        other_dataset: A SingleCellMemMapDataset or a list of\n        SingleCellMemMapDatasets\n\n    Raises:\n       ValueError if the other dataset(s) are not of the same version or\n       something of another type is passed in.\n    \"\"\"\n    # Verify the other dataset or datasets are of the same type.\n    match other_dataset:\n        case self.__class__():\n            other_dataset = [other_dataset]\n        case list():\n            pass\n        case _:\n            raise ValueError(\n                f\"Expecting either a {SingleCellMemMapDataset} or a list thereof. Actually got: {type(other_dataset)}\"\n            )\n\n    for dataset in other_dataset:\n        if self.version() != dataset.version():\n            raise ValueError(\n                f\"\"\"Incompatable versions: input version: {dataset.version()},\n        this version:  {self.version}\"\"\"\n            )\n\n    # Set our mode:\n    self.mode: Mode = Mode.READ_APPEND\n\n    mmaps = []\n    mmaps.extend(other_dataset)\n    # Calculate the size of our new dataset arrays\n    total_num_elements = (self.number_nonzero_values() if self.number_of_rows() &gt; 0 else 0) + sum(\n        [m.number_nonzero_values() for m in mmaps]\n    )\n    total_num_rows = self.number_of_rows() + sum([m.number_of_rows() for m in mmaps])\n\n    # Create new arrays to store the data, colptr, and rowptr.\n    with tempfile.TemporaryDirectory(prefix=\"_tmp\", dir=self.data_path) as tmp:\n        data_arr, col_arr, row_arr = _create_compressed_sparse_row_memmaps(\n            num_elements=total_num_elements,\n            num_rows=total_num_rows,\n            memmap_dir_path=Path(tmp),\n            mode=Mode.CREATE_APPEND,\n            dtypes=self.dtypes,\n        )\n        # Copy the data from self and other into the new arrays.\n        cumulative_elements = 0\n        cumulative_rows = 0\n        if self.number_of_rows() &gt; 0:\n            data_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.data.data\n            col_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.col_index.data\n            row_arr[cumulative_rows : cumulative_rows + self.number_of_rows() + 1] = self.row_index.data\n            cumulative_elements += self.number_nonzero_values()\n            cumulative_rows += self.number_of_rows()\n        for mmap in mmaps:\n            # Fill the data array for the span of this scmmap\n            data_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.data.data\n            # fill the col array for the span of this scmmap\n            col_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.col_index.data\n            # Fill the row array for the span of this scmmap\n            row_arr[cumulative_rows : cumulative_rows + mmap.number_of_rows() + 1] = (\n                mmap.row_index + int(cumulative_elements)\n            ).data\n\n            self._feature_index.concat(mmap._feature_index)\n            # Update counters\n            cumulative_elements += mmap.number_nonzero_values()\n            cumulative_rows += mmap.number_of_rows()\n        # The arrays are swapped to ensure that the data remains stored at self.data_path and\n        # not at a temporary filepath.\n        _swap_mmap_array(\n            data_arr,\n            f\"{tmp}/{FileNames.DATA.value}\",\n            self.data,\n            f\"{self.data_path}/{FileNames.DATA.value}\",\n            destroy_src=True,\n        )\n        _swap_mmap_array(\n            col_arr,\n            f\"{tmp}/{FileNames.COLPTR.value}\",\n            self.col_index,\n            f\"{self.data_path}/{FileNames.COLPTR.value}\",\n            destroy_src=True,\n        )\n        _swap_mmap_array(\n            row_arr,\n            f\"{tmp}/{FileNames.ROWPTR.value}\",\n            self.row_index,\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n            destroy_src=True,\n        )\n        # Reopen the data, colptr, and rowptr arrays\n        self.data = np.memmap(\n            f\"{self.data_path}/{FileNames.DATA.value}\",\n            dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n            shape=(cumulative_elements,),\n            mode=Mode.READ_APPEND.value,\n        )\n        self.row_index = np.memmap(\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"],\n            shape=(cumulative_rows + 1,),\n            mode=Mode.READ_APPEND.value,\n        )\n        self.col_index = np.memmap(\n            f\"{self.data_path}/{FileNames.COLPTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"],\n            shape=(cumulative_elements,),\n            mode=Mode.READ_APPEND.value,\n        )\n\n    self.save()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.features","title":"<code>features()</code>","text":"<p>Return the corresponding RowFeatureIndex.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def features(self) -&gt; Optional[RowFeatureIndex]:\n    \"\"\"Return the corresponding RowFeatureIndex.\"\"\"\n    return self._feature_index\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row","title":"<code>get_row(index, return_features=False, feature_vars=None)</code>","text":"<p>Returns a given row in the dataset along with optional features.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned. This is in the range of [0, num_rows)</p> required <code>return_features</code> <code>bool</code> <p>boolean that indicates whether to return features</p> <code>False</code> <code>feature_vars</code> <code>Optional[List[str]]</code> <p>Optional, feature variables to extract</p> <code>None</code> <p>Return:     [Tuple[np.ndarray, np.ndarray]: data values and column pointes     List[np.ndarray]: optional, corresponding features.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row(\n    self,\n    index: int,\n    return_features: bool = False,\n    feature_vars: Optional[List[str]] = None,\n) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], List[np.ndarray]]:\n    \"\"\"Returns a given row in the dataset along with optional features.\n\n    Args:\n        index: The row to be returned. This is in the range of [0, num_rows)\n        return_features: boolean that indicates whether to return features\n        feature_vars: Optional, feature variables to extract\n    Return:\n        [Tuple[np.ndarray, np.ndarray]: data values and column pointes\n        List[np.ndarray]: optional, corresponding features.\n    \"\"\"\n    start = self.row_index[index]\n    end = self.row_index[index + 1]\n    values = self.data[start:end]\n    columns = self.col_index[start:end]\n    ret = (values, columns)\n    if return_features:\n        return ret, self._feature_index.lookup(index, select_features=feature_vars)[0]\n    else:\n        return ret, None\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_column","title":"<code>get_row_column(index, column, impute_missing_zeros=True)</code>","text":"<p>Returns the value at a given index and the corresponding column.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index to be returned</p> required <code>column</code> <code>int</code> <p>The column to be returned</p> required <code>impute_missing_zeros</code> <code>bool</code> <p>boolean that indicates whether to set missing</p> <code>True</code> <p>Return:     A float that is the value in the array or None.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_column(self, index: int, column: int, impute_missing_zeros: bool = True) -&gt; Optional[float]:\n    \"\"\"Returns the value at a given index and the corresponding column.\n\n    Args:\n        index: The index to be returned\n        column: The column to be returned\n        impute_missing_zeros: boolean that indicates whether to set missing\n        data to 0\n    Return:\n        A float that is the value in the array or None.\n    \"\"\"\n    (row_values, row_column_pointer), _ = self.get_row(index)\n    if column is not None:\n        for col_index, col in enumerate(row_column_pointer):\n            if col == column:\n                # return the value at this position\n                return row_values[col_index]\n            elif col &gt; column:\n                try:\n                    raise ValueError(f\"Column pointer {col} is larger than the column {column}.\")\n                except ValueError:\n                    break\n        return 0.0 if impute_missing_zeros else None\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_padded","title":"<code>get_row_padded(index, return_features=False, feature_vars=None)</code>","text":"<p>Returns a padded version of a row in the dataset.</p> <p>A padded version is one where the a sparse array representation is converted to a conventional represenentation. Optionally, features are returned.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned</p> required <code>return_features</code> <code>bool</code> <p>boolean that indicates whether to return features</p> <code>False</code> <code>feature_vars</code> <code>Optional[List[str]]</code> <p>Optional, feature variables to extract</p> <code>None</code> <p>Return:     np.ndarray: conventional row representation     List[np.ndarray]: optional, corresponding features.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_padded(\n    self,\n    index: int,\n    return_features: bool = False,\n    feature_vars: Optional[List[str]] = None,\n) -&gt; Tuple[np.ndarray, List[np.ndarray]]:\n    \"\"\"Returns a padded version of a row in the dataset.\n\n    A padded version is one where the a sparse array representation is\n    converted to a conventional represenentation. Optionally, features are\n    returned.\n\n    Args:\n        index: The row to be returned\n        return_features: boolean that indicates whether to return features\n        feature_vars: Optional, feature variables to extract\n    Return:\n        np.ndarray: conventional row representation\n        List[np.ndarray]: optional, corresponding features.\n    \"\"\"\n    (row_values, row_column_pointer), features = self.get_row(index, return_features, feature_vars)\n    return (\n        _pad_sparse_array(row_values, row_column_pointer, self._feature_index.number_vars_at_row(index)),\n        features,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.load","title":"<code>load(stored_path)</code>","text":"<p>Loads the data at store_path that is an np.memmap format.</p> <p>Parameters:</p> Name Type Description Default <code>stored_path</code> <code>str</code> <p>directory with np.memmap files</p> required <p>Raises:     FileNotFoundError if the corresponding directory or files are not     found, or if the metadata file is not present.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def load(self, stored_path: str) -&gt; None:\n    \"\"\"Loads the data at store_path that is an np.memmap format.\n\n    Args:\n        stored_path: directory with np.memmap files\n    Raises:\n        FileNotFoundError if the corresponding directory or files are not\n        found, or if the metadata file is not present.\n    \"\"\"\n    if not os.path.exists(stored_path):\n        raise FileNotFoundError(\n            f\"\"\"Error: the specified data path to the mmap files {stored_path} does not exist.\n                                Specify an updated filepath or provide an h5ad path to the dataset. The data can\n                                be loaded with SingleCellMemMapDataset.load_h5ad. Alternatively, the class can be instantiated\n                                with  SingleCellMemMapDataset(&lt;path to data that will be created&gt;, h5ad_path=&lt;path to h5ad file&gt;\"\"\"\n        )\n    self.data_path = stored_path\n    self.mode = Mode.READ_APPEND\n\n    # Metadata is required, so we must check if it exists and fail if not.\n    if not os.path.exists(f\"{self.data_path}/{FileNames.METADATA.value}\"):\n        raise FileNotFoundError(\n            f\"Error: the metadata file {self.data_path}/{FileNames.METADATA.value} does not exist.\"\n        )\n\n    with open(f\"{self.data_path}/{FileNames.METADATA.value}\", Mode.READ_APPEND.value) as mfi:\n        self.metadata = json.load(mfi)\n\n    if os.path.exists(f\"{self.data_path}/{FileNames.FEATURES.value}\"):\n        self._feature_index = RowFeatureIndex.load(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n    if os.path.exists(f\"{self.data_path}/{FileNames.DTYPE.value}\"):\n        with open(f\"{self.data_path}/{FileNames.DTYPE.value}\") as dfi:\n            self.dtypes = json.load(dfi)\n\n    # mmap the existing arrays\n    self.data = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.DATA.value}\", self.dtypes[f\"{FileNames.DATA.value}\"]\n    )\n    self.row_index = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.ROWPTR.value}\", dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"]\n    )\n    self.col_index = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.COLPTR.value}\", dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"]\n    )\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.load_h5ad","title":"<code>load_h5ad(anndata_path)</code>","text":"<p>Loads an existing AnnData archive from disk.</p> <p>This creates a new backing data structure which is saved. Note: the storage utilized will roughly double. Currently, the data must be in a scipy.sparse.spmatrix format.</p> <p>Parameters:</p> Name Type Description Default <code>anndata_path</code> <code>str</code> <p>location of data to load</p> required <p>Raises:     FileNotFoundError if the data path does not exist.     NotImplementedError if the data is not in scipy.sparse.spmatrix     format     ValueError it there is not count data</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; None:\n    \"\"\"Loads an existing AnnData archive from disk.\n\n    This creates a new backing data structure which is saved.\n    Note: the storage utilized will roughly double. Currently, the data must\n    be in a scipy.sparse.spmatrix format.\n\n    Args:\n        anndata_path: location of data to load\n    Raises:\n        FileNotFoundError if the data path does not exist.\n        NotImplementedError if the data is not in scipy.sparse.spmatrix\n        format\n        ValueError it there is not count data\n    \"\"\"\n    if not os.path.exists(anndata_path):\n        raise FileNotFoundError(f\"Error: could not find h5ad path {anndata_path}\")\n    file_size_MB = os.path.getsize(anndata_path) / (1_024**2)\n\n    if file_size_MB &lt; self.paginated_load_cutoff:\n        features_df, num_rows = self.regular_load_h5ad(anndata_path)\n    else:\n        features_df, num_rows = self.paginated_load_h5ad(anndata_path)\n    if len(features_df.columns) &gt; 0:\n        features = {col: np.array(features_df[col].values) for col in features_df.columns}\n    elif len(features_df.index) &gt; 0:\n        features = {self.feature_index_name: features_df.index.values}\n    else:\n        features = {}\n    self._feature_index.append_features(n_obs=num_rows, features=features, label=anndata_path)\n    self.save()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_nonzero_values","title":"<code>number_nonzero_values()</code>","text":"<p>Number of non zero entries in the dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_nonzero_values(self) -&gt; int:\n    \"\"\"Number of non zero entries in the dataset.\"\"\"\n    return self.data.size\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the dataset</p> <p>Raises:     ValueError if the length of the number of rows in the feature     index does not correspond to the number of stored rows.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the dataset.\n\n    Returns:\n        The number of rows in the dataset\n    Raises:\n        ValueError if the length of the number of rows in the feature\n        index does not correspond to the number of stored rows.\n    \"\"\"\n    if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != self.row_index.size - 1:\n        raise ValueError(\n            f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                         does not correspond to the number of rows in the row_index {self.row_index.size - 1}\"\"\"\n        )\n    return self._feature_index.number_of_rows()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Get the total number of values in the array.</p> <p>For each index, the length of the corresponding np.ndarray of features is counted.</p> <p>Returns:</p> Type Description <code>int</code> <p>The sum of lengths of the features in every row</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_values(self) -&gt; int:\n    \"\"\"Get the total number of values in the array.\n\n    For each index, the length of the corresponding np.ndarray of features is counted.\n\n    Returns:\n        The sum of lengths of the features in every row\n    \"\"\"\n    return sum(self._feature_index.number_of_values())\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_variables","title":"<code>number_of_variables()</code>","text":"<p>Get the number of features in every entry in the dataset.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>A list containing the lengths of the features in every row</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_variables(self) -&gt; List[int]:\n    \"\"\"Get the number of features in every entry in the dataset.\n\n    Returns:\n        A list containing the lengths of the features in every row\n    \"\"\"\n    feats = self._feature_index\n    if len(feats) == 0:\n        return [0]\n    num_vars = feats.column_dims()\n    return num_vars\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.paginated_load_h5ad","title":"<code>paginated_load_h5ad(anndata_path)</code>","text":"<p>Method for block loading a larger h5ad file and converting it to the SCDL format.</p> <p>This should be used in the case when the entire anndata file cannot be loaded into memory. The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk is converted into numpy memory maps which are then concatenated together.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>pd.DataFrame: var variables for features</p> <code>int</code> <code>int</code> <p>number of rows in the dataframe.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def paginated_load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; Tuple[pd.DataFrame, int]:\n    \"\"\"Method for block loading a larger h5ad file and converting it to the SCDL format.\n\n    This should be used in the case when the entire anndata file cannot be loaded into memory.\n    The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk\n    is converted into numpy memory maps which are then concatenated together.\n\n    Raises:\n        NotImplementedError if the data is not loaded in the CSRDataset format.\n\n    Returns:\n        pd.DataFrame: var variables for features\n        int: number of rows in the dataframe.\n    \"\"\"\n    adata = ad.read_h5ad(anndata_path, backed=True)\n\n    if not isinstance(adata.X, ad.experimental.CSRDataset):\n        raise NotImplementedError(\"Non-sparse format cannot be loaded: {type(adata.X)}.\")\n    num_rows = adata.X.shape[0]\n\n    self.dtypes[f\"{FileNames.DATA.value}\"] = adata.X.dtype\n\n    # Read the row indices into a memory map.\n    mode = Mode.CREATE_APPEND\n    self.row_index = _create_row_memmaps(num_rows, Path(self.data_path), mode, self.dtypes)\n    self.row_index[:] = adata.X._indptr.astype(int)\n\n    # The data from each column and data chunk of the original anndata file is read in. This is saved into the final\n    # location of the memmap file. In this step, it is saved in the binary file format.\n    memmap_dir_path = Path(self.data_path)\n    with (\n        open(f\"{memmap_dir_path}/{FileNames.COLPTR.value}\", \"wb\") as col_file,\n        open(f\"{memmap_dir_path}/{FileNames.DATA.value}\", \"wb\") as data_file,\n    ):\n        n_elements = 0\n        for row_start in range(0, num_rows, self.load_block_row_size):\n            # Write each array's data to the file in binary format\n            col_block = adata.X[row_start : row_start + self.load_block_row_size].indices\n            col_file.write(col_block.tobytes())\n\n            data_block = adata.X[row_start : row_start + self.load_block_row_size].data\n            data_file.write(data_block.tobytes())\n\n            n_elements += len(data_block)\n\n    # The column and data files are re-opened as memory-mapped arrays with the final shape\n    mode = Mode.READ_APPEND\n    self.col_index = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.COLPTR.value}\",\n        self.dtypes[f\"{FileNames.COLPTR.value}\"],\n        mode=mode,\n        shape=(n_elements,),\n    )\n    self.data = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.DATA.value}\",\n        dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n        mode=mode,\n        shape=(n_elements,),\n    )\n    return adata.var, num_rows\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.regular_load_h5ad","title":"<code>regular_load_h5ad(anndata_path)</code>","text":"<p>Method for loading an h5ad file into memorySu and converting it to the SCDL format.</p> <p>Parameters:</p> Name Type Description Default <code>anndata_path</code> <code>str</code> <p>location of data to load</p> required <p>Raises:     NotImplementedError if the data is not in scipy.sparse.spmatrix format     ValueError it there is not count data Returns:     pd.DataFrame: var variables for features     int: number of rows in the dataframe.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def regular_load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; Tuple[pd.DataFrame, int]:\n    \"\"\"Method for loading an h5ad file into memorySu and converting it to the SCDL format.\n\n    Args:\n        anndata_path: location of data to load\n    Raises:\n        NotImplementedError if the data is not in scipy.sparse.spmatrix format\n        ValueError it there is not count data\n    Returns:\n        pd.DataFrame: var variables for features\n        int: number of rows in the dataframe.\n\n    \"\"\"\n    adata = ad.read_h5ad(anndata_path)  # slow\n\n    if not isinstance(adata.X, scipy.sparse.spmatrix):\n        raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n    # Check if raw data is present\n    raw = getattr(adata, \"raw\", None)\n    count_data = None\n    if raw is not None:\n        # If it is, attempt to get the counts in the raw data.\n        count_data = getattr(raw, \"X\", None)\n\n    if count_data is None:\n        # No raw counts were present, resort to normalized\n        count_data = getattr(adata, \"X\")\n    if count_data is None:\n        raise ValueError(\"This file does not have count data\")\n\n    shape = count_data.shape\n    num_rows = shape[0]\n\n    num_elements_stored = count_data.nnz\n\n    self.dtypes[f\"{FileNames.DATA.value}\"] = count_data.dtype\n\n    # Create the arrays.\n    self._init_arrs(num_elements_stored, num_rows)\n    # Store data\n    self.data[0:num_elements_stored] = count_data.data\n\n    # Store the col idx array\n    self.col_index[0:num_elements_stored] = count_data.indices.astype(int)\n\n    # Store the row idx array\n    self.row_index[0 : num_rows + 1] = count_data.indptr.astype(int)\n\n    return adata.var, num_rows\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.save","title":"<code>save(output_path=None)</code>","text":"<p>Saves the class to a given output path.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Optional[str]</code> <p>The location to save - not yet implemented and should</p> <code>None</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def save(self, output_path: Optional[str] = None) -&gt; None:\n    \"\"\"Saves the class to a given output path.\n\n    Args:\n        output_path: The location to save - not yet implemented and should\n        be self.data_path\n\n    Raises:\n       NotImplementedError if output_path is not None.\n    \"\"\"\n    if f\"{METADATA.NUM_ROWS.value}\" not in self.metadata:\n        self.metadata[f\"{METADATA.NUM_ROWS.value}\"] = self.number_of_rows()\n\n    self._write_metadata()\n    # Write the feature index. This may not exist.\n    self._feature_index.save(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n    # Ensure the object is in a valid state. These are saved at creation!\n    for postfix in [\n        f\"{FileNames.VERSION.value}\",\n        f\"{FileNames.DATA.value}\",\n        f\"{FileNames.COLPTR.value}\",\n        f\"{FileNames.ROWPTR.value}\",\n        f\"{FileNames.FEATURES.value}\",\n    ]:\n        if not os.path.exists(f\"{self.data_path}/{postfix}\"):\n            raise FileNotFoundError(f\"This file should exist from object creation: {self.data_path}/{postfix}\")\n\n    self.data.flush()\n    self.row_index.flush()\n    self.col_index.flush()\n\n    if output_path is not None:\n        raise NotImplementedError(\"Saving to separate path is not yet implemented.\")\n\n    return True\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.shape","title":"<code>shape()</code>","text":"<p>Get the shape of the dataset.</p> <p>This is the number of entries by the the length of the feature index corresponding to that variable.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of elements in the dataset</p> <code>List[int]</code> <p>A list containing the number of variables for each row.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Get the shape of the dataset.\n\n    This is the number of entries by the the length of the feature index\n    corresponding to that variable.\n\n    Returns:\n        The number of elements in the dataset\n        A list containing the number of variables for each row.\n    \"\"\"\n    return self.number_of_rows(), self.number_of_variables()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset._create_compressed_sparse_row_memmaps","title":"<code>_create_compressed_sparse_row_memmaps(num_elements, num_rows, memmap_dir_path, mode, dtypes)</code>","text":"<p>Create a set of CSR-format numpy arrays.</p> <p>They are saved to memmap_dir_path. This is an efficient way of indexing into a sparse matrix. Only non- zero values of the data are stored.</p> <p>To get the data for a specific row, slice row_idx[idx, idx+1] and then get the elements in data[row_idx[idx]:row_idx[idx+1]] which are in the corresponding columns col_index[row_idx[idx], row_idx[row_idx+1]]</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def _create_compressed_sparse_row_memmaps(\n    num_elements: int,\n    num_rows: int,\n    memmap_dir_path: Path,\n    mode: Mode,\n    dtypes: Dict[FileNames, str],\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Create a set of CSR-format numpy arrays.\n\n    They are saved to memmap_dir_path. This is an efficient way of indexing\n    into a sparse matrix. Only non- zero values of the data are stored.\n\n    To get the data for a specific row, slice row_idx[idx, idx+1]\n    and then get the elements in data[row_idx[idx]:row_idx[idx+1]]\n    which are in the corresponding columns col_index[row_idx[idx], row_idx[row_idx+1]]\n\n    \"\"\"\n    if num_elements &lt;= 0:\n        raise ValueError(f\"n_elements is set to {num_elements}. It must be postive to create CSR matrices.\")\n\n    if num_rows &lt;= 0:\n        raise ValueError(f\"num_rows is set to {num_rows}. It must be postive to create CSR matrices.\")\n\n    memmap_dir_path.mkdir(parents=True, exist_ok=True)\n    data_arr, col_arr = _create_data_col_memmaps(\n        num_elements,\n        memmap_dir_path,\n        mode,\n        dtypes,\n    )\n\n    row_arr = _create_row_memmaps(\n        num_rows,\n        memmap_dir_path,\n        mode,\n        dtypes,\n    )\n    return data_arr, col_arr, row_arr\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset._create_data_col_memmaps","title":"<code>_create_data_col_memmaps(num_elements, memmap_dir_path, mode, dtypes)</code>","text":"<p>Records a pointer into the data and column arrays.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def _create_data_col_memmaps(\n    num_elements: int,\n    memmap_dir_path: Path,\n    mode: Mode,\n    dtypes: Dict[FileNames, str],\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Records a pointer into the data and column arrays.\"\"\"\n    # Records the value at index[i]\n    data_arr = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.DATA.value}\",\n        dtype=dtypes[f\"{FileNames.DATA.value}\"],\n        shape=(num_elements,),\n        mode=mode,\n    )\n    # Records the column the data resides in at index [i]\n    col_arr = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.COLPTR.value}\",\n        dtype=dtypes[f\"{FileNames.COLPTR.value}\"],\n        shape=(num_elements,),\n        mode=mode.value,\n    )\n    return data_arr, col_arr\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset._create_row_memmaps","title":"<code>_create_row_memmaps(num_rows, memmap_dir_path, mode, dtypes)</code>","text":"<p>Records a pointer into the data and column arrays.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def _create_row_memmaps(\n    num_rows: int,\n    memmap_dir_path: Path,\n    mode: Mode,\n    dtypes: Dict[FileNames, str],\n) -&gt; np.ndarray:\n    \"\"\"Records a pointer into the data and column arrays.\"\"\"\n    return np.memmap(\n        f\"{str(memmap_dir_path.absolute())}/{FileNames.ROWPTR.value}\",\n        dtype=dtypes[f\"{FileNames.ROWPTR.value}\"],\n        shape=(num_rows + 1,),\n        mode=mode.value,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset._pad_sparse_array","title":"<code>_pad_sparse_array(row_values, row_col_ptr, n_cols)</code>","text":"<p>Creates a conventional array from a sparse one.</p> <p>Convert a sparse matrix representation of a 1d matrix to a conventional numpy representation.</p> <p>Parameters:</p> Name Type Description Default <code>row_values</code> <p>The row indices of the entries</p> required <code>row_col_ptr</code> <p>The corresponding column pointers</p> required <code>n_cols</code> <code>int</code> <p>The number of columns in the dataset.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The full 1d numpy array representation.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def _pad_sparse_array(row_values, row_col_ptr, n_cols: int) -&gt; np.ndarray:\n    \"\"\"Creates a conventional array from a sparse one.\n\n    Convert a sparse matrix representation of a 1d matrix to a conventional\n    numpy representation.\n\n    Args:\n        row_values: The row indices of the entries\n        row_col_ptr: The corresponding column pointers\n        n_cols: The number of columns in the dataset.\n\n    Returns:\n        The full 1d numpy array representation.\n    \"\"\"\n    ret = np.zeros(n_cols)\n    for row_ptr in range(0, len(row_values)):\n        col = row_col_ptr[row_ptr]\n        ret[col] = row_values[row_ptr]\n    return ret\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset._swap_mmap_array","title":"<code>_swap_mmap_array(src_array, src_path, dest_array, dest_path, destroy_src=False)</code>","text":"<p>Function that swaps the location of two mmap arrays.</p> <p>This is used when concatanating SingleCellMemMapDataset. This emables the newly merged arrays to be stored in the same place as the original dataset.</p> <p>Parameters:</p> Name Type Description Default <code>src_array</code> <code>memmap</code> <p>the first memmap array</p> required <code>src_path</code> <code>str</code> <p>location of the first memmap array</p> required <code>dest_array</code> <code>memmap</code> <p>the second memmap array</p> required <code>dest_path</code> <code>str</code> <p>location of the first second array</p> required <code>destroy_src</code> <code>bool</code> <p>set to True if the source array is destroyed</p> <code>False</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def _swap_mmap_array(\n    src_array: np.memmap,\n    src_path: str,\n    dest_array: np.memmap,\n    dest_path: str,\n    destroy_src: bool = False,\n) -&gt; None:\n    \"\"\"Function that swaps the location of two mmap arrays.\n\n    This is used when concatanating SingleCellMemMapDataset. This emables the\n    newly merged arrays to be stored in the same place as the original dataset.\n\n    Args:\n        src_array: the first memmap array\n        src_path: location of the first memmap array\n        dest_array: the second memmap array\n        dest_path: location of the first second array\n        destroy_src: set to True if the source array is destroyed\n\n    Raises:\n        FileNotFoundError if the source or destination path are not found.\n    \"\"\"\n    if not os.path.isfile(src_path):\n        raise FileNotFoundError(f\"The source file {src_path} does not exist\")\n    if not os.path.isfile(dest_path):\n        raise FileNotFoundError(f\"The destination file {dest_path} does not exist\")\n\n    # Flush and close arrays\n    src_array.flush()\n    dest_array.flush()\n\n    del src_array\n    del dest_array\n\n    # Swap the file locations on disk using a tmp file.\n    with tempfile.TemporaryDirectory() as tempdir:\n        temp_file_name = f\"{tempdir}/arr_temp\"\n        shutil.move(src_path, temp_file_name)\n        shutil.move(dest_path, src_path)\n        shutil.move(temp_file_name, dest_path)\n\n    if destroy_src:\n        os.remove(src_path)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/scripts/convert_h5ad_to_scdl/","title":"Convert h5ad to scdl","text":""},{"location":"API_reference/bionemo/scdl/scripts/convert_h5ad_to_scdl/#bionemo.scdl.scripts.convert_h5ad_to_scdl.main","title":"<code>main()</code>","text":"<p>Parse the arguments to process the single cell collection.</p> Source code in <code>bionemo/scdl/scripts/convert_h5ad_to_scdl.py</code> <pre><code>def main():\n    \"\"\"Parse the arguments to process the single cell collection.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--num-workers\", type=int, default=4, help=\"The number of AnnData loaders to run in parallel [4].\"\n    )\n    parser.add_argument(\n        \"--use-mp\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use a subprocess for each worker rather than a lightweight OS thread [False].\",\n    )\n    parser.add_argument(\n        \"--data-path\",\n        type=str,\n        required=True,\n        help=\"A path containing AnnData files. Note: These will all be concatenated.\",\n    )\n    parser.add_argument(\n        \"--save-path\", required=True, type=str, help=\"An output path where an SCDataset will be stored.\"\n    )\n    args = parser.parse_args()\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        coll = SingleCellCollection(temp_dir)\n        coll.load_h5ad_multi(args.data_path, max_workers=args.num_workers, use_processes=args.use_mp)\n        coll.flatten(args.save_path, destroy_on_copy=True)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/","title":"Async worker queue","text":""},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue","title":"<code>AsyncWorkQueue</code>","text":"<p>Implements an asynchronous queue.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>class AsyncWorkQueue:\n    \"\"\"Implements an asynchronous queue.\"\"\"\n\n    def __init__(self, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n        \"\"\"Initialize the AsyncWorkQueue.\n\n        Args:\n            max_workers: The maximum number of worker threads or processes.\n            use_processes: If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.\n        \"\"\"\n        self.use_processes = use_processes\n        if use_processes:\n            self.executor: Union[concurrent.futures.ThreadPoolExecutor, concurrent.futures.ProcessPoolExecutor] = (\n                concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n            )\n        else:\n            self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n        self.lock = threading.Lock()\n        self.tasks: List[concurrent.futures.Future] = []\n\n    def submit_task(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; concurrent.futures.Future:\n        \"\"\"Submit a task to the work queue.\n\n        Args:\n            func: The function to be executed asynchronously.\n            args: Positional arguments to pass to the function.\n            kwargs: Keyword arguments to pass to the function.\n            A Future object representing the execution of the function.\n\n        Returns:\n            Future: placeholder for the asynchronous operation.\n        \"\"\"\n        with self.lock:\n            future = self.executor.submit(func, *args, **kwargs)\n            self.tasks.append(future)\n            return future\n\n    def shutdown(self, wait: bool = True) -&gt; None:\n        \"\"\"Shutdown the executor and wait for the tasks to complete.\n\n        Args:\n            wait: If True, wait for all tasks to complete before shutting down.\n        \"\"\"\n        self.executor.shutdown(wait=wait)\n\n    def get_completed_tasks(self) -&gt; List[concurrent.futures.Future]:\n        \"\"\"Get the list of completed tasks.\n\n        Returns:\n            A list of Future objects that are completed.\n        \"\"\"\n        with self.lock:\n            completed_tasks = [task for task in self.tasks if task.done()]\n            return completed_tasks\n\n    def get_pending_tasks(self) -&gt; List[concurrent.futures.Future]:\n        \"\"\"Get the list of pending tasks.\n\n        Returns:\n            A list of Future objects that are not yet completed.\n        \"\"\"\n        with self.lock:\n            pending_tasks = [task for task in self.tasks if not task.done()]\n            return pending_tasks\n\n    def get_task_results(self) -&gt; List[Any]:\n        \"\"\"Get the results of all completed tasks.\n\n        Returns:\n            A list of results from the completed tasks.\n\n        Raises:\n            Exception: This would be expected if the task fails to complete or\n            if is cancelled.\n        \"\"\"\n        completed_tasks = self.get_completed_tasks()\n        results = []\n        for task in completed_tasks:\n            try:\n                results.append(task.result())\n            except Exception as e:\n                results.append(e)\n        return results\n\n    def wait(self) -&gt; List[Any]:\n        \"\"\"Wait for all submitted tasks to complete and return their results.\n\n        Returns:\n            A list of results from all completed tasks.\n        \"\"\"\n        # Wait for all tasks to complete\n        concurrent.futures.wait(self.tasks)\n\n        # Collect results from all tasks\n        results = []\n        for task in self.tasks:\n            try:\n                results.append(task.result())\n            except Exception as e:\n                results.append(e)\n\n        return results\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.__init__","title":"<code>__init__(max_workers=5, use_processes=False)</code>","text":"<p>Initialize the AsyncWorkQueue.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>The maximum number of worker threads or processes.</p> <code>5</code> <code>use_processes</code> <code>bool</code> <p>If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.</p> <code>False</code> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def __init__(self, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n    \"\"\"Initialize the AsyncWorkQueue.\n\n    Args:\n        max_workers: The maximum number of worker threads or processes.\n        use_processes: If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.\n    \"\"\"\n    self.use_processes = use_processes\n    if use_processes:\n        self.executor: Union[concurrent.futures.ThreadPoolExecutor, concurrent.futures.ProcessPoolExecutor] = (\n            concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n        )\n    else:\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n    self.lock = threading.Lock()\n    self.tasks: List[concurrent.futures.Future] = []\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_completed_tasks","title":"<code>get_completed_tasks()</code>","text":"<p>Get the list of completed tasks.</p> <p>Returns:</p> Type Description <code>List[Future]</code> <p>A list of Future objects that are completed.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_completed_tasks(self) -&gt; List[concurrent.futures.Future]:\n    \"\"\"Get the list of completed tasks.\n\n    Returns:\n        A list of Future objects that are completed.\n    \"\"\"\n    with self.lock:\n        completed_tasks = [task for task in self.tasks if task.done()]\n        return completed_tasks\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_pending_tasks","title":"<code>get_pending_tasks()</code>","text":"<p>Get the list of pending tasks.</p> <p>Returns:</p> Type Description <code>List[Future]</code> <p>A list of Future objects that are not yet completed.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_pending_tasks(self) -&gt; List[concurrent.futures.Future]:\n    \"\"\"Get the list of pending tasks.\n\n    Returns:\n        A list of Future objects that are not yet completed.\n    \"\"\"\n    with self.lock:\n        pending_tasks = [task for task in self.tasks if not task.done()]\n        return pending_tasks\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_task_results","title":"<code>get_task_results()</code>","text":"<p>Get the results of all completed tasks.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>A list of results from the completed tasks.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>This would be expected if the task fails to complete or</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_task_results(self) -&gt; List[Any]:\n    \"\"\"Get the results of all completed tasks.\n\n    Returns:\n        A list of results from the completed tasks.\n\n    Raises:\n        Exception: This would be expected if the task fails to complete or\n        if is cancelled.\n    \"\"\"\n    completed_tasks = self.get_completed_tasks()\n    results = []\n    for task in completed_tasks:\n        try:\n            results.append(task.result())\n        except Exception as e:\n            results.append(e)\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.shutdown","title":"<code>shutdown(wait=True)</code>","text":"<p>Shutdown the executor and wait for the tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>If True, wait for all tasks to complete before shutting down.</p> <code>True</code> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def shutdown(self, wait: bool = True) -&gt; None:\n    \"\"\"Shutdown the executor and wait for the tasks to complete.\n\n    Args:\n        wait: If True, wait for all tasks to complete before shutting down.\n    \"\"\"\n    self.executor.shutdown(wait=wait)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.submit_task","title":"<code>submit_task(func, *args, **kwargs)</code>","text":"<p>Submit a task to the work queue.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to be executed asynchronously.</p> required <code>args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Future</code> <code>Future</code> <p>placeholder for the asynchronous operation.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def submit_task(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; concurrent.futures.Future:\n    \"\"\"Submit a task to the work queue.\n\n    Args:\n        func: The function to be executed asynchronously.\n        args: Positional arguments to pass to the function.\n        kwargs: Keyword arguments to pass to the function.\n        A Future object representing the execution of the function.\n\n    Returns:\n        Future: placeholder for the asynchronous operation.\n    \"\"\"\n    with self.lock:\n        future = self.executor.submit(func, *args, **kwargs)\n        self.tasks.append(future)\n        return future\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.wait","title":"<code>wait()</code>","text":"<p>Wait for all submitted tasks to complete and return their results.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>A list of results from all completed tasks.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def wait(self) -&gt; List[Any]:\n    \"\"\"Wait for all submitted tasks to complete and return their results.\n\n    Returns:\n        A list of results from all completed tasks.\n    \"\"\"\n    # Wait for all tasks to complete\n    concurrent.futures.wait(self.tasks)\n\n    # Collect results from all tasks\n    results = []\n    for task in self.tasks:\n        try:\n            results.append(task.result())\n        except Exception as e:\n            results.append(e)\n\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/torch_dataloader_utils/","title":"Torch dataloader utils","text":""},{"location":"API_reference/bionemo/scdl/util/torch_dataloader_utils/#bionemo.scdl.util.torch_dataloader_utils.collate_sparse_matrix_batch","title":"<code>collate_sparse_matrix_batch(batch)</code>","text":"<p>Collate function to create a batch out of sparse tensors.</p> <p>This is necessary to collate sparse matrices of various lengths.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[Tensor]</code> <p>A list of Tensors to collate into a batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensors collated into a CSR (Compressed Sparse Row) Format.</p> Source code in <code>bionemo/scdl/util/torch_dataloader_utils.py</code> <pre><code>def collate_sparse_matrix_batch(batch: list[torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"Collate function to create a batch out of sparse tensors.\n\n    This is necessary to collate sparse matrices of various lengths.\n\n    Args:\n        batch: A list of Tensors to collate into a batch.\n\n    Returns:\n        The tensors collated into a CSR (Compressed Sparse Row) Format.\n    \"\"\"\n    batch_rows = torch.cumsum(\n        torch.tensor([0] + [sparse_representation.shape[1] for sparse_representation in batch]), dim=0\n    )\n    batch_cols = torch.cat([sparse_representation[1] for sparse_representation in batch]).to(torch.int32)\n    batch_values = torch.cat([sparse_representation[0] for sparse_representation in batch])\n    if len(batch_cols) == 0:\n        max_pointer = 0\n    else:\n        max_pointer = int(batch_cols.max().item() + 1)\n    batch_sparse_tensor = torch.sparse_csr_tensor(batch_rows, batch_cols, batch_values, size=(len(batch), max_pointer))\n    return batch_sparse_tensor\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/","title":"Sampler","text":""},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler","title":"<code>BucketBatchSampler</code>","text":"<p>               Bases: <code>Sampler[List[int]]</code></p> <p>A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.</p> <p>Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements. Then, a base batch sampler is used for each bucket to create mini-batches.</p> <p>The bucket ranges are specified by <code>bucket_boundaries</code>, which will be first sorted internally and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals. e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created with ranges: [0, 5), [5, 10), [10, 16).</p> <p>The base batch sampler will be created by passing the element indices in each bucket as the data source, and <code>base_batch_sampler_shared_kwargs</code> and <code>base_batch_sampler_individual_kwargs</code> to the constructor of the base batch sampler class specified as <code>base_batch_sampler_class</code>. e.g. <code>base_batch_sampler_shared_kwargs = {'drop_last': True}</code> and <code>base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}</code> will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like <code>base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)</code>.</p> <p>In the <code>__iter__</code> method, if <code>shuffle</code> is <code>True</code>, the element indices in each bucket will be shuffled, and a bucket is randomly selected each time to create a mini-batch. If <code>shuffle</code> is <code>False</code>, there is no shuffle on element indices, and the bucket is selected in ascending order of its interval boundaries.</p> <p>This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.</p> <p>Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n&gt;&gt;&gt; # Define the sizes for a dataset\n&gt;&gt;&gt; sizes = torch.arange(25)\n&gt;&gt;&gt; # Define bucket ranges\n&gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n&gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n&gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=False,\n    )\n\n&gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n&gt;&gt;&gt; print(list(batch_sampler))\n[[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n&gt;&gt;&gt; # randomize the dataset and buckets\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(batch_sampler))\n[[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n&gt;&gt;&gt; print(list(batch_sampler))\n[[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n&gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n&gt;&gt;&gt; item_costs = sizes.tolist()\n&gt;&gt;&gt; def cost_of_element(index):\n        return item_costs[index]\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=SizeAwareBatchSampler,\n        base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n        base_batch_sampler_individual_kwargs={},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(iter(batch_sampler)))\n[[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>class BucketBatchSampler(Sampler[List[int]]):\n    \"\"\"A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.\n\n    Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements.\n    Then, a base batch sampler is used for each bucket to create mini-batches.\n\n    The bucket ranges are specified by `bucket_boundaries`, which will be first sorted internally and used to create\n    `len(bucket_boundaries) - 1` left-closed right-open intervals.\n    e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created\n    with ranges: [0, 5), [5, 10), [10, 16).\n\n    The base batch sampler will be created by passing the element indices in each bucket as the data source, and\n    `base_batch_sampler_shared_kwargs` and `base_batch_sampler_individual_kwargs`\n    to the constructor of the base batch sampler class specified as `base_batch_sampler_class`.\n    e.g. `base_batch_sampler_shared_kwargs = {'drop_last': True}` and `base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}`\n    will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like\n    `base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)`.\n\n    In the `__iter__` method, if `shuffle` is `True`, the element indices in each bucket will be shuffled, and a bucket\n    is randomly selected each time to create a mini-batch. If `shuffle` is `False`, there is no shuffle on element indices,\n    and the bucket is selected in ascending order of its interval boundaries.\n\n    This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.\n\n    Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n    &gt;&gt;&gt; # Define the sizes for a dataset\n    &gt;&gt;&gt; sizes = torch.arange(25)\n    &gt;&gt;&gt; # Define bucket ranges\n    &gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n    &gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n    &gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=torch.utils.data.BatchSampler,\n            base_batch_sampler_shared_kwargs={'drop_last': False},\n            base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n            shuffle=False,\n        )\n\n    &gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n    &gt;&gt;&gt; # randomize the dataset and buckets\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=torch.utils.data.BatchSampler,\n            base_batch_sampler_shared_kwargs={'drop_last': False},\n            base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n            shuffle=True,\n            generator=torch.Generator().manual_seed(0),\n        )\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n    &gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n    &gt;&gt;&gt; item_costs = sizes.tolist()\n    &gt;&gt;&gt; def cost_of_element(index):\n            return item_costs[index]\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=SizeAwareBatchSampler,\n            base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n            base_batch_sampler_individual_kwargs={},\n            shuffle=True,\n            generator=torch.Generator().manual_seed(0),\n        )\n    &gt;&gt;&gt; print(list(iter(batch_sampler)))\n    [[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        sizes: torch.Tensor,\n        bucket_boundaries: torch.Tensor,\n        base_batch_sampler_class: Type[S],\n        base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n        base_batch_sampler_individual_kwargs: Optional[Dict[str, Iterable]] = None,\n        shuffle: Optional[bool] = True,\n        generator: Optional[torch.Generator] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the BucketBatchSampler.\n\n        Args:\n            sizes: A 1D tensor of real numbers representing the size of each element in the dataset.\n            bucket_boundaries: A 1D tensor of real numbers representing the boundaries of the bucket ranges.\n                It will be first sorted and used to create `len(bucket_boundaries) - 1` left-closed right-open intervals as bucket ranges.\n                It should not contain any duplicate values.\n            base_batch_sampler_class: Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,\n                `base_batch_sampler_shared_kwargs` and the corresponding `base_batch_sampler_individual_kwargs`.\n            base_batch_sampler_shared_kwargs: Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.\n                Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_individual_kwargs`. Default to  {}.\n            base_batch_sampler_individual_kwargs: Keyword argument dictionary used to initialize\n                each bucket batch sampler with the corresponding key value pairs.\n                Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).\n                Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_shared_kwargs`.\n                Default to  {}.\n            shuffle: A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.\n            generator: Generator used in sampling. Defaults to None.\n\n        Raises:\n            ValueError: If `sizes` is not a 1D tensor of real numbers.\n            ValueError: If `bucket_boundaries` is not a 1D tensor of real numbers.\n            ValueError: If `base_batch_sampler_individual_kwargs` or `base_batch_sampler_individual_kwargs` is not a keyword argument dictionary.\n            ValueError: If the length of values in the dict of `base_batch_sampler_individual_kwargs` must be equal to len(bucket_boundaries) - 1.\n            RuntimeError: If there is no elements with sizes inside the ranges specified by `bucket_boundaries`.\n\n        \"\"\"\n        if not torch.is_tensor(sizes):\n            raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n        if sizes.ndim != 1:\n            raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n        if not torch.is_floating_point(sizes) and sizes.dtype not in TorchIntegerDataTypes:\n            raise ValueError(\n                f\"sizes should contain only integers or floating point numbers, but got sizes.dtype={sizes.dtype}\"\n            )\n\n        if not torch.is_tensor(bucket_boundaries):\n            raise TypeError(\n                f\"bucket_boundaries should be a torch tensor, but got bucket_boundaries={bucket_boundaries}\"\n            )\n\n        if bucket_boundaries.ndim != 1:\n            raise ValueError(\n                f\"bucket_boundaries should be a 2D tensor, but got bucket_boundaries with shape {bucket_boundaries.shape}\"\n            )\n\n        if len(bucket_boundaries) &lt; 2:\n            raise ValueError(\n                f\"bucket_boundaries should have at least 2 numbers, but got bucket_boundaries={bucket_boundaries.shape}\"\n            )\n\n        if not torch.is_floating_point(bucket_boundaries) and bucket_boundaries.dtype not in TorchIntegerDataTypes:\n            raise ValueError(\n                f\"bucket_boundaries should contain only integers or floating point numbers, but got bucket_boundaries.dtype={bucket_boundaries.dtype}\"\n            )\n\n        bucket_boundaries = torch.sort(bucket_boundaries)[0]\n\n        if torch.any(bucket_boundaries[:-1] &gt;= bucket_boundaries[1:]):\n            raise ValueError(\n                f\"bucket_boundaries should not have duplicate values, and should specify the lower endpoint of each interval smaller than the upper endpoint, but got sorted bucket_boundaries={bucket_boundaries}\"\n            )\n\n        if not isinstance(shuffle, bool):\n            raise TypeError(f\"shuffle should be a boolean value, but got shuffle={shuffle}\")\n\n        self.sizes = sizes\n        self.bucket_boundaries = bucket_boundaries\n        self.num_buckets = len(bucket_boundaries) - 1\n        self.shuffle = shuffle\n        self.generator = generator\n        if self.shuffle and self.generator is None:\n            self.generator = torch.Generator().manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n\n        if not issubclass(base_batch_sampler_class, Sampler):\n            raise TypeError(\n                f\"base_batch_sampler_class should be a batch sampler class inherited from torch.utils.data.Sampler, but got base_batch_sampler_class={base_batch_sampler_class}\"\n            )\n\n        base_batch_sampler_shared_kwargs = (\n            {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n        )\n        base_batch_sampler_individual_kwargs = (\n            {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n        )\n        if not isinstance(base_batch_sampler_shared_kwargs, dict):\n            raise TypeError(\n                f\"base_batch_sampler_shared_kwargs should be a dictionary, but got base_batch_sampler_shared_kwargs={base_batch_sampler_shared_kwargs}\"\n            )\n\n        if not all(isinstance(key, str) for key in base_batch_sampler_shared_kwargs.keys()):\n            raise TypeError(\n                f\"base_batch_sampler_shared_kwargs should have string keys, but got keys={list(base_batch_sampler_shared_kwargs.keys())}\"\n            )\n\n        if not isinstance(base_batch_sampler_individual_kwargs, dict):\n            raise TypeError(\n                f\"base_batch_sampler_individual_kwargs should be a dictionary, but got base_batch_sampler_individual_kwargs={base_batch_sampler_individual_kwargs}\"\n            )\n\n        if not all(isinstance(key, str) for key in base_batch_sampler_individual_kwargs.keys()):\n            raise TypeError(\n                f\"base_batch_sampler_individual_kwargs should have string keys, but got keys={list(base_batch_sampler_individual_kwargs.keys())}\"\n            )\n\n        if not all(len(list(value)) == self.num_buckets for value in base_batch_sampler_individual_kwargs.values()):\n            raise ValueError(\n                f\"Each value in base_batch_sampler_individual_kwargs should have a length of {self.num_buckets}, \"\n                f\"but got lengths {[len(list(value)) for value in base_batch_sampler_individual_kwargs.values()]}\"\n            )\n\n        self.base_batch_sampler_class = base_batch_sampler_class\n        self.base_batch_sampler_shared_kwargs = (\n            {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n        )\n        base_batch_sampler_individual_kwargs = (\n            {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n        )\n        self.base_batch_sampler_individual_kwargs = [\n            {key: list(base_batch_sampler_individual_kwargs[key])[k] for key in base_batch_sampler_individual_kwargs}\n            for k in range(self.num_buckets)\n        ]\n\n        self.bucket_sizes: torch.Tensor  # number of elements in each bucket\n        self.bucket_element_indices: List[List[int]]  # List of elements' indices for each bucket\n\n        # bucket index for each element\n        element_bucket_indices = torch.bucketize(sizes, bucket_boundaries, right=True)\n\n        # element indices reordered for each bucket\n        reordered_element_indices = torch.argsort(element_bucket_indices, stable=True)\n\n        # bucket sizes, including the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n        bucket_sizes = torch.bincount(element_bucket_indices, minlength=len(bucket_boundaries) + 1)\n\n        # bucket segments\n        bucket_segments = torch.cumsum(bucket_sizes, dim=0)[:-1]\n\n        self.bucket_element_indices = []\n        # exclude the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n        for bucket_idx in range(self.num_buckets):\n            self.bucket_element_indices.append(\n                reordered_element_indices[bucket_segments[bucket_idx] : bucket_segments[bucket_idx + 1]].tolist()\n            )\n        self.bucket_sizes = bucket_sizes[1 : (self.num_buckets + 1)]\n\n        self.num_samples = torch.sum(self.bucket_sizes).item()\n        if self.num_samples == 0:\n            raise RuntimeError(\"The sizes of all elements in the dataset are outside the bucket ranges provided\")\n        if self.num_samples &lt; len(self.sizes):\n            warnings.warn(\n                f\"{len(self.sizes) - self.num_samples} elements are outside the buckets provided and will be skipped\"\n            )\n\n        self.base_batch_samplers: List[Sampler] = self._init_base_batch_samplers()\n\n    def _init_base_batch_samplers(self) -&gt; list[Sampler[List[int]]]:\n        \"\"\"Initialize batch samplers for each bucket.\n\n        Returns:\n            List of batch samplers.\n        \"\"\"\n        base_batch_samplers = []\n        for k in range(self.num_buckets):\n            base_batch_samplers.append(\n                self.base_batch_sampler_class(\n                    self.bucket_element_indices[k],\n                    **self.base_batch_sampler_shared_kwargs,\n                    **self.base_batch_sampler_individual_kwargs[k],\n                )\n            )\n        return base_batch_samplers\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of batches.\n\n        Can only be called if the `base_batch_sampler_class` has __len__() implemented\n\n        Returns:\n            int: Number of batches\n        \"\"\"\n        num_batches = sum(len(sampler) for sampler in self.base_batch_samplers)  # type: ignore\n        return num_batches\n\n    def __iter__(self) -&gt; Iterator[List[int]]:\n        \"\"\"Iterate over batches of indices.\n\n        This function yields batches of indices of elements with sizes from each bucket range.\n\n        Yields:\n            List[int]: A batch of indices of elements with sizes from each bucket range.\n        \"\"\"\n        if self.shuffle:\n            for indices in self.bucket_element_indices:\n                idx = torch.randperm(len(indices), generator=self.generator)\n                indices[:] = torch.tensor(indices)[idx].tolist()\n\n        base_batch_sampler_iters = [iter(batch_sampler) for batch_sampler in self.base_batch_samplers]\n        bucket_remaining_elements = self.bucket_sizes.clone()\n        total_remaining_elements = self.num_samples\n\n        while total_remaining_elements &gt; 0:\n            if self.shuffle:\n                bucket_idx = torch.multinomial(\n                    bucket_remaining_elements / total_remaining_elements, 1, generator=self.generator\n                )\n            else:\n                bucket_idx = torch.argmax((bucket_remaining_elements &gt; 0).to(int))  # type: ignore\n\n            try:\n                batch = next(base_batch_sampler_iters[bucket_idx])\n                bucket_remaining_elements[bucket_idx] -= len(batch)\n                total_remaining_elements -= len(batch)\n                yield batch\n            except StopIteration:\n                bucket_remaining_elements[bucket_idx] = 0\n                total_remaining_elements = torch.sum(bucket_remaining_elements)\n                continue\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__init__","title":"<code>__init__(sizes, bucket_boundaries, base_batch_sampler_class, base_batch_sampler_shared_kwargs=None, base_batch_sampler_individual_kwargs=None, shuffle=True, generator=None)</code>","text":"<p>Initializes the BucketBatchSampler.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Tensor</code> <p>A 1D tensor of real numbers representing the size of each element in the dataset.</p> required <code>bucket_boundaries</code> <code>Tensor</code> <p>A 1D tensor of real numbers representing the boundaries of the bucket ranges. It will be first sorted and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals as bucket ranges. It should not contain any duplicate values.</p> required <code>base_batch_sampler_class</code> <code>Type[S]</code> <p>Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices, <code>base_batch_sampler_shared_kwargs</code> and the corresponding <code>base_batch_sampler_individual_kwargs</code>.</p> required <code>base_batch_sampler_shared_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Shared keyword argument dictionary used to initialize all base batch samplers for all buckets. Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_individual_kwargs</code>. Default to  {}.</p> <code>None</code> <code>base_batch_sampler_individual_kwargs</code> <code>Optional[Dict[str, Iterable]]</code> <p>Keyword argument dictionary used to initialize each bucket batch sampler with the corresponding key value pairs. Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets). Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_shared_kwargs</code>. Default to  {}.</p> <code>None</code> <code>shuffle</code> <code>Optional[bool]</code> <p>A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.</p> <code>True</code> <code>generator</code> <code>Optional[Generator]</code> <p>Generator used in sampling. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>sizes</code> is not a 1D tensor of real numbers.</p> <code>ValueError</code> <p>If <code>bucket_boundaries</code> is not a 1D tensor of real numbers.</p> <code>ValueError</code> <p>If <code>base_batch_sampler_individual_kwargs</code> or <code>base_batch_sampler_individual_kwargs</code> is not a keyword argument dictionary.</p> <code>ValueError</code> <p>If the length of values in the dict of <code>base_batch_sampler_individual_kwargs</code> must be equal to len(bucket_boundaries) - 1.</p> <code>RuntimeError</code> <p>If there is no elements with sizes inside the ranges specified by <code>bucket_boundaries</code>.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __init__(\n    self,\n    sizes: torch.Tensor,\n    bucket_boundaries: torch.Tensor,\n    base_batch_sampler_class: Type[S],\n    base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n    base_batch_sampler_individual_kwargs: Optional[Dict[str, Iterable]] = None,\n    shuffle: Optional[bool] = True,\n    generator: Optional[torch.Generator] = None,\n) -&gt; None:\n    \"\"\"Initializes the BucketBatchSampler.\n\n    Args:\n        sizes: A 1D tensor of real numbers representing the size of each element in the dataset.\n        bucket_boundaries: A 1D tensor of real numbers representing the boundaries of the bucket ranges.\n            It will be first sorted and used to create `len(bucket_boundaries) - 1` left-closed right-open intervals as bucket ranges.\n            It should not contain any duplicate values.\n        base_batch_sampler_class: Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,\n            `base_batch_sampler_shared_kwargs` and the corresponding `base_batch_sampler_individual_kwargs`.\n        base_batch_sampler_shared_kwargs: Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.\n            Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_individual_kwargs`. Default to  {}.\n        base_batch_sampler_individual_kwargs: Keyword argument dictionary used to initialize\n            each bucket batch sampler with the corresponding key value pairs.\n            Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).\n            Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_shared_kwargs`.\n            Default to  {}.\n        shuffle: A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.\n        generator: Generator used in sampling. Defaults to None.\n\n    Raises:\n        ValueError: If `sizes` is not a 1D tensor of real numbers.\n        ValueError: If `bucket_boundaries` is not a 1D tensor of real numbers.\n        ValueError: If `base_batch_sampler_individual_kwargs` or `base_batch_sampler_individual_kwargs` is not a keyword argument dictionary.\n        ValueError: If the length of values in the dict of `base_batch_sampler_individual_kwargs` must be equal to len(bucket_boundaries) - 1.\n        RuntimeError: If there is no elements with sizes inside the ranges specified by `bucket_boundaries`.\n\n    \"\"\"\n    if not torch.is_tensor(sizes):\n        raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n    if sizes.ndim != 1:\n        raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n    if not torch.is_floating_point(sizes) and sizes.dtype not in TorchIntegerDataTypes:\n        raise ValueError(\n            f\"sizes should contain only integers or floating point numbers, but got sizes.dtype={sizes.dtype}\"\n        )\n\n    if not torch.is_tensor(bucket_boundaries):\n        raise TypeError(\n            f\"bucket_boundaries should be a torch tensor, but got bucket_boundaries={bucket_boundaries}\"\n        )\n\n    if bucket_boundaries.ndim != 1:\n        raise ValueError(\n            f\"bucket_boundaries should be a 2D tensor, but got bucket_boundaries with shape {bucket_boundaries.shape}\"\n        )\n\n    if len(bucket_boundaries) &lt; 2:\n        raise ValueError(\n            f\"bucket_boundaries should have at least 2 numbers, but got bucket_boundaries={bucket_boundaries.shape}\"\n        )\n\n    if not torch.is_floating_point(bucket_boundaries) and bucket_boundaries.dtype not in TorchIntegerDataTypes:\n        raise ValueError(\n            f\"bucket_boundaries should contain only integers or floating point numbers, but got bucket_boundaries.dtype={bucket_boundaries.dtype}\"\n        )\n\n    bucket_boundaries = torch.sort(bucket_boundaries)[0]\n\n    if torch.any(bucket_boundaries[:-1] &gt;= bucket_boundaries[1:]):\n        raise ValueError(\n            f\"bucket_boundaries should not have duplicate values, and should specify the lower endpoint of each interval smaller than the upper endpoint, but got sorted bucket_boundaries={bucket_boundaries}\"\n        )\n\n    if not isinstance(shuffle, bool):\n        raise TypeError(f\"shuffle should be a boolean value, but got shuffle={shuffle}\")\n\n    self.sizes = sizes\n    self.bucket_boundaries = bucket_boundaries\n    self.num_buckets = len(bucket_boundaries) - 1\n    self.shuffle = shuffle\n    self.generator = generator\n    if self.shuffle and self.generator is None:\n        self.generator = torch.Generator().manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n\n    if not issubclass(base_batch_sampler_class, Sampler):\n        raise TypeError(\n            f\"base_batch_sampler_class should be a batch sampler class inherited from torch.utils.data.Sampler, but got base_batch_sampler_class={base_batch_sampler_class}\"\n        )\n\n    base_batch_sampler_shared_kwargs = (\n        {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n    )\n    base_batch_sampler_individual_kwargs = (\n        {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n    )\n    if not isinstance(base_batch_sampler_shared_kwargs, dict):\n        raise TypeError(\n            f\"base_batch_sampler_shared_kwargs should be a dictionary, but got base_batch_sampler_shared_kwargs={base_batch_sampler_shared_kwargs}\"\n        )\n\n    if not all(isinstance(key, str) for key in base_batch_sampler_shared_kwargs.keys()):\n        raise TypeError(\n            f\"base_batch_sampler_shared_kwargs should have string keys, but got keys={list(base_batch_sampler_shared_kwargs.keys())}\"\n        )\n\n    if not isinstance(base_batch_sampler_individual_kwargs, dict):\n        raise TypeError(\n            f\"base_batch_sampler_individual_kwargs should be a dictionary, but got base_batch_sampler_individual_kwargs={base_batch_sampler_individual_kwargs}\"\n        )\n\n    if not all(isinstance(key, str) for key in base_batch_sampler_individual_kwargs.keys()):\n        raise TypeError(\n            f\"base_batch_sampler_individual_kwargs should have string keys, but got keys={list(base_batch_sampler_individual_kwargs.keys())}\"\n        )\n\n    if not all(len(list(value)) == self.num_buckets for value in base_batch_sampler_individual_kwargs.values()):\n        raise ValueError(\n            f\"Each value in base_batch_sampler_individual_kwargs should have a length of {self.num_buckets}, \"\n            f\"but got lengths {[len(list(value)) for value in base_batch_sampler_individual_kwargs.values()]}\"\n        )\n\n    self.base_batch_sampler_class = base_batch_sampler_class\n    self.base_batch_sampler_shared_kwargs = (\n        {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n    )\n    base_batch_sampler_individual_kwargs = (\n        {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n    )\n    self.base_batch_sampler_individual_kwargs = [\n        {key: list(base_batch_sampler_individual_kwargs[key])[k] for key in base_batch_sampler_individual_kwargs}\n        for k in range(self.num_buckets)\n    ]\n\n    self.bucket_sizes: torch.Tensor  # number of elements in each bucket\n    self.bucket_element_indices: List[List[int]]  # List of elements' indices for each bucket\n\n    # bucket index for each element\n    element_bucket_indices = torch.bucketize(sizes, bucket_boundaries, right=True)\n\n    # element indices reordered for each bucket\n    reordered_element_indices = torch.argsort(element_bucket_indices, stable=True)\n\n    # bucket sizes, including the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n    bucket_sizes = torch.bincount(element_bucket_indices, minlength=len(bucket_boundaries) + 1)\n\n    # bucket segments\n    bucket_segments = torch.cumsum(bucket_sizes, dim=0)[:-1]\n\n    self.bucket_element_indices = []\n    # exclude the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n    for bucket_idx in range(self.num_buckets):\n        self.bucket_element_indices.append(\n            reordered_element_indices[bucket_segments[bucket_idx] : bucket_segments[bucket_idx + 1]].tolist()\n        )\n    self.bucket_sizes = bucket_sizes[1 : (self.num_buckets + 1)]\n\n    self.num_samples = torch.sum(self.bucket_sizes).item()\n    if self.num_samples == 0:\n        raise RuntimeError(\"The sizes of all elements in the dataset are outside the bucket ranges provided\")\n    if self.num_samples &lt; len(self.sizes):\n        warnings.warn(\n            f\"{len(self.sizes) - self.num_samples} elements are outside the buckets provided and will be skipped\"\n        )\n\n    self.base_batch_samplers: List[Sampler] = self._init_base_batch_samplers()\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over batches of indices.</p> <p>This function yields batches of indices of elements with sizes from each bucket range.</p> <p>Yields:</p> Type Description <code>List[int]</code> <p>List[int]: A batch of indices of elements with sizes from each bucket range.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __iter__(self) -&gt; Iterator[List[int]]:\n    \"\"\"Iterate over batches of indices.\n\n    This function yields batches of indices of elements with sizes from each bucket range.\n\n    Yields:\n        List[int]: A batch of indices of elements with sizes from each bucket range.\n    \"\"\"\n    if self.shuffle:\n        for indices in self.bucket_element_indices:\n            idx = torch.randperm(len(indices), generator=self.generator)\n            indices[:] = torch.tensor(indices)[idx].tolist()\n\n    base_batch_sampler_iters = [iter(batch_sampler) for batch_sampler in self.base_batch_samplers]\n    bucket_remaining_elements = self.bucket_sizes.clone()\n    total_remaining_elements = self.num_samples\n\n    while total_remaining_elements &gt; 0:\n        if self.shuffle:\n            bucket_idx = torch.multinomial(\n                bucket_remaining_elements / total_remaining_elements, 1, generator=self.generator\n            )\n        else:\n            bucket_idx = torch.argmax((bucket_remaining_elements &gt; 0).to(int))  # type: ignore\n\n        try:\n            batch = next(base_batch_sampler_iters[bucket_idx])\n            bucket_remaining_elements[bucket_idx] -= len(batch)\n            total_remaining_elements -= len(batch)\n            yield batch\n        except StopIteration:\n            bucket_remaining_elements[bucket_idx] = 0\n            total_remaining_elements = torch.sum(bucket_remaining_elements)\n            continue\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of batches.</p> <p>Can only be called if the <code>base_batch_sampler_class</code> has len() implemented</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of batches</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of batches.\n\n    Can only be called if the `base_batch_sampler_class` has __len__() implemented\n\n    Returns:\n        int: Number of batches\n    \"\"\"\n    num_batches = sum(len(sampler) for sampler in self.base_batch_samplers)  # type: ignore\n    return num_batches\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler._init_base_batch_samplers","title":"<code>_init_base_batch_samplers()</code>","text":"<p>Initialize batch samplers for each bucket.</p> <p>Returns:</p> Type Description <code>list[Sampler[List[int]]]</code> <p>List of batch samplers.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def _init_base_batch_samplers(self) -&gt; list[Sampler[List[int]]]:\n    \"\"\"Initialize batch samplers for each bucket.\n\n    Returns:\n        List of batch samplers.\n    \"\"\"\n    base_batch_samplers = []\n    for k in range(self.num_buckets):\n        base_batch_samplers.append(\n            self.base_batch_sampler_class(\n                self.bucket_element_indices[k],\n                **self.base_batch_sampler_shared_kwargs,\n                **self.base_batch_sampler_individual_kwargs[k],\n            )\n        )\n    return base_batch_samplers\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler","title":"<code>SizeAwareBatchSampler</code>","text":"<p>               Bases: <code>Sampler[List[int]]</code></p> <p>Varriying-size batching data sampler class that ensures batch size doesn't exceed maximum.</p> <p>A sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</p> <p>This is useful when dealing with datasets where each element has a different size, such as graphs or sequences of varying lengths. The sampler uses a provided <code>sizeof</code> function to determine the size of each element in the dataset and ensures that the total size of each batch does not exceed the specified <code>max_total_size</code>.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n&gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n&gt;&gt;&gt; def sizeof(index):\n...     return dataset[index].numel()\n\n\n&gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n&gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n...     sampler=torch.utils.data.SequentialSampler(dataset),\n...     sizeof=sizeof,\n...     max_total_size=4\n... )\n\n\n&gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n&gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4]]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>class SizeAwareBatchSampler(Sampler[List[int]]):\n    \"\"\"Varriying-size batching data sampler class that ensures batch size doesn't exceed maximum.\n\n    A sampler that batches elements of varying sizes while ensuring\n    that the total size of each batch does not exceed a specified maximum.\n\n    This is useful when dealing with datasets where each element has a\n    different size, such as graphs or sequences of varying lengths.\n    The sampler uses a provided `sizeof` function to determine the size\n    of each element in the dataset and ensures that the total size of\n    each batch does not exceed the specified `max_total_size`.\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n    &gt;&gt;&gt; # Define a sample dataset with torch.tensor\n    &gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n    ...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n    &gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n    &gt;&gt;&gt; def sizeof(index):\n    ...     return dataset[index].numel()\n\n\n    &gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n    &gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n    ...     sampler=torch.utils.data.SequentialSampler(dataset),\n    ...     sizeof=sizeof,\n    ...     max_total_size=4\n    ... )\n\n\n    &gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n    &gt;&gt;&gt; print(list(batch_sampler))\n        [[0, 1], [2, 3], [4]]\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler: Union[Sampler[List[int]], Iterable[int]],\n        sizeof: Callable[[int], Real],\n        max_total_size: Real,\n        info_logger: Optional[Callable[[str], None]] = None,\n        warn_logger: Optional[Callable[[str], None]] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the SizeAwareBatchSampler.\n\n        Args:\n            sampler: The underlying sampler.\n            sizeof: A function that returns the size at each index. E.g., this can used to\n                determine how much memory an element consumes. Its return type must be\n                comparable with `max_total_size` and it must be addable (operator `+`).\n            max_total_size: The maximum total size of a mini-batch. The semantics of \"size\"\n                is defined by the `sizeof` argument. The type of this value must be comparable\n                with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n            info_logger: A function to log info. Defaults to None.\n            warn_logger: A function to log warnings. Defaults None.\n\n        Raises:\n            TypeError: If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.\n            ValueError: If max_total_size is not a positive number.\n\n        \"\"\"\n        if not (isinstance(sampler, Sampler) or (isinstance(sampler, Iterable) and not isinstance(sampler, str))):\n            raise TypeError(\"sampler should be an instance of torch.utils.data.Sampler or Iterable\")\n\n        if not isinstance(max_total_size, Real):\n            raise ValueError(f\"max_total_size should be int or float but got {type(max_total_size)}\")\n\n        self._info_logger = info_logger\n        self._warn_logger = warn_logger\n\n        self._is_sizeof_callable = callable(sizeof)\n\n        if not self._is_sizeof_callable:\n            raise TypeError(\"sizeof must be a callable\")\n\n        self._sampler = sampler\n        self._sizeof = sizeof\n        self._max_total_size = max_total_size\n\n    def __iter__(self) -&gt; Iterator[List[int]]:\n        \"\"\"Iterate over batches of indices.\n\n        This function yields batches of indices that do not exceed the maximum total size.\n\n        Yields:\n            A batch of indices that do not exceed the maximum total size.\n        \"\"\"\n        return size_aware_batching(\n            self._sampler,\n            self._sizeof,\n            self._max_total_size,\n            collate_fn=None,\n            info_logger=self._info_logger,\n            warn_logger=self._warn_logger,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler.__init__","title":"<code>__init__(sampler, sizeof, max_total_size, info_logger=None, warn_logger=None)</code>","text":"<p>Initializes the SizeAwareBatchSampler.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[Sampler[List[int]], Iterable[int]]</code> <p>The underlying sampler.</p> required <code>sizeof</code> <code>Callable[[int], Real]</code> <p>A function that returns the size at each index. E.g., this can used to determine how much memory an element consumes. Its return type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</p> required <code>max_total_size</code> <code>Real</code> <p>The maximum total size of a mini-batch. The semantics of \"size\" is defined by the <code>sizeof</code> argument. The type of this value must be comparable with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</p> required <code>info_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log info. Defaults to None.</p> <code>None</code> <code>warn_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log warnings. Defaults None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.</p> <code>ValueError</code> <p>If max_total_size is not a positive number.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __init__(\n    self,\n    sampler: Union[Sampler[List[int]], Iterable[int]],\n    sizeof: Callable[[int], Real],\n    max_total_size: Real,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None,\n) -&gt; None:\n    \"\"\"Initializes the SizeAwareBatchSampler.\n\n    Args:\n        sampler: The underlying sampler.\n        sizeof: A function that returns the size at each index. E.g., this can used to\n            determine how much memory an element consumes. Its return type must be\n            comparable with `max_total_size` and it must be addable (operator `+`).\n        max_total_size: The maximum total size of a mini-batch. The semantics of \"size\"\n            is defined by the `sizeof` argument. The type of this value must be comparable\n            with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n        info_logger: A function to log info. Defaults to None.\n        warn_logger: A function to log warnings. Defaults None.\n\n    Raises:\n        TypeError: If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.\n        ValueError: If max_total_size is not a positive number.\n\n    \"\"\"\n    if not (isinstance(sampler, Sampler) or (isinstance(sampler, Iterable) and not isinstance(sampler, str))):\n        raise TypeError(\"sampler should be an instance of torch.utils.data.Sampler or Iterable\")\n\n    if not isinstance(max_total_size, Real):\n        raise ValueError(f\"max_total_size should be int or float but got {type(max_total_size)}\")\n\n    self._info_logger = info_logger\n    self._warn_logger = warn_logger\n\n    self._is_sizeof_callable = callable(sizeof)\n\n    if not self._is_sizeof_callable:\n        raise TypeError(\"sizeof must be a callable\")\n\n    self._sampler = sampler\n    self._sizeof = sizeof\n    self._max_total_size = max_total_size\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over batches of indices.</p> <p>This function yields batches of indices that do not exceed the maximum total size.</p> <p>Yields:</p> Type Description <code>List[int]</code> <p>A batch of indices that do not exceed the maximum total size.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __iter__(self) -&gt; Iterator[List[int]]:\n    \"\"\"Iterate over batches of indices.\n\n    This function yields batches of indices that do not exceed the maximum total size.\n\n    Yields:\n        A batch of indices that do not exceed the maximum total size.\n    \"\"\"\n    return size_aware_batching(\n        self._sampler,\n        self._sizeof,\n        self._max_total_size,\n        collate_fn=None,\n        info_logger=self._info_logger,\n        warn_logger=self._warn_logger,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.size_aware_batching","title":"<code>size_aware_batching(dataset, sizeof, max_total_size, collate_fn=None, info_logger=None, warn_logger=None)</code>","text":"<p>Creates a batching iterator where each batch size varries (within a max limit) according to memory consumption.</p> <p>A generator that batches elements from an iterable while ensuring that the total size of each batch does not exceed a specified maximum. Here the size can be a measurement of memory consumption of the elements in the batch. This can be useful for both indexible data or non-indexible but iterable data.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Iterable[Data]</code> <p>The input iterable.</p> required <code>sizeof</code> <code>Callable[[Data], Real]</code> <p>A function or mapping that returns the \"size\" of each element in <code>dataset</code>. E.g., this can used to determine how much memory an element consumes. Its return type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</p> required <code>max_total_size</code> <code>Real</code> <p>The maximum total \"size\" of each batch. The semantics of \"size\" is defined by the <code>sizeof</code> argument. The type of this value must be comparable with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</p> required <code>collate_fn</code> <code>Optional[Callable[[Iterable[Data]], BatchCollated]]</code> <p>An optional function to collate batches. Defaults to None, in which case each batch is a list of elements from the input dataset</p> <code>None</code> <code>info_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log info. Defaults to None.</p> <code>None</code> <code>warn_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log warnings. Defaults to None.</p> <code>None</code> <p>Yields:</p> Type Description <code>Union[List[Data], BatchCollated]</code> <p>A generator that yields batches from <code>dataset</code>.</p> <p>Assumptions 1. Linear complexity. This function consumes the given Iterable of data (<code>dataset</code>) once,    by going over the data item one by one to build a batch and yield it as soon as the    addition of the next data item to the batch would exceed <code>max_total_size</code> or if the    batch is the last one (end of iteration) 2. Additive size measurement. For the general usage case of building mini-batches with    a threshold of the batch's memory consumption, it assumes that the size of the batch is    the sum of all elements in the batch (additive property). 3. Comparable type of <code>max_total_size</code> and <code>sizeof</code>'s return. <code>sizeof</code>'s return values    must be compared with <code>max_total_size</code> to threshold the size of batches</p> <p>Caveat 1: The generated batch sizes may have large variance    - how to workaround: filter the output of this generator using a batch size threshold 2: The number of batches may vary a lot across different epochs.    - how to workaround: increase the number of steps that compose an epoch,      e.g., in the Lightning training/validation loop, which effectively increases the input      dataset size per epoch</p> <p>Example: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import default_collate\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n&gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n&gt;&gt;&gt; def sizeof(x):\n...     return x.numel()\n\n&gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n&gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n&gt;&gt;&gt; batches = list(gen)\n&gt;&gt;&gt; print(batches)\n    [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def size_aware_batching(\n    dataset: Iterable[Data],\n    sizeof: Callable[[Data], Real],\n    max_total_size: Real,\n    collate_fn: Optional[Callable[[Iterable[Data]], BatchCollated]] = None,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None,\n) -&gt; Iterator[Union[List[Data], BatchCollated]]:\n    \"\"\"Creates a batching iterator where each batch size varries (within a max limit) according to memory consumption.\n\n    A generator that batches elements from an iterable while ensuring that the\n    total size of each batch does not exceed a specified maximum. Here the size\n    can be a measurement of memory consumption of the elements in the batch.\n    This can be useful for both indexible data or non-indexible but iterable data.\n\n    Args:\n        dataset: The input iterable.\n        sizeof: A function or mapping that returns the \"size\" of each element in `dataset`.\n            E.g., this can used to determine how much memory an element consumes. Its return\n            type must be comparable with `max_total_size` and it must be addable (operator `+`).\n        max_total_size: The maximum total \"size\" of each batch. The semantics of \"size\"\n            is defined by the `sizeof` argument. The type of this value must be comparable\n            with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n        collate_fn: An optional function to collate batches. Defaults to None, in which case\n            each batch is a list of elements from the input dataset\n        info_logger: A function to log info. Defaults to None.\n        warn_logger: A function to log warnings. Defaults to None.\n\n    Yields:\n        A generator that yields batches from `dataset`.\n\n    -----------\n    Assumptions\n    1. Linear complexity. This function consumes the given Iterable of data (`dataset`) once,\n       by going over the data item one by one to build a batch and yield it as soon as the\n       addition of the next data item to the batch would exceed `max_total_size` or if the\n       batch is the last one (end of iteration)\n    2. Additive size measurement. For the general usage case of building mini-batches with\n       a threshold of the batch's memory consumption, it assumes that the size of the batch is\n       the sum of all elements in the batch (additive property).\n    3. Comparable type of `max_total_size` and `sizeof`'s return. `sizeof`'s return values\n       must be compared with `max_total_size` to threshold the size of batches\n\n\n    ------\n    Caveat\n    1: The generated batch sizes may have large variance\n       - how to workaround: filter the output of this generator using a batch size threshold\n    2: The number of batches may vary a lot across different epochs.\n       - how to workaround: increase the number of steps that compose an epoch,\n         e.g., in the Lightning training/validation loop, which effectively increases the input\n         dataset size per epoch\n\n\n    -------\n\n    Example:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from torch.utils.data import default_collate\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n    &gt;&gt;&gt; # Define a sample dataset with torch.tensor\n    &gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n    ...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n    &gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n    &gt;&gt;&gt; def sizeof(x):\n    ...     return x.numel()\n\n    &gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n    &gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n    &gt;&gt;&gt; batches = list(gen)\n    &gt;&gt;&gt; print(batches)\n        [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n    ```\n\n    \"\"\"\n    is_sizeof_callable = callable(sizeof)\n    has_collate_fn = collate_fn is not None and callable(collate_fn)\n\n    if not is_sizeof_callable:\n        raise TypeError(\"sizeof must be a callable\")\n\n    batch_total_size = 0\n    batch = []\n    n_samples = 0\n    n_samples_batched = 0\n    n_batches = 0\n    for data in dataset:\n        n_samples += 1\n        try:\n            new_size = sizeof(data)\n        except Exception as e:\n            raise RuntimeError(f\"sizeof raises error at data={data}: {e}\") from e\n        if new_size &gt; max_total_size:\n            if warn_logger is not None:\n                warn_logger(\n                    f\"Size of element {data} exceeds max_total_size\" f\" ({new_size} &gt; {max_total_size}), skipping\"\n                )\n            continue\n        if new_size + batch_total_size &gt; max_total_size:\n            n_batches += 1\n            if has_collate_fn:\n                yield collate_fn(batch)\n            else:\n                yield batch\n            batch_total_size = 0\n            batch = []\n        batch.append(data)\n        n_samples_batched += 1\n        batch_total_size += new_size\n\n    # return the remaining batch if there is\n    if len(batch) &gt; 0:\n        n_batches += 1\n        if has_collate_fn:\n            yield collate_fn(batch)\n        else:\n            yield batch\n\n    if warn_logger is not None and n_samples_batched &lt; n_samples:\n        warn_logger(\n            f\"{n_samples_batched} samples were batched from {n_samples} \"\n            f\"of the input data. Missing samples are due to exceeding max_total_size={max_total_size})\"\n        )\n\n    if info_logger is not None:\n        info_logger(\n            f\"Batched {n_samples_batched} samples into {n_batches} batches. \"\n            f\"If this doesn't match the your expectation, consider adjusting \"\n            f\"max_total_size or the sizeof functor\"\n        )\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.Buckets","title":"<code>Buckets</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A container for storing bucket boundaries and sizes.</p> <p>Attributes:</p> Name Type Description <code>bucket_boundaries</code> <code>Tensor</code> <p>A 1D tensor with the boundaries of all the bucket.</p> <code>bucket_sizes</code> <code>Tensor</code> <p>The number of elements in each bucket.</p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>class Buckets(NamedTuple):\n    \"\"\"A container for storing bucket boundaries and sizes.\n\n    Attributes:\n        bucket_boundaries (torch.Tensor): A 1D tensor with the boundaries of all the bucket.\n        bucket_sizes (torch.Tensor): The number of elements in each bucket.\n    \"\"\"\n\n    bucket_boundaries: torch.Tensor\n    bucket_sizes: torch.Tensor\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.collect_cuda_peak_alloc","title":"<code>collect_cuda_peak_alloc(dataset, work, device, cleanup=None)</code>","text":"<p>Collects CUDA peak memory allocation statistics for a given workflow.</p> <p>This function iterates through the provided dataset, applies the given feature function to each data point, and records the peak CUDA memory allocation during this process. The features extracted from the data points are collected along with their corresponding memory usage statistics.</p> <p>Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Iterable[Data]</code> <p>An iterable containing the input data.</p> required <code>work</code> <code>Callable[[Data], Feature]</code> <p>A function that takes a data point and returns its corresponding feature. This is where the main computation happens and memory allocations are tracked.</p> required <code>device</code> <code>device</code> <p>The target Torch CUDA device.</p> required <code>cleanup</code> <code>Optional[Callable[[], None]]</code> <p>A function that is called after each iteration to perform any necessary cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[Feature], List[int]]</code> <p>A tuple containing the collected features and their corresponding memory usage statistics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided device is not a CUDA device.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n&gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n&gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n&gt;&gt;&gt; dataset, model, optimizer = ...\n&gt;&gt;&gt; # Set the target Torch CUDA device.\n&gt;&gt;&gt; device = torch.device(\"cuda:0\")\n&gt;&gt;&gt; model = model.to(device)\n\n&gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n&gt;&gt;&gt; # do a training step\n&gt;&gt;&gt; def work(data):\n...     # example body of a training loop\n...     optimizer.zero_grad()\n...     output = model(data.to(device))\n...     loss = compute_loss(output)\n...     loss.backward()\n...     optimizer.step()\n...     # extract the feature for later to be modeled or analyzed\n...     return featurize(data)\n\n&gt;&gt;&gt; # can optionally use a cleanup function to release the references\n&gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n&gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n&gt;&gt;&gt; def cleanup():\n...     model.zero_grad(set_to_none=True)\n\n&gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n&gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n...     dataset=batches,\n...     work=work,\n...     device=device,\n...     cleanup=cleanup,\n... )\n\n\n&gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n&gt;&gt;&gt; # that can use these statistics to predict memory usage\n&gt;&gt;&gt; memory_model = ...\n&gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>def collect_cuda_peak_alloc(\n    dataset: Iterable[Data],\n    work: Callable[[Data], Feature],\n    device: torch.device,\n    cleanup: Optional[Callable[[], None]] = None,\n) -&gt; Tuple[List[Feature], List[int]]:\n    \"\"\"Collects CUDA peak memory allocation statistics for a given workflow.\n\n    This function iterates through the provided dataset, applies the given feature function to each data point,\n    and records the peak CUDA memory allocation during this process. The features extracted from the data points\n    are collected along with their corresponding memory usage statistics.\n\n    Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized\n    data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.\n\n    Args:\n        dataset: An iterable containing the input data.\n        work: A function that takes a data point and returns its corresponding feature. This is where\n            the main computation happens and memory allocations are tracked.\n        device: The target Torch CUDA device.\n        cleanup: A function that is called after each iteration to perform any necessary cleanup.\n\n    Returns:\n        A tuple containing the collected features and their corresponding memory usage statistics.\n\n    Raises:\n        ValueError: If the provided device is not a CUDA device.\n\n    -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n    &gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n    &gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n    &gt;&gt;&gt; dataset, model, optimizer = ...\n    &gt;&gt;&gt; # Set the target Torch CUDA device.\n    &gt;&gt;&gt; device = torch.device(\"cuda:0\")\n    &gt;&gt;&gt; model = model.to(device)\n\n    &gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n    &gt;&gt;&gt; # do a training step\n    &gt;&gt;&gt; def work(data):\n    ...     # example body of a training loop\n    ...     optimizer.zero_grad()\n    ...     output = model(data.to(device))\n    ...     loss = compute_loss(output)\n    ...     loss.backward()\n    ...     optimizer.step()\n    ...     # extract the feature for later to be modeled or analyzed\n    ...     return featurize(data)\n\n    &gt;&gt;&gt; # can optionally use a cleanup function to release the references\n    &gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n    &gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n    &gt;&gt;&gt; def cleanup():\n    ...     model.zero_grad(set_to_none=True)\n\n    &gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n    &gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n    ...     dataset=batches,\n    ...     work=work,\n    ...     device=device,\n    ...     cleanup=cleanup,\n    ... )\n\n\n    &gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n    &gt;&gt;&gt; # that can use these statistics to predict memory usage\n    &gt;&gt;&gt; memory_model = ...\n    &gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n    ```\n\n\n    \"\"\"\n    if device.type != \"cuda\":\n        raise ValueError(\"This function is intended for CUDA devices only.\")\n\n    features = []\n    alloc_peaks = []\n\n    for data in dataset:\n        try:\n            torch.cuda.reset_peak_memory_stats(device)\n            feature = work(data)\n            alloc_peak = torch.cuda.memory_stats(device)[\"allocated_bytes.all.peak\"]\n            alloc_peaks.append(alloc_peak)\n            features.append(feature)\n        except torch.cuda.OutOfMemoryError:\n            print(\"Encounter CUDA out-of-memory error. Skipping sample\", file=sys.stderr, flush=True)\n            continue\n        finally:\n            # ensures cleanup is done next round even in case of exception\n            del data\n            if \"feature\" in locals():\n                del feature\n            if cleanup is not None:\n                cleanup()\n            gc.collect()\n            torch.cuda.empty_cache()\n            torch.cuda.reset_peak_memory_stats(device)\n    return features, alloc_peaks\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.create_buckets","title":"<code>create_buckets(sizes, max_width, min_bucket_count)</code>","text":"<p>Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.</p> <p>It will return a named tuple containing the bucket boundaries and the actual bucket sizes. e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Tensor</code> <p>An 1D tensor of integers.</p> required <code>max_width</code> <code>int</code> <p>The maximum width of a bucket, should be a positive integer.</p> required <code>min_bucket_count</code> <code>int</code> <p>The minimum count of a bucket, should be a positive integer. Bucket size may be smaller than min_bucket_count if its width reaches max_width.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided sizes is empty, or not integers.</p> <code>ValueError</code> <p>If max_width is not a positive integer or min_bucket_count is not a positive integer.</p> <p>Returns:</p> Type Description <code>Buckets</code> <p>A namedtuple containing bucket boundaries in ascending order and the number of elements in each bucket.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n&gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n&gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 1,  6, 11, 16, 21, 23])\n\n&gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([12,  0,  0,  0,  4])\n\n&gt;&gt;&gt; sizes = torch.arange(20)\n&gt;&gt;&gt; # min_bucket_count is used to control bucket size\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 0,  5, 10, 15, 20])\n\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([5, 5, 5, 5])\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>def create_buckets(sizes: torch.Tensor, max_width: int, min_bucket_count: int) -&gt; Buckets:\n    \"\"\"Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.\n\n    It will return a named tuple containing the bucket boundaries and the actual bucket sizes.\n    e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements\n    and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.\n\n\n    Args:\n        sizes: An 1D tensor of integers.\n        max_width: The maximum width of a bucket, should be a positive integer.\n        min_bucket_count: The minimum count of a bucket, should be a positive integer.\n            Bucket size may be smaller than min_bucket_count if its width reaches max_width.\n\n    Raises:\n        ValueError: If the provided sizes is empty, or not integers.\n        ValueError: If max_width is not a positive integer or min_bucket_count is not a positive integer.\n\n    Returns:\n        A namedtuple containing bucket boundaries in ascending order and the number of elements in each bucket.\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n    &gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n    &gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n    &gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n    &gt;&gt;&gt; print(buckets.bucket_boundaries)\n    tensor([ 1,  6, 11, 16, 21, 23])\n\n    &gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n    &gt;&gt;&gt; print(buckets.bucket_sizes)\n    tensor([12,  0,  0,  0,  4])\n\n    &gt;&gt;&gt; sizes = torch.arange(20)\n    &gt;&gt;&gt; # min_bucket_count is used to control bucket size\n    &gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n    &gt;&gt;&gt; print(buckets.bucket_boundaries)\n    tensor([ 0,  5, 10, 15, 20])\n\n    &gt;&gt;&gt; print(buckets.bucket_sizes)\n    tensor([5, 5, 5, 5])\n    ```\n\n    \"\"\"\n    if not torch.is_tensor(sizes):\n        raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n    if sizes.ndim != 1:\n        raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n    if sizes.dtype not in TorchIntegerDataTypes:\n        raise ValueError(f\"sizes should contain only integers, but got sizes.dtype={sizes.dtype}\")\n\n    if len(sizes) == 0:\n        raise ValueError(\"sizes should not be empty\")\n\n    if not isinstance(max_width, int) or max_width &lt;= 0:\n        raise ValueError(f\"max_width should be a positive integer but got max_width={max_width}\")\n\n    if not isinstance(min_bucket_count, int) or min_bucket_count &lt;= 0:\n        raise ValueError(f\"min_bucket_count should be a positive integer but got min_bucket_count={min_bucket_count}\")\n\n    unique_values, counts = torch.unique(sizes, return_counts=True, sorted=True)\n\n    bucket_boundaries = [unique_values[0]]\n    bucket_sizes = []\n    start = 0\n    end = 0\n    upper_bound = unique_values[0] + 1\n    bucket_count = 0\n\n    while start &lt; len(unique_values):\n        while (\n            end &lt; len(unique_values)\n            and bucket_count &lt; min_bucket_count\n            and unique_values[end] - bucket_boundaries[-1] &lt; max_width\n        ):\n            bucket_count += counts[end]\n            end += 1\n\n        bucket_sizes.append(sum(counts[start:end]))\n        if end == len(unique_values):\n            upper_bound = unique_values[-1] + 1\n        else:\n            upper_bound = unique_values[end]\n\n        # Adjust the end of the range to ensure that no width exceeds 'max_width'\n        n_empty_buckets = (upper_bound - bucket_boundaries[-1]) // max_width\n        if n_empty_buckets &gt; 0:\n            bucket_boundaries.extend(\n                list(\n                    range(\n                        bucket_boundaries[-1] + max_width,\n                        bucket_boundaries[-1] + max_width * (n_empty_buckets + 1),\n                        max_width,\n                    )\n                )\n            )\n            bucket_sizes.extend([0] * (n_empty_buckets - 1))\n        else:\n            bucket_boundaries.append(upper_bound)\n\n        start = end\n        end = start + 1\n        # index start may be out of bounds\n        bucket_count = counts[start:end].sum()\n\n    bucket_boundaries = torch.tensor(bucket_boundaries)\n    bucket_sizes = torch.tensor(bucket_sizes)\n\n    return Buckets(bucket_boundaries, bucket_sizes)\n</code></pre>"},{"location":"API_reference/bionemo/testing/callbacks/","title":"Callbacks","text":""},{"location":"API_reference/bionemo/testing/lightning/","title":"Lightning","text":""},{"location":"API_reference/bionemo/testing/lightning/#bionemo.testing.lightning.get_random_microbatch","title":"<code>get_random_microbatch(microbatch_size, max_sequence_length, vocab_size, seed, mask_index=MLM_LOSS_IGNORE_INDEX)</code>","text":"<p>Generate random microbatches for testing.</p> <p>Note that this follows the convention that token_logits are s,b, while other fields are b,s.</p> Source code in <code>bionemo/testing/lightning.py</code> <pre><code>def get_random_microbatch(\n    microbatch_size: int,\n    max_sequence_length: int,\n    vocab_size: int,\n    seed: int,\n    mask_index: int = MLM_LOSS_IGNORE_INDEX,\n) -&gt; Dict[str, Dict[str, torch.Tensor]]:\n    \"\"\"Generate random microbatches for testing.\n\n    Note that this follows the convention that token_logits are s,b, while other fields are b,s.\n    \"\"\"\n    generator = torch.Generator(device=torch.cuda.current_device()).manual_seed(seed)\n    labels = torch.randint(\n        low=0,\n        high=vocab_size,\n        size=(microbatch_size, max_sequence_length),\n        generator=generator,\n        device=torch.cuda.current_device(),\n    )  # [b s]\n    loss_mask = torch.randint(\n        low=1,\n        high=1 + 1,\n        size=(microbatch_size, max_sequence_length),\n        dtype=torch.long,\n        device=torch.cuda.current_device(),\n        generator=generator,\n    )  # [b s]\n    token_logits = torch.rand(\n        max_sequence_length, microbatch_size, vocab_size, device=torch.cuda.current_device(), generator=generator\n    )  # [s b v]\n    labels[loss_mask == 0] = mask_index  # propagate masking to labels\n    microbatch_output = {\n        \"batch\": {\"labels\": labels, \"loss_mask\": loss_mask},\n        \"forward_out\": {\"token_logits\": token_logits},\n    }\n    return microbatch_output\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/","title":"Megatron dataset compatibility","text":""},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.DatasetDistributedNondeterministic","title":"<code>DatasetDistributedNondeterministic</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Datasets are not locally deterministic.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>class DatasetDistributedNondeterministic(AssertionError):\n    \"\"\"Datasets are not locally deterministic.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.DatasetLocallyNondeterministic","title":"<code>DatasetLocallyNondeterministic</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Datasets are not locally deterministic.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>class DatasetLocallyNondeterministic(AssertionError):\n    \"\"\"Datasets are not locally deterministic.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dataset_compatible_with_megatron","title":"<code>assert_dataset_compatible_with_megatron(dataset, index=0, assert_elements_equal=assert_dict_tensors_approx_equal)</code>","text":"<p>Make sure that a dataset passes some basic sanity checks for megatron determinism constraints.</p> Constraints tested <ul> <li>dataset[i] returns the same element regardless of device</li> <li>dataset[i] doesn't make calls to known problematic randomization procedures (currently <code>torch.manual_seed</code>).</li> </ul> <p>As more constraints are discovered, they should be added to this test.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dataset_compatible_with_megatron(\n    dataset: torch.utils.data.Dataset[TensorCollectionOrTensor],\n    index: Index = 0,\n    assert_elements_equal: Callable[\n        [TensorCollectionOrTensor, TensorCollectionOrTensor], None\n    ] = assert_dict_tensors_approx_equal,\n):\n    \"\"\"Make sure that a dataset passes some basic sanity checks for megatron determinism constraints.\n\n    Constraints tested:\n        * dataset[i] returns the same element regardless of device\n        * dataset[i] doesn't make calls to known problematic randomization procedures (currently `torch.manual_seed`).\n\n    As more constraints are discovered, they should be added to this test.\n    \"\"\"\n    # 1. Make sure the dataset is deterministic when you ask for the same elements.\n    n_elements = len(dataset)  # type: ignore\n    assert n_elements &gt; 0, \"Need one element or more to test\"\n    try:\n        assert_elements_equal(dataset[index], dataset[index])\n    except AssertionError as e_0:\n        raise DatasetLocallyNondeterministic(e_0)\n    with (\n        patch(\"torch.manual_seed\") as mock_manual_seed,\n        patch(\"torch.cuda.manual_seed\") as mock_cuda_manual_seed,\n        patch(\"torch.cuda.manual_seed_all\") as mock_cuda_manual_seed_all,\n    ):\n        _ = dataset[index]\n    if mock_manual_seed.call_count &gt; 0 or mock_cuda_manual_seed.call_count &gt; 0 or mock_cuda_manual_seed_all.call_count:\n        raise DatasetDistributedNondeterministic(\n            \"You cannot safely use torch.manual_seed in a cluster with model parallelism. Use torch.Generator directly.\"\n            \" See https://github.com/NVIDIA/Megatron-LM/blob/dddecd19/megatron/core/tensor_parallel/random.py#L198-L199\"\n        )\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dataset_elements_not_equal","title":"<code>assert_dataset_elements_not_equal(dataset, index_a=0, index_b=1, assert_elements_equal=assert_dict_tensors_approx_equal)</code>","text":"<p>Test the case where two indices return different elements on datasets that employ randomness, like masking.</p> <p>NOTE: if you have a dataset without any kinds of randomness, just use the <code>assert_dataset_compatible_with_megatron</code> test and skip this one. This test is for the case when you want to test that a dataset that applies a random transform to your elements as a function of index actually does so with two different indices that map to the same underlying object. This test also runs <code>assert_dataset_compatible_with_megatron</code> behind the scenes so if you do this you do not need to also do the other.</p> <p>With epoch upsampling approaches, some underlying index, say index=0, will be called multiple times by some wrapping dataset object. For example if you have a dataset of length 1, and you wrap it in an up-sampler that maps it to length 2 by mapping index 0 to 0 and 1 to 0, then in that wrapper we apply randomness to the result and we expect different masks to be used for each call, even though the underlying object is the same. Again this test only applies to a dataset that employs randomness. Another approach some of our datasets take is to use a special index that captures both the underlying index, and the epoch index. This tuple of indices is used internally to seed the mask. If that kind of dataset is used, then index_a could be (epoch=0, idx=0) and index_b could be (epoch=1, idx=0), for example. We expect those to return different random features.</p> <p>The idea for using this test effectively is to identify cases where you have two indices that return the same underlying object, but where you expect different randomization to be applied to each by the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[TensorCollectionOrTensor]</code> <p>dataset object with randomness (eg masking) to test.</p> required <code>index_a</code> <code>Index</code> <p>index for some element. Defaults to 0.</p> <code>0</code> <code>index_b</code> <code>Index</code> <p>index for a different element. Defaults to 1.</p> <code>1</code> <code>assert_elements_equal</code> <code>Callable[[TensorCollectionOrTensor, TensorCollectionOrTensor], None]</code> <p>Function to compare two returned batch elements. Defaults to <code>assert_dict_tensors_approx_equal</code> which works for both tensors and dictionaries of tensors.</p> <code>assert_dict_tensors_approx_equal</code> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dataset_elements_not_equal(\n    dataset: torch.utils.data.Dataset[TensorCollectionOrTensor],\n    index_a: Index = 0,\n    index_b: Index = 1,\n    assert_elements_equal: Callable[\n        [TensorCollectionOrTensor, TensorCollectionOrTensor], None\n    ] = assert_dict_tensors_approx_equal,\n):\n    \"\"\"Test the case where two indices return different elements on datasets that employ randomness, like masking.\n\n    NOTE: if you have a dataset without any kinds of randomness, just use the `assert_dataset_compatible_with_megatron`\n    test and skip this one. This test is for the case when you want to test that a dataset that applies a random\n    transform to your elements as a function of index actually does so with two different indices that map to the same\n    underlying object. This test also runs `assert_dataset_compatible_with_megatron` behind the scenes so if you\n    do this you do not need to also do the other.\n\n    With epoch upsampling approaches, some underlying index, say index=0, will be called multiple times by some wrapping\n    dataset object. For example if you have a dataset of length 1, and you wrap it in an up-sampler that maps it to\n    length 2 by mapping index 0 to 0 and 1 to 0, then in that wrapper we apply randomness to the result and we expect\n    different masks to be used for each call, even though the underlying object is the same. Again this test only\n    applies to a dataset that employs randomness. Another approach some of our datasets take is to use a special index\n    that captures both the underlying index, and the epoch index. This tuple of indices is used internally to seed the\n    mask. If that kind of dataset is used, then index_a could be (epoch=0, idx=0) and index_b could be (epoch=1, idx=0),\n    for example. We expect those to return different random features.\n\n    The idea for using this test effectively is to identify cases where you have two indices that return the same\n    underlying object, but where you expect different randomization to be applied to each by the dataset.\n\n    Args:\n        dataset: dataset object with randomness (eg masking) to test.\n        index_a: index for some element. Defaults to 0.\n        index_b: index for a different element. Defaults to 1.\n        assert_elements_equal: Function to compare two returned batch elements. Defaults to\n            `assert_dict_tensors_approx_equal` which works for both tensors and dictionaries of tensors.\n    \"\"\"\n    # 0, first sanity check for determinism/compatibility on idx0 and idx1\n    assert_dataset_compatible_with_megatron(dataset, index=index_a, assert_elements_equal=assert_elements_equal)\n    assert_dataset_compatible_with_megatron(dataset, index=index_b, assert_elements_equal=assert_elements_equal)\n    # 1, now check that index_a != index_b\n    with pytest.raises(AssertionError):\n        assert_elements_equal(dataset[index_a], dataset[index_b])\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dict_tensors_approx_equal","title":"<code>assert_dict_tensors_approx_equal(actual, expected)</code>","text":"<p>Assert that two tensors are equal.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dict_tensors_approx_equal(actual: TensorCollectionOrTensor, expected: TensorCollectionOrTensor) -&gt; None:\n    \"\"\"Assert that two tensors are equal.\"\"\"\n    if isinstance(actual, dict) and isinstance(expected, dict):\n        a_keys, b_keys = actual.keys(), expected.keys()\n        assert a_keys == b_keys\n        for key in a_keys:\n            torch.testing.assert_close(actual=actual[key], expected=expected[key])\n    else:\n        torch.testing.assert_close(actual=actual, expected=expected)\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/","title":"Megatron parallel state utils","text":"<p>This package contains utilities for managing the state of distributed model parallelism in Megatron and Apex.</p> <p>In general you should just use the context manager <code>distributed_model_parallel_state</code> to manage the state of your test. This context manager will handle the setup and teardown of the distributed model parallel state for you.</p> <p>Example usage: <pre><code>from bionemo.testing import megatron_parallel_state_utils\n\ndef my_test():\n    with megatron_parallel_state_utils.distributed_model_parallel_state():\n        # your test code that requires megatron/apex parallel state to be set up here\n</code></pre></p>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils._MockMegatronParallelStateSingleton","title":"<code>_MockMegatronParallelStateSingleton</code>","text":"Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>class _MockMegatronParallelStateSingleton:\n    _instance = None\n\n    def __init__(\n        self,\n        world_size=torch.cuda.device_count(),\n        rank=int(os.getenv(\"LOCAL_RANK\", 0)),\n        inited=False,\n        store=FakeStore(),\n    ):\n        \"\"\"A singleton to deal with global megatron state for simulating a fake cluster.\n\n        Args:\n            world_size: the cluster size. Defaults to torch.cuda.device_count().\n            rank: rank of this node. Defaults to int(os.getenv(\"LOCAL_RANK\", 0)).\n            inited: if this global cluster has been initiated. Defaults to False.\n            store: the FakeStore for process groups. Defaults to FakeStore().\n        \"\"\"\n        self.world_size = world_size\n        self.rank = rank\n        self.inited = inited\n        # Fake store idea: see https://github.com/pytorch/pytorch/blob/main/test/distributed/test_fake_pg.py\n        self.store = store\n\n    def __new__(cls):\n        # Makes this a singleton\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n    def initialize_distributed(self):\n        torch.cuda.set_device(self.rank % self.world_size)\n        # Fake store idea: see https://github.com/pytorch/pytorch/blob/main/test/distributed/test_fake_pg.py\n        torch.distributed.init_process_group(\n            backend=\"fake\",\n            world_size=self.world_size,\n            rank=self.rank,\n            store=self.store,\n        )\n        self.inited = True\n\n    def set_world_size(self, world_size=None, rank=None):\n        self.world_size = torch.cuda.device_count() if world_size is None else world_size\n        if torch.distributed.is_initialized() and self.world_size != torch.distributed.get_world_size():\n            torch.distributed.destroy_process_group()\n\n        if rank is None:\n            self.rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n            if self.rank &gt;= self.world_size:\n                self.rank = -1\n        else:\n            self.rank = rank\n\n    def destroy_model_parallel(self):\n        if not self.inited:\n            return\n        # torch.distributed.barrier()\n        parallel_state.destroy_model_parallel()\n        self.inited = False\n        torch.distributed.destroy_process_group()\n\n    def initialize_model_parallel(\n        self,\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        virtual_pipeline_model_parallel_size=None,\n        **kwargs,\n    ):\n        parallel_state.destroy_model_parallel()\n        self.initialize_distributed()\n        parallel_state.initialize_model_parallel(\n            tensor_model_parallel_size,\n            pipeline_model_parallel_size,\n            virtual_pipeline_model_parallel_size,\n            **kwargs,\n        )\n        self.inited = True\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils._MockMegatronParallelStateSingleton.__init__","title":"<code>__init__(world_size=torch.cuda.device_count(), rank=int(os.getenv('LOCAL_RANK', 0)), inited=False, store=FakeStore())</code>","text":"<p>A singleton to deal with global megatron state for simulating a fake cluster.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <p>the cluster size. Defaults to torch.cuda.device_count().</p> <code>device_count()</code> <code>rank</code> <p>rank of this node. Defaults to int(os.getenv(\"LOCAL_RANK\", 0)).</p> <code>int(getenv('LOCAL_RANK', 0))</code> <code>inited</code> <p>if this global cluster has been initiated. Defaults to False.</p> <code>False</code> <code>store</code> <p>the FakeStore for process groups. Defaults to FakeStore().</p> <code>FakeStore()</code> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>def __init__(\n    self,\n    world_size=torch.cuda.device_count(),\n    rank=int(os.getenv(\"LOCAL_RANK\", 0)),\n    inited=False,\n    store=FakeStore(),\n):\n    \"\"\"A singleton to deal with global megatron state for simulating a fake cluster.\n\n    Args:\n        world_size: the cluster size. Defaults to torch.cuda.device_count().\n        rank: rank of this node. Defaults to int(os.getenv(\"LOCAL_RANK\", 0)).\n        inited: if this global cluster has been initiated. Defaults to False.\n        store: the FakeStore for process groups. Defaults to FakeStore().\n    \"\"\"\n    self.world_size = world_size\n    self.rank = rank\n    self.inited = inited\n    # Fake store idea: see https://github.com/pytorch/pytorch/blob/main/test/distributed/test_fake_pg.py\n    self.store = store\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils._reset_microbatch_calculator","title":"<code>_reset_microbatch_calculator()</code>","text":"<p>Resets _GLOBAL_NUM_MICROBATCHES_CALCULATOR in megatron which is used in NeMo to initilised model parallel in nemo.collections.nlp.modules.common.megatron.megatron_init.initialize_model_parallel_for_nemo</p> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>def _reset_microbatch_calculator():\n    \"\"\"Resets _GLOBAL_NUM_MICROBATCHES_CALCULATOR in megatron which is used in NeMo to initilised model parallel in\n    nemo.collections.nlp.modules.common.megatron.megatron_init.initialize_model_parallel_for_nemo\n    \"\"\"  # noqa: D205, D415\n    megatron.core.num_microbatches_calculator._GLOBAL_NUM_MICROBATCHES_CALCULATOR = None\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.clean_parallel_state_context","title":"<code>clean_parallel_state_context()</code>","text":"<p>Puts you into a clean parallel state, and again tears it down at the end.</p> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef clean_parallel_state_context():\n    \"\"\"Puts you into a clean parallel state, and again tears it down at the end.\"\"\"\n    try:\n        clean_up_distributed_and_parallel_states()\n        yield\n    except Exception as e:\n        # TODO (@skothenhill) verify this is a problem and that this is a solution. Had issues with keyboard interrupts being ignored inside context manager.\n        raise Exception from e\n    finally:\n        clean_up_distributed_and_parallel_states()\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.clean_up_distributed_and_parallel_states","title":"<code>clean_up_distributed_and_parallel_states()</code>","text":"<p>Clean up parallel states, torch.distributed and torch cuda cache.</p> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>def clean_up_distributed_and_parallel_states():\n    \"\"\"Clean up parallel states, torch.distributed and torch cuda cache.\"\"\"\n    _reset_microbatch_calculator()\n    parallel_state.destroy_model_parallel()  # destroy parallel state before distributed\n    if torch.distributed.is_initialized():\n        torch.distributed.destroy_process_group()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.distributed_model_parallel_state","title":"<code>distributed_model_parallel_state(seed=42, rank=0, world_size=1, backend='nccl', **initialize_model_parallel_kwargs)</code>","text":"<p>Context manager for torch distributed and parallel state testing.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>random seed to be passed into tensor_parallel.random (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/random.py). default to 42.</p> <code>42</code> <code>rank</code> <code>int</code> <p>global rank of the current cuda device. default to 0.</p> <code>0</code> <code>world_size</code> <code>int</code> <p>world size or number of devices. default to 1.</p> <code>1</code> <code>backend</code> <code>str</code> <p>backend to torch.distributed.init_process_group. default to 'nccl'.</p> <code>'nccl'</code> <code>**initialize_model_parallel_kwargs</code> <p>kwargs to be passed into initialize_model_parallel (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py).</p> <code>{}</code> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef distributed_model_parallel_state(\n    seed: int = 42,\n    rank: int = 0,\n    world_size: int = 1,\n    backend: str = \"nccl\",\n    **initialize_model_parallel_kwargs,\n):\n    \"\"\"Context manager for torch distributed and parallel state testing.\n\n    Args:\n        seed (int): random seed to be passed into tensor_parallel.random (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/tensor_parallel/random.py). default to 42.\n        rank (int): global rank of the current cuda device. default to 0.\n        world_size (int): world size or number of devices. default to 1.\n        backend (str): backend to torch.distributed.init_process_group. default to 'nccl'.\n        **initialize_model_parallel_kwargs: kwargs to be passed into initialize_model_parallel (https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/parallel_state.py).\n    \"\"\"\n    with MonkeyPatch.context() as context:\n        initial_states = None\n        try:\n            clean_up_distributed_and_parallel_states()\n\n            # distributed and parallel state set up\n            if not os.environ.get(\"MASTER_ADDR\", None):\n                context.setenv(\"MASTER_ADDR\", DEFAULT_MASTER_ADDR)\n            if not os.environ.get(\"MASTER_PORT\", None):\n                context.setenv(\"MASTER_PORT\", DEFAULT_MASTER_PORT)\n            if not os.environ.get(\"NCCL_TIMEOUT\", None):\n                context.setenv(\"NCCL_TIMEOUT\", DEFAULT_NCCL_TIMEOUT)\n            context.setenv(\"RANK\", str(rank))\n\n            torch.distributed.init_process_group(backend=backend, world_size=world_size)\n            parallel_state.initialize_model_parallel(**initialize_model_parallel_kwargs)\n\n            # tensor parallel random seed set up\n            # do not call torch.cuda.manual_seed after so!\n            if tp_random.get_cuda_rng_tracker().is_initialized():\n                initial_states = tp_random.get_cuda_rng_tracker().get_states()\n            if seed is not None:\n                tp_random.model_parallel_cuda_manual_seed(seed)\n\n            yield\n        finally:\n            # restore/unset tensor parallel random seed\n            if initial_states is not None:\n                tp_random.get_cuda_rng_tracker().set_states(initial_states)\n            else:\n                # Reset to the unset state\n                tp_random.get_cuda_rng_tracker().reset()\n\n            clean_up_distributed_and_parallel_states()\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.mock_distributed_parallel_state","title":"<code>mock_distributed_parallel_state(world_size=8, rank=0, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, context_parallel_size=1, expert_model_parallel_size=1, seed=42)</code>","text":"<p>A context manager that facilitates easy mocking of torch.distributed for an arbitrary GPU in a simulated cluster.</p> Key functions that are mocked <ul> <li><code>torch.distributed.new_group</code> when <code>backend=\"gloo\"</code> which doesn't support a <code>backend=\"fake\"</code></li> <li><code>torch.distributed.destroy_process_group</code> when <code>backend=\"gloo\"</code> since new \"gloo\" groups are not actually made</li> <li><code>torch._C._cuda_setDevice</code> which changes the current device behind the scenes. We assign devices round-robin     to support <code>world_size &gt; torch.cuda.device_count()</code>.</li> </ul> <p>Outside of this mocking, a fake cluster is initialized using <code>backend=\"fake\"</code> in <code>torch.distributed</code>. This sets up     enough global state and environment for megatron to think that it is initializing a larger cluster with some     settings where the current context has some user defined rank. You can then test the megatron state on a     hypothetical rank in some large world size.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <code>int</code> <p>The world size (cluster size). Defaults to 8.</p> <code>8</code> <code>rank</code> <code>int</code> <p>the GPU number globally in the cluster. Defaults to 0.</p> <code>0</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>tensor model parallel setting for megatron. Defaults to 1.</p> <code>1</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>pipeline model parallel setting for megatron. Defaults to 1.</p> <code>1</code> <code>virtual_pipeline_model_parallel_size</code> <code>Optional[int]</code> <p>virtual pipeline model parallel size for megatron. Defaults to None.</p> <code>None</code> <code>context_parallel_size</code> <code>int</code> <p>context parallel size. Defaults to 1.</p> <code>1</code> <code>expert_model_parallel_size</code> <code>int</code> <p>expert model parallel size. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int | None</code> <p>seed for RNG state. Defaults to 42.</p> <code>42</code> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef mock_distributed_parallel_state(\n    world_size: int = 8,\n    rank: int = 0,\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    virtual_pipeline_model_parallel_size: Optional[int] = None,\n    context_parallel_size: int = 1,\n    expert_model_parallel_size: int = 1,\n    seed: int | None = 42,\n):\n    \"\"\"A context manager that facilitates easy mocking of torch.distributed for an arbitrary GPU in a simulated cluster.\n\n    Key functions that are mocked:\n        * `torch.distributed.new_group` when `backend=\"gloo\"` which doesn't support a `backend=\"fake\"`\n        * `torch.distributed.destroy_process_group` when `backend=\"gloo\"` since new \"gloo\" groups are not actually made\n        * `torch._C._cuda_setDevice` which changes the current device behind the scenes. We assign devices round-robin\n            to support `world_size &gt; torch.cuda.device_count()`.\n\n    Outside of this mocking, a fake cluster is initialized using `backend=\"fake\"` in `torch.distributed`. This sets up\n        enough global state and environment for megatron to think that it is initializing a larger cluster with some\n        settings where the current context has some user defined rank. You can then test the megatron state on a\n        hypothetical rank in some large world size.\n\n    Args:\n        world_size: The world size (cluster size). Defaults to 8.\n        rank: the GPU number globally in the cluster. Defaults to 0.\n        tensor_model_parallel_size: tensor model parallel setting for megatron. Defaults to 1.\n        pipeline_model_parallel_size: pipeline model parallel setting for megatron. Defaults to 1.\n        virtual_pipeline_model_parallel_size: virtual pipeline model parallel size for megatron. Defaults to None.\n        context_parallel_size: context parallel size. Defaults to 1.\n        expert_model_parallel_size: expert model parallel size. Defaults to 1.\n        seed: seed for RNG state. Defaults to 42.\n    \"\"\"\n    # First set up mocks for torch.distributed state/info\n    ori_device_count = torch.cuda.device_count()\n    # Conditionally mock torch.distributed.new_group based on backend argument\n    ori_dist_new_group = torch.distributed.new_group\n\n    def mock_new_group(*args, **kwargs):\n        if kwargs.get(\"backend\") == \"gloo\":\n            # Return a specific mock if backend is 'gloo'\n            return MagicMock(name=\"gloo_group\")\n        else:\n            # Return another mock or a different behavior for other backends\n            return ori_dist_new_group(*args, **kwargs)\n\n    ori_destroy_pg = torch.distributed.destroy_process_group\n\n    def mock_destroy_gloo_group(pg=None):\n        if isinstance(pg, MagicMock):\n            return None\n        ori_destroy_pg(pg)\n\n    # The next mock is required to \"set the device\" to one that is greater than the number of actual GPUs\n    #  the consequence of this mock is that the device is always dev 0\n    ori_set_device = torch._C._cuda_setDevice\n\n    def mock_set_device(device):\n        if ori_device_count &gt; 0:\n            ori_set_device(device % ori_device_count)  # wrap around the request\n\n    with (\n        mock.patch(\"torch.distributed.new_group\", side_effect=mock_new_group),\n        mock.patch(\"torch.distributed.destroy_process_group\", side_effect=mock_destroy_gloo_group),\n        mock.patch(\"torch._C._cuda_setDevice\", side_effect=mock_set_device),\n    ):\n        # Next set up state etc\n        state_util = _MockMegatronParallelStateSingleton()  # static singleton class\n        state_util.world_size = world_size\n        state_util.rank = rank\n        initial_states: Optional[Any] = None\n        try:\n            state_util.set_world_size(world_size=world_size, rank=rank)\n            state_util.initialize_model_parallel(\n                tensor_model_parallel_size=tensor_model_parallel_size,\n                pipeline_model_parallel_size=pipeline_model_parallel_size,\n                virtual_pipeline_model_parallel_size=virtual_pipeline_model_parallel_size,\n                context_parallel_size=context_parallel_size,\n                expert_model_parallel_size=expert_model_parallel_size,\n            )\n            # Our goal is to set required state on entry, and then restore current state on exit for the RNGs.\n            #  there are two possibilities that are handled below:\n            # 1. If the RNG state is not initialized, we need to set it up and then\n            #     unset it on exit to restore the current state. We track that this is the case when `initial_states` is `None`.\n            # 2. If the RNG state is initialized, we need to track this state and reset it on exit to be what it was on entry.\n            #    We track that this is the case when `initial_states` is not `None`.\n            if tp_random.get_cuda_rng_tracker().is_initialized():\n                initial_states = tp_random.get_cuda_rng_tracker().get_states()\n            if seed is not None:\n                # Set the seed if provided, this case is valid whether or not the RNG had state previously.\n                #  on exit the RNG state will be restored to what it was on entry.\n                tp_random.model_parallel_cuda_manual_seed(seed)\n            else:\n                # This is the case where the RNG state is not initialized and no seed was provided.\n                #  We need to raise an error in this case, as we cannot restore the RNG state on exit and we need a seed\n                #  to initialize the RNG state to. This only happens if the user overrides the default seed and sets it\n                #  to None, and additionally if the RNG state was not initialized externally, as there is a default seed of 42.\n                if initial_states is None:\n                    raise ValueError(\n                        \"You must provide a seed if the initial parallel state is unset. \"\n                        \"Either provide a seed or leave the default seed (rather setting to None) \"\n                        \"or initialize the RNG state externally.\"\n                    )\n            yield\n        finally:\n            if initial_states is not None:\n                tp_random.get_cuda_rng_tracker().set_states(initial_states)\n            else:\n                # Reset to the unset state\n                tp_random.get_cuda_rng_tracker().reset()\n            state_util.destroy_model_parallel()\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/","title":"Testing callbacks","text":""},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback","title":"<code>AbstractStopAndGoCallback</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseInterruptedVsContinuousCallback</code></p> <p>Abstract base class for stop-and-go callback to compare metadata before pausing and after resuming training.</p> <p>This base class provides utility methods to help streamline stop and go comparison.</p> Provided methods <ul> <li>init: initializes the callback with the given mode.</li> <li>get_metadata: abstract method that should be overridden to get metadata from the trainer and pl_module.</li> </ul> Default behaviors <ul> <li>in stop mode, metadata is gotten and compared on_validation_epoch_end.</li> <li>in go mode, metadata is gotten and saved on_train_epoch_start.</li> </ul> <p>Override these behaviors if necessary.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class AbstractStopAndGoCallback(ABC, BaseInterruptedVsContinuousCallback):\n    \"\"\"Abstract base class for stop-and-go callback to compare metadata before pausing and after resuming training.\n\n    This base class provides utility methods to help streamline stop and go comparison.\n\n    Provided methods:\n        - __init__: initializes the callback with the given mode.\n        - get_metadata: abstract method that should be overridden to get metadata from the trainer and pl_module.\n\n    Default behaviors:\n        - in stop mode, metadata is gotten and compared on_validation_epoch_end.\n        - in go mode, metadata is gotten and saved on_train_epoch_start.\n\n    Override these behaviors if necessary.\n    \"\"\"\n\n    def __init__(self, mode: Mode = Mode.STOP):\n        \"\"\"Initialize StopAndGoCallback.\n\n        Args:\n            mode (str, optional): Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.\n\n        Notes:\n            User must override get_metadata to get metadata from the trainer and pl_module.\n        \"\"\"\n        if mode not in [Mode.STOP, Mode.RESUME]:\n            raise ValueError(f\"mode must be 'stop' or 'go', got {mode}\")\n        self.mode = mode\n        super().__init__()\n\n    @abstractmethod\n    def get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n        \"\"\"Get metadata from trainer and pl_module.\"\"\"\n        raise NotImplementedError\n\n    def on_train_epoch_start(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if self.mode == Mode.RESUME:\n            self.data = self.get_metadata(trainer, pl_module)\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if not trainer.sanity_checking and self.mode == Mode.STOP:\n            self.data = self.get_metadata(trainer, pl_module)\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback.__init__","title":"<code>__init__(mode=Mode.STOP)</code>","text":"<p>Initialize StopAndGoCallback.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.</p> <code>STOP</code> Notes <p>User must override get_metadata to get metadata from the trainer and pl_module.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(self, mode: Mode = Mode.STOP):\n    \"\"\"Initialize StopAndGoCallback.\n\n    Args:\n        mode (str, optional): Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.\n\n    Notes:\n        User must override get_metadata to get metadata from the trainer and pl_module.\n    \"\"\"\n    if mode not in [Mode.STOP, Mode.RESUME]:\n        raise ValueError(f\"mode must be 'stop' or 'go', got {mode}\")\n    self.mode = mode\n    super().__init__()\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback.get_metadata","title":"<code>get_metadata(trainer, pl_module)</code>  <code>abstractmethod</code>","text":"<p>Get metadata from trainer and pl_module.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>@abstractmethod\ndef get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n    \"\"\"Get metadata from trainer and pl_module.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback","title":"<code>BaseInterruptedVsContinuousCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code>, <code>IOMixin</code></p> <p>Base class for serializable stop-and-go callback to compare continuous to interrupted training.</p> <p>This class is used by extending a callback and collecting data into the <code>self.data</code> attribute. This data is then compared between continuous and interrupted training.</p> <p>See nemo.lightning.megatron_parallel.CallbackMethods for the available callback methods.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class BaseInterruptedVsContinuousCallback(Callback, CallbackMethods, io.IOMixin):\n    \"\"\"Base class for serializable stop-and-go callback to compare continuous to interrupted training.\n\n    This class is used by extending a callback and collecting data into the `self.data` attribute. This data is then\n    compared between continuous and interrupted training.\n\n    See nemo.lightning.megatron_parallel.CallbackMethods for the available callback methods.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the callback.\"\"\"\n        self.data = []\n\n    def __deepcopy__(self, memo):\n        \"\"\"Don't actually attempt to copy this data when this callback is being serialized.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Don't actually attempt to copy this data when this callback is being serialized.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"Don't actually attempt to copy this data when this callback is being serialized.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the callback.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the callback.\"\"\"\n    self.data = []\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ConsumedSamplesCallback","title":"<code>ConsumedSamplesCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback to check consumed samples before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ConsumedSamplesCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback to check consumed samples before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            data_sampler = step.trainer.datamodule.data_sampler\n            consumed_samples = data_sampler.compute_consumed_samples(\n                step.trainer.global_step - step.trainer.datamodule.init_global_step\n            )\n            self.data.append(np.array(consumed_samples))\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ConsumedSamplesCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        data_sampler = step.trainer.datamodule.data_sampler\n        consumed_samples = data_sampler.compute_consumed_samples(\n            step.trainer.global_step - step.trainer.datamodule.init_global_step\n        )\n        self.data.append(np.array(consumed_samples))\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.GlobalStepStateCallback","title":"<code>GlobalStepStateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback for global_step before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class GlobalStepStateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback for global_step before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get learning rate as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(np.array(step.trainer.global_step))\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.GlobalStepStateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get learning rate as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get learning rate as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(np.array(step.trainer.global_step))\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.LearningRateCallback","title":"<code>LearningRateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback for learning rate before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class LearningRateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback for learning rate before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get learning rate as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(np.array(step.trainer.optimizers[0].param_groups[0][\"lr\"]))\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.LearningRateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get learning rate as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get learning rate as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(np.array(step.trainer.optimizers[0].param_groups[0][\"lr\"]))\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.OptimizerStateCallback","title":"<code>OptimizerStateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback to check optimizer states before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class OptimizerStateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback to check optimizer states before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get optimizer states as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(\n                recursive_detach(\n                    [\n                        optimizer.mcore_optimizer.optimizer.state_dict()[\"state\"]\n                        for optimizer in step.trainer.optimizers\n                    ]\n                )\n            )\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.OptimizerStateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get optimizer states as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get optimizer states as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(\n            recursive_detach(\n                [\n                    optimizer.mcore_optimizer.optimizer.state_dict()[\"state\"]\n                    for optimizer in step.trainer.optimizers\n                ]\n            )\n        )\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.SignalAfterGivenStepCallback","title":"<code>SignalAfterGivenStepCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code></p> <p>A callback that emits a given signal to the current process at the defined step.</p> <p>Use this callback for pytest based Stop and go tests.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class SignalAfterGivenStepCallback(Callback, CallbackMethods):\n    \"\"\"A callback that emits a given signal to the current process at the defined step.\n\n    Use this callback for pytest based Stop and go tests.\n    \"\"\"\n\n    def __init__(self, stop_step: int, signal_: signal.Signals = signal.SIGUSR2):\n        \"\"\"Initializes the callback with the given stop_step.\"\"\"\n        self.stop_step = stop_step\n        self.signal = signal_\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Stop training if the global step is greater than or equal to the stop_step.\"\"\"\n        if step.trainer.global_step &gt;= self.stop_step:\n            os.kill(os.getpid(), self.signal)\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.SignalAfterGivenStepCallback.__init__","title":"<code>__init__(stop_step, signal_=signal.SIGUSR2)</code>","text":"<p>Initializes the callback with the given stop_step.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(self, stop_step: int, signal_: signal.Signals = signal.SIGUSR2):\n    \"\"\"Initializes the callback with the given stop_step.\"\"\"\n    self.stop_step = stop_step\n    self.signal = signal_\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.SignalAfterGivenStepCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Stop training if the global step is greater than or equal to the stop_step.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Stop training if the global step is greater than or equal to the stop_step.\"\"\"\n    if step.trainer.global_step &gt;= self.stop_step:\n        os.kill(os.getpid(), self.signal)\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.StopAfterValidEpochEndCallback","title":"<code>StopAfterValidEpochEndCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code></p> <p>A callback that stops training after the validation epoch.</p> <p>Use this callback for pytest based Stop and go tests.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class StopAfterValidEpochEndCallback(Callback, CallbackMethods):\n    \"\"\"A callback that stops training after the validation epoch.\n\n    Use this callback for pytest based Stop and go tests.\n    \"\"\"\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if trainer.sanity_checking:\n            return\n        trainer.should_stop = True\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainInputCallback","title":"<code>TrainInputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training input samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainInputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training input samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainInputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainLossCallback","title":"<code>TrainLossCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training loss samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainLossCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training loss samples for comparison.\"\"\"\n\n    def on_megatron_step_end(\n        self,\n        step: MegatronStep,\n        microbatch_outputs: List[Any],\n        reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainLossCallback.on_megatron_step_end","title":"<code>on_megatron_step_end(step, microbatch_outputs, reduced=None)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_end(\n    self,\n    step: MegatronStep,\n    microbatch_outputs: List[Any],\n    reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainOutputCallback","title":"<code>TrainOutputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training output samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainOutputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training output samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainOutputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback","title":"<code>TrainValInitConsumedSamplesStopAndGoCallback</code>","text":"<p>               Bases: <code>AbstractStopAndGoCallback</code></p> <p>Stop-and-go callback to check consumed samples before pausing and after resuming training.</p> <p>This is currently the only callback that doesn't fit with the new pattern of directly comparing continuous and interrupted training, since the dataloaders don't track their consumed_samples before and after checkpoint resumption.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainValInitConsumedSamplesStopAndGoCallback(AbstractStopAndGoCallback):\n    \"\"\"Stop-and-go callback to check consumed samples before pausing and after resuming training.\n\n    This is currently the only callback that doesn't fit with the new pattern of directly comparing continuous and\n    interrupted training, since the dataloaders don't track their consumed_samples before and after checkpoint\n    resumption.\n    \"\"\"\n\n    @override\n    def get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        # return trainer.datamodule.state_dict()[\"consumed_samples\"]  # TODO why state_dict can be empty despite working lines below\n        train_data_sampler: MegatronPretrainingSampler = trainer.train_dataloader.batch_sampler\n        val_data_sampler: MegatronPretrainingSampler = trainer.val_dataloaders.batch_sampler\n        return train_data_sampler.consumed_samples, val_data_sampler.consumed_samples\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback.get_metadata","title":"<code>get_metadata(trainer, pl_module)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>@override\ndef get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    # return trainer.datamodule.state_dict()[\"consumed_samples\"]  # TODO why state_dict can be empty despite working lines below\n    train_data_sampler: MegatronPretrainingSampler = trainer.train_dataloader.batch_sampler\n    val_data_sampler: MegatronPretrainingSampler = trainer.val_dataloaders.batch_sampler\n    return train_data_sampler.consumed_samples, val_data_sampler.consumed_samples\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidInputCallback","title":"<code>ValidInputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect validation input samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidInputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect validation input samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidInputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidLossCallback","title":"<code>ValidLossCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training loss samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidLossCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training loss samples for comparison.\"\"\"\n\n    def on_megatron_step_end(\n        self,\n        step: MegatronStep,\n        microbatch_outputs: List[Any],\n        reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidLossCallback.on_megatron_step_end","title":"<code>on_megatron_step_end(step, microbatch_outputs, reduced=None)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_end(\n    self,\n    step: MegatronStep,\n    microbatch_outputs: List[Any],\n    reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidOutputCallback","title":"<code>ValidOutputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect validation output samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidOutputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect validation output samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidOutputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/torch/","title":"Torch","text":""},{"location":"API_reference/bionemo/testing/torch/#bionemo.testing.torch.check_fp8_support","title":"<code>check_fp8_support(device_id=0)</code>","text":"<p>Check if FP8 is supported on the current GPU.</p> <p>FP8 requires compute capability 8.9+ (Ada Lovelace/Hopper architecture or newer).</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def check_fp8_support(device_id: int = 0) -&gt; tuple[bool, str, str]:\n    \"\"\"Check if FP8 is supported on the current GPU.\n\n    FP8 requires compute capability 8.9+ (Ada Lovelace/Hopper architecture or newer).\n    \"\"\"\n    if not torch.cuda.is_available():\n        return False, \"0.0\", \"CUDA not available\"\n    device_props = torch.cuda.get_device_properties(device_id)\n    compute_capability = f\"{device_props.major}.{device_props.minor}\"\n    device_name = device_props.name\n    # FP8 is supported on compute capability 8.9+ (Ada Lovelace/Hopper architecture)\n    is_supported = (device_props.major &gt; 8) or (device_props.major == 8 and device_props.minor &gt;= 9)\n    return is_supported, compute_capability, f\"Device: {device_name}, Compute Capability: {compute_capability}\"\n</code></pre>"},{"location":"API_reference/bionemo/testing/torch/#bionemo.testing.torch.recursive_assert_approx_equal","title":"<code>recursive_assert_approx_equal(x, y, atol=0.0001, rtol=0.0001)</code>","text":"<p>Assert that all tensors in a nested structure are approximately equal.</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def recursive_assert_approx_equal(x, y, atol=1e-4, rtol=1e-4):\n    \"\"\"Assert that all tensors in a nested structure are approximately equal.\"\"\"\n    if isinstance(x, torch.Tensor):\n        torch.testing.assert_close(x, y, atol=atol, rtol=rtol)\n    elif isinstance(x, np.ndarray):\n        np.testing.assert_allclose(x, y, atol=atol, rtol=rtol)\n    elif isinstance(x, (list, tuple)):\n        assert len(x) == len(y), f\"Length mismatch: {len(x)} vs {len(y)}\"\n        for x_item, y_item in zip(x, y):\n            recursive_assert_approx_equal(x_item, y_item, atol=atol, rtol=rtol)\n    elif isinstance(x, dict):\n        assert x.keys() == y.keys()\n        for key in x:\n            recursive_assert_approx_equal(x[key], y[key], atol=atol, rtol=rtol)\n    else:\n        assert x == y\n</code></pre>"},{"location":"API_reference/bionemo/testing/torch/#bionemo.testing.torch.recursive_detach","title":"<code>recursive_detach(x)</code>","text":"<p>Detach all tensors in a nested structure.</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def recursive_detach(x):\n    \"\"\"Detach all tensors in a nested structure.\"\"\"\n    if isinstance(x, torch.Tensor):\n        return x.detach().cpu()\n    elif isinstance(x, (list, tuple)):\n        return type(x)(recursive_detach(item) for item in x)\n    elif isinstance(x, dict):\n        return {key: recursive_detach(value) for key, value in x.items()}\n    else:\n        return x\n</code></pre>"},{"location":"API_reference/bionemo/testing/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/testing/utils/#bionemo.testing.utils.assert_matrix_correlation_above_value","title":"<code>assert_matrix_correlation_above_value(actual, expected, mask=None, min_correlation=0.95, msg='')</code>","text":"<p>Assert that two tensors are close with a root mean squared error (RMSE)     relative to the scaled root mean square values for each matrix. This tells     you if the RMSE implies that the two matrices are more similar to eachother     as-is than would be the case if values were randomly permuted.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Tensor</code> <p>The actual tensor.</p> required <code>expected</code> <code>Tensor</code> <p>The expected tensor.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>If there are only some values you want to compare, apply this mask and RMSE will be computed on the unmasked items only.</p> <code>None</code> <code>min_relative_rmse</code> <p>The relative tolerance parameter.</p> required Source code in <code>bionemo/testing/utils.py</code> <pre><code>def assert_matrix_correlation_above_value(  # noqa: D417\n    actual: torch.Tensor,\n    expected: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    min_correlation: float = 0.95,\n    msg: str = \"\",\n) -&gt; None:\n    \"\"\"Assert that two tensors are close with a root mean squared error (RMSE)\n        relative to the scaled root mean square values for each matrix. This tells\n        you if the RMSE implies that the two matrices are more similar to eachother\n        as-is than would be the case if values were randomly permuted.\n\n    Args:\n        actual: The actual tensor.\n        expected: The expected tensor.\n        mask: If there are only some values you want to compare,\n            apply this mask and RMSE will be computed on the unmasked items only.\n        min_relative_rmse: The relative tolerance parameter.\n    \"\"\"  # noqa: D205\n    if mask is None:\n        mask = torch.ones_like(actual)\n    else:\n        if len(mask.shape) &lt; len(actual.shape):\n            mask = mask[..., None]\n    masked_actual = actual[mask.expand_as(actual).to(bool)]\n    masked_expected = expected[mask.expand_as(expected).to(bool)]\n    corr = torch.corrcoef(torch.stack([masked_actual, masked_expected]))[0, 1]\n    if corr &lt; min_correlation:\n        raise AssertionError(f\"Correlation below threshold: {corr} &lt; {min_correlation}. {msg}\")\n</code></pre>"},{"location":"API_reference/bionemo/testing/utils/#bionemo.testing.utils.assert_matrix_mape_below_value","title":"<code>assert_matrix_mape_below_value(actual, expected, mask=None, max_mape=0.1, eps=0.001, msg='')</code>","text":"<p>Assert that two tensors are close with a root mean squared error (RMSE)     relative to the scaled root mean square values for each matrix. This tells     you if the RMSE implies that the two matrices are more similar to eachother     as-is than would be the case if values were randomly permuted.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Tensor</code> <p>The actual tensor.</p> required <code>expected</code> <code>Tensor</code> <p>The expected tensor.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>If there are only some values you want to compare, apply this mask and RMSE will be computed on the unmasked items only.</p> <code>None</code> <code>min_relative_rmse</code> <p>The relative tolerance parameter.</p> required Source code in <code>bionemo/testing/utils.py</code> <pre><code>def assert_matrix_mape_below_value(  # noqa: D417\n    actual: torch.Tensor,\n    expected: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    max_mape: float = 0.1,\n    eps: float = 1e-3,\n    msg: str = \"\",\n) -&gt; None:\n    \"\"\"Assert that two tensors are close with a root mean squared error (RMSE)\n        relative to the scaled root mean square values for each matrix. This tells\n        you if the RMSE implies that the two matrices are more similar to eachother\n        as-is than would be the case if values were randomly permuted.\n\n    Args:\n        actual: The actual tensor.\n        expected: The expected tensor.\n        mask: If there are only some values you want to compare,\n            apply this mask and RMSE will be computed on the unmasked items only.\n        min_relative_rmse: The relative tolerance parameter.\n    \"\"\"  # noqa: D205\n    if mask is None:\n        mask = torch.ones_like(actual)\n    else:\n        if len(mask.shape) &lt; len(actual.shape):\n            mask = mask[..., None]\n    masked_actual = actual[mask.expand_as(actual).to(bool)]\n    masked_expected = expected[mask.expand_as(expected).to(bool)]\n    mape = (\n        torch.mean(\n            torch.abs(masked_actual - masked_expected)\n            / torch.maximum(torch.abs(masked_expected), torch.zeros_like(masked_expected) + eps)\n        )\n        * 100.0\n    )\n    if mape &gt; max_mape:\n        raise AssertionError(f\"MAPE below threshold: {mape} &gt; {max_mape}. {msg}\")\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/esm2/","title":"Esm2","text":""},{"location":"API_reference/bionemo/testing/data/esm2/#bionemo.testing.data.esm2.create_mock_parquet_train_val_inputs","title":"<code>create_mock_parquet_train_val_inputs(tmp_path)</code>","text":"<p>Create a mock protein train and val cluster parquet.</p> Source code in <code>bionemo/testing/data/esm2.py</code> <pre><code>def create_mock_parquet_train_val_inputs(tmp_path):\n    \"\"\"Create a mock protein train and val cluster parquet.\"\"\"\n    train_cluster_path = tmp_path / \"train_clusters.parquet\"\n    train_clusters = pd.DataFrame(\n        {\n            \"ur90_id\": [[\"UniRef90_A\"], [\"UniRef90_B\", \"UniRef90_C\"]],\n        }\n    )\n    train_clusters.to_parquet(train_cluster_path)\n\n    valid_cluster_path = tmp_path / \"valid_clusters.parquet\"\n    valid_clusters = pd.DataFrame(\n        {\n            \"ur50_id\": [\"UniRef50_A\", \"UniRef50_B\", \"UniRef90_A\", \"UniRef90_B\"],\n        }\n    )\n    valid_clusters.to_parquet(valid_cluster_path)\n    return train_cluster_path, valid_cluster_path\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/esm2/#bionemo.testing.data.esm2.create_mock_protein_dataset","title":"<code>create_mock_protein_dataset(tmp_path)</code>","text":"<p>Create a mock protein dataset.</p> Source code in <code>bionemo/testing/data/esm2.py</code> <pre><code>def create_mock_protein_dataset(tmp_path):\n    \"\"\"Create a mock protein dataset.\"\"\"\n    db_file = tmp_path / \"protein_dataset.db\"\n    conn = sqlite3.connect(str(db_file))\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        CREATE TABLE protein (\n            id TEXT PRIMARY KEY,\n            sequence TEXT\n        )\n    \"\"\"\n    )\n\n    proteins = [\n        (\"UniRef90_A\", \"ACDEFGHIKLMNPQRSTVWY\"),\n        (\"UniRef90_B\", \"DEFGHIKLMNPQRSTVWYAC\"),\n        (\"UniRef90_C\", \"MGHIKLMNPQRSTVWYACDE\"),\n        (\"UniRef50_A\", \"MKTVRQERLKSIVRI\"),\n        (\"UniRef50_B\", \"MRILERSKEPVSGAQLA\"),\n    ]\n    cursor.executemany(\"INSERT INTO protein VALUES (?, ?)\", proteins)\n\n    conn.commit()\n    conn.close()\n\n    return db_file\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/fasta/","title":"Fasta","text":""},{"location":"API_reference/bionemo/testing/data/fasta/#bionemo.testing.data.fasta.create_fasta_file","title":"<code>create_fasta_file(fasta_file_path, num_sequences, sequence_length=None, sequence_lengths=None, repeating_dna_pattern=ALU_SEQUENCE, max_line_length=80)</code>","text":"<p>Creates a fasta file with the given number of sequences, sequence length, and repeating dna pattern. Each contig uses a shifted version of the repeating pattern.</p> Source code in <code>bionemo/testing/data/fasta.py</code> <pre><code>def create_fasta_file(\n    fasta_file_path: Path,\n    num_sequences: int,\n    sequence_length: int | None = None,\n    sequence_lengths: list[int] | None = None,\n    repeating_dna_pattern: str = ALU_SEQUENCE,\n    max_line_length: int = 80,\n) -&gt; Path:\n    \"\"\"Creates a fasta file with the given number of sequences, sequence length, and repeating dna pattern. Each contig uses a shifted version of the repeating pattern.\"\"\"\n    assert sequence_length is not None or sequence_lengths is not None\n    with open(fasta_file_path, \"w\") as f:\n        if sequence_lengths is not None:\n            assert len(sequence_lengths) == num_sequences\n        else:\n            assert sequence_length is not None\n            sequence_lengths: list[int] = [sequence_length] * num_sequences\n        for i in range(num_sequences):\n            # get the repeating pattern shifted by i for this contig\n            repeat_pattern_for_contig = repeating_dna_pattern[i:] + repeating_dna_pattern[:i]\n            # repeat the pattern enough times to reach the desired sequence length\n            if sequence_lengths[i] &lt;= len(repeat_pattern_for_contig):\n                contig_output = repeat_pattern_for_contig[: sequence_lengths[i]]\n            else:\n                # Calculate how many complete repeats we need\n                num_repeats = sequence_lengths[i] // len(repeat_pattern_for_contig)\n                remainder = sequence_lengths[i] % len(repeat_pattern_for_contig)\n                contig_output = repeat_pattern_for_contig * num_repeats + repeat_pattern_for_contig[:remainder]\n            # verify the length of the contig is as expected\n            assert len(contig_output) == sequence_lengths[i]\n            # Fold the contig output into lines of max_line_length\n            contig_output = \"\\n\".join(\n                contig_output[i : i + max_line_length] for i in range(0, sequence_lengths[i], max_line_length)\n            )\n            # write to the fasta file with the actual contig_output, not the repeating pattern\n            f.write(f\"&gt;contig_{i}\\n{contig_output}\\n\")\n    return fasta_file_path\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/","title":"Load","text":""},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.default_ngc_client","title":"<code>default_ngc_client(use_guest_if_api_key_invalid=True)</code>","text":"<p>Create a default NGC client.</p> <p>This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_ngc_client(use_guest_if_api_key_invalid: bool = True) -&gt; \"ngcsdk.Client\":\n    \"\"\"Create a default NGC client.\n\n    This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.\n    \"\"\"\n    import ngcsdk\n\n    client = ngcsdk.Client()\n\n    try:\n        client.configure()\n\n    except ValueError as e:\n        if use_guest_if_api_key_invalid:\n            logger.error(f\"Error configuring NGC client: {e}, signing in as guest.\")\n            client = ngcsdk.Client(\"no-apikey\")\n            client.configure(\n                api_key=\"no-apikey\",  # pragma: allowlist secret\n                org_name=\"no-org\",\n                team_name=\"no-team\",\n                ace_name=\"no-ace\",\n            )\n\n        else:\n            raise\n\n    return client\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.default_pbss_client","title":"<code>default_pbss_client()</code>","text":"<p>Create a default S3 client for PBSS.</p> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def default_pbss_client():\n    \"\"\"Create a default S3 client for PBSS.\"\"\"\n    import boto3\n\n    retry_config = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"})\n    return boto3.client(\"s3\", endpoint_url=\"https://pbss.s8k.io\", config=retry_config)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.load","title":"<code>load(model_or_data_tag, source=DEFAULT_SOURCE, resources=None, cache_dir=None)</code>","text":"<p>Download a resource from PBSS or NGC.</p> <p>Parameters:</p> Name Type Description Default <code>model_or_data_tag</code> <code>str</code> <p>A pointer to the desired resource. Must be a key in the resources dictionary.</p> required <code>source</code> <code>SourceOptions</code> <p>Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".</p> <code>DEFAULT_SOURCE</code> <code>resources</code> <code>dict[str, Resource] | None</code> <p>A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)</p> <code>None</code> <code>cache_dir</code> <code>Path | None</code> <p>The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the desired tag was not found, or if an NGC url was requested but not provided.</p> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object pointing either at the downloaded file, or at a decompressed folder containing the</p> <code>Path</code> <p>file(s).</p> <p>Examples:</p> <p>For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:</p> <pre><code>&gt;&gt;&gt; load(\"filename/tag\")\nPosixPath(/tmp/bionemo/downloaded-file-name)\n</code></pre> Source code in <code>bionemo/core/data/load.py</code> <pre><code>def load(\n    model_or_data_tag: str,\n    source: SourceOptions = DEFAULT_SOURCE,\n    resources: dict[str, Resource] | None = None,\n    cache_dir: Path | None = None,\n) -&gt; Path:\n    \"\"\"Download a resource from PBSS or NGC.\n\n    Args:\n        model_or_data_tag: A pointer to the desired resource. Must be a key in the resources dictionary.\n        source: Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".\n        resources: A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)\n        cache_dir: The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)\n\n    Raises:\n        ValueError: If the desired tag was not found, or if an NGC url was requested but not provided.\n\n    Returns:\n        A Path object pointing either at the downloaded file, or at a decompressed folder containing the\n        file(s).\n\n    Examples:\n        For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:\n        &gt;&gt;&gt; load(\"filename/tag\")\n        PosixPath(/tmp/bionemo/downloaded-file-name)\n    \"\"\"\n    if resources is None:\n        resources = get_all_resources()\n\n    if cache_dir is None:\n        cache_dir = BIONEMO_CACHE_DIR\n\n    if model_or_data_tag not in resources:\n        raise ValueError(f\"Resource '{model_or_data_tag}' not found.\")\n\n    if source == \"ngc\" and resources[model_or_data_tag].ngc is None:\n        raise ValueError(f\"Resource '{model_or_data_tag}' does not have an NGC URL.\")\n\n    resource = resources[model_or_data_tag]\n    filename = str(resource.pbss).split(\"/\")[-1]\n\n    extension = \"\".join(Path(filename).suffixes)\n    processor = _get_processor(extension, resource.unpack, resource.decompress)\n\n    if source == \"pbss\":\n        download_fn = _s3_download\n        url = resource.pbss\n\n    elif source == \"ngc\":\n        assert resource.ngc_registry is not None\n        download_fn = NGCDownloader(filename=filename, ngc_registry=resource.ngc_registry)\n        url = resource.ngc\n\n    else:\n        raise ValueError(f\"Source '{source}' not supported.\")\n\n    download = pooch.retrieve(\n        url=str(url),\n        fname=f\"{resource.sha256}-{filename}\",\n        known_hash=resource.sha256,\n        path=cache_dir,\n        downloader=download_fn,\n        processor=processor,\n    )\n\n    # Pooch by default returns a list of unpacked files if they unpack a zipped or tarred directory. Instead of that, we\n    # just want the unpacked, parent folder.\n    if isinstance(download, list):\n        return Path(processor.extract_dir)  # type: ignore\n\n    else:\n        return Path(download)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/resource/","title":"Resource","text":""},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class that represents a remote resource for downloading and caching test data.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>class Resource(pydantic.BaseModel):\n    \"\"\"Class that represents a remote resource for downloading and caching test data.\"\"\"\n\n    model_config = pydantic.ConfigDict(use_attribute_docstrings=True)\n\n    tag: Annotated[str, pydantic.StringConstraints(pattern=r\"^[^/]*/[^/]*$\")]  # Only slash between filename and tag.\n    \"\"\"A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").\"\"\"\n\n    ngc: Annotated[str, pydantic.AfterValidator(_validate_ngc_resource)] | None = None\n    \"\"\"The NGC URL for the resource.\n\n    Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.\n    \"\"\"\n\n    ngc_registry: Literal[\"model\", \"resource\"] | None = None\n    \"\"\"The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.\"\"\"\n\n    pbss: Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(allowed_schemes=[\"s3\"])]\n    \"\"\"The PBSS (NVIDIA-internal) URL of the resource.\"\"\"\n\n    sha256: str | None\n    \"\"\"The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).\"\"\"\n\n    owner: pydantic.NameEmail\n    \"\"\"The owner or primary point of contact for the resource, in the format \"Name &lt;email&gt;\".\"\"\"\n\n    description: str | None = None\n    \"\"\"A description of the file(s).\"\"\"\n\n    unpack: Literal[False, None] = None\n    \"\"\"Whether the resource should be unpacked after download. If None, will defer to the file extension.\"\"\"\n\n    decompress: Literal[False, None] = None\n    \"\"\"Whether the resource should be decompressed after download. If None, will defer to the file extension.\"\"\"\n\n    @pydantic.model_validator(mode=\"after\")\n    def _validate_ngc_registry(self):\n        if self.ngc and not self.ngc_registry:\n            raise ValueError(f\"ngc_registry must be provided if ngc is not None: {self.tag}\")\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.decompress","title":"<code>decompress = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be decompressed after download. If None, will defer to the file extension.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A description of the file(s).</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.ngc","title":"<code>ngc = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC URL for the resource.</p> <p>Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.ngc_registry","title":"<code>ngc_registry = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.owner","title":"<code>owner</code>  <code>instance-attribute</code>","text":"<p>The owner or primary point of contact for the resource, in the format \"Name \"."},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.pbss","title":"<code>pbss</code>  <code>instance-attribute</code>","text":"<p>The PBSS (NVIDIA-internal) URL of the resource.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.sha256","title":"<code>sha256</code>  <code>instance-attribute</code>","text":"<p>The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.tag","title":"<code>tag</code>  <code>instance-attribute</code>","text":"<p>A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.unpack","title":"<code>unpack = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be unpacked after download. If None, will defer to the file extension.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.get_all_resources","title":"<code>get_all_resources(resource_path=None)</code>  <code>cached</code>","text":"<p>Return a dictionary of all resources.</p> Source code in <code>bionemo/core/data/resource.py</code> <pre><code>@functools.cache\ndef get_all_resources(resource_path: Path | None = None) -&gt; dict[str, Resource]:\n    \"\"\"Return a dictionary of all resources.\"\"\"\n    if not resource_path:\n        resource_path = Path(files(\"bionemo.core.data\").joinpath(\"resources\"))  # type: ignore\n\n    resources_files = itertools.chain(resource_path.glob(\"*.yaml\"), resource_path.glob(\"*.yml\"))\n\n    all_resources = [resource for file in resources_files for resource in _parse_resource_file(file)]\n\n    resource_list = pydantic.TypeAdapter(list[Resource]).validate_python(all_resources)\n    resource_dict = {resource.tag: resource for resource in resource_list}\n\n    if len(resource_dict) != len(resource_list):\n        # Show the # of and which ones are duplicated so that a user can begin debugging and resolve the issue.\n        tag_counts = Counter([resource.tag for resource in resource_list])\n        raise ValueError(f\"Duplicate resource tags found!: {[tag for tag, count in tag_counts.items() if count &gt; 1]}\")\n\n    return resource_dict\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/mode/","title":"Mode","text":""},{"location":"API_reference/bionemo/testing/harnesses/mode/#bionemo.testing.harnesses.mode.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Mode for stop-go testing.</p> Source code in <code>bionemo/testing/harnesses/mode.py</code> <pre><code>class Mode(Enum):\n    \"\"\"Mode for stop-go testing.\"\"\"\n\n    STOP = auto()\n    RESUME = auto()\n    CONTINUOUS = auto()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/","title":"Stop and go","text":""},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness","title":"<code>StopAndGoHarness</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for testing consistency between interrupted and continuous training.</p> <p>Users should override cls.setup_model and update cls.setup_class to customize the downstream test cases. Metadata are collected through callbacks and users can add new unit tests by comparing the metadata for the interrupted and continuous cases.</p> <p>By default, learning rate, global step, optimizer state, consumed samples, input and output tensors, and loss are compared. Users can add additional metrics by adding new callbacks to <code>cls.callbacks</code> and associated test functions.</p> Stop and go tests act as follows <ul> <li>setup a clean model for a brief training run, set callbacks to track.</li> <li>interrupt training via the StopAndGoException in the callback Raise.</li> <li>train the model resumed from the checkpoint with the same set of callbacks.</li> <li>train the model continuously without interruption with a new set of the same callbacks.</li> <li>compare each pair of interrupted and continuous callbacks to check for equality.</li> </ul> Considerations when implementing this class <ul> <li>The derived test name should start with <code>Test</code>, and test methods should start with <code>test_</code> to enable pytest   discovery.</li> <li>devices, pipeline_model_parallel, and tensor_model_parallel may impact the setup of DataModule. Certain     datasets expect a known global batch size, which depends on the number of devices and conditional tensor     model parallel/ pipeline model parallel settings. By default, we are testing only on single device without     parallelism.</li> <li>'mode' is useful in some cases, but not in all cases. Implement conditions based on these when useful. As an     example, it may be useful to implement a test that stops and resumes.<ul> <li>changing callbacks to test metadata integrity (core feature of stop-and-go tests).</li> <li>changing the model construction to use different hyperparameters.</li> <li>... etc Each of the above tests cases may be useful for automated testing of various expected behavior.</li> </ul> </li> <li>stop(), resume(), continuous() or collectively run_stop_and_go() are provided methods which execute the actual   tests, leveraging the conditions in the various setup methods, respecting 'mode' where necessary.</li> </ul> <p>Attributes:</p> Name Type Description <code>root_dir</code> <p>The root directory.</p> <code>val_check_interval</code> <code>int</code> <p>The validation check interval. Stored as an attribute to ensure consistency.</p> <code>exp_name</code> <code>str</code> <p>The experiment name.</p> <code>extra_metrics_dict</code> <code>str</code> <p>A dictionary of metrics and their corresponding functions.</p> <p>See Also: bionemo.testing.callbacks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>class StopAndGoHarness(ABC):\n    \"\"\"Abstract base class for testing consistency between interrupted and continuous training.\n\n    Users should override cls.setup_model and update cls.setup_class to customize the downstream test cases. Metadata\n    are collected through callbacks and users can add new unit tests by comparing the metadata for the interrupted and\n    continuous cases.\n\n    By default, learning rate, global step, optimizer state, consumed samples, input and output tensors, and loss are\n    compared. Users can add additional metrics by adding new callbacks to `cls.callbacks` and associated test functions.\n\n    Stop and go tests act as follows:\n        - setup a clean model for a brief training run, set callbacks to track.\n        - interrupt training via the StopAndGoException in the callback Raise.\n        - train the model resumed from the checkpoint with the same set of callbacks.\n        - train the model continuously without interruption with a new set of the same callbacks.\n        - compare each pair of interrupted and continuous callbacks to check for equality.\n\n    Considerations when implementing this class:\n        - The derived test name should start with `Test`, and test methods should start with `test_` to enable pytest\n          discovery.\n        - devices, pipeline_model_parallel, and tensor_model_parallel may impact the setup of DataModule. Certain\n            datasets expect a known global batch size, which depends on the number of devices and conditional tensor\n            model parallel/ pipeline model parallel settings. By default, we are testing only on single device without\n            parallelism.\n        - 'mode' is useful in some cases, but not in all cases. Implement conditions based on these when useful. As an\n            example, it may be useful to implement a test that stops and resumes.\n            - changing callbacks to test metadata integrity (core feature of stop-and-go tests).\n            - changing the model construction to use different hyperparameters.\n            - ... etc\n            Each of the above tests cases may be useful for automated testing of various expected behavior.\n        - stop(), resume(), continuous() or collectively run_stop_and_go() are provided methods which execute the actual\n          tests, leveraging the conditions in the various setup methods, respecting 'mode' where necessary.\n\n    Attributes:\n        root_dir: The root directory.\n        val_check_interval: The validation check interval. Stored as an attribute to ensure consistency.\n        exp_name: The experiment name.\n        extra_metrics_dict: A dictionary of metrics and their corresponding functions.\n\n    See Also: bionemo.testing.callbacks.\n    \"\"\"\n\n    # class variables that need to be overridden\n    num_steps: int\n    val_check_interval: int\n    limit_val_batches: int\n    lr: float = 1e-4\n    precision: Literal[\"16-mixed\", \"bf16-mixed\", \"32\"]\n    output_tensor_atol: float = 1e-3  # Absolute tolerance for model precision between output tensors.\n    output_tensor_rtol: float = 1e-4  # Relative tolerance for model precision between output tensors.\n\n    # class variables that will be setup in setUpClass\n    tempdir: tempfile.TemporaryDirectory\n    metadata_dir: pathlib.Path\n    exp_name: str\n    callbacks: CallbackDict\n    nemo_logger: NeMoLogger\n\n    @classmethod\n    def setup_class(cls) -&gt; None:\n        \"\"\"Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.\"\"\"\n        cls.tempdir = tempfile.TemporaryDirectory()\n        cls.metadata_dir = pathlib.Path(cls.tempdir.name) / \"metadata\"\n        cls.exp_name = cls.__name__\n\n        cls.callbacks = cls.get_default_callbacks()\n\n        cls.nemo_logger = NeMoLogger(\n            log_dir=cls.tempdir.name,\n            name=cls.exp_name,\n            use_datetime_version=False,\n            version=None,\n            tensorboard=None,\n            wandb=None,\n            ckpt=None,\n        )\n\n    @classmethod\n    def teardown_class(cls) -&gt; None:\n        \"\"\"Tears down the class by cleaning up the temporary directory.\"\"\"\n        cls.tempdir.cleanup()\n\n    @classmethod\n    @abstractmethod\n    def setup_model(cls, mode: Mode) -&gt; tuple[pl.LightningModule, pl.LightningDataModule, nl.MegatronOptimizerModule]:\n        \"\"\"Constructs the model, data, and optimizer for the test harness.\n\n        Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged\n        to use the same code path for both.\n\n        Args:\n            mode: The mode indicating whether to stop or go.\n\n        Returns:\n            tuple: A tuple containing the model, data, and optimizer.\n        \"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def setup_trainer(\n        cls,\n        mode: Mode,\n    ) -&gt; nl.Trainer:\n        \"\"\"Setup trainer by passing stop, resume, or continuous callbacks according to mode.\n\n        Args:\n            mode (Mode): The mode indicating whether to stop, resume, or train continuously.\n\n        Returns:\n            (nl.Trainer): NeMo Lightning trainer object.\n        \"\"\"\n        strategy = MegatronStrategy(\n            ddp=\"megatron\",\n            find_unused_parameters=True,\n            ckpt_include_optimizer=True,\n            ckpt_async_save=False,\n        )\n\n        trainer = nl.Trainer(\n            devices=1,\n            max_steps=cls.num_steps,\n            accelerator=\"gpu\",\n            strategy=strategy,\n            limit_val_batches=cls.limit_val_batches,\n            val_check_interval=cls.val_check_interval,\n            log_every_n_steps=cls.val_check_interval,\n            num_nodes=1,\n            callbacks=list(cls.callbacks[mode].values()),\n            plugins=nl.MegatronMixedPrecision(precision=cls.precision),\n        )\n        return trainer\n\n    @classmethod\n    def get_default_callbacks(cls) -&gt; CallbackDict:\n        \"\"\"Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.\n\n        To extend this method, call the super and append to the callbacks, depending on which mode you are in:\n\n        ```python\n        callbacks = super().get_callbacks()\n        callbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\n        return callbacks\n        ```\n\n        Returns:\n            A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback\n            object.\n        \"\"\"\n        callbacks: CallbackDict = {}\n\n        def make_callbacks() -&gt; Dict[Type[pl.Callback], pl.Callback]:\n            return {\n                testing_callbacks.LearningRateCallback: testing_callbacks.LearningRateCallback(),\n                testing_callbacks.GlobalStepStateCallback: testing_callbacks.GlobalStepStateCallback(),\n                testing_callbacks.ConsumedSamplesCallback: testing_callbacks.ConsumedSamplesCallback(),\n                testing_callbacks.OptimizerStateCallback: testing_callbacks.OptimizerStateCallback(),\n                testing_callbacks.TrainInputCallback: testing_callbacks.TrainInputCallback(),\n                testing_callbacks.TrainOutputCallback: testing_callbacks.TrainOutputCallback(),\n                testing_callbacks.TrainLossCallback: testing_callbacks.TrainLossCallback(),\n                testing_callbacks.ValidInputCallback: testing_callbacks.ValidInputCallback(),\n                testing_callbacks.ValidOutputCallback: testing_callbacks.ValidOutputCallback(),\n                testing_callbacks.ValidLossCallback: testing_callbacks.ValidLossCallback(),\n            }\n\n        interrupted_callbacks = make_callbacks()\n        callbacks[Mode.CONTINUOUS] = make_callbacks()\n\n        for mode in [Mode.STOP, Mode.RESUME]:\n            consumed_samples_cls = testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n            callbacks[mode] = {\n                consumed_samples_cls: consumed_samples_cls(mode=mode),\n                **interrupted_callbacks,\n            }\n\n        callbacks[Mode.STOP].update(\n            {\n                testing_callbacks.StopAfterValidEpochEndCallback: testing_callbacks.StopAfterValidEpochEndCallback(),\n                nl_callbacks.ModelCheckpoint: nl_callbacks.ModelCheckpoint(\n                    save_last=True,\n                    monitor=\"val_loss\",\n                    save_top_k=2,\n                    always_save_context=True,\n                    filename=\"{epoch}-{step}-{val_loss:.2f}\",\n                ),\n            }\n        )\n\n        return callbacks\n\n    # stop() and resume() are provided methods and run the requisite methods with the appropriate mode.\n    @classmethod\n    def stop(cls) -&gt; None:\n        \"\"\"Runs pre-training and 'stops' after the first checkpoint is saved.\n\n        This method sets up the model, data, and optimizer for the Mode.STOP mode.\n        It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics.\n        The training process is executed using the `llm.train` function, passing the model, data, trainer, logger, optimizer, and resume options.\n        If a `testing_callbacks.StopAndGoException` is raised during training, it is caught and no action is taken.\n\n        Raises:\n            testing_callbacks.StopAndGoException: If a stop and go exception occurs during training.\n        \"\"\"\n        logging.info(\"Running stop()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.STOP)\n        trainer = cls.setup_trainer(Mode.STOP)\n        with distributed_model_parallel_state():\n            llm.train(\n                model=model,\n                data=data,\n                trainer=trainer,\n                log=cls.nemo_logger,\n                optim=opt,\n                resume=resume.AutoResume(\n                    resume_if_exists=False,  # Looks for the -last checkpoint to continue training.\n                    resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n                ),\n            )\n\n    @classmethod\n    def resume(cls) -&gt; None:\n        \"\"\"Resumes the model from the checkpoint saved at the end of `stop()` and verifies the metadata integrity.\"\"\"\n        logging.info(\"Running resume()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.RESUME)\n        trainer = cls.setup_trainer(Mode.RESUME)\n        with distributed_model_parallel_state():\n            llm.train(\n                model=model,\n                data=data,\n                trainer=trainer,\n                log=cls.nemo_logger,\n                optim=opt,\n                resume=resume.AutoResume(\n                    resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n                    resume_ignore_no_checkpoint=False,  # When false this will throw an error with no existing checkpoint.\n                ),\n            )\n\n    @classmethod\n    def continuous(cls) -&gt; None:\n        \"\"\"Trains the model in one continuous path without stopping.\"\"\"\n        logging.info(\"Running continuous()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.CONTINUOUS)\n        trainer = cls.setup_trainer(Mode.CONTINUOUS)\n        with distributed_model_parallel_state():\n            llm.train(model=model, data=data, trainer=trainer, log=cls.nemo_logger, optim=opt)\n\n    @classmethod\n    def run_stop_and_go(cls):\n        \"\"\"Executes training both continuously and with a checkpoint interruption.\"\"\"\n        # Interrupted model training\n        cls.stop()\n        cls.resume()\n\n        # Cleanup and reinitialize the temporary directory so we don't conflict with a previous checkpoint.\n        cls.tempdir.cleanup()\n        cls.tempdir = tempfile.TemporaryDirectory()\n\n        # Continuous model training.\n        cls.continuous()\n\n    @pytest.mark.parametrize(\n        \"callback_type\",\n        [\n            testing_callbacks.LearningRateCallback,\n            testing_callbacks.GlobalStepStateCallback,\n            testing_callbacks.ConsumedSamplesCallback,\n            testing_callbacks.OptimizerStateCallback,\n            testing_callbacks.TrainInputCallback,\n            testing_callbacks.TrainOutputCallback,\n            testing_callbacks.TrainLossCallback,\n            testing_callbacks.ValidInputCallback,\n            testing_callbacks.ValidOutputCallback,\n            testing_callbacks.ValidLossCallback,\n        ],\n    )\n    def test_stop_and_go_consistency(self, callback_type):\n        \"\"\"Tests the consistency of the callback data between the interrupted and continuous checks.\"\"\"\n        interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n        continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n        assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n        if callback_type in {testing_callbacks.TrainOutputCallback, testing_callbacks.ValidOutputCallback}:\n            atol, rtol = self.output_tensor_atol, self.output_tensor_rtol\n        else:\n            atol, rtol = 1e-4, 1e-4\n\n        recursive_assert_approx_equal(\n            interrupted_callback.data,\n            continuous_callback.data,\n            atol=atol,\n            rtol=rtol,\n        )\n\n    def test_train_val_init_consumed_samples(self):\n        \"\"\"Tests the initial consumed samples in stop-and-go scenario.\"\"\"\n        train_consumed_stop, val_consumed_stop = get_callback(\n            self.callbacks, Mode.STOP, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        ).data\n        train_consumed_go, val_consumed_go = get_callback(\n            self.callbacks, Mode.RESUME, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        ).data\n\n        assert val_consumed_stop == 0\n        assert val_consumed_go == 0\n        assert train_consumed_stop == 0\n        assert train_consumed_go &gt; 0\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.continuous","title":"<code>continuous()</code>  <code>classmethod</code>","text":"<p>Trains the model in one continuous path without stopping.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef continuous(cls) -&gt; None:\n    \"\"\"Trains the model in one continuous path without stopping.\"\"\"\n    logging.info(\"Running continuous()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.CONTINUOUS)\n    trainer = cls.setup_trainer(Mode.CONTINUOUS)\n    with distributed_model_parallel_state():\n        llm.train(model=model, data=data, trainer=trainer, log=cls.nemo_logger, optim=opt)\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.get_default_callbacks","title":"<code>get_default_callbacks()</code>  <code>classmethod</code>","text":"<p>Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.</p> <p>To extend this method, call the super and append to the callbacks, depending on which mode you are in:</p> <pre><code>callbacks = super().get_callbacks()\ncallbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\nreturn callbacks\n</code></pre> <p>Returns:</p> Type Description <code>CallbackDict</code> <p>A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback</p> <code>CallbackDict</code> <p>object.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef get_default_callbacks(cls) -&gt; CallbackDict:\n    \"\"\"Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.\n\n    To extend this method, call the super and append to the callbacks, depending on which mode you are in:\n\n    ```python\n    callbacks = super().get_callbacks()\n    callbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\n    return callbacks\n    ```\n\n    Returns:\n        A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback\n        object.\n    \"\"\"\n    callbacks: CallbackDict = {}\n\n    def make_callbacks() -&gt; Dict[Type[pl.Callback], pl.Callback]:\n        return {\n            testing_callbacks.LearningRateCallback: testing_callbacks.LearningRateCallback(),\n            testing_callbacks.GlobalStepStateCallback: testing_callbacks.GlobalStepStateCallback(),\n            testing_callbacks.ConsumedSamplesCallback: testing_callbacks.ConsumedSamplesCallback(),\n            testing_callbacks.OptimizerStateCallback: testing_callbacks.OptimizerStateCallback(),\n            testing_callbacks.TrainInputCallback: testing_callbacks.TrainInputCallback(),\n            testing_callbacks.TrainOutputCallback: testing_callbacks.TrainOutputCallback(),\n            testing_callbacks.TrainLossCallback: testing_callbacks.TrainLossCallback(),\n            testing_callbacks.ValidInputCallback: testing_callbacks.ValidInputCallback(),\n            testing_callbacks.ValidOutputCallback: testing_callbacks.ValidOutputCallback(),\n            testing_callbacks.ValidLossCallback: testing_callbacks.ValidLossCallback(),\n        }\n\n    interrupted_callbacks = make_callbacks()\n    callbacks[Mode.CONTINUOUS] = make_callbacks()\n\n    for mode in [Mode.STOP, Mode.RESUME]:\n        consumed_samples_cls = testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        callbacks[mode] = {\n            consumed_samples_cls: consumed_samples_cls(mode=mode),\n            **interrupted_callbacks,\n        }\n\n    callbacks[Mode.STOP].update(\n        {\n            testing_callbacks.StopAfterValidEpochEndCallback: testing_callbacks.StopAfterValidEpochEndCallback(),\n            nl_callbacks.ModelCheckpoint: nl_callbacks.ModelCheckpoint(\n                save_last=True,\n                monitor=\"val_loss\",\n                save_top_k=2,\n                always_save_context=True,\n                filename=\"{epoch}-{step}-{val_loss:.2f}\",\n            ),\n        }\n    )\n\n    return callbacks\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.resume","title":"<code>resume()</code>  <code>classmethod</code>","text":"<p>Resumes the model from the checkpoint saved at the end of <code>stop()</code> and verifies the metadata integrity.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef resume(cls) -&gt; None:\n    \"\"\"Resumes the model from the checkpoint saved at the end of `stop()` and verifies the metadata integrity.\"\"\"\n    logging.info(\"Running resume()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.RESUME)\n    trainer = cls.setup_trainer(Mode.RESUME)\n    with distributed_model_parallel_state():\n        llm.train(\n            model=model,\n            data=data,\n            trainer=trainer,\n            log=cls.nemo_logger,\n            optim=opt,\n            resume=resume.AutoResume(\n                resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n                resume_ignore_no_checkpoint=False,  # When false this will throw an error with no existing checkpoint.\n            ),\n        )\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.run_stop_and_go","title":"<code>run_stop_and_go()</code>  <code>classmethod</code>","text":"<p>Executes training both continuously and with a checkpoint interruption.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef run_stop_and_go(cls):\n    \"\"\"Executes training both continuously and with a checkpoint interruption.\"\"\"\n    # Interrupted model training\n    cls.stop()\n    cls.resume()\n\n    # Cleanup and reinitialize the temporary directory so we don't conflict with a previous checkpoint.\n    cls.tempdir.cleanup()\n    cls.tempdir = tempfile.TemporaryDirectory()\n\n    # Continuous model training.\n    cls.continuous()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_class","title":"<code>setup_class()</code>  <code>classmethod</code>","text":"<p>Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef setup_class(cls) -&gt; None:\n    \"\"\"Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.\"\"\"\n    cls.tempdir = tempfile.TemporaryDirectory()\n    cls.metadata_dir = pathlib.Path(cls.tempdir.name) / \"metadata\"\n    cls.exp_name = cls.__name__\n\n    cls.callbacks = cls.get_default_callbacks()\n\n    cls.nemo_logger = NeMoLogger(\n        log_dir=cls.tempdir.name,\n        name=cls.exp_name,\n        use_datetime_version=False,\n        version=None,\n        tensorboard=None,\n        wandb=None,\n        ckpt=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_model","title":"<code>setup_model(mode)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs the model, data, and optimizer for the test harness.</p> <p>Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged to use the same code path for both.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop or go.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[LightningModule, LightningDataModule, MegatronOptimizerModule]</code> <p>A tuple containing the model, data, and optimizer.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\n@abstractmethod\ndef setup_model(cls, mode: Mode) -&gt; tuple[pl.LightningModule, pl.LightningDataModule, nl.MegatronOptimizerModule]:\n    \"\"\"Constructs the model, data, and optimizer for the test harness.\n\n    Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged\n    to use the same code path for both.\n\n    Args:\n        mode: The mode indicating whether to stop or go.\n\n    Returns:\n        tuple: A tuple containing the model, data, and optimizer.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_trainer","title":"<code>setup_trainer(mode)</code>  <code>classmethod</code>","text":"<p>Setup trainer by passing stop, resume, or continuous callbacks according to mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop, resume, or train continuously.</p> required <p>Returns:</p> Type Description <code>Trainer</code> <p>NeMo Lightning trainer object.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef setup_trainer(\n    cls,\n    mode: Mode,\n) -&gt; nl.Trainer:\n    \"\"\"Setup trainer by passing stop, resume, or continuous callbacks according to mode.\n\n    Args:\n        mode (Mode): The mode indicating whether to stop, resume, or train continuously.\n\n    Returns:\n        (nl.Trainer): NeMo Lightning trainer object.\n    \"\"\"\n    strategy = MegatronStrategy(\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n        ckpt_async_save=False,\n    )\n\n    trainer = nl.Trainer(\n        devices=1,\n        max_steps=cls.num_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        limit_val_batches=cls.limit_val_batches,\n        val_check_interval=cls.val_check_interval,\n        log_every_n_steps=cls.val_check_interval,\n        num_nodes=1,\n        callbacks=list(cls.callbacks[mode].values()),\n        plugins=nl.MegatronMixedPrecision(precision=cls.precision),\n    )\n    return trainer\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.stop","title":"<code>stop()</code>  <code>classmethod</code>","text":"<p>Runs pre-training and 'stops' after the first checkpoint is saved.</p> <p>This method sets up the model, data, and optimizer for the Mode.STOP mode. It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics. The training process is executed using the <code>llm.train</code> function, passing the model, data, trainer, logger, optimizer, and resume options. If a <code>testing_callbacks.StopAndGoException</code> is raised during training, it is caught and no action is taken.</p> <p>Raises:</p> Type Description <code>StopAndGoException</code> <p>If a stop and go exception occurs during training.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef stop(cls) -&gt; None:\n    \"\"\"Runs pre-training and 'stops' after the first checkpoint is saved.\n\n    This method sets up the model, data, and optimizer for the Mode.STOP mode.\n    It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics.\n    The training process is executed using the `llm.train` function, passing the model, data, trainer, logger, optimizer, and resume options.\n    If a `testing_callbacks.StopAndGoException` is raised during training, it is caught and no action is taken.\n\n    Raises:\n        testing_callbacks.StopAndGoException: If a stop and go exception occurs during training.\n    \"\"\"\n    logging.info(\"Running stop()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.STOP)\n    trainer = cls.setup_trainer(Mode.STOP)\n    with distributed_model_parallel_state():\n        llm.train(\n            model=model,\n            data=data,\n            trainer=trainer,\n            log=cls.nemo_logger,\n            optim=opt,\n            resume=resume.AutoResume(\n                resume_if_exists=False,  # Looks for the -last checkpoint to continue training.\n                resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n            ),\n        )\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.teardown_class","title":"<code>teardown_class()</code>  <code>classmethod</code>","text":"<p>Tears down the class by cleaning up the temporary directory.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef teardown_class(cls) -&gt; None:\n    \"\"\"Tears down the class by cleaning up the temporary directory.\"\"\"\n    cls.tempdir.cleanup()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_stop_and_go_consistency","title":"<code>test_stop_and_go_consistency(callback_type)</code>","text":"<p>Tests the consistency of the callback data between the interrupted and continuous checks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@pytest.mark.parametrize(\n    \"callback_type\",\n    [\n        testing_callbacks.LearningRateCallback,\n        testing_callbacks.GlobalStepStateCallback,\n        testing_callbacks.ConsumedSamplesCallback,\n        testing_callbacks.OptimizerStateCallback,\n        testing_callbacks.TrainInputCallback,\n        testing_callbacks.TrainOutputCallback,\n        testing_callbacks.TrainLossCallback,\n        testing_callbacks.ValidInputCallback,\n        testing_callbacks.ValidOutputCallback,\n        testing_callbacks.ValidLossCallback,\n    ],\n)\ndef test_stop_and_go_consistency(self, callback_type):\n    \"\"\"Tests the consistency of the callback data between the interrupted and continuous checks.\"\"\"\n    interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n    continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n    assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n    if callback_type in {testing_callbacks.TrainOutputCallback, testing_callbacks.ValidOutputCallback}:\n        atol, rtol = self.output_tensor_atol, self.output_tensor_rtol\n    else:\n        atol, rtol = 1e-4, 1e-4\n\n    recursive_assert_approx_equal(\n        interrupted_callback.data,\n        continuous_callback.data,\n        atol=atol,\n        rtol=rtol,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_train_val_init_consumed_samples","title":"<code>test_train_val_init_consumed_samples()</code>","text":"<p>Tests the initial consumed samples in stop-and-go scenario.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>def test_train_val_init_consumed_samples(self):\n    \"\"\"Tests the initial consumed samples in stop-and-go scenario.\"\"\"\n    train_consumed_stop, val_consumed_stop = get_callback(\n        self.callbacks, Mode.STOP, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n    ).data\n    train_consumed_go, val_consumed_go = get_callback(\n        self.callbacks, Mode.RESUME, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n    ).data\n\n    assert val_consumed_stop == 0\n    assert val_consumed_go == 0\n    assert train_consumed_stop == 0\n    assert train_consumed_go &gt; 0\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.get_callback","title":"<code>get_callback(callbacks, mode, callback_type)</code>","text":"<p>Returns the callback with the given name and mode.</p> <p>Convenience function to make type hinting easier.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>CallbackDict</code> <p>The dictionary of callbacks.</p> required <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop or go.</p> required <code>callback_type</code> <code>Type[Callback]</code> <p>The type of the callback.</p> required <p>Returns:</p> Type Description <code>Callback</code> <p>pl.Callback: The callback with the given name and mode.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>def get_callback(callbacks: CallbackDict, mode: Mode, callback_type: Type[Callback]) -&gt; Callback:\n    \"\"\"Returns the callback with the given name and mode.\n\n    Convenience function to make type hinting easier.\n\n    Args:\n        callbacks: The dictionary of callbacks.\n        mode: The mode indicating whether to stop or go.\n        callback_type: The type of the callback.\n\n    Returns:\n        pl.Callback: The callback with the given name and mode.\n    \"\"\"\n    return callbacks[mode][callback_type]  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS","title":"<code>PickledDataWDS</code>","text":"<p>               Bases: <code>WebDataModule</code></p> <p>A LightningDataModule to process pickled data into webdataset tar files.</p> <p><code>PickledDataWDS</code> is a LightningDataModule to process pickled data into webdataset tar files and setup dataset and dataloader. This inherits the webdataset setup from its parent module <code>WebDataModule</code>. This data module takes a directory of pickled data files, data filename prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files by globbing the specific pickle data files <code>{dir_pickles}/{name_subset[split]}.{suffix_pickles}</code> and outputing to webdataset tar file with the dict structure: <pre><code>    {\"__key__\" : name.replace(\".\", \"-\"),\n     suffix_pickles : pickled.dumps(data) }\n</code></pre> NOTE: this assumes only one pickled file is processed for each sample. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS--examples","title":"Examples:","text":"<ol> <li>create the data module with a directory of pickle files and the file name prefix thereof for different splits to used by <code>Lightning.Trainer.fit()</code></li> </ol> <pre><code>&gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n&gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n&gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n&gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n&gt;&gt;&gt; # validation dataset\n\n&gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n&gt;&gt;&gt; names_subset = {\n&gt;&gt;&gt;     Split.train: [sample1, sample2],\n&gt;&gt;&gt;     Split.val: [sample4, sample5],\n&gt;&gt;&gt; }\n\n&gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n&gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n&gt;&gt;&gt; n_tars_wds = 5\n&gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n&gt;&gt;&gt; output_dir_tar_files = {\n        Split.train : \"/path/to/output/tars/dir-train\",\n        Split.val : \"/path/to/output/tars/dir-val\",\n        Split.test : \"/path/to/output/tars/dir-test\",\n    }\n\n&gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n&gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n&gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n&gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n&gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; # create the data module\n&gt;&gt;&gt; data_module = PickledDataWDS(\n&gt;&gt;&gt;     dir_pickles,\n&gt;&gt;&gt;     names_subset,\n&gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n&gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n&gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n&gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wds=invoke_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wld=invoke_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class PickledDataWDS(WebDataModule):\n    \"\"\"A LightningDataModule to process pickled data into webdataset tar files.\n\n    `PickledDataWDS` is a LightningDataModule to process pickled data into webdataset tar files\n    and setup dataset and dataloader. This inherits the webdataset setup from its parent module\n    `WebDataModule`. This data module takes a directory of pickled data files, data filename\n    prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files\n    by globbing the specific pickle data files `{dir_pickles}/{name_subset[split]}.{suffix_pickles}`\n    and outputing to webdataset tar file with the dict structure:\n    ```\n        {\"__key__\" : name.replace(\".\", \"-\"),\n         suffix_pickles : pickled.dumps(data) }\n    ```\n    NOTE: this assumes only one pickled file is processed for each sample. In\n    its setup() function, it creates the webdataset object chaining up the input\n    `pipeline_wds` workflow. In its train/val/test_dataloader(), it creates the\n    WebLoader object chaining up the `pipeline_prebatch_wld` workflow.\n\n    Examples:\n    --------\n    1. create the data module with a directory of pickle files and the file name\n    prefix thereof for different splits to used by `Lightning.Trainer.fit()`\n\n    ```python\n    &gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n    &gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n    &gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n    &gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n    &gt;&gt;&gt; # validation dataset\n\n    &gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n    &gt;&gt;&gt; names_subset = {\n    &gt;&gt;&gt;     Split.train: [sample1, sample2],\n    &gt;&gt;&gt;     Split.val: [sample4, sample5],\n    &gt;&gt;&gt; }\n\n    &gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n    &gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n    &gt;&gt;&gt; n_tars_wds = 5\n    &gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n    &gt;&gt;&gt; output_dir_tar_files = {\n            Split.train : \"/path/to/output/tars/dir-train\",\n            Split.val : \"/path/to/output/tars/dir-val\",\n            Split.test : \"/path/to/output/tars/dir-test\",\n        }\n\n    &gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n    &gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n    &gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n    &gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n    &gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; invoke_wds = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; invoke_wld = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; # create the data module\n    &gt;&gt;&gt; data_module = PickledDataWDS(\n    &gt;&gt;&gt;     dir_pickles,\n    &gt;&gt;&gt;     names_subset,\n    &gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n    &gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n    &gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n    &gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     invoke_wds=invoke_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     invoke_wld=invoke_wld, # `WebDataModule` kwargs\n    &gt;&gt;&gt; )\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        dir_pickles: str,\n        names_subset: Dict[Split, List[str]],\n        *args,\n        n_tars_wds: Optional[int] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Args:\n            dir_pickles: input directory of pickled data files\n            names_subset: list of filename prefix of\n                the data samples to be loaded in the dataset and dataloader for\n                each of the split\n            *args: arguments passed to the parent WebDataModule\n            n_tars_wds: attempt to create at least this number of\n                webdataset shards\n            **kwargs: arguments passed to the parent WebDataModule\n        \"\"\"\n        super().__init__(\n            *args,\n            **kwargs,\n        )\n\n        self._dir_pickles = dir_pickles\n\n        self._names_subset = names_subset\n\n        self._n_tars_wds = n_tars_wds\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"This is called only by the main process by the Lightning workflow.\n\n        Do not rely on this data module object's state update here as there is no\n        way to communicate the state update to other subprocesses. The nesting\n        `pickles_to_tars` function goes through the data name prefixes in the\n        different splits, read the corresponding pickled file and output a\n        webdataset tar archive with the dict structure: {\"__key__\" :\n        name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.\n        \"\"\"\n        for split in self._names_subset.keys():\n            # create wds shards (tar files) for train set\n            pickles_to_tars(\n                self._dir_pickles,\n                self._names_subset[split],\n                self._suffix_keys_wds,\n                self._dirs_tars_wds[split],\n                self._prefix_tars_wds,\n                min_num_shards=self._n_tars_wds,\n            )\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS.__init__","title":"<code>__init__(dir_pickles, names_subset, *args, n_tars_wds=None, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dir_pickles</code> <code>str</code> <p>input directory of pickled data files</p> required <code>names_subset</code> <code>Dict[Split, List[str]]</code> <p>list of filename prefix of the data samples to be loaded in the dataset and dataloader for each of the split</p> required <code>*args</code> <p>arguments passed to the parent WebDataModule</p> <code>()</code> <code>n_tars_wds</code> <code>Optional[int]</code> <p>attempt to create at least this number of webdataset shards</p> <code>None</code> <code>**kwargs</code> <p>arguments passed to the parent WebDataModule</p> <code>{}</code> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def __init__(\n    self,\n    dir_pickles: str,\n    names_subset: Dict[Split, List[str]],\n    *args,\n    n_tars_wds: Optional[int] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Args:\n        dir_pickles: input directory of pickled data files\n        names_subset: list of filename prefix of\n            the data samples to be loaded in the dataset and dataloader for\n            each of the split\n        *args: arguments passed to the parent WebDataModule\n        n_tars_wds: attempt to create at least this number of\n            webdataset shards\n        **kwargs: arguments passed to the parent WebDataModule\n    \"\"\"\n    super().__init__(\n        *args,\n        **kwargs,\n    )\n\n    self._dir_pickles = dir_pickles\n\n    self._names_subset = names_subset\n\n    self._n_tars_wds = n_tars_wds\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. The nesting <code>pickles_to_tars</code> function goes through the data name prefixes in the different splits, read the corresponding pickled file and output a webdataset tar archive with the dict structure: {\"key\" : name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"This is called only by the main process by the Lightning workflow.\n\n    Do not rely on this data module object's state update here as there is no\n    way to communicate the state update to other subprocesses. The nesting\n    `pickles_to_tars` function goes through the data name prefixes in the\n    different splits, read the corresponding pickled file and output a\n    webdataset tar archive with the dict structure: {\"__key__\" :\n    name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.\n    \"\"\"\n    for split in self._names_subset.keys():\n        # create wds shards (tar files) for train set\n        pickles_to_tars(\n            self._dir_pickles,\n            self._names_subset[split],\n            self._suffix_keys_wds,\n            self._dirs_tars_wds[split],\n            self._prefix_tars_wds,\n            min_num_shards=self._n_tars_wds,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.Split","title":"<code>Split</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Names for each data split.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class Split(Enum):\n    \"\"\"Names for each data split.\"\"\"\n\n    train = auto()\n    val = auto()\n    test = auto()\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule","title":"<code>WebDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for using webdataset tar files.</p> <p><code>WebDataModule</code> is a <code>LightningDataModule</code> for using webdataset tar files to setup PyTorch datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file directory and vaiours webdataset config settings. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule--examples","title":"Examples:","text":"<ol> <li> <p>create the data module with input directory to webdataset tar files. Depending on which of the downstream Lightning.Trainer methods are called, e.g., <code>Trainer.fit()</code>, <code>Trainer.validate()</code>, <code>Trainer.test()</code> or <code>Trainer.predict()</code>, only a subset of the train, val and test splits need to be specified in the various input options to the data module:</p> </li> <li> <p><code>Trainer.fit()</code> requires the <code>train</code> and <code>val</code> splits</p> </li> <li><code>Trainer.validate()</code> requires the <code>val</code> split</li> <li><code>Trainer.test()</code> requires the <code>test</code> splits</li> <li><code>Trainer.predict()</code> requires the <code>test</code> splits</li> </ol> <p>Here is an example of constructing the data module for <code>Trainer.fit()</code>: <pre><code>&gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; tar_file_prefix = \"shards\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; dirs_of_tar_files = {\n&gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n&gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; n_samples {\n&gt;&gt;&gt;     Split.train: 1000,\n&gt;&gt;&gt;     Split.val: 100,\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n&gt;&gt;&gt; # webdataset file (see\n&gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n&gt;&gt;&gt; # for details)\n&gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; seed = 27193781\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n&gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n&gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n&gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n&gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n&gt;&gt;&gt; # for details.\n&gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n&gt;&gt;&gt; # file parsing rule.\n&gt;&gt;&gt;\n&gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n&gt;&gt;&gt;\n&gt;&gt;&gt; from webdatast import shuffle\n&gt;&gt;&gt; pipeline_wds = {\n&gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n&gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n&gt;&gt;&gt;     Split.val: untuple\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n&gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n&gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n&gt;&gt;&gt; # user can customize their batching routines here\n&gt;&gt;&gt;\n&gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                    list_samples : torch.vstack(list_samples))\n&gt;&gt;&gt; pipeline_prebatch_wld = {\n        Split.train: [shuffle(n_samples[Split.train],\n                              rng=random.Random(seed_rng_shfl)), batch],\n        Split.val : batch,\n        Split.test : batch\n    }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n&gt;&gt;&gt; # WebLoader\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wds = {\n&gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n&gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n&gt;&gt;&gt;              'seed' : seed_rng_shfl}\n&gt;&gt;&gt;     for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wld = {\n&gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wds = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5})] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wld = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5}] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # construct the data module\n&gt;&gt;&gt; data_module = WebDataModule(suffix_keys_wds,\n                                dirs_of_tar_files,\n                                prefix_tars_wds=tar_file_prefix,\n                                pipeline_wds=pipeline_wds,\n                                pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                kwargs_wds=kwargs_wds,\n                                kwargs_wld=kwargs_wld,\n                                invoke_wds=invoke_wds,\n                                invoke_wld=invoke_wld,\n                                )\n</code></pre></p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class WebDataModule(L.LightningDataModule):\n    \"\"\"A LightningDataModule for using webdataset tar files.\n\n    `WebDataModule` is a `LightningDataModule` for using webdataset tar files to setup PyTorch\n    datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file\n    directory and vaiours webdataset config settings. In its setup() function, it creates the\n    webdataset object chaining up the input `pipeline_wds` workflow. In its train/val/test_dataloader(),\n    it creates the WebLoader object chaining up the `pipeline_prebatch_wld` workflow.\n\n    Examples:\n    --------\n    1. create the data module with input directory to webdataset tar files.\n    Depending on which of the downstream Lightning.Trainer methods are called,\n    e.g., `Trainer.fit()`, `Trainer.validate()`, `Trainer.test()` or\n    `Trainer.predict()`, only a subset of the train, val and test splits need to\n    be specified in the various input options to the data module:\n\n    - `Trainer.fit()` requires the `train` and `val` splits\n    - `Trainer.validate()` requires the `val` split\n    - `Trainer.test()` requires the `test` splits\n    - `Trainer.predict()` requires the `test` splits\n\n    Here is an example of constructing the data module for `Trainer.fit()`:\n    ```python\n    &gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; tar_file_prefix = \"shards\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; dirs_of_tar_files = {\n    &gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n    &gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; n_samples {\n    &gt;&gt;&gt;     Split.train: 1000,\n    &gt;&gt;&gt;     Split.val: 100,\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n    &gt;&gt;&gt; # webdataset file (see\n    &gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n    &gt;&gt;&gt; # for details)\n    &gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; seed = 27193781\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n    &gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n    &gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n    &gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n    &gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n    &gt;&gt;&gt; # for details.\n    &gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n    &gt;&gt;&gt; # file parsing rule.\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; from webdatast import shuffle\n    &gt;&gt;&gt; pipeline_wds = {\n    &gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n    &gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n    &gt;&gt;&gt;     Split.val: untuple\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n    &gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n    &gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n    &gt;&gt;&gt; # user can customize their batching routines here\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                        list_samples : torch.vstack(list_samples))\n    &gt;&gt;&gt; pipeline_prebatch_wld = {\n            Split.train: [shuffle(n_samples[Split.train],\n                                  rng=random.Random(seed_rng_shfl)), batch],\n            Split.val : batch,\n            Split.test : batch\n        }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n    &gt;&gt;&gt; # WebLoader\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; kwargs_wds = {\n    &gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n    &gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n    &gt;&gt;&gt;              'seed' : seed_rng_shfl}\n    &gt;&gt;&gt;     for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; kwargs_wld = {\n    &gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; invoke_wds = {\n    &gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5})] for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; invoke_wld = {\n    &gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5}] for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # construct the data module\n    &gt;&gt;&gt; data_module = WebDataModule(suffix_keys_wds,\n                                    dirs_of_tar_files,\n                                    prefix_tars_wds=tar_file_prefix,\n                                    pipeline_wds=pipeline_wds,\n                                    pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                    kwargs_wds=kwargs_wds,\n                                    kwargs_wld=kwargs_wld,\n                                    invoke_wds=invoke_wds,\n                                    invoke_wld=invoke_wld,\n                                    )\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        suffix_keys_wds: Union[str, Iterable[str]],\n        dirs_tars_wds: Dict[Split, str],\n        prefix_tars_wds: str = \"wdshards\",\n        pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n        pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n        kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n        kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n        invoke_wds: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n        invoke_wld: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n    ):\n        \"\"\"Constructor.\n\n        Args:\n            suffix_keys_wds: a set of keys each\n                corresponding to a data object in the webdataset tar file\n                dictionary. The data objects of these keys will be extracted and\n                tupled for each sample in the tar files\n            dirs_tars_wds: input dictionary: Split -&gt; tar file\n                directory that contains the webdataset tar files for each split\n        Kwargs:\n            prefix_tars_wds: name prefix of the input webdataset tar\n                files. The input tar files are globbed by\n                \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"\n            pipeline_wds: a dictionary of webdatast composable, i.e.,\n                functor that maps a iterator to another iterator that\n                transforms the data sample yield from the dataset object, for\n                different splits, or an iterable to such a sequence of such\n                iterators. For example, this can be used to transform the\n                sample in the worker before sending it to the main process of\n                the dataloader\n            pipeline_prebatch_wld: a dictionary\n                of webloader composable, i.e., functor that maps a iterator to\n                another iterator that transforms the data sample yield from the\n                WebLoader object, for different splits, or an iterable to a\n                seuqnence of such iterators. For example, this can be used for\n                batching the samples. NOTE: this is applied before batching is\n                yield from the WebLoader\n            kwargs_wds: kwargs for the WebDataset.__init__()\n            kwargs_wld : kwargs for the WebLoader.__init__(), e.g., num_workers, of each split\n            invoke_wds: a dictionary of WebDataset methods to be called upon WebDataset\n                construction. These methods must return the WebDataset object itself. Examples\n                are .with_length() and .with_epoch(). These methods will be applied towards\n                the end of returning the WebDataset object, i.e., after the pipline_wds\n                have been applied. The inner list of tuples each has its first element as the\n                method name and the second element as the corresponding method's kwargs.\n            invoke_wld: a dictionary of WebLoader methods to be called upon WebLoader\n                construction. These methods must return the WebLoader object itself. Examples\n                are .with_length() and .with_epoch(). These methods will be applied towards\n                the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld\n                have been applied. The inner list of tuples each has its first element as the\n                method name and the second element as the corresponding method's kwargs.\n        \"\"\"\n        super().__init__()\n\n        self._dirs_tars_wds = dirs_tars_wds\n\n        if not isinstance(suffix_keys_wds, get_args(Union[str, Iterable])):\n            raise TypeError(\"suffix_keys_wds can only be str or Iterable[str]\")\n\n        self._suffix_keys_wds = suffix_keys_wds\n\n        self._prefix_tars_wds = prefix_tars_wds\n        self._pipeline_wds = pipeline_wds\n        self._pipeline_prebatch_wld = pipeline_prebatch_wld\n\n        self._kwargs_wld = kwargs_wld\n\n        self._kwargs_wds = kwargs_wds\n\n        self._invoke_wds = invoke_wds\n        self._invoke_wld = invoke_wld\n\n        # to be created later in setup\n        self._dataset = {}\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"This is called only by the main process by the Lightning workflow.\n\n        Do not rely on this data module object's state update here as there is no\n        way to communicate the state update to other subprocesses. Is a **no-op**.\n        \"\"\"\n        pass\n\n    def _setup_wds(self, split: Split) -&gt; wds.WebDataset:\n        \"\"\"Setup webdataset and webloader. This is called by setup().\n\n        Args:\n            split (Split): train, val or test split\n\n        Returns:\n            WebDataset\n\n        \"\"\"\n        if split not in self._dirs_tars_wds.keys():\n            raise RuntimeError(f\"_setup_wds() is called with {split} \" f\"split that doesn't have the input tar dir\")\n        urls = sorted(glob.glob(f\"{self._dirs_tars_wds[split]}/{self._prefix_tars_wds}-*.tar\"))\n        kwargs = self._kwargs_wds[split] if self._kwargs_wds is not None else None\n        dataset = wds.WebDataset(urls, **(kwargs if kwargs is not None else {})).decode()\n        if isinstance(self._suffix_keys_wds, str):\n            dataset = dataset.extract_keys(f\"*.{self._suffix_keys_wds}\")\n        else:\n            dataset = dataset.extract_keys(*[f\"*.{key}\" for key in self._suffix_keys_wds])\n\n        if self._pipeline_wds is not None and self._pipeline_wds[split] is not None:\n            if isinstance(self._pipeline_wds[split], Iterable):\n                dataset = dataset.compose(*self._pipeline_wds[split])\n            else:\n                dataset = dataset.compose(self._pipeline_wds[split])\n\n        if self._invoke_wds is not None and self._invoke_wds[split] is not None:\n            for method in self._invoke_wds[split]:\n                name_method, kwargs_method = method\n                dataset = getattr(dataset, name_method)(**kwargs_method)\n        return dataset\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"This is called on all Lightning-managed nodes in a multi-node training session.\n\n        Args:\n            stage: \"fit\", \"test\" or \"predict\"\n        \"\"\"\n        if stage == \"fit\":\n            self._dataset[Split.train] = self._setup_wds(Split.train)\n            self._dataset[Split.val] = self._setup_wds(Split.val)\n        elif stage == \"validate\":\n            self._dataset[Split.val] = self._setup_wds(Split.val)\n        elif stage == \"test\":\n            self._dataset[Split.test] = self._setup_wds(Split.test)\n        elif stage == \"predict\":\n            self._dataset[Split.test] = self._setup_wds(Split.test)\n        else:\n            raise NotImplementedError(f\"Data setup with {stage=} is not implemented.\")\n\n    def _setup_dataloader(self, split: Split) -&gt; wds.WebLoader:\n        \"\"\"Setup the dataloader for the input dataset split.\n\n        Args:\n            split (Split): input split type\n\n        Returns:\n             WebLoader object\n\n        Raises:\n            ValueError if `split` doesn't correspond to a known dataset.\n        \"\"\"\n        if self._dataset[split] is None:\n            raise ValueError(\n                f\"_setup_dataloader() is called with {split} split without setting up the corresponding dataset.\"\n            )\n        dataset = self._dataset[split]\n        kwargs = self._kwargs_wld[split] if self._kwargs_wld is not None else None\n        loader = wds.WebLoader(dataset, **(kwargs if kwargs is not None else {}))\n\n        if self._pipeline_prebatch_wld is not None and self._pipeline_prebatch_wld[split] is not None:\n            if isinstance(self._pipeline_prebatch_wld[split], Iterable):\n                loader = loader.compose(*self._pipeline_prebatch_wld[split])\n            else:\n                loader = loader.compose(self._pipeline_prebatch_wld[split])\n\n        if self._invoke_wld is not None and self._invoke_wld[split] is not None:\n            for method in self._invoke_wld[split]:\n                name_method, kwargs_method = method\n                loader = getattr(loader, name_method)(**kwargs_method)\n\n        return loader\n\n    def train_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the training data.\"\"\"\n        return self._setup_dataloader(Split.train)\n\n    def val_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the validation data.\"\"\"\n        return self._setup_dataloader(Split.val)\n\n    def test_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the test data.\"\"\"\n        return self._setup_dataloader(Split.test)\n\n    def predict_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Alias for :func:`test_dataloader`.\"\"\"\n        return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.__init__","title":"<code>__init__(suffix_keys_wds, dirs_tars_wds, prefix_tars_wds='wdshards', pipeline_wds=None, pipeline_prebatch_wld=None, kwargs_wds=None, kwargs_wld=None, invoke_wds=None, invoke_wld=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>suffix_keys_wds</code> <code>Union[str, Iterable[str]]</code> <p>a set of keys each corresponding to a data object in the webdataset tar file dictionary. The data objects of these keys will be extracted and tupled for each sample in the tar files</p> required <code>dirs_tars_wds</code> <code>Dict[Split, str]</code> <p>input dictionary: Split -&gt; tar file directory that contains the webdataset tar files for each split</p> required <p>Kwargs:     prefix_tars_wds: name prefix of the input webdataset tar         files. The input tar files are globbed by         \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"     pipeline_wds: a dictionary of webdatast composable, i.e.,         functor that maps a iterator to another iterator that         transforms the data sample yield from the dataset object, for         different splits, or an iterable to such a sequence of such         iterators. For example, this can be used to transform the         sample in the worker before sending it to the main process of         the dataloader     pipeline_prebatch_wld: a dictionary         of webloader composable, i.e., functor that maps a iterator to         another iterator that transforms the data sample yield from the         WebLoader object, for different splits, or an iterable to a         seuqnence of such iterators. For example, this can be used for         batching the samples. NOTE: this is applied before batching is         yield from the WebLoader     kwargs_wds: kwargs for the WebDataset.init()     kwargs_wld : kwargs for the WebLoader.init(), e.g., num_workers, of each split     invoke_wds: a dictionary of WebDataset methods to be called upon WebDataset         construction. These methods must return the WebDataset object itself. Examples         are .with_length() and .with_epoch(). These methods will be applied towards         the end of returning the WebDataset object, i.e., after the pipline_wds         have been applied. The inner list of tuples each has its first element as the         method name and the second element as the corresponding method's kwargs.     invoke_wld: a dictionary of WebLoader methods to be called upon WebLoader         construction. These methods must return the WebLoader object itself. Examples         are .with_length() and .with_epoch(). These methods will be applied towards         the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld         have been applied. The inner list of tuples each has its first element as the         method name and the second element as the corresponding method's kwargs.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def __init__(\n    self,\n    suffix_keys_wds: Union[str, Iterable[str]],\n    dirs_tars_wds: Dict[Split, str],\n    prefix_tars_wds: str = \"wdshards\",\n    pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n    pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n    kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n    kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n    invoke_wds: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n    invoke_wld: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n):\n    \"\"\"Constructor.\n\n    Args:\n        suffix_keys_wds: a set of keys each\n            corresponding to a data object in the webdataset tar file\n            dictionary. The data objects of these keys will be extracted and\n            tupled for each sample in the tar files\n        dirs_tars_wds: input dictionary: Split -&gt; tar file\n            directory that contains the webdataset tar files for each split\n    Kwargs:\n        prefix_tars_wds: name prefix of the input webdataset tar\n            files. The input tar files are globbed by\n            \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"\n        pipeline_wds: a dictionary of webdatast composable, i.e.,\n            functor that maps a iterator to another iterator that\n            transforms the data sample yield from the dataset object, for\n            different splits, or an iterable to such a sequence of such\n            iterators. For example, this can be used to transform the\n            sample in the worker before sending it to the main process of\n            the dataloader\n        pipeline_prebatch_wld: a dictionary\n            of webloader composable, i.e., functor that maps a iterator to\n            another iterator that transforms the data sample yield from the\n            WebLoader object, for different splits, or an iterable to a\n            seuqnence of such iterators. For example, this can be used for\n            batching the samples. NOTE: this is applied before batching is\n            yield from the WebLoader\n        kwargs_wds: kwargs for the WebDataset.__init__()\n        kwargs_wld : kwargs for the WebLoader.__init__(), e.g., num_workers, of each split\n        invoke_wds: a dictionary of WebDataset methods to be called upon WebDataset\n            construction. These methods must return the WebDataset object itself. Examples\n            are .with_length() and .with_epoch(). These methods will be applied towards\n            the end of returning the WebDataset object, i.e., after the pipline_wds\n            have been applied. The inner list of tuples each has its first element as the\n            method name and the second element as the corresponding method's kwargs.\n        invoke_wld: a dictionary of WebLoader methods to be called upon WebLoader\n            construction. These methods must return the WebLoader object itself. Examples\n            are .with_length() and .with_epoch(). These methods will be applied towards\n            the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld\n            have been applied. The inner list of tuples each has its first element as the\n            method name and the second element as the corresponding method's kwargs.\n    \"\"\"\n    super().__init__()\n\n    self._dirs_tars_wds = dirs_tars_wds\n\n    if not isinstance(suffix_keys_wds, get_args(Union[str, Iterable])):\n        raise TypeError(\"suffix_keys_wds can only be str or Iterable[str]\")\n\n    self._suffix_keys_wds = suffix_keys_wds\n\n    self._prefix_tars_wds = prefix_tars_wds\n    self._pipeline_wds = pipeline_wds\n    self._pipeline_prebatch_wld = pipeline_prebatch_wld\n\n    self._kwargs_wld = kwargs_wld\n\n    self._kwargs_wds = kwargs_wds\n\n    self._invoke_wds = invoke_wds\n    self._invoke_wld = invoke_wld\n\n    # to be created later in setup\n    self._dataset = {}\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule._setup_dataloader","title":"<code>_setup_dataloader(split)</code>","text":"<p>Setup the dataloader for the input dataset split.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>Split</code> <p>input split type</p> required <p>Returns:</p> Type Description <code>WebLoader</code> <p>WebLoader object</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def _setup_dataloader(self, split: Split) -&gt; wds.WebLoader:\n    \"\"\"Setup the dataloader for the input dataset split.\n\n    Args:\n        split (Split): input split type\n\n    Returns:\n         WebLoader object\n\n    Raises:\n        ValueError if `split` doesn't correspond to a known dataset.\n    \"\"\"\n    if self._dataset[split] is None:\n        raise ValueError(\n            f\"_setup_dataloader() is called with {split} split without setting up the corresponding dataset.\"\n        )\n    dataset = self._dataset[split]\n    kwargs = self._kwargs_wld[split] if self._kwargs_wld is not None else None\n    loader = wds.WebLoader(dataset, **(kwargs if kwargs is not None else {}))\n\n    if self._pipeline_prebatch_wld is not None and self._pipeline_prebatch_wld[split] is not None:\n        if isinstance(self._pipeline_prebatch_wld[split], Iterable):\n            loader = loader.compose(*self._pipeline_prebatch_wld[split])\n        else:\n            loader = loader.compose(self._pipeline_prebatch_wld[split])\n\n    if self._invoke_wld is not None and self._invoke_wld[split] is not None:\n        for method in self._invoke_wld[split]:\n            name_method, kwargs_method = method\n            loader = getattr(loader, name_method)(**kwargs_method)\n\n    return loader\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule._setup_wds","title":"<code>_setup_wds(split)</code>","text":"<p>Setup webdataset and webloader. This is called by setup().</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>Split</code> <p>train, val or test split</p> required <p>Returns:</p> Type Description <code>WebDataset</code> <p>WebDataset</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def _setup_wds(self, split: Split) -&gt; wds.WebDataset:\n    \"\"\"Setup webdataset and webloader. This is called by setup().\n\n    Args:\n        split (Split): train, val or test split\n\n    Returns:\n        WebDataset\n\n    \"\"\"\n    if split not in self._dirs_tars_wds.keys():\n        raise RuntimeError(f\"_setup_wds() is called with {split} \" f\"split that doesn't have the input tar dir\")\n    urls = sorted(glob.glob(f\"{self._dirs_tars_wds[split]}/{self._prefix_tars_wds}-*.tar\"))\n    kwargs = self._kwargs_wds[split] if self._kwargs_wds is not None else None\n    dataset = wds.WebDataset(urls, **(kwargs if kwargs is not None else {})).decode()\n    if isinstance(self._suffix_keys_wds, str):\n        dataset = dataset.extract_keys(f\"*.{self._suffix_keys_wds}\")\n    else:\n        dataset = dataset.extract_keys(*[f\"*.{key}\" for key in self._suffix_keys_wds])\n\n    if self._pipeline_wds is not None and self._pipeline_wds[split] is not None:\n        if isinstance(self._pipeline_wds[split], Iterable):\n            dataset = dataset.compose(*self._pipeline_wds[split])\n        else:\n            dataset = dataset.compose(self._pipeline_wds[split])\n\n    if self._invoke_wds is not None and self._invoke_wds[split] is not None:\n        for method in self._invoke_wds[split]:\n            name_method, kwargs_method = method\n            dataset = getattr(dataset, name_method)(**kwargs_method)\n    return dataset\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Alias for :func:<code>test_dataloader</code>.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Alias for :func:`test_dataloader`.\"\"\"\n    return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. Is a no-op.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"This is called only by the main process by the Lightning workflow.\n\n    Do not rely on this data module object's state update here as there is no\n    way to communicate the state update to other subprocesses. Is a **no-op**.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>This is called on all Lightning-managed nodes in a multi-node training session.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>\"fit\", \"test\" or \"predict\"</p> required Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"This is called on all Lightning-managed nodes in a multi-node training session.\n\n    Args:\n        stage: \"fit\", \"test\" or \"predict\"\n    \"\"\"\n    if stage == \"fit\":\n        self._dataset[Split.train] = self._setup_wds(Split.train)\n        self._dataset[Split.val] = self._setup_wds(Split.val)\n    elif stage == \"validate\":\n        self._dataset[Split.val] = self._setup_wds(Split.val)\n    elif stage == \"test\":\n        self._dataset[Split.test] = self._setup_wds(Split.test)\n    elif stage == \"predict\":\n        self._dataset[Split.test] = self._setup_wds(Split.test)\n    else:\n        raise NotImplementedError(f\"Data setup with {stage=} is not implemented.\")\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Webdataset for the test data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the test data.\"\"\"\n    return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Webdataset for the training data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the training data.\"\"\"\n    return self._setup_dataloader(Split.train)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Webdataset for the validation data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the validation data.\"\"\"\n    return self._setup_dataloader(Split.val)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/webdatamodule/utils/#bionemo.webdatamodule.utils.pickles_to_tars","title":"<code>pickles_to_tars(dir_input, input_prefix_subset, input_suffix, dir_output, output_prefix, func_output_data=lambda prefix, suffix_to_data: {'__key__': prefix, None: suffix_to_data}, min_num_shards=None)</code>","text":"<p>Convert a subset of pickle files from a directory to Webdataset tar files.</p> <p>Input path and name pattern for sample 0: f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[0]}\" f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[1]}\" Input path and name pattern for sample 1: f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[0]}\" f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[1]}\" ... Output path and name pattern: f\"{dir_output}/{output_prefix}-%06d.tar\".</p> <p>The webdataset tar archive is specified by the dictionary: {     \"key\" : sample_filename_preifx,     sample_filename_suffix_1 : data_1,     sample_filename_suffix_2 : data_2,     ... } so that parsing the tar archive is equivalent of reading {sample_filename_preifx}.{sample_filename_suffix_1} etc.</p> <p>Here, each sample data get its name prefix from one element of <code>input_prefix_subset</code> and its name suffixes from the list <code>input_suffix</code>. Per the webdataset file format specification, the <code>sample_filename_preifx</code> can't contain dots '.' so this function removes it for the user by calling .replace(\".\", \"-\") on the elements of <code>input_prefix_subset</code></p> <p>Parameters:</p> Name Type Description Default <code>dir_input</code> <code>str</code> <p>Input directory</p> required <code>input_prefix_subset</code> <code>List[str]</code> <p>Input subset of pickle files' prefix</p> required <code>input_suffix</code> <code>Union[str, Iterable[str]]</code> <p>Input pickle file name suffixes, each for one type of data object, for all the samples</p> required <code>dir_output</code> <code>str</code> <p>Output directory</p> required <code>output_prefix</code> <code>str</code> <p>Output tar file name prefix</p> required <code>func_output_data</code> <code>Callable[[str, Dict[str, Any]], Dict[str, Any]]</code> <p>function that maps the name prefix, name suffix and data object to a webdataset tar archive dictionary. Refer to the webdataset github repo for the archive file format specification.</p> <code>lambda prefix, suffix_to_data: {'__key__': prefix, None: suffix_to_data}</code> <code>min_num_shards</code> <p>create at least this number of tar files. WebDataset has bugs when reading small number of tar files in a multi-node lightening + DDP setting so this option can be used to guarantee the tar file counts</p> <code>None</code> Source code in <code>bionemo/webdatamodule/utils.py</code> <pre><code>def pickles_to_tars(\n    dir_input: str,\n    input_prefix_subset: List[str],\n    input_suffix: Union[str, Iterable[str]],\n    dir_output: str,\n    output_prefix: str,\n    func_output_data: Callable[[str, Dict[str, Any]], Dict[str, Any]] = lambda prefix, suffix_to_data: {\n        \"__key__\": prefix,\n        **suffix_to_data,\n    },\n    min_num_shards: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Convert a subset of pickle files from a directory to Webdataset tar files.\n\n    Input path and name pattern for sample 0:\n    f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[0]}\"\n    f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[1]}\"\n    Input path and name pattern for sample 1:\n    f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[0]}\"\n    f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[1]}\"\n    ...\n    Output path and name pattern:\n    f\"{dir_output}/{output_prefix}-%06d.tar\".\n\n    The webdataset tar archive is specified by the dictionary:\n    {\n        \"__key__\" : sample_filename_preifx,\n        sample_filename_suffix_1 : data_1,\n        sample_filename_suffix_2 : data_2,\n        ...\n    }\n    so that parsing the tar archive is equivalent of reading\n    {sample_filename_preifx}.{sample_filename_suffix_1} etc.\n\n    Here, each sample data get its name prefix from one element of\n    `input_prefix_subset` and its name suffixes from the list `input_suffix`.\n    Per the webdataset file format specification, the `sample_filename_preifx`\n    can't contain dots '.' so this function removes it for the user by calling\n    .replace(\".\", \"-\") on the elements of `input_prefix_subset`\n\n    Args:\n        dir_input: Input directory\n        input_prefix_subset: Input subset of pickle files' prefix\n        input_suffix: Input pickle file name\n            suffixes, each for one type of data object, for all the samples\n        dir_output: Output directory\n        output_prefix: Output tar file name prefix\n        func_output_data: function that maps the name prefix, name suffix and\n            data object to a webdataset tar archive dictionary. Refer to the webdataset\n            github repo for the archive file format specification.\n        min_num_shards : create at least this number of tar files.\n            WebDataset has bugs when reading small number of tar files in a\n            multi-node lightening + DDP setting so this option can be used to\n            guarantee the tar file counts\n    \"\"\"\n    if not isinstance(input_suffix, get_args(Union[str, Iterable])):\n        raise TypeError(\"input_suffix can only be str or Iterable[str]\")\n    os.makedirs(dir_output, exist_ok=True)\n    wd_subset_pattern = os.path.join(dir_output, f\"{output_prefix}-%06d.tar\")\n    n_samples_per_shard_max = 100000\n    if min_num_shards is not None:\n        if min_num_shards &lt;= 0:\n            raise ValueError(f\"Invalid min_num_shards = {min_num_shards} &lt;= 0\")\n        n_samples_per_shard_max = len(input_prefix_subset) // min_num_shards\n    with wds.ShardWriter(\n        wd_subset_pattern,\n        encoder=False,\n        maxcount=n_samples_per_shard_max,\n        compress=False,\n        mode=0o777,\n    ) as sink:\n        for name in input_prefix_subset:\n            try:\n                if isinstance(input_suffix, str):\n                    suffix_to_data = {\n                        input_suffix: pickle.dumps(\n                            pickle.loads((Path(dir_input) / f\"{name}.{input_suffix}\").read_bytes())\n                        )\n                    }\n                else:\n                    suffix_to_data = {\n                        suffix: pickle.dumps(pickle.loads((Path(dir_input) / f\"{name}.{suffix}\").read_bytes()))\n                        for suffix in input_suffix\n                    }\n                # the prefix name shouldn't contain any \".\" per webdataset's\n                # specification\n                sample = func_output_data(name.replace(\".\", \"-\"), suffix_to_data)\n                sink.write(sample)\n            except ModuleNotFoundError as e:\n                raise RuntimeError(\n                    \"Can't process pickle file due to\\\n                                   missing dependencies\"\n                ) from e\n            except Exception as e:\n                raise RuntimeError(f\"Failed to write {name} into tar files.\") from e\n</code></pre>"},{"location":"datasets/","title":"BioNeMo Framework: Available Datasets","text":"<p>The BioNeMo Framework provides access to a variety of high-quality datasets for bioinformatics and cheminformatics research. These datasets cover a range of biological and chemical modalities, supporting various research applications. The following table lists the currently available datasets:</p> Dataset Modality Uses CELLxGENE Single Cell Single-Cell Gene Expression UniProt Protein Protein Sequence and Function Analysis <p>For more information about the datasets included in the BioNeMo Framework, refer to the Dataset Cards linked in the table above or the original sources referenced in the respective dataset descriptions.</p>"},{"location":"datasets/CELLxGENE/","title":"CELLxGENE","text":""},{"location":"datasets/CELLxGENE/#description","title":"Description","text":"<p>CELLxGENE is an aggregation of publicly available single-cell datasets collected by CZI.</p>"},{"location":"datasets/CELLxGENE/#dataset-attributes-of-version-2023-12-15","title":"Dataset attributes of version 2023-12-15","text":"<p>Data was downloaded using the CELLxGENE Discover Census version <code>2023-12-15</code>. We first downloaded cellxgene census version 2023-12-15 using the <code>cellxgene_census</code> python API. We limited cell data to <code>organism=\u201dHomo sapiens\u201d</code>, with a non \u201cna\u201d <code>suspension_type</code>, <code>is_primary_data=True</code>, and <code>disease=\u201dnormal\u201d</code> to limit to non-diseased tissues that are also the primary data source per cell to make sure that cells are only included once in the download. We tracked metadata including \u201cassay\u201d, \u201csex\u201d, \u201cdevelopment_stage\u201d, \u201ctissue_general\u201d, \u201cdataset_id\u201d and \u201cself_reported_ethnicity\u201d. The metadata \u201cassay\u201d, \u201ctissue_general\u201d, and \u201cdataset_id\u201d were used to construct dataset splits into train, validation, and test sets. The training set represented 99% of the downloaded cells. We partitioned the data by dataset_id into a train set (99%) and a hold-out set (1%), to make sure that the hold-out datasets were independently collected single cell experiments, which helps evaluate generalizability to new future datasets. In this training split, we made sure that all \u201cassay\u201d and \u201ctissue_general\u201d labels were present in the training set so that our model would have maximal visibility into different tissues and assay biases. Finally the 1% hold-out set was split further into a validation and test set. This final split was mostly done randomly by cell, however we set aside a full dataset into the test split so that we could evaluate performance after training on a completely unseen dataset, including when monitoring the validation loss during training.</p> <p>These parameters resulted in 23.87 Million single cells collected from a variety of public datasets, all hosted by CZI cell x gene census. After the splitting procedure we had:</p> <ul> <li>23.64 Million cells in the training split</li> <li>0.13 Million cells in the validation split</li> <li>0.11 Million cells in the test split</li> </ul>"},{"location":"datasets/CELLxGENE/#distributions-of-donor-covariates","title":"Distributions of donor covariates","text":"<p>There are various biases apparent in this dataset.</p>"},{"location":"datasets/CELLxGENE/#tissue-distribution","title":"Tissue distribution","text":"<p>At a high level tissues were heavily biased toward the nervous system, which made up nearly 40 percent of the data.</p> <p></p>"},{"location":"datasets/CELLxGENE/#assay-distribution","title":"Assay distribution","text":"<p>Assays were also imbalanced in this dataset. As the 10x machine is fairly high throughput and currently popular, it makes sense that the majority of cells present would be from this instrument. Various versions of the 10x instrument made up 18M of the 24M cells while the next largest category was <code>sci-RNA-seq</code>. </p>"},{"location":"datasets/CELLxGENE/#sex-distribution","title":"Sex distribution","text":"<p>A bias exists in this dataset for sex. Most of the donor's cells were male-derived at 52%, while female donor's cell contribution made up 42%, and the remaining 6% were not annotated. .</p>"},{"location":"datasets/CELLxGENE/#reported-ethnicity-distribution","title":"Reported ethnicity distribution","text":"<p>The dataset has a heavy bias toward cells derived from donors with european ethnicity at 40%, while the next largest category, asian, made up 8%. When considering that nearly 50% were unknown, we might expect that as much as 75% of this dataset is made up of cells extracted from donors of self reported european ethnicity. </p>"},{"location":"datasets/CELLxGENE/#age-distribution","title":"Age distribution","text":"<p>This dataset is very heavily balanced toward younger donors. Many of the cells are derived from donors that are under a year of age (over 25%). After that the remaining 75% of cells are dispersed roughly under a normal distribution with a mode of 51-60 other than an additional peak in the 21-30 range. Donors over 61 years old make up approximately 15% of the data.</p> <p></p>"},{"location":"datasets/CELLxGENE/#assay-size-distribution","title":"Assay size distribution","text":"<p>Different assays have different ranges of reported gene measurements. On the low end <code>BD Rapsody Targetted mRNA</code> has only a few genes reported, while 10x instruments tend to report on 30,000 genes.</p> <p></p>"},{"location":"datasets/CELLxGENE/#dataset-distribution","title":"Dataset distribution","text":"<p>Dataset (eg a publication that produces data and uploads to cellxgene) leads to known batch effects due to different handling proceedures, collection procedures, etc. We stratify our training vs hold-out split by this covariate for this reason. Exploring the breakdown of datasets we see that the top 10 datsets represent approximately 10 million cells of the full cellxgene datset. The largest dataset alone has 4 million cells.</p> <p></p> <p>Looking at the makeup of these top datasets, we see that most represent single tissue categories predominately. Most of these tend to be nervous system datsets with the exception of one which is balanced between many cell types. </p>"},{"location":"datasets/CELLxGENE/#references","title":"References","text":"<ul> <li>CZ CELLxGENE Discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data CZI Single-Cell Biology, et al. bioRxiv 2023.10.30; doi: https://doi.org/10.1101/2023.10.30.563174</li> </ul>"},{"location":"datasets/CELLxGENE/#data-license","title":"Data License","text":"<p>The data in CELLxGENE are made available by the study authors and Chan Zuckerberg Initiative under the creative commons CC BY 4.0 license. Study authors agree prior to submission that their data is not identifiable, lacking any direct personal identifiers in the metadata. More information may be found in the CELLxGENE Data Submission Policy. Our training, validation and test data, including subsets made available for testing and demonstration purposes, was contributed to CELLxGENE through one or more of the following sources:</p> <ul> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/01fee550-877c-4a13-97b2-96bb43e5a2a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6b701826-37bb-4356-9792-ff41fc4c3161</li> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/9959402b-2e69-4aeb-ba39-efdfa5e0de1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/10bf5c50-8d85-4c5f-94b4-22c1363d9f31</li> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/bc484ee8-b3cc-47a3-8f4f-f95aa1fec803.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a98b828a-622a-483a-80e0-15703678befd</li> <li>Publication Reference: Ahern et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.01.012 Dataset Version: https://datasets.cellxgene.cziscience.com/e72d0170-3399-4aa0-8c56-da7b4f0ced6b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/8f126edf-5405-4731-8374-b5ce11f53e82</li> <li>Publication Reference: Andrews et al. (2022) Hepatology Communications; Publication: https://doi.org/10.1002/hep4.1854 Dataset Version: https://datasets.cellxgene.cziscience.com/19b364f7-db0c-430b-bc16-9b31cbd45a58.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/44531dd9-1388-4416-a117-af0a99de2294</li> <li>Publication Reference: Arutyunyan et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05869-0 Dataset Version: https://datasets.cellxgene.cziscience.com/5720f13d-fc15-4859-90df-447637fb37c4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e2c257e7-6f79-487c-b81c-39451cd4ab3c</li> <li>Publication Reference: Bhaduri et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03910-8 Dataset Version: https://datasets.cellxgene.cziscience.com/677082ca-48d3-44b8-b5c4-84dffafbba23.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c8565c6a-01a1-435b-a549-f11b452a83a8</li> <li>Publication Reference: Bhat-Nakshatri et al. (2021) Cell Reports Medicine; Publication: https://doi.org/10.1016/j.xcrm.2021.100219 Dataset Version: https://datasets.cellxgene.cziscience.com/f72aae6e-c997-484c-bffd-6d09e41ef9a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c9706a92-0e5f-46c1-96d8-20e42467f287</li> <li>Publication Reference: Bondoc et al. (2021) Commun Biol; Publication: https://doi.org/10.1038/s42003-021-02562-8 Dataset Version: https://datasets.cellxgene.cziscience.com/20d54624-2098-4ed5-89f8-6da2bb460c3c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a261413d-835b-4f1e-ab0c-dada55ea6afd</li> <li>Publication Reference: Burclaff et al. (2022) Cellular and Molecular Gastroenterology and Hepatology; Publication: https://doi.org/10.1016/j.jcmgh.2022.02.007 Dataset Version: https://datasets.cellxgene.cziscience.com/e00e3a74-038a-46fd-8931-f6dc8c90fd13.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/64b24fda-6591-4ce1-89e7-33eb6c43ad7b</li> <li>Publication Reference: Calandrelli et al. (2020) Nat Commun; Publication: https://doi.org/10.1038/s41467-020-18957-w Dataset Version: https://datasets.cellxgene.cziscience.com/c3189372-fceb-493d-98be-23abe1947253.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/db468083-041c-41ca-8f6f-bf991a070adf</li> <li>Publication Reference: Cao et al. (2020) Science; Publication: https://doi.org/10.1126/science.aba7721 Dataset Version: https://datasets.cellxgene.cziscience.com/8f6296d0-5b29-4dca-8061-b97147df5fcc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c114c20f-1ef4-49a5-9c2e-d965787fb90c</li> <li>Publication Reference: Chan Zuckerberg Initiative Single-Cell COVID-19 Consortia et al. (2020) medRxiv; Publication: https://doi.org/10.1101/2020.11.20.20227355 Dataset Version: https://datasets.cellxgene.cziscience.com/6c779b98-f437-4cbf-9b81-a3e6be637419.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0434a9d4-85fd-4554-b8e3-cf6c582bb2fa</li> <li>Publication Reference: Chan et al. (2021) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2021.09.008 Dataset Version: https://datasets.cellxgene.cziscience.com/1ba7d495-c1a8-4809-b56d-548fbea77c8a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62e8f058-9c37-48bc-9200-e767f318a8ec</li> <li>Publication Reference: Chan et al. (2021) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2021.09.008 Dataset Version: https://datasets.cellxgene.cziscience.com/c40911a4-47de-460e-be86-52e39800654c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62e8f058-9c37-48bc-9200-e767f318a8ec</li> <li>Publication Reference: Cheng et al. (2018) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2018.09.006 Dataset Version: https://datasets.cellxgene.cziscience.com/912d943b-9060-4fd3-a12c-ad641a89f0e4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/43d4bb39-21af-4d05-b973-4c1fed7b916c</li> <li>Publication Reference: Cowan et al. (2020) Cell; Publication: https://doi.org/10.1016/j.cell.2020.08.013 Dataset Version: https://datasets.cellxgene.cziscience.com/b1989183-5808-46ab-87f5-978febb2d26e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2f4c738f-e2f3-4553-9db2-0582a38ea4dc</li> <li>Publication Reference: Cowan et al. (2020) Cell; Publication: https://doi.org/10.1016/j.cell.2020.08.013 Dataset Version: https://datasets.cellxgene.cziscience.com/c0d3867e-1a7b-4e57-af62-c563f1934226.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2f4c738f-e2f3-4553-9db2-0582a38ea4dc</li> <li>Publication Reference: Dom\\u00ednguez Conde et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl5197 Dataset Version: https://datasets.cellxgene.cziscience.com/08f58b32-a01b-4300-8ebc-2b93c18f26f7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62ef75e4-cbea-454e-a0ce-998ec40223d3</li> <li>Publication Reference: Easter et al. (2024) Nat Commun; Publication: https://doi.org/10.1038/s41467-024-49037-y Dataset Version: https://datasets.cellxgene.cziscience.com/221dff56-a47d-4563-90ed-51b60e2f16d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/71f4bccf-53d4-4c12-9e80-e73bfb89e398</li> <li>Publication Reference: Egozi et al. (2021) Nat Med; Publication: https://doi.org/10.1038/s41591-021-01586-1 Dataset Version: https://datasets.cellxgene.cziscience.com/e3a84fef-b6df-49b2-b0ca-ecaf444773ec.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7651ac1a-f947-463a-9223-a9e408a41989</li> <li>Publication Reference: Elmentaite et al. (2020) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2020.11.010 Dataset Version: https://datasets.cellxgene.cziscience.com/3aedefc0-401a-4ee8-a1b5-a0ffc20e1ff2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/17481d16-ee44-49e5-bcf0-28c0780d8c4a</li> <li>Publication Reference: Elmentaite et al. (2020) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2020.11.010 Dataset Version: https://datasets.cellxgene.cziscience.com/5d27ffd6-1769-4564-961f-9bb32d9ca3a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/17481d16-ee44-49e5-bcf0-28c0780d8c4a</li> <li>Publication Reference: Elmentaite et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03852-1 Dataset Version: https://datasets.cellxgene.cziscience.com/c463b937-dbdc-48ff-8037-dd191ea4e41e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e33ffcd3-7cbf-4b8c-b0f4-85587ad5019a</li> <li>Publication Reference: Eraslan et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl4290 Dataset Version: https://datasets.cellxgene.cziscience.com/355ed159-f7d7-45e9-bc55-95639f0ab8b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a3ffde6c-7ad2-498a-903c-d58e732f7470</li> <li>Publication Reference: Fan et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-11036-9 Dataset Version: https://datasets.cellxgene.cziscience.com/9b2536db-4576-4906-ae9b-a01a623462f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2902f08c-f83c-470e-a541-e463e25e5058</li> <li>Publication Reference: Fasolino et al. (2022) Nat Metab; Publication: https://doi.org/10.1038/s42255-022-00531-x Dataset Version: https://datasets.cellxgene.cziscience.com/d39144df-fa59-4b63-b07b-9b34613b5c84.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/51544e44-293b-4c2b-8c26-560678423380</li> <li>Publication Reference: Fawkner-Corbett et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2020.12.016 Dataset Version: https://datasets.cellxgene.cziscience.com/e8473c46-eada-43b7-bd1c-e0fed1c4c913.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/60358420-6055-411d-ba4f-e8ac80682a2e</li> <li>Publication Reference: Gabitto et al. (2023) bioRxiv; Publication: https://doi.org/10.1101/2023.05.08.539485 Dataset Version: https://datasets.cellxgene.cziscience.com/291ce735-8d18-4a2f-a6bc-98f75f8d6bc0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</li> <li>Publication Reference: Gabitto et al. (2023) bioRxiv; Publication: https://doi.org/10.1101/2023.05.08.539485 Dataset Version: https://datasets.cellxgene.cziscience.com/e9bffe1d-9f07-4467-9230-c080b362e737.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</li> <li>Publication Reference: Garcia-Alonso et al. (2021) Nat Genet; Publication: https://doi.org/10.1038/s41588-021-00972-2 Dataset Version: https://datasets.cellxgene.cziscience.com/15f77a91-aadc-4e63-81c7-a8614e9ad33d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/32f2fd23-ec74-486f-9544-e5b2f41725f5</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/366847dc-8fc4-42c3-9c27-9704929c6792.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/4b894b0d-6b27-4ab4-a48a-0205e4aaf348.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/c74c3be0-1a80-4af9-8241-d560afc67886.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Gray et al. (2022) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2022.05.003 Dataset Version: https://datasets.cellxgene.cziscience.com/9fecd056-d8c8-4ec6-8522-8d40f19c90a8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/99f1515b-46a2-4bc4-94c3-f62659dc1eb4</li> <li>Publication Reference: Guilliams et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2021.12.018 Dataset Version: https://datasets.cellxgene.cziscience.com/5f2d618d-2a5f-4c31-8750-982342d7dd04.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/74e10dc4-cbb2-4605-a189-8a1cd8e44d8c</li> <li>Publication Reference: Han et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2157-4 Dataset Version: https://datasets.cellxgene.cziscience.com/7a455e3b-dd79-499b-95c9-8b1b2dde5339.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/38833785-fac5-48fd-944a-0f62a4c23ed1</li> <li>Publication Reference: Han et al. (2022) Blood Cancer Discovery; Publication: https://doi.org/10.1158/2643-3230.BCD-21-0075 Dataset Version: https://datasets.cellxgene.cziscience.com/f905e484-1162-467e-b8c5-835dcfe9bd5c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/968834a0-1895-40df-8720-666029b3bbac</li> <li>Publication Reference: Hao et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.048 Dataset Version: https://datasets.cellxgene.cziscience.com/55c120dc-6a20-4caf-9513-f5970b24b1be.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0cf0afa-ec40-4d65-b570-ed4ceacc6813</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/256d6049-b499-47a9-9c9a-20b92f9c6ba6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/25dbd3da-8cb0-4b9d-814c-85ae0a710353.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/2b55f5c0-aa82-41e8-9d84-915b1d5a797b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/c4122ff1-79d9-405b-92f1-c1c27234a125.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: James et al. (2020) Nat Immunol; Publication: https://doi.org/10.1038/s41590-020-0602-z Dataset Version: https://datasets.cellxgene.cziscience.com/6e2ab5f9-bb51-459a-9fd7-605d29661823.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7681c7d7-0168-4892-a547-6f02a6430ace</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/7c452616-bc00-4499-b511-bfd5de1b7cd6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/8ef64f32-461d-4d50-81b6-5718b506a7a8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/f402b857-7871-45f4-9ad1-ff943552285a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jin et al. (2021) iScience; Publication: https://doi.org/10.1016/j.isci.2021.103115 Dataset Version: https://datasets.cellxgene.cziscience.com/753caa81-61c5-4126-a61c-df2c546b16d1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b9fc3d70-5a72-4479-a046-c2cc1ab19efc</li> <li>Publication Reference: Jorstad et al. (2023) Science; Publication: https://doi.org/10.1126/science.adf6812 Dataset Version: https://datasets.cellxgene.cziscience.com/97a035e0-b9d3-4e7b-adc2-b318316da7f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d17249d2-0e6e-4500-abb8-e6c93fa1ac6f</li> <li>Publication Reference: Joseph et al. (2021) J. Pathol.; Publication: https://doi.org/10.1002/path.5751 Dataset Version: https://datasets.cellxgene.cziscience.com/fb812806-ae3a-45de-9102-0b2f801424e2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4b54248f-2165-477c-a027-dd55082e8818</li> <li>Publication Reference: Kamath et al. (2022) Nat Neurosci; Publication: https://doi.org/10.1038/s41593-022-01061-1 Dataset Version: https://datasets.cellxgene.cziscience.com/4936be1a-c766-46e2-815f-1c994aed7a4f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0f0b447-ac37-45b0-b1bf-5c0b7d871120</li> <li>Publication Reference: Kamath et al. (2022) Nat Neurosci; Publication: https://doi.org/10.1038/s41593-022-01061-1 Dataset Version: https://datasets.cellxgene.cziscience.com/dd206caf-13ca-4598-86af-339189adff0d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0f0b447-ac37-45b0-b1bf-5c0b7d871120</li> <li>Publication Reference: Kanemaru et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06311-1 Dataset Version: https://datasets.cellxgene.cziscience.com/1a7a9fb0-aee1-437f-8a7c-9d132253a4db.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3116d060-0a8e-4767-99bb-e866badea1ed</li> <li>Publication Reference: King et al. (2021) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abe6291 Dataset Version: https://datasets.cellxgene.cziscience.com/02675fa7-5f13-4d89-a07d-9d0ff7996f0d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3a2af25b-2338-4266-aad3-aa8d07473f50</li> <li>Publication Reference: King et al. (2021) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abe6291 Dataset Version: https://datasets.cellxgene.cziscience.com/6c8a5c10-2617-4cb1-8ed6-20a5e25065ef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3a2af25b-2338-4266-aad3-aa8d07473f50</li> <li>Publication Reference: Knight-Schrijver et al. (2022) Nat Cardiovasc Res; Publication: https://doi.org/10.1038/s44161-022-00183-w Dataset Version: https://datasets.cellxgene.cziscience.com/c1e3c998-4961-46c9-929d-d011900964e8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/43b45a20-a969-49ac-a8e8-8c84b211bd01</li> <li>Publication Reference: Kock et al. (2024) bioRxiv; Publication: https://doi.org/10.1101/2024.06.30.601119 Dataset Version: https://datasets.cellxgene.cziscience.com/a3850101-1e15-4ae2-82f7-bb9289e911d4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ced320a1-29f3-47c1-a735-513c7084d508</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/30d6b4f5-1475-4b86-a47c-48609d6706c2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/41bdc5da-b436-485a-a753-9ae297057ee6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/485d1aee-db62-4373-854c-12a34237e97b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/8db9570a-3f80-41c9-b927-ae3eb115ba1d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/dedfeef9-bfb8-4c2f-a196-1afea9a846d8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/ef7d48a5-c56b-4f13-9903-fb3327924445.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kumar et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06252-9 Dataset Version: https://datasets.cellxgene.cziscience.com/24ee53d1-e5ed-47ae-8b8e-7a0d62d91513.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4195ab4c-20bd-4cd3-8b3d-65601277e731</li> <li>Publication Reference: Kumar et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06252-9 Dataset Version: https://datasets.cellxgene.cziscience.com/b8b5be07-061b-4390-af0a-f9ced877a068.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4195ab4c-20bd-4cd3-8b3d-65601277e731</li> <li>Publication Reference: Kuppe et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-05060-x Dataset Version: https://datasets.cellxgene.cziscience.com/c1f6034b-7973-45e1-85e7-16933d0550bc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/8191c283-0816-424b-9b61-c3e1d6258a77</li> <li>Publication Reference: Lake et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05769-3 Dataset Version: https://datasets.cellxgene.cziscience.com/1cbe52c1-0567-4188-9c18-9d7271c56055.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bcb61471-2a44-4d00-a0af-ff085512674c</li> <li>Publication Reference: Lake et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05769-3 Dataset Version: https://datasets.cellxgene.cziscience.com/d0ddf40e-dc4b-4134-8439-bfb8bf7a81f4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bcb61471-2a44-4d00-a0af-ff085512674c</li> <li>Publication Reference: Lavaert et al. (2020) Immunity; Publication: https://doi.org/10.1016/j.immuni.2020.03.019 Dataset Version: https://datasets.cellxgene.cziscience.com/22e3ee6e-e47b-4502-9b2c-21b4c30a455f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/83ed3be8-4cb9-43e6-9aaa-3fbbf5d1bd3a</li> <li>Publication Reference: Ledergor et al. (2018) Nat Med; Publication: https://doi.org/10.1038/s41591-018-0269-2 Dataset Version: https://datasets.cellxgene.cziscience.com/680c8801-ccdd-4018-a14a-cefda2da3848.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2a0b02c0-fea6-47bd-92b9-9b03f5d2580c</li> <li>Publication Reference: Lee et al. (2020) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abd1554 Dataset Version: https://datasets.cellxgene.cziscience.com/b0a99b60-7480-4b5c-93c5-11020c36adb2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4f889ffc-d4bc-4748-905b-8eb9db47a2ed</li> <li>Publication Reference: Lengyel et al. (2022) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2022.111838 Dataset Version: https://datasets.cellxgene.cziscience.com/da638059-73e0-4a3b-a6fc-5fb8e47d4bff.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d36ca85c-3e8b-444c-ba3e-a645040c6185</li> <li>Publication Reference: Lengyel et al. (2022) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2022.111838 Dataset Version: https://datasets.cellxgene.cziscience.com/de2a800c-249f-4072-8454-cde3d6bfb5b4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d36ca85c-3e8b-444c-ba3e-a645040c6185</li> <li>Publication Reference: Li et al. (2022) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2022.11.001 Dataset Version: https://datasets.cellxgene.cziscience.com/39196e03-e248-4724-a618-b3bef017d6b2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/f7cecffa-00b4-4560-a29a-8ad626b8ee08</li> <li>Publication Reference: Liang et al. (2023) Cell Genomics; Publication: https://doi.org/10.1016/j.xgen.2023.100298 Dataset Version: https://datasets.cellxgene.cziscience.com/0da7c2ec-246f-4ffb-9c25-1aa059870a0a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/af893e86-8e9f-41f1-a474-ef05359b1fb7</li> <li>Publication Reference: Liang et al. (2023) Cell Genomics; Publication: https://doi.org/10.1016/j.xgen.2023.100298 Dataset Version: https://datasets.cellxgene.cziscience.com/9a199269-5a65-4ade-aa85-07ab4cfb4c26.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/af893e86-8e9f-41f1-a474-ef05359b1fb7</li> <li>Publication Reference: Litvi\\u0148ukov\\u00e1 et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2797-4 Dataset Version: https://datasets.cellxgene.cziscience.com/a5618935-5bbd-494d-b300-9ecc2402d5b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b52eb423-5d0d-4645-b217-e1c6d38b2e72</li> <li>Publication Reference: Liu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.02.018 Dataset Version: https://datasets.cellxgene.cziscience.com/504b4b53-f0fb-43a6-8b8d-6254e8d81e85.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ed9185e3-5b82-40c7-9824-b2141590c7f0</li> <li>Publication Reference: Liu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.02.018 Dataset Version: https://datasets.cellxgene.cziscience.com/6d14e6f5-9b9f-4e8e-8b31-cee9020e18a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ed9185e3-5b82-40c7-9824-b2141590c7f0</li> <li>Publication Reference: Lukassen et al. (2020) EMBO J; Publication: https://doi.org/10.15252/embj.20105114 Dataset Version: https://datasets.cellxgene.cziscience.com/a6c47325-eae6-4111-9e47-89a550ef99af.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6ff3401b-d72c-4940-a00c-3f0792397082</li> <li>Publication Reference: Lukassen et al. (2020) EMBO J; Publication: https://doi.org/10.15252/embj.20105114 Dataset Version: https://datasets.cellxgene.cziscience.com/b0c0edd1-a8ba-436c-bef4-667bdf3fc8f1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6ff3401b-d72c-4940-a00c-3f0792397082</li> <li>Publication Reference: Lukowski et al. (2019) EMBO J; Publication: https://doi.org/10.15252/embj.2018100811 Dataset Version: https://datasets.cellxgene.cziscience.com/9b910901-bcf8-4d51-a85b-268ccbd54544.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3472f32d-4a33-48e2-aad5-666d4631bf4c</li> <li>Publication Reference: MacParland et al. (2018) Nat Commun; Publication: https://doi.org/10.1038/s41467-018-06318-7 Dataset Version: https://datasets.cellxgene.cziscience.com/f07d2b1d-2e04-4f30-92d2-7d55e22da909.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bd5230f4-cd76-4d35-9ee5-89b3e7475659</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/2c6ab5e2-09c9-410b-a472-7559e45e553a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/55abbbab-bb53-4f92-92cd-483d8f74a881.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/ee76e44a-8b7a-4ddc-8e29-c8a355254033.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Melms et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03569-1 Dataset Version: https://datasets.cellxgene.cziscience.com/739bae5d-2d6f-4664-a79f-d065610da5ae.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e4c9ed14-e560-4900-a3bf-b0f8d2ce6a10</li> <li>Publication Reference: Menon et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-12780-8 Dataset Version: https://datasets.cellxgene.cziscience.com/f34734b0-5de7-4b48-a3ba-bd9bb8c48e73.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1a486c4c-c115-4721-8c9f-f9f096e10857</li> <li>Publication Reference: Muto et al. (2021) Nat Commun; Publication: https://doi.org/10.1038/s41467-021-22368-w Dataset Version: https://datasets.cellxgene.cziscience.com/3b84b0b5-3d0a-41a1-860a-bbadad3717bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9b02383a-9358-4f0f-9795-a891ec523bcc</li> <li>Publication Reference: Nowicki-Osuch et al. (2023) Cancer Discovery; Publication: https://doi.org/10.1158/2159-8290.cd-22-0824 Dataset Version: https://datasets.cellxgene.cziscience.com/074caa96-9205-4bc6-b5ba-eef787f131dc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a18474f4-ff1e-4864-af69-270b956cee5b</li> <li>Publication Reference: Orozco et al. (2020) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2019.12.082 Dataset Version: https://datasets.cellxgene.cziscience.com/56334044-9f3a-4541-be24-28bed94c2d4a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/939769a8-d8d2-4d01-abfc-55699893fd49</li> <li>Publication Reference: Otero-Garcia et al. (2022) Neuron; Publication: https://doi.org/10.1016/j.neuron.2022.06.021 Dataset Version: https://datasets.cellxgene.cziscience.com/70170717-45c4-4891-9b14-fb795ecc3d94.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b953c942-f5d8-434f-9da7-e726ba7c1481</li> <li>Publication Reference: Otero-Garcia et al. (2022) Neuron; Publication: https://doi.org/10.1016/j.neuron.2022.06.021 Dataset Version: https://datasets.cellxgene.cziscience.com/b6ddb1cd-f058-450e-8e66-e81f6e8a3c4a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b953c942-f5d8-434f-9da7-e726ba7c1481</li> <li>Publication Reference: Park et al. (2020) Science; Publication: https://doi.org/10.1126/science.aay3224 Dataset Version: https://datasets.cellxgene.cziscience.com/59d5b3c5-9a55-44ae-a7fa-c14567e02755.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/de13e3e2-23b6-40ed-a413-e9e12d7d3910</li> <li>Publication Reference: Park et al. (2020) Science; Publication: https://doi.org/10.1126/science.aay3224 Dataset Version: https://datasets.cellxgene.cziscience.com/c6e08ab6-ab3b-41dc-8058-8e6442e081ec.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/de13e3e2-23b6-40ed-a413-e9e12d7d3910</li> <li>Publication Reference: Perez et al. (2022) Science; Publication: https://doi.org/10.1126/science.abf1970 Dataset Version: https://datasets.cellxgene.cziscience.com/8f2e4a29-e397-4f33-bc9a-3181e06b64b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/436154da-bcf1-4130-9c8b-120ff9a888f2</li> <li>Publication Reference: Reed et al. (2024) Nat Genet; Publication: https://doi.org/10.1038/s41588-024-01688-9 Dataset Version: https://datasets.cellxgene.cziscience.com/5a611776-aae0-41b9-9f2b-aaf5f83771a3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/48259aa8-f168-4bf5-b797-af8e88da6637</li> <li>Publication Reference: Reichart et al. (2022) Science; Publication: https://doi.org/10.1126/science.abo1984 Dataset Version: https://datasets.cellxgene.cziscience.com/e0251a80-0058-4686-9ab8-b2e751313e18.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e75342a8-0f3b-4ec5-8ee1-245a23e0f7cb</li> <li>Publication Reference: Ren et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.01.053 Dataset Version: https://datasets.cellxgene.cziscience.com/ae18e694-6f45-4635-affa-63ac0e29323d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0a839c4b-10d0-4d64-9272-684c49a2c8ba</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/0ac22902-dfaa-4ceb-9cad-03afbd96f6d4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/5b4b134c-42f6-415b-a4fd-7f2a60128ff1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/d6e742c5-f6e5-42f4-8064-622783542f6b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Salcher et al. (2022) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2022.10.008 Dataset Version: https://datasets.cellxgene.cziscience.com/99040b08-7e1a-4d81-911c-4d2fd2335757.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/edb893ee-4066-4128-9aec-5eb2b03f8287</li> <li>Publication Reference: Seeker et al. (2023) acta neuropathol commun; Publication: https://doi.org/10.1186/s40478-023-01568-z Dataset Version: https://datasets.cellxgene.cziscience.com/32c319ef-10e2-4948-8b40-093d2f9d7cb5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9d63fcf1-5ca0-4006-8d8f-872f3327dbe9</li> <li>Publication Reference: Sikkema et al. (2023) Nat Med; Publication: https://doi.org/10.1038/s41591-023-02327-2 Dataset Version: https://datasets.cellxgene.cziscience.com/8d84ba15-d367-4dce-979c-85da70b868a2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6f6d381a-7701-4781-935c-db10d30de293</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/005aba12-a5af-4fcd-9b80-e28d845885c8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/05c60502-d980-4b5f-b093-d27cdf1360cd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/09df20c1-3c87-465d-b67c-73dc5f3d1bc8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/0eb52059-c5a4-4b6a-aa95-e9bec179e13c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/0f509022-ab14-4151-a435-37bcf582e20d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/137f5623-e19a-4cb4-9466-05ba393c3552.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1a5d4cf1-e1a7-4081-918a-8d9cb441cd54.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1eef8184-4629-406d-b04e-13f1c34b1071.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1faafbca-0062-4fc0-8b50-a3d32a111d39.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/214485af-4c67-4fda-b430-995162804fbf.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/21fd9d75-dfec-443c-b634-9803792a081c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/229ddec2-a177-4325-bb2c-0ec11092982a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/25a92b24-76c5-4bdb-a431-3478a8b38a69.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/25cc85a9-8645-43cf-a552-d7487c04159e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/2a8c8c02-e64b-4b39-9b1a-e46247031c1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/2baa0697-1757-4675-b1ae-51672b2d9bce.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/34c82f0d-04a7-4b13-8d4f-ae06065a2b1c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/364a5dc5-2d78-4931-9eeb-477c56ba63e7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/366254d0-6aef-4c49-a814-bc195a5bb6c6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3a64f908-b132-4d0c-aac6-28012cb6fd11.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3f0713f8-2fd6-4b77-8ea2-010a0d9b51ae.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3f1a2741-7b58-4eef-91ad-becc1646b1a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3ff56875-32be-4008-be41-c9c6526fc730.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/443f8b59-a15d-4d22-9b18-9ec721560c19.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/45d8a451-d313-465d-b66d-bdea50800ce8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/47a0e562-2745-4619-9623-d6fa431b4026.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/4a65c6dd-cb77-4cc3-8d95-06957a668c1f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/4df9fbc5-871b-43be-b796-92cd3f592e32.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/54095aef-f5c6-4dfa-b2e3-00fa8bb8d86a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/544c6998-8d55-4c59-bd18-18c81a3a8b37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/560a125a-9dd4-4fae-b58a-f24c932c5dd9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/587acfff-2fb3-46eb-8511-8a1cd2e65ad4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/58f6d3e6-aedd-479f-b2f5-9026fac2011c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/595f2363-dd4a-4184-a75a-c8b0fb62e3d1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5ae914f7-a02e-4f08-8bf5-45023b1a23d9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5dac55f7-6650-4d24-b8c0-bbb9672ac819.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5e07340f-a505-4272-a688-de9e104de2d6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/60034ae4-27a7-46f3-a5aa-668eba808b6f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/609942ad-2a18-4567-a9b7-8bf126cd1732.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/61ee7a9e-216b-4119-93fa-740fe41a8a0a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/63319798-c980-4fd6-866c-46d9fc1f6d28.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/64f05ce7-291c-487e-9b00-498a2279291d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6918480f-892a-42bf-a5cd-5e988f6e46b9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6a65d707-2c28-4d47-8b77-12f8dd3d66e2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6be7d5fc-9024-46a9-84aa-26a230ac6733.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6cc6a9b9-d53a-4b25-9b3f-d1f00e092d85.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/702d58ff-08f2-48ef-85b7-6132e77efdc4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/705b76b8-68a5-44fb-89ef-fb5273d2ec88.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/717b249c-46e3-43b5-9b3e-93bbdf879874.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/73ec94b8-e1c8-4de8-966a-8dea2842b068.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/744be8ae-ddd2-4822-9537-50030b340bef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/74903ae9-7470-4795-9508-057a058b9447.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/777952cd-2f82-408b-8890-f084f5825d90.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/7d5e92ed-87fa-4e85-831c-0b98479f3ec6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/80869ae5-15a7-4ddc-9c2e-4f40dffb2aeb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/81d55237-e3df-4212-84f8-5ed7a1b5b62e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/81eb9dd8-2b33-4ff5-aa3f-a8842b68ddca.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/821fa877-c2b1-488f-bc12-606e804f6f4b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/8268fa76-4996-4a21-b687-ad67c5dc383d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/82a043bf-b6c0-4746-9d69-39437796282e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/82faf671-658a-4c88-8a7d-618fc7f68fad.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/83b8522d-f362-4aca-8c64-2dfb125b87e1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/88bacb21-c9d7-4d59-a1d9-ac71d75628b3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/8faf8f5a-284a-4b2f-b8fe-4a71cb38a79a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9993afca-cfb9-4ae9-851b-07528edd8b20.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9b42ebb8-5ebe-4707-bb75-7d64a5fdcafb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9f0e0ca6-43ed-4e52-9d85-93cbe24346a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9f90a216-3fac-44ff-b0ca-7b77cf53ef07.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a1446790-c439-4a23-904d-488028e4d34c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a4581a49-fe69-4039-97dd-a2c54e676159.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a497b47b-c4a2-42fe-a7a4-8abc8aed4ec5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a561b8a5-9c26-4b52-bb0b-dca989c432cd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a737721b-e311-40da-9835-fc60d991be9a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a7f64bb1-6198-406f-ae78-ddb884633937.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ab10decf-4e42-44ac-bb45-660771526f96.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ad40f328-75a3-4b5e-8212-308b7d734d45.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ae98b1bf-fd73-49a0-9732-41f74da1832b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/aeb99282-b851-469d-bb77-4c550ae71b37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b18522ea-2a30-4f9d-b5c1-aa93a9f5677d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b38e94df-9801-4cf1-a8e2-8b39f25f56b4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b47045e2-d8ad-4c72-9412-aa4824f532d3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b893efbd-c660-404d-9055-6717b7eba280.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b9ecd819-1a5b-4e8e-9f2d-833b1891af37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/bc0c001e-5c7f-43ef-9003-40f2e3015ff7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/bfbe13d3-d6b8-4fd9-aeb7-c35e80219e1d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/c2b0f135-3723-4351-83e5-c4eef19401f4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/c821776d-a9fd-4a9b-b0ef-65bee87c7dbc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cb5f81ff-4676-4ead-be89-e7da582ddd94.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cc1379b7-5d6e-4944-85ab-6b3ddb602730.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ccef089c-a215-4607-9617-8e00f40ece67.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cf701447-cfdd-40fd-b8fe-76d181dad7f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/d693ce29-f45e-4871-83e5-c60c001c190d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/d8b0c448-6d8c-4286-aa8b-55372af3ad1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/dcf1e7b7-6e47-4052-a096-2b2ef8deafce.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ddf86877-360c-44d0-af09-e6645bd0432a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/de350331-df0e-4b8d-88dc-0bc12c4ca845.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/df9fb19c-3e91-484a-bee6-6db14d649e05.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e0763860-958c-4f2c-928e-9625f7a37a39.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e1669dcb-672e-4bca-bf68-6ff242618aa0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e683f70e-10b0-4189-ae3f-7007d32ca57a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e9f575c0-bf54-4252-9551-1d4c69eda1bf.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f143d458-e4d0-4b44-9e25-d13e8578c9a6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f146e689-f045-4b3a-999d-ab0b287391bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f867c94f-c15b-4e40-9ffe-b0c598b90881.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/fdda2f51-9ed9-4959-9897-9581595c1bfb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Smillie et al. (2019) Cell; Publication: https://doi.org/10.1016/j.cell.2019.06.029 Dataset Version: https://datasets.cellxgene.cziscience.com/6c483976-30de-4835-97f0-2b9bc93614e7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/33d19f34-87f5-455b-8ca5-9023a2e5453d</li> <li>Publication Reference: Smith et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2023333118 Dataset Version: https://datasets.cellxgene.cziscience.com/bf50dbfb-9ca0-4f0d-8deb-a1a810a0e313.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e02201d7-f49f-401f-baf0-1eb1406546c0</li> <li>Publication Reference: Smith et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2023333118 Dataset Version: https://datasets.cellxgene.cziscience.com/ff7778bf-7a65-4d23-a9f4-b26c47926c28.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e02201d7-f49f-401f-baf0-1eb1406546c0</li> <li>Publication Reference: Sol\\u00e9-Boldo et al. (2020) Commun Biol; Publication: https://doi.org/10.1038/s42003-020-0922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/bc8d7152-3b69-4153-9314-7342ae58fbde.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c353707f-09a4-4f12-92a0-cb741e57e5f0</li> <li>Publication Reference: Stephenson et al. (2021) Nat Med; Publication: https://doi.org/10.1038/s41591-021-01329-2 Dataset Version: https://datasets.cellxgene.cziscience.com/46586a98-b75d-4557-9cc4-839fc28e67d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ddfad306-714d-4cc0-9985-d9072820c530</li> <li>Publication Reference: Stewart et al. (2019) Science; Publication: https://doi.org/10.1126/science.aat5031 Dataset Version: https://datasets.cellxgene.cziscience.com/40ebb8e4-1a25-4a33-b8ff-02d1156e4e9b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/120e86b4-1195-48c5-845b-b98054105eec</li> <li>Publication Reference: Stewart et al. (2019) Science; Publication: https://doi.org/10.1126/science.aat5031 Dataset Version: https://datasets.cellxgene.cziscience.com/fe7e4408-7390-4f93-95aa-ffe472843421.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/120e86b4-1195-48c5-845b-b98054105eec</li> <li>Publication Reference: Strati et al. (2023) Cell Reports Medicine; Publication: https://doi.org/10.1016/j.xcrm.2023.101158 Dataset Version: https://datasets.cellxgene.cziscience.com/21d11474-fe9a-420d-9661-5b88ba407bc1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/26b5b4f6-828c-4791-b4a3-abb19e3b1952</li> <li>Publication Reference: Suo et al. (2022) Science; Publication: https://doi.org/10.1126/science.abo0510 Dataset Version: https://datasets.cellxgene.cziscience.com/fe340fe0-f2e8-4e7b-8879-0161248129d3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b1a879f6-5638-48d3-8f64-f6592c1b1561</li> <li>Publication Reference: Szabo et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-12464-3 Dataset Version: https://datasets.cellxgene.cziscience.com/71c5026d-9567-4d0f-a808-edf29440df43.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/24d42e5e-ce6d-45ff-a66b-a3b3b715deaf</li> <li>Publication Reference: The Tabula Sapiens Consortium* et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl4896 Dataset Version: https://datasets.cellxgene.cziscience.com/981bcf57-30cb-4a85-b905-e04373432fef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e5f58829-1a66-40b5-a624-9046778e74f5</li> <li>Publication Reference: Travaglini et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/0fd9c007-8ba2-4d87-8568-c938d2631fba.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5d445965-6f1a-4b68-ba3a-b8f765155d3a</li> <li>Publication Reference: Travaglini et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/6dde7580-6b89-4cf9-83b4-c778f06eda7c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5d445965-6f1a-4b68-ba3a-b8f765155d3a</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/3967ab0f-a63c-4318-809e-73329341ba5e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/61f15353-e598-43b5-bb5a-80ac44a0cf0b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/9397b7fa-a9b5-4b56-8570-c30cb09d9df8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Tritschler et al. (2022) Molecular Metabolism; Publication: https://doi.org/10.1016/j.molmet.2022.101595 Dataset Version: https://datasets.cellxgene.cziscience.com/9abfdde4-1d63-4c9a-8ec5-2fff1bd6f387.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0a77d4c0-d5d0-40f0-aa1a-5e1429bcbd7e</li> <li>Publication Reference: Ulrich et al. (2021) bioRxiv; Publication: https://doi.org/10.1101/2021.09.16.460628 Dataset Version: https://datasets.cellxgene.cziscience.com/7fa27624-7eda-454a-a066-4de49d5788bd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/fc77d2ae-247d-44d7-aa24-3f4859254c2c</li> <li>Publication Reference: Velmeshev et al. (2023) Science; Publication: https://doi.org/10.1126/science.adf0834 Dataset Version: https://datasets.cellxgene.cziscience.com/b126d3e9-a1a2-419a-a049-b4a27d3ce3ab.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bacccb91-066d-4453-b70e-59de0b4598cd</li> <li>Publication Reference: Vento-Tormo et al. (2018) Nature; Publication: https://doi.org/10.1038/s41586-018-0698-6 Dataset Version: https://datasets.cellxgene.cziscience.com/7a3bdf11-fbb1-4966-8742-38b574c47317.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a9254216-6cd8-4186-b32c-349363777584</li> <li>Publication Reference: Wang et al. (2020) eLife; Publication: https://doi.org/10.7554/eLife.62522 Dataset Version: https://datasets.cellxgene.cziscience.com/b0afc679-2ec0-4c8d-9e3e-19cd693f8462.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/625f6bf4-2f33-4942-962e-35243d284837</li> <li>Publication Reference: Wang et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.032 Dataset Version: https://datasets.cellxgene.cziscience.com/07d4feab-33de-4bb2-8a5d-452044bc066d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03cdc7f4-bd08-49d0-a395-4487c0e5a168</li> <li>Publication Reference: Wang et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.032 Dataset Version: https://datasets.cellxgene.cziscience.com/20f29b78-5b74-4e6d-bc88-679116733988.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03cdc7f4-bd08-49d0-a395-4487c0e5a168</li> <li>Publication Reference: Wiedemann et al. (2023) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2023.111994 Dataset Version: https://datasets.cellxgene.cziscience.com/1ffe8f40-d258-4c05-885e-46375c483fc7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6d203948-a779-4b69-9b3f-1ee1dadc3980</li> <li>Publication Reference: Wilk et al. (2020) Nat Med; Publication: https://doi.org/10.1038/s41591-020-0944-y Dataset Version: https://datasets.cellxgene.cziscience.com/419da3c2-9141-4654-817f-ee6472df4be3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a72afd53-ab92-4511-88da-252fb0e26b9a</li> <li>Publication Reference: Wilson et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-32972-z Dataset Version: https://datasets.cellxgene.cziscience.com/14e77ad6-38ce-4aad-8385-68b21aff0737.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b3e2c6e3-9b05-4da9-8f42-da38a664b45b</li> <li>Publication Reference: Xiang et al. (2020) Front. Cardiovasc. Med.; Publication: https://doi.org/10.3389/fcvm.2020.00052 Dataset Version: https://datasets.cellxgene.cziscience.com/4dc06a70-6d39-4da6-aa8d-2f3fcdbbc1ff.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9c8808ce-1138-4dbe-818c-171cff10e650</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/60eec096-34b9-45c2-b433-3caffb84f955.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/6d75c1db-1fe2-4a0e-b55f-680c6df9c99e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/8de7d30b-caeb-4def-aad2-592c39cb3f3c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/96c7866c-7111-4687-b2c4-fbd445842a30.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/a9d71d53-66a1-4681-ba97-9671c05dff6d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/b67dcfb1-ccae-404c-b0d4-d681ac227858.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yazar et al. (2022) Science; Publication: https://doi.org/10.1126/science.abf3041 Dataset Version: https://datasets.cellxgene.cziscience.com/b3e8792e-a31b-404b-a866-250c43bc06d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dde06e0f-ab3b-46be-96a2-a8082383c4a1</li> <li>Publication Reference: Yoshida et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-021-04345-x Dataset Version: https://datasets.cellxgene.cziscience.com/69be948c-03f4-46e8-896e-530c79080808.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03f821b4-87be-4ff4-b65a-b5fc00061da7</li> <li>Publication Reference: Yoshida et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-021-04345-x Dataset Version: https://datasets.cellxgene.cziscience.com/d911082e-b5e2-40e6-8f08-fb53c7894622.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03f821b4-87be-4ff4-b65a-b5fc00061da7</li> <li>Publication Reference: Yu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.028 Dataset Version: https://datasets.cellxgene.cziscience.com/c7bec699-47c3-44ee-a311-ae8507adf6bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dfc09a93-bce0-4c77-893d-e153d1b7f9fa</li> <li>Publication Reference: Yu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.028 Dataset Version: https://datasets.cellxgene.cziscience.com/da7ca6a3-079c-428f-8453-9b21ecce87a7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dfc09a93-bce0-4c77-893d-e153d1b7f9fa</li> <li>Publication Reference: Zhang et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2103240118 Dataset Version: https://datasets.cellxgene.cziscience.com/b6a3566e-a7bf-4fb3-b20c-858c6330e380.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1df8c90d-d299-4b2e-a54d-a5a80f36e780</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/47573dc2-1dfc-4ca8-8c6e-b705a6656159.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/9fa63ab8-6316-4460-8283-af290fec1654.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/a76ab167-d254-4128-b5ca-86960560074a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/e999edf8-f5e6-4af7-975c-1d8f0a5e8d3e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/f5f99f11-5a99-4a72-bd0d-0a87c5b6b406.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van der Wijst et al. (2021) Sci. Transl. Med.; Publication: https://doi.org/10.1126/scitranslmed.abh2624 Dataset Version: https://datasets.cellxgene.cziscience.com/02a1eee1-e290-47d1-8d9d-bc7f51c13670.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7d7cabfd-1d1f-40af-96b7-26a0825a306d</li> </ul>"},{"location":"datasets/uniprot/","title":"UniProt Dataset","text":"<p>The UniProt Knowledgebase (UniProtKB) is an open database of protein sequences curated from translated genomic data [1]. The UniProt Reference Cluster (UniRef) databases provide clustered sets of sequences from UniProtKB [2], which have been used in previous large language model training studies to improve diversity in protein training data. UniRef clusters proteins hierarchically. At the highest level, UniRef100 groups proteins with identical primary sequences from the UniProt Archive (UniParc). UniRef90 clusters these unique sequences into buckets with 90% sequence similarity, selecting a single sequence from within each cluster as the representative sequence. UniRef50 is then built by clustering these UniRef90 representative sequences into groups with 50% sequence similarity.</p>"},{"location":"datasets/uniprot/#data-used-for-esm-2-pre-training","title":"Data Used for ESM-2 Pre-training","text":"<p>Since the original train/test splits from ESM-2 were not available [3], we replicated the ESM-2 pre-training experiments with UniProt's 2024_03 release. Following the approach described by the ESM-2 authors, we removed artificial sequences and reserved 0.5% of UniRef50 clusters for validation. From the 65,672,139 UniRef50 clusters, this resulted in 328,360 validation sequences. We then ran MMSeqs to further ensure no contamination of the training set with sequences similar to the validation set. This resulted in 65,182,365 training UniRef50 clusters comprising 187,382,018 UniRef90 sequences.</p> <p>Pretraining batches were formed by uniformly sampling each UniRef50 cluster from the training database, taking a randomly chosen UniRef90 sequence from each.</p>"},{"location":"datasets/uniprot/#data-availability","title":"Data Availability","text":"<p>Two versions of the dataset are distributed, a full training dataset (~80Gb) and a 10,000 UniRef50 cluster random slice (~150Mb). To load and use the sanity dataset, the bionemo.core.data.load function can be used to materialize the sanity dataset in the BioNeMo2 cache directory:</p> <pre><code>from bionemo.core.data.load import load\n\nsanity_data_dir = load(\"esm2/testdata_esm2_pretrain:2.0\")\n</code></pre>"},{"location":"datasets/uniprot/#ngc-resource-links","title":"NGC Resource Links","text":"<ul> <li>Sanity Dataset</li> <li>[Full Dataset]</li> </ul>"},{"location":"datasets/uniprot/#reference","title":"Reference","text":"<ol> <li> <p>UniProt Consortium. (2023). UniProt: The universal protein knowledgebase in 2023. Nucleic Acids Research, 51(D1),    D523\u2013D531. doi:10.1093/nar/gkac1052</p> </li> <li> <p>Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu, C. H., &amp; UniProt Consortium. (2015). UniRef clusters: a    comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics (Oxford, England),    31(6), 926\u2013932. doi:10.1093/bioinformatics/btu739</p> </li> <li> <p>Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., \u2026 Rives, A. (2023). Evolutionary-scale prediction of    atomic-level protein structure with a language model. Science (New York, N.Y.), 379(6637), 1123\u20131130.    doi:10.1126/science.ade2574</p> </li> </ol>"},{"location":"models/","title":"BioNeMo Framework: Available Models","text":"<p>State-of-the-art models are continually integrated into the BioNeMo Framework. The BioNeMo Framework currently offers the following pre-trained models:</p> Model Modality Uses ESM-2 Protein Representation Learning Geneformer Single Cell Representation Learning <p>For more information about the models included in BioNeMo Framework, refer to the Model Cards linked in the table above or the original publications referenced in the respective model descriptions.</p>"},{"location":"models/geneformer/","title":"Geneformer","text":"<p>Current checkpoints trained in BioNeMo1</p> <p>This document references performance numbers and runtime engines that are from the bionemo v1 variant of the model. These numbers will be updated in a coming release to reflect the new bionemo v2 codebase. The model architecture and training information will be the same, as checkpoints are converted from bionemo v1 format to v2 format. Benchmarks below are annotated with which version of bionemo generated them. Accuracy should be the same within a small epsilon since we have tests in place showing model equivalency between the two versions.</p>"},{"location":"models/geneformer/#model-overview","title":"Model Overview","text":""},{"location":"models/geneformer/#description","title":"Description:","text":"<p>Geneformer generates a dense representation of a sc-RNA cell by learning co-expression patterns within single cells. Geneformer is a tabular count model trained on sc-RNA from the Chan Zuckerberg Cell x Gene census. Geneformer computes a complete embedding for each cell over the top 1024 expressed genes. The embeddings are used as features for a variety of predictive tasks. This model is ready for both commercial and academic use.</p>"},{"location":"models/geneformer/#references","title":"References:","text":"<ul> <li>Geneformer, reference foundation model for single-cell RNA: Transfer learning enables predictions in network biology | Nature</li> <li>scGPT, alternative foundation model for single-cell RNA: scGPT: toward building a foundation model for single-cell multi-omics using generative AI | Nature Methods</li> <li>scBERT, alternative foundation model for single-cell RNA: scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data | Nature Machine Intelligence</li> <li>scFoundation, alternative foundation model for single-cell RNA: Large Scale Foundation Model on Single-cell Transcriptomics | bioRxiv   Cell x Gene census, public repository for sc-RNA experiments: CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com)</li> </ul>"},{"location":"models/geneformer/#model-architecture","title":"Model Architecture:","text":"<p>Architecture Type: Bidirectional Encoder Representations from Transformers (BERT) Network Architecture: Geneformer </p>"},{"location":"models/geneformer/#input","title":"Input:","text":"<p>Input Type(s): Number (Row represents cell, containing gene names and single cell expression counts)  Input Format(s): Array AnnData Input Parameters: 1D </p>"},{"location":"models/geneformer/#output","title":"Output:","text":"<p>Output Type(s): Vector (Dense Embedding Predictions)embeddings.  Output Format: NumPy  Output Parameters: 1D  Other Properties Related to Output: Numeric floating point vector (fp16, bf16, or fp32); geneformer-10M-240530 outputs 256 dimensional embeddings; geneformer-106M-240530 outputs 768 dimensional embeddings </p>"},{"location":"models/geneformer/#software-integration","title":"Software Integration:","text":"<p>Runtime Engine(s):</p> <ul> <li>BioNeMo, NeMo 1.2 </li> </ul> <p>Supported Hardware Microarchitecture Compatibility: </p> <ul> <li>Ampere </li> <li>Hopper </li> <li>Volta </li> </ul> <p>[Preferred/Supported] Operating System(s): </p> <ul> <li>Linux </li> </ul>"},{"location":"models/geneformer/#model-versions","title":"Model Version(s):","text":"<ul> <li>geneformer-10M-240530 </li> <li>10.3M parameter geneformer variant.</li> <li>25429 ensemble ID based gene tokens</li> <li>256 hidden dimensions with 4 heads, 6 layers and an 512 dimensional FFN</li> <li>relu activation</li> <li>1e-12 EPS layernorm</li> <li>bf16 mixed precision training with 32 bit residual connections</li> <li>2% hidden dropout, 10% attention dropout</li> <li>geneformer-106M-240530</li> <li>106M parameter geneformer variant.</li> <li>25429 ensemble ID based gene tokens</li> <li>768 hidden dimensions with 12 heads, 12 layers and an 3072 dimensional FFN</li> <li>relu activation</li> <li>1e-12 EPS layernorm</li> <li>bf16 mixed precision training with 32 bit residual connections</li> <li>2% hidden dropout, 10% attention dropout</li> </ul>"},{"location":"models/geneformer/#training-evaluation","title":"Training &amp; Evaluation:","text":""},{"location":"models/geneformer/#training-dataset","title":"Training Dataset:","text":"<p>Single cell expression counts from CELLxGENE Census used for the direct download of data matching similar criteria to those described in the geneformer publication. limiting cell data to organism=\u201dHomo sapiens\u201d, with a non \u201cna\u201d suspension_type, is_primary_data=True, and disease=\u201dnormal\u201d to limit to non-diseased tissues that are also the primary data source per cell to make sure that cells are only included once in the download. We tracked metadata including \u201cassay\u201d, \u201csex\u201d, \u201cdevelopment_stage\u201d, \u201ctissue_general\u201d, \u201cdataset_id\u201d and \u201cself_reported_ethnicity\u201d. The metadata \u201cassay\u201d, \u201ctissue_general\u201d, and \u201cdataset_id\u201d were used to construct dataset splits into train, validation, and test sets.</p> <p>The training set represented 99% of the downloaded cells. We partitioned the data by dataset_id into a train set (99%) and a hold-out set (1%), to make sure that the hold-out datasets were independently collected single cell experiments, which helps evaluate generalizability to new future datasets.</p> <p>In this training split, we made sure that all \u201cassay\u201d and \u201ctissue_general\u201d labels were present in the training set so that our model would have maximal visibility into different tissues and assay biases.</p> <p>The 1% hold-out evaluation set was split further into a validation and test set. This final split was mostly done randomly by cell; however, we set aside a full dataset into the test split so that we could evaluate performance after training on a completely unseen dataset, including when monitoring the validation loss during training.</p> <p>Link: Datasets downloaded from CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com)  ** Data Collection Method by dataset </p> <ul> <li>[Human] </li> </ul> <p>** Labeling Method by dataset </p> <ul> <li>Hybrid: Automated, Human </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)): 23.64 million non-diseased and human-derived single cells were chosen from the CZI CELLxGENE census, which is characterized as follows: </p> <ul> <li>Assay Bias:</li> <li>The vast majority of the dataset is one of the 10x genomics assays, Approximately 20M of 26M cells are genomic assays, 4M are sci-RNA-seq, while Remaining assays (microwell-seq, drop-seq, bd rhapsody, smart-seq, seq-well, and MARS-seq) represent small fractions of the full datasets.</li> <li>Sex:</li> <li>12.5M are male-derived cells; 10M are female derived cells. The remaining cells are not annotated.</li> <li>Self-Reported Ethnicity:</li> <li>Approximately 12M cells are not annotated; 9M are annotated as \u201cEuropean.\u201d .5M are annotated as \u201cHan Chinese.\u201d followed by \u201cAfrican American\u201d.</li> <li>Age Bias:</li> <li>The dataset is heavily biased toward donors less than one year. The next highest group would be the segment that includes ages 21-30.</li> <li>Tissue Type Bias:</li> <li>9M cells are \u201cbrain\u201d derived. 4M are blood derived, followed by \u201clung\u201d, \u201cbreast\u201d, \u201cheart\u201d and \u201ceye\u201d at approximately 1M cells each.</li> </ul> <p>Dataset was derived from a limited number of public sources where methods and protocols may not represent sufficiently diverse sources to capture the full scope of gene expression.</p>"},{"location":"models/geneformer/#evaluation-dataset","title":"Evaluation Dataset:","text":"<p>Adamson et al 2016 PERTURB-seq dataset, accessed by Harvard dataverse. Link: adamson.zip - Harvard Dataverse  ** Data Collection Method by dataset </p> <ul> <li>Human </li> </ul> <p>** Labeling Method by dataset </p> <ul> <li>Automated - Molecular Barcoding </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)): There are ~20k single cells, half of which represent unperturbed control samples, and the other half which contain an additional datatable containing the CRISPR knock-out targets for each cell.</p> <p>Link: CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com)  ** Data Collection Method by dataset </p> <ul> <li>Human </li> </ul> <p>** Labeling Method by dataset </p> <ul> <li>Hybrid: Automated, Human </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)):</p> <ul> <li>240,000 single cells were chosen from the CZI cell x gene census such that they did not share a <code>dataset_id</code> with any cell in the training data described previously.</li> </ul>"},{"location":"models/geneformer/#inference","title":"Inference:","text":"<p>Engine: BioNeMo, NeMo  Test Hardware: </p> <ul> <li>Ampere </li> <li>Hopper </li> <li>Volta </li> </ul> <p>*Additional description content may be included here</p>"},{"location":"models/geneformer/#ethical-considerations","title":"Ethical Considerations:","text":"<p>NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety &amp; Security, and Privacy Subcards [Insert Link to Model Card++ here]. Please report security vulnerabilities or NVIDIA AI Concerns here.</p>"},{"location":"models/geneformer/#training-diagnostics","title":"Training diagnostics","text":""},{"location":"models/geneformer/#geneformer-10m-240530","title":"geneformer-10M-240530","text":"<p>This checkpoint was trained for approximately 11 epochs through the CELLxGENE split. Training was performed on 8 servers with 8 A100 GPUs each for a total of 115430 steps of per-gpu micro batch size 32 and global batch size of 2048. Training took a total of 1 day, 20 hours and 19 minutes of wallclock time. As can be seen in the following image, training and validation curves both decreased fairly smoothly throughout the course of training. In fact validation (blue) and training (orange) loss were both still decreasing at the end of 11 epochs through the dataset. The model could likely be trained for more epochs without overfitting. </p> <p>Training curves from BioNeMo1</p> <p>Note that these curves were generated on BioNeMo1. We see the same general training curves in our initial testing of BioNeMo2, however. In the following figure the blue line is the previous training run of the 10M model and the red curve is an equivalent training run on BioNeMo2. As we release new checkpoints they will be trained on BioNeMo2.</p> <p></p>"},{"location":"models/geneformer/#geneformer-106m-240530","title":"geneformer-106M-240530","text":"<p>This checkpoint was trained for approximately 11 epochs through the CELLxGENE split. Training was performed on 16 servers with 8 A100 GPUs each for a total of 115430 steps of per-gpu micro batch size 16 and global batch size of 2048. Training took a total of 3 days, 18 hours and 55 minutes of wallclock time. As can be seen in the following image, training and validation curves both decreased fairly smoothly throughout the course of training. In fact validation (blue) and training (orange) loss were both still decreasing at the end of 11 epochs through the dataset. The model could likely be trained for more epochs without overfitting. </p> <p>Additionally, validation loss decreased both faster and continued to decrease at the same improved rate throughout training in the 106M parameter model (red) as compared to the 10M parameter model (blue). It would be interesting to test even larger models to see if we continue to observe improved performance in larger models. </p> <p>!! note \"Training curves from BioNeMo1\"</p> <pre><code>As stated in the previous section, the figures are from our BioNeMo1 code base where these checkpoints were originally\ntrained. As we release new checkpoints they will be trained on BioNeMo2.\n</code></pre>"},{"location":"models/geneformer/#benchmarking","title":"Benchmarking","text":""},{"location":"models/geneformer/#accuracy-benchmarks","title":"Accuracy Benchmarks","text":""},{"location":"models/geneformer/#masked-language-model-mlm-loss","title":"Masked language model (MLM) loss","text":"<p>The following describes the bert MLM token loss. Like in the original BERT paper, and the geneformer paper, 15% of all tokens are included in the loss. Of the included tokens, 80% are <code>\"[MASK]\"</code> token, 2% are a random gene token, and 18% are the correct output token. Note that this was an unintentional deviation from the original publication, but so far it seems to be working well. In the future we will test the intended 80%/10%/10% mixture proposed in the paper. The token loss in the following table is the mean cross entropy loss of the 15% of tokens included in the loss mask averaged across cells. As a baseline geneformer was downloaded from the ctheodoris/Geneformer page on hugging face on 2024/11/04 and applied to the same masking/unmasking problem on this dataset, but with model-specific cell representations due to the updated tokenizer and medians dictionary used to train, and the update from training with 2048 tokens to 4096 tokens per cell. The held-out <code>test</code> dataset from our training splits described previously was used, and it should be noted that some of these cells may have been involved in training the baseline geneformer.</p> Model Description Token Loss (lower is better) Baseline geneformer 2.26* geneformer-10M-240530 2.64 geneformer-106M-240530 2.34 <p>Baseline Geneformer was recently updated on huggingface making loss comparisons challenging.</p> <p>Geneformer was recently updated on hugging face to a new version. In a future release we will make checkpoint conversion scripts available so that the public model can be ran directly. Some key differences follow:</p> <ul> <li>Trained on a much larger 95M cell dataset. Our current checkpoints were trained with 23M cells.</li> <li>The new 12 layer baseline geneformer variant sits between our 10M and 106M parameter models in parameter count with   approximately 38M parameters.</li> <li>The model is trained with a 4096 context rather than a 2048 context. When forcing the model to make predictions   with a 2048 context, the MLM loss drops to 2.76, which is probably unfair because this may be \"out of domain\" for   training. It is really hard to compare these loss numbers directly is the only take-home here.</li> <li>The model was trained on a set of 20,275 genes, rather than the older set of 25,426 genes. This would also be   expected to give a boost in loss since there are fewer tokens to choose from.</li> </ul>"},{"location":"models/geneformer/#downstream-task-accuracy","title":"Downstream task accuracy","text":"<p>Here we benchmark four models, with two baselines. These models are tasked with cell type classification, using the Chron's disease small intestine dataset from Elmentaite et al. (2020), Developmental Cell. This dataset contains approximately 22,500 single cells from both healthy children aged 4-13 and chidlren with Chron's disease. This dataset contains 31 unique cell types which we assume to be annotated accurately. This dataset was held out of our pre-training dataset as all diseased samples were removed.</p> <ul> <li>Baseline 1) scRNA workflow: this model uses PCA with 10 components and random forest on normalized and log transformed expression counts to produce a result.</li> <li>Baseline 2) geneformer-qa, a model trained for approximately 100 steps with approximately random weights. We expect this model to perform no differently than working on counts directly.</li> <li>geneformer-10M-240530 and geneformer-106M-240530 as described above.</li> </ul> <p>For more details see the example notebook titled Geneformer-celltype-classification-example.ipynb</p> <p> </p>"},{"location":"models/geneformer/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>The 106M parameter variant of Geneformer achieves over 50 TFLOPS per GPU during training. This is consistent whether trained with 1 or 8 A100s.</p> <p></p> <p>TFLOPS are from BioNeMo1, early evidence of speedups in BioNeMo2</p> <p>We have observed equivalent or better performance in BioNeMo2 vs BioNeMo1 for the Geneformer model. One example is a cluster run where we see a time-per step at 0.26 seconds per iteration with a batch size of 64 through Geneformer. An older BioNeMo1 run that was properly configured had a time-per-step of 0.09 with a batch size of 16. When you consider samples per second this would imply a significant speedup in BioNeMo2, however this is anecdotal and a more thorough comparison is forthcoming.</p>"},{"location":"models/ESM-2/","title":"ESM-2","text":""},{"location":"models/ESM-2/#model-overview","title":"Model Overview","text":""},{"location":"models/ESM-2/#description","title":"Description","text":"<p>ESM-2 is a pre-trained, bi-directional encoder (BERT-style model) over amino acid sequences. ESM-2 models provide embeddings for amino acids that have led to state-of-the-art performance on downstream tasks such as structure and function prediction. ESM-2 has been trained at a number of different model sizes. BioNeMo2 includes converted checkpoints for the 650M and 3B parameter variants. The 650M model has 33 layers, 20 attention heads, and a hidden space dimension of 1280. The 3B model has 36 layers, 40 attention heads, and a hidden space dimension of 2,560.</p> <p>These models are ready for commercial use.</p>"},{"location":"models/ESM-2/#third-party-community-consideration","title":"Third-Party Community Consideration","text":"<p>This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party\u2019s requirements for this application and use case [1]; see link to Non-NVIDIA Model Card for ESM-2 3B model and non-NVIDIA Model Card for ESM-2 650M model</p>"},{"location":"models/ESM-2/#references","title":"References","text":"<p>[1] Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y. and dos Santos Costa, A., 2023. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637), pp.1123-1130.</p> <p>[2] \"UniProt: the universal protein knowledgebase in 2021.\" Nucleic acids research 49, no. D1 (2021): D480-D489.</p> <p>[3] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>"},{"location":"models/ESM-2/#model-architecture","title":"Model Architecture","text":"<p>Architecture Type: BERT</p> <p>Network Architecture: ESM-2</p>"},{"location":"models/ESM-2/#input","title":"Input","text":"<p>Input Type(s): Text (Protein Sequences)</p> <p>Input Parameters: 1D</p> <p>Other Properties Related to Input: Protein sequence represented as a string of canonical amino acids, of maximum length 1022. Longer sequences are automatically truncated to this length.</p>"},{"location":"models/ESM-2/#output","title":"Output","text":"<p>Output Type(s): Embeddings (Amino-acid and sequence-level)</p> <p>Output Parameters: 1D</p> <p>Other Properties Related to Output: Numeric vector with floating-point values corresponding to an embedding for each amino acid in the input protein sequence. Maximum output length is 1022 embeddings - one embedding vector per amino acid.</p>"},{"location":"models/ESM-2/#software-integration","title":"Software Integration","text":"<p>Runtime Engine(s)</p> <ul> <li>BioNeMo, NeMo, Megatron, TransformerEngine</li> </ul> <p>Supported Hardware Microarchitecture Compatibility</p> <ul> <li>[Ampere]</li> <li>[Hopper]</li> <li>[Volta]</li> </ul> <p>[Preferred/Supported] Operating System(s)</p> <ul> <li>[Linux]</li> </ul>"},{"location":"models/ESM-2/#model-versions","title":"Model Version(s)","text":"<ul> <li>esm2/650m:2.0</li> <li>esm2/3b:2.0</li> </ul>"},{"location":"models/ESM-2/#training-evaluation","title":"Training &amp; Evaluation","text":""},{"location":"models/ESM-2/#training-dataset","title":"Training Dataset","text":"<p>Original ESM-2 checkpoints from HuggingFace were trained with the UniProt 2021_04 sequence database. For more details on the training dataset, see Lin et al. 2023. The train / test splits used by the original authors were not distributed. A pre-training database compiled by NVIDIA following a similar approach is described in UniProt Dataset.</p>"},{"location":"models/ESM-2/#inference","title":"Inference","text":"<p>Engine: BioNeMo, NeMo</p> <p>Test Hardware</p> <ul> <li>[Ampere]</li> <li>[Hopper]</li> <li>[Volta]</li> </ul>"},{"location":"models/ESM-2/#license","title":"License","text":"<p>ESM-2 is as provided under the Apache 2.0 license.</p>"},{"location":"models/ESM-2/#competitive-benchmarking","title":"Competitive Benchmarking","text":""},{"location":"models/ESM-2/#accuracy","title":"Accuracy","text":"<p>A validation set of 328,360 UniRef50 representative sequences were randomly selected from UniRef 2024_03 (see UniProt Dataset). This validation set was used to ensure that the output of BioNeMo-converted checkpoints is consistent with their outputs when evaluated with the HuggingFace Transformers library.</p> Checkpoint HuggingFace BioNeMo2 Lin et al. 2023 650M 7.001 7.002 6.95  3B 6.003 6.004 6.49  <p>Different Validation Sets</p> <p>The HuggingFace and converted BioNeMo2 checkpoints were evaluated on a newly curated validation set. Perplexities from Lin et al. 2023 are reported for comparison, but the original train/test splits are not available.</p>"},{"location":"models/ESM-2/#training-performance","title":"Training Performance","text":""},{"location":"models/ESM-2/#single-node-training-performance","title":"Single-node Training Performance","text":"<p>The pure-pytorch baseline (compiled with <code>torch.compile()</code>) raised an out-of-memory error for batch sizes larger than 16 at the ESM2-650M model size. The <code>bionemo2</code> model could handle batch sizes of 46, reaching a model flops utilization of 59.2% on an NVIDIA A100.</p>"},{"location":"models/ESM-2/#model-scaling","title":"Model Scaling","text":"<p>Training ESM-2 at the 650M, 3B, and 15B model variants show improved performance with the BioNeMo2 framework over the pure-pytorch baseline. These experiments were conducted on 16x NVIDIA A100 or 16x NVIDIA H100 GPUs split across two nodes. <sup>Note:* 15B model variants were trained on 64 GPUs with the BioNeMo2 framework."},{"location":"models/ESM-2/#device-scaling","title":"Device Scaling","text":"<p>Training ESM-3B on 256 NVIDIA A100s on 32 nodes achieved 96.85% of the theoretical linear throughput expected from extrapolating single-node (8 GPU) performance, representing a model flops utilization of 60.6% at 256 devices.</p>"},{"location":"models/ESM-2/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Model Overview</li> <li>Pre-trained Checkpoints</li> </ul>"},{"location":"models/ESM-2/pre-training/","title":"Pre-training ESM-2","text":"<p>Pre-trained checkpoints for ESM-2 are available at the 8M, 650M, and 3B model sizes. These models were trained by the bionemo-framework team to reproduce the original training results from Lin et al, Science (2023), with more recent UniProt data and leveraging the bionemo training infrastructure. The full pre-training data and train/test splits are available.</p>"},{"location":"models/ESM-2/pre-training/#model-convergence","title":"Model Convergence","text":"<p>Validation perplexity evaluated on the NVIDIA validation set.</p> <p></p> Model Size Perplexity at 500k updates 8M 10.26 650M 7.14 3B 6.42"},{"location":"models/ESM-2/pre-training/#pre-training-recipes","title":"Pre-training recipes","text":"8M650M3B <pre><code>esm2_8m_ckpt_path = load(\"esm2/8m:2.0\")\n</code></pre> <pre><code>esm2_650m_ckpt_path = load(\"esm2/nv_650m:2.1\")\n</code></pre> <pre><code>esm2_3b_ckpt_path = load(\"esm2/nv_3b:2.1\")\n</code></pre>"},{"location":"models/ESM-2/pre-training/#training-script","title":"Training Script","text":"Training Parameters Value # of GPUs 32 GPU Type A100 Batch size (per device) 64 <pre><code>train_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=&lt;wandb-project-name&gt; \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\  # (1)!\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=64 \\\n  --num-nodes=4 \\\n  --num-gpus=8 \\\n  --val-check-interval=10000 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_8m \\\n  --experiment-name=esm2_pretrain_8m \\\n  --num-layers=6 \\\n  --hidden-size=320 \\\n  --num-attention-heads=20 \\\n  --ffn-hidden-size=1280;\n</code></pre> <ol> <li>Paths here must be mounted into the <code>bionemo-framework</code> docker image.</li> </ol>"},{"location":"models/ESM-2/pre-training/#training-script_1","title":"Training Script","text":"Training Parameters Value # of GPUs 64 GPU Type H100 Batch size (per device) 32 <pre><code>train_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=&lt;wandb-project-name&gt; \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\  # (1)!\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=32 \\\n  --num-nodes=8 \\\n  --num-gpus=8 \\\n  --val-check-interval=10000 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_650m \\\n  --experiment-name=esm2_pretrain_650m \\\n  --min-seq-length=1024 \\\n  --max-seq-length=1024 \\\n  --num-layers=33 \\\n  --hidden-size=1280 \\\n  --num-attention-heads=20 \\\n  --ffn-hidden-size=5120;\n</code></pre> <ol> <li>Paths here must be mounted into the <code>bionemo-framework</code> docker image.</li> </ol>"},{"location":"models/ESM-2/pre-training/#training-script_2","title":"Training Script","text":"Training Parameters Value # of GPUs 128 GPU Type H100 Batch size (per device) 16 warmup steps 20,000 <pre><code>train_esm2 \\\n  --create-tensorboard-logger \\\n  --resume-if-exists \\\n  --wandb-project=&lt;wandb-project-name&gt; \\\n  --save-top-k=10 \\\n  --train-cluster-path=/data/train_clusters.parquet \\  # (2)!\n  --train-database-path=/data/train.db \\\n  --valid-cluster-path=/data/valid_clusters.parquet \\\n  --valid-database-path=/data/validation.db \\\n  --num-steps=500_000 \\\n  --warmup-steps=20_000 \\  # (1)!\n  --metric-to-monitor-for-checkpoints=val_loss \\\n  --micro-batch-size=16 \\\n  --num-nodes=16 \\\n  --num-gpus=8 \\\n  --val-check-interval=2500 \\\n  --limit-val-batches=1.0 \\\n  --result-dir=/results/esm2_pretrain_3b \\\n  --experiment-name=esm2_pretrain_3b \\\n  --min-seq-length=1024 \\\n  --max-seq-length=1024 \\\n  --num-layers=36 \\\n  --hidden-size=2560 \\\n  --num-attention-heads=40 \\\n  --ffn-hidden-size=10240;\n</code></pre> <ol> <li> <p>We had to increase the number of warmup steps 10x over the published training recipe for ESM-2 3B, which was    likely trained with fp16 precision. This gave us an overall similar initial curve, but avoided convergence issues    at around 2,000 steps.</p> </li> <li> <p>Paths here must be mounted into the <code>bionemo-framework</code> docker image.</p> </li> </ol>"},{"location":"user-guide/","title":"What is BioNeMo?","text":"<p>BioNeMo is a software ecosystem produced by NVIDIA for the development and deployment of life sciences-oriented artificial intelligence models. BioNeMo provides a set of tools to help researchers build, train, and deploy AI models for various biological applications. The main components of BioNeMo are:</p> <ul> <li> <p>BioNeMo Framework: a free-to-use collection of programming tools and packages offering access to optimized, pre-trained biomolecular models and workflows. The framework enables building and customizing models, including training and fine-tuning. Capabilities span various workloads and therapeutic modalities, such as molecular generation, protein structure prediction, protein-ligand, and representation learning.</p> </li> <li> <p>BioNeMo NIMs: easy-to-use, enterprise-ready inference microservices with built-in API endpoints. NIMs are engineered for scalable, self- or cloud-hosted deployment of optimized, production-grade biomolecular foundation models. Check out the growing list of BioNeMo NIMs here.</p> </li> </ul> <p>When choosing between the BioNeMo Framework and BioNeMo NIMs, consider your project's specific requirements. The Framework is ideal for scenarios that require model training, fine-tuning, or customization, offering a comprehensive suite of tools and packages. In contrast, NIMs are optimized for inference-only workflows, providing easy-to-use, enterprise-ready microservices with built-in API endpoints. As a rule, use the Framework for custom model development or high-control modeling, and NIMs for inference against existing models.</p>"},{"location":"user-guide/#bionemo-user-success-stories","title":"BioNeMo User Success Stories","text":"<p>Enhancing Biologics Discovery and Development With Generative AI - Amgen leverages BioNeMo and DGX Cloud to train large language models (LLMs) on proprietary protein sequence data, predicting protein properties and designing biologics with enhanced capabilities. By using BioNeMo, Amgen achieved faster training and up to 100X faster post-training analysis, accelerating the drug discovery process.</p> <p>Cognizant to apply generative AI to enhance drug discovery for pharmaceutical clients with NVIDIA BioNeMo - Cognizant leverages BioNeMo to enhance drug discovery for pharmaceutical clients using generative AI technology. This collaboration enables researchers to rapidly analyze vast datasets, predict interactions between drug compounds, and create new development pathways, aiming to improve productivity, reduce costs, and accelerate the development of life-saving treatments.</p> <p>Cadence and NVIDIA Unveil Groundbreaking Generative AI and Accelerated Compute-Driven Innovations - Cadence's Orion molecular design platform will integrate with BioNeMo generative AI tool to accelerate therapeutic design and shorten time to trusted results in drug discovery. The combined platform will enable pharmaceutical companies to quickly generate and assess design hypotheses across various therapeutic modalities using on-demand GPU access.</p> <p>Find more user stories on NVIDIA's Customer Stories and Technical Blog sites.</p>"},{"location":"user-guide/SUMMARY/","title":"SUMMARY","text":"<ul> <li>What is BioNeMo?</li> <li>Getting Started</li> <li>Background</li> <li>Developer Guide</li> <li>Tutorials</li> <li>Developer Guide</li> <li>Contributing</li> <li>Appendix</li> </ul>"},{"location":"user-guide/appendix/FAQ/","title":"Frequently Asked Questions","text":""},{"location":"user-guide/appendix/FAQ/#is-bionemo-framework-free-to-use","title":"Is BioNeMo Framework free to use?","text":"<p>Yes, BioNeMo Framework is free to use. BioNeMo Framework code is licensed under the Apache 2.0 License. The Apache 2.0 License is a permissive open-source license that allows users to freely use, modify, and distribute software. With this license, users have the right to use the software for any purpose, including commercial use, without requiring royalties or attribution. Overall, our choice of the Apache 2.0 License allows for wide adoption and use of BioNeMo Framework, while also providing a high degree of freedom and flexibility for users.</p> <p>For users that would like NVIDIA AI Enterprise support for BioNeMo Framework container usage, refer to the NVAIE Landing Page for more information.</p>"},{"location":"user-guide/appendix/FAQ/#how-do-i-install-bionemo-framework","title":"How do I install BioNeMo Framework?","text":"<p>BioNeMo Framework is distributed as a Docker container through NVIDIA NGC. To download the pre-built Docker container and data assets, you will need a free NVIDIA NGC account.</p> <p>Alternatively, you can install individual sub-packages from within BioNeMo Framework by following the corresponding README pages the BioNeMo Framework GitHub. Please note that this is a beta feature and may require some additional effort to install seamlessly. We are actively working on testing this functionality and expect it will be a fully supported feature in future releases. You can review our release notes to stay up to date on our releases.</p>"},{"location":"user-guide/appendix/FAQ/#how-do-i-update-bionemo-framework-to-the-latest-version","title":"How do I update BioNeMo Framework to the latest version?","text":"<p>To update the BioNeMo Framework Docker container, you need to pull the latest version of the Docker image using the command <code>docker pull</code>. For available tags, refer to the BioNeMo Framework page in the NGC Catalog.</p>"},{"location":"user-guide/appendix/FAQ/#what-are-the-system-requirements-for-bionemo-framework","title":"What are the system requirements for BioNeMo Framework?","text":"<p>Generally, BioNeMo Framework should run on any NVIDIA GPU with Compute Capability \u22658.0. For a full list of supported hardware, refer to the Hardware and Software Prerequisites.</p>"},{"location":"user-guide/appendix/FAQ/#can-i-contribute-code-or-models-to-bionemo-framework","title":"Can I contribute code or models to BioNeMo Framework?","text":"<p>Yes, BioNeMo Framework is open source and we welcome contributions from organizations and individuals. You can do so either by forking the repository and directly opening a PR against our <code>main</code> branch from your fork or by contacting us fo r further assistance. BioNeMo Framework's mission is to stay extremely light weight and primarily support building blocks required for various AI models. As such, we currently prioritize feature extensions, bug fixes, and new independent modules such as dataloaders, tokenizers, custom architecture blocks, and other reusable features over end-to-end model implementations. We might consider end-to-end model implementations on a case-by-case basis. If you're interested in this contribution of this kind, we recommend reaching out to us first</p> <p>For more information about external contributions, refer to the Contributing and Code Review pages.</p>"},{"location":"user-guide/appendix/FAQ/#how-do-i-report-bugs-or-suggest-new-features","title":"How do I report bugs or suggest new features?","text":"<p>To report a bug or suggest a new feature, open an issue on the BioNeMo Framework GitHub site. For the fastest turnaround, thoroughly describe your issue, including any steps and/or minimal data sets necessary to reproduce (when possible), as well as the expected behavior.</p>"},{"location":"user-guide/appendix/FAQ/#can-i-train-models-in-jupyter-notebooks-using-bionemo-framework","title":"Can I train models in Jupyter notebooks using BioNeMo Framework?","text":"<p>At the current time, notebook-based training is not supported due to restrictions imposed by the Megatron framework that underpins the BioNeMo Framework models. However, the user may call training scripts using a subprocess, either through the use of the Python Subprocess module or through Jupyter's Shell Assignment or Bash Cell Magic. For the latter two options, we caution the user to be careful when using Python and shell variables as we have observed unpredictable and unreproducible behavior in certain instances.</p>"},{"location":"user-guide/appendix/releasenotes-fw/","title":"Release Notes","text":""},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v23","title":"BioNeMo Framework v2.3","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features","title":"New Features","text":"<ul> <li>Distributed Inference Support for ESM2 and Geneformer</li> <li>Enables linear inference throughput as GPU number is increased</li> <li>See ESM2 inference notebook and use <code>--num-gpus</code> parameter.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#updates-improvements","title":"Updates &amp; Improvements","text":"<ul> <li>Prior Geneformer inference on H100 accuracy regression fixed.</li> <li>Base image updated to <code>nvcr.io/nvidia/pytorch:24.12-py3</code>; python updated to 3.12 among other core dependency upgrades (base container release notes here).</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v22","title":"BioNeMo Framework v2.2","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_1","title":"New Features","text":"<ul> <li>Small Molecule Featurization</li> <li>Implemented elementary and advanced atom, bond, and full molecule featurizers.</li> <li>GH200 Support for BioNeMo</li> <li>Added a <code>Dockerfile.arm</code> that builds a BioNeMo container that runs on GH200 machines.</li> <li>Publish a version of the BioNeMo container that supports multiple architectures to NGC.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#updates-improvements_1","title":"Updates &amp; Improvements","text":"<ul> <li>Single-Cell Dataloader (SCDL)</li> <li>Changed metadata storage to <code>parquet</code> files, which creates a 30x speed up when iterating over a large dataset.</li> <li>Added functionality to concatenate several <code>anndata</code> files without doubling disk memory usage.</li> <li>ESM2</li> <li>Added support for <code>SIGTERM</code> preemption checkpoint saving.</li> <li>Moved ESM-2 and Geneformer training scripts to new executables, <code>train_esm2</code> and <code>train_geneformer</code>, respectively.</li> <li>Moved inference script to a new executable <code>infer_esm2</code>, and deprecated the inference example in the fine-tuning tutorial.</li> <li>Added new Jupyter notebook tutorials for inference and zero-shot protein design. These notebooks can be deployed on the cloud resources as a brev.dev launchable.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues","title":"Known Issues:","text":"<ul> <li>Loading a checkpoint for Geneformer inference on H100 has a known regression in accuracy. Work is in progress to resolve by next release.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v21","title":"BioNeMo Framework v2.1","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_2","title":"New Features:","text":"<ul> <li>ESM2 Implementation</li> <li>Updated the ESM-2 Model Card with detailed performance benchmarks comparing BioNeMo2 training against vanilla pytorch.</li> <li>Added ESM-2 inference endpoint for evaluating pre-trained models</li> <li>Size-Aware Batching</li> <li>Added SizeAwareBatchSampler, a pytorch data sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>Added BucketBatchSampler, another pytorch data sampler that groups elements of varying sizes based on predefined bucket ranges, and create batches with elements from each bucket to ensure that each batch has elements with homogeneous sizes.</li> <li>CLI Support</li> <li>Added pydantic interface for pretraining jobs via parsing JSON configuration files that enables passing customized Model and DataModules classes.</li> <li>Implemented pydantic configuration for Geneformer and ESM2 pretraining and finetuning.</li> <li>Added 'recipes' for generating validated JSON files to be used with pydantic interface.</li> <li>Added installable scripts for 2/3 respectively, bionemo-esm2-recipe, bionemo-esm2-train, bionemo-geneformer-recipe, bionemo-geneformer-train.</li> <li>Geneformer support in BioNeMo2:</li> <li>Tested pre-training scripts and fine-tuning example scripts that can be used as a starting point for users to create custom derivative models.</li> <li>Geneformer 10M and 106M checkpoints ported from BioNeMo v1 into BioNeMo v2 available and included in documentation.</li> <li>Added inference scripts</li> <li>Documentation</li> <li>Cell type classification example notebook which covers the process of converting anndata into our internal format, and running inference on that data with a geneformer checkpoint, as well as making use of the inference results.</li> <li>Updated Getting Started guide, ESM-2 tutorials</li> <li>Added Frequently Asked Questions (FAQ) page</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v20","title":"BioNeMo Framework v2.0","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_3","title":"New Features:","text":"<ul> <li>ESM-2 implementation</li> <li>State of the art training performance and equivalent accuracy to the reference implementation</li> <li>650M, and 3B scale checkpoints available which mirror the reference model</li> <li>Flexible fine-tuning examples that can be copied and modified to accomplish a wide variety of downstream tasks</li> <li>First version of our NeMo v2 based reference implementation which re-imagines bionemo as a repository of megatron models, dataloaders, and training recipes which make use of NeMo v2 for training loops.</li> <li>Modular design and permissible Apache 2 OSS licenses enables the import and use of our framework in proprietary applications.</li> <li>NeMo2 training abstractions allows the user to focus on the model implementation while the training strategy handles distribution and model parallelism.</li> <li>Documentation and documentation build system for BioNeMo 2.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_1","title":"Known Issues:","text":"<ul> <li>PEFT support is not yet fully functional.</li> <li>Partial implementation of Geneformer is present, use at your own risk. It will be optimized and officially released in the future.</li> <li>Command line interface is currently based on one-off training recipes and scripts. We are working on a configuration based approach that will be released in the future.</li> <li>Fine-tuning workflow is implemented for BERT based architectures and could be adapted for others, but it requires you to inherit from the biobert base model config. You can follow similar patterns in the short term to load weights from an old checkpoint partially into a new model, however in the future we will have a more direct API which is easier to follow.</li> <li>Slow memory leak occurs during ESM-2 pretraining, which can cause OOM during long pretraining runs. Training with a   microbatch size of 48 on 40 A100s raised an out-of-memory error after 5,800 training steps.</li> <li>Possible workarounds include calling <code>gc.collect(); torch.cuda.empty_cache()</code> at every ~1,000 steps, which appears     to reclaim the consumed memory; or training with a lower microbatch size and re-starting training from a saved     checkpoint periodically.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v19","title":"BioNeMo Framework v1.9","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_4","title":"New Features","text":"<ul> <li>[Documentation] Updated, executable ESM-2nv notebooks demonstrating: Data preprocessing and model training with custom datasets, Fine-tuning on FLIP data, Inference on OAS sequences, Pre-training from scratch and continuing training</li> <li>[Documentation] New notebook demonstrating Zero-Shot Protein Design Using ESM-2nv. Thank you to @awlange from A-Alpha Bio for contributing the original version of this recipe!</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements","title":"Bug fixes and Improvements","text":"<ul> <li>[Geneformer] Fixed bug in preprocessing due to a relocation of dependent artifacts.</li> <li>[Geneformer] Fixes bug in finetuning to use the newer preprocessing constructor.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v18","title":"BioNeMo Framework v1.8","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_5","title":"New Features","text":"<ul> <li>[Documentation] Updated, executable MolMIM notebooks demonstrating: Training on custom data, Inference and downstream prediction, ZINC15 dataset preprocesing, and CMA-ES optimization</li> <li>[Dependencies] Upgraded the framework to NeMo v1.23, which updates PyTorch to version 2.2.0a0+81ea7a4 and CUDA to version 12.3.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_1","title":"Bug fixes and Improvements","text":"<ul> <li>[ESM2] Fixed a bug in gradient accumulation in encoder fine-tuning</li> <li>[MegaMolBART] Make MegaMolBART encoder finetuning respect random seed set by user</li> <li>[MegaMolBART] Finetuning with val_check_interval=1 bug fix</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_2","title":"Known Issues","text":"<ul> <li>Minor training speed regression observed for models DNABERT, Geneformer, MolMIM</li> <li>Two known critical CVEs GHSA-cgwc-qvrx-rf7f, GHSA-mr7h-w2qc-ffc2. The vulnerabilities arise within a package that's installed by lightning by default. We do not use that package in bionemo framework container. we are also unable to remove the package in question as it's installed as a side-effect of installing lightning.</li> <li>Two known High CVEs from pytorch : GHSA-pg7h-5qx3-wjr3, GHSA-5pcm-hx3q-hm94.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v17","title":"BioNeMo Framework v1.7","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models","title":"New Models","text":"<ul> <li>DSMBind, developed under the BioNeMo framework, is a model which can produce comparative values for ranking protein-ligand binding affinities. This release features the capability to perform inference using a newly trained checkpoint.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_6","title":"New Features","text":"<ul> <li>[EquiDock] Remove steric clashes as a post-processing step after equidock inference.</li> <li>[Documentation] Updated Getting Started section which sequentially describes prerequisites, BioNeMo Framework access, startup instructions, and next steps.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_3","title":"Known Issues","text":"<ul> <li>There is a known security vulnerability with NLTK that can allow for arbitrary code execution via pickle files that are external assets downloaded via nltk.download() (https://github.com/nltk/nltk/issues/3266). BioNeMo itself does not use this dependency in any way, however parts of NeMo text-to-speech (nemo.collections.tts) does use this vulnerable codepath. Since NeMo is installed in the BioNeMo release containers, users are urged to exercise caution when using  nemo.collections.tts or nltk.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v16","title":"BioNeMo Framework v1.6","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_7","title":"New Features","text":"<ul> <li>[Model Fine-tuning] <code>model.freeze_layers</code> fine-tuning config parameter added to freeze a specified number of layers. Thank you to github user @nehap25!</li> <li>[ESM2]  Loading pre-trained ESM-2 weights and continue pre-training on the MLM objective on a custom FASTA dataset is now supported.</li> <li>[OpenFold] MLPerf feature 3.2 bug (mha_fused_gemm) fix has merged.</li> <li>[OpenFold] MLPerf feature 3.10 integrated into bionemo framework.</li> <li>[DiffDock] Updated data loading module for DiffDock model training, changing from sqlite3 backend to webdataset.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v15","title":"BioNeMo Framework v1.5","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_1","title":"New Models","text":"<ul> <li>Geneformer is out of Beta status. This release includes newly trained checkpoints and benchmarks, including a variant based on the publication with 10M parameters, and the largest variant of geneformer publically available to date with 106M parameters.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v14","title":"BioNeMo Framework v1.4","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_2","title":"New Models","text":"<ul> <li>Beta Geneformer a foundation model for single-cell data that encodes each cell as represented by an ordered list of differentially expressed genes for that cell.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_8","title":"New Features","text":"<ul> <li>Beta Geneformer pretraining with custom datasets</li> <li>Low-Rank Adaptation (LoRA) finetuning for ESM2</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_2","title":"Bug fixes and Improvements","text":"<ul> <li>OpenFold training improved benchmarks and validation of optimizations</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_4","title":"Known Issues","text":"<ul> <li>BioNeMo Framework v24.04 container is vulnerable to GHSA-whh8-fjgc-qp73 in onnx 1.14.0. Users are advised not to open untrusted onnx files with this image. Restrict your mount point to minimize directory traversal impact. A fix for this is scheduled in the 24.05 (May) release.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v13","title":"BioNeMo Framework v1.3","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_3","title":"New Models","text":"<ul> <li>MolMIM implementation under BioNeMo framework, a small molecule model developed at NVIDIA which can be used to produce embeddings and novel molecules.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_9","title":"New Features","text":"<ul> <li>MolMIM re-trained on more data is now available in the framework, and achieves state of the art performance.</li> <li>MolMIM property guided tutorial notebook covering property guided optimization using our new framework model.</li> <li>MolMIM training tutorial available walking users through either training from scratch or from an existing checkpoint on your own data.</li> <li>MolMIM tutorial notebook covering molecular sampling and property prediction is also now available.</li> <li>Numerous optimizations from NVIDIA's entry to the MLPerf competition have been added to OpenFold. Documentation and detailed benchmarks are works in progress and will be published in upcoming releases. This release contains the following performance optimizations:<ul> <li>Fused GEMMs in multi-head attention (MHA)</li> <li>Non-blocking data pipeline</li> <li>BF16 precision training</li> <li>Fused MHA gating</li> <li>Inductor Compiled LayerNorm</li> <li>OpenAI Triton LayerNorm kernels</li> <li>OpenAI Triton MHA</li> </ul> </li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_3","title":"Bug fixes and Improvements","text":"<ul> <li>NeMo upgraded to v1.22 (see NeMo release notes),</li> <li>PyTorch Lightning upgraded to 2.0.7</li> <li>NGC CLI has been removed from the release container. If users     download models from inside the container (e.g. using <code>bionemo_data_download</code> or via running specific unit tests),     the NGC CLI will be auto-installed to pull the models from NGC.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_5","title":"Known Issues","text":"<ul> <li>BioNeMo Framework v24.03 container is vulnerable to GHSA-whh8-fjgc-qp73 in onnx 1.14.0. Users are advised not to open untrusted onnx files with this image. Restrict your mount point to minimize directory traversal impact.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v12","title":"BioNeMo Framework v1.2","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_4","title":"New Models","text":"<ul> <li>OpenFold implementation under BioNeMo framework, derived from public OpenFold and DeepMind AlphaFold-2.</li> <li>DNABERT implementation for computing embeddings for each nucleotide in the input DNA sequence.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_10","title":"New Features","text":"<ul> <li>Training recipes for DNABERT and OpenFold, including automated data processing and full configuration for training.</li> <li>Example tutorials for running inference using OpenFold.</li> <li>Splice Prediction downstream task example for DNABERT.</li> <li>Wrapper scripts for DNABERT and OpenFold to launch jobs on BCP.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_4","title":"Bug fixes and Improvements","text":"<ul> <li>Interface improvements for ESM-2 data ingestion and pre-processing. The interface allows for explicit specification of training, validation, and test sets. The user may set <code>config.model.data.default_dataset_path</code> to maintain prior behavior, or set <code>config.model.data.train.dataset_path</code>, <code>config.model.data.val.dataset_path</code>, <code>config.model.data.test.dataset_path</code> which may all be unique.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_6","title":"Known Issues","text":"<ul> <li>OpenFold training speed does not yet include MLPerf optimizations, and these will be released in the subsequent release.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v11","title":"BioNeMo Framework v1.1","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_5","title":"New Models","text":"<ul> <li>EquiDock for protein-protein docking pose prediction</li> <li>DiffDock for protein-ligand blind docking pose generation</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_11","title":"New Features","text":"<ul> <li>Training recipes for EquiDock and DiffDock, including automated data processing and full configuration for training.</li> <li>Accelerated inference and training for DiffDock via fast tensor-product kernels.</li> <li>Example tutorials for running inference using EquiDock and DiffDock.</li> <li>Recipes for running EquiDock and DiffDock on BCP and Slurm.</li> <li>Pipeline parallel supported for ESM-2nv.</li> <li>Migration of inference notebooks to using pytriton.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_5","title":"Bug fixes and Improvements","text":"<ul> <li>Faster pre-processing of data on BCP.</li> <li>Refactor of download_models.sh to download_models.py for easier CLI use.</li> <li>Refactor of install structure to move from /opt/nvidia to /workspace/bionemo. The environment variable $BIONEMO_HOME now points to the repo base and is required to be set for tests to pass.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#security-notice","title":"Security Notice","text":"<p>SchedMD Slurm in the release container is shipped with a security vulnerability, CVE-2022-29501, and therefore this version of Slurm should not be used to run a Slurm cluster (specifically, the processes <code>slurmdbd</code>, <code>slurmctld</code>, and <code>slurmd</code>.</p> <p>In general, the BioNeMo Framework release is designed to ship code and an environment that would be executed on local workstations, or deployed on clusters for large scale training jobs. This container is not designed to run as a service with public facing APIs. A full summary of security vulnerabilities can be found here.</p>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v10","title":"BioNeMo Framework v1.0","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_6","title":"New Models","text":"<ul> <li>ESM-2nv for protein sequence representations, pretrained weights of ESM-2 650M and ESM-2 3B converted from HF checkpoint available.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_12","title":"New Features","text":"<ul> <li>Pre-training recipes for ESM-2nv, including automated data processing and full configuration for training</li> <li>Fine-tuning of ESM-2nv with encoder frozen or trainable</li> <li>Downstream task finetuning support for single-value classification (e.g. subcellular localization), single-value regression (e.g. meltome) and per-token classification (e.g. secondary structure)</li> <li>Validation in loop to evaluate performance on downstream tasks during training</li> <li>Example tutorials for pre-training, fine tuning, and downstream tasks</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v040","title":"BioNeMo Framework v0.4.0","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_7","title":"New Models","text":"<ul> <li>ESM-1nv for protein sequence representations, pretrained weights available</li> <li>ProtT5nv for protein sequence representation and sequence-to-sequence tasks, pretrained weights available</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_13","title":"New Features","text":"<ul> <li>Pre-training for all models, including automated data processing and full configuration for training</li> <li>Fine-tuning of MegaMolBART, ESM-1nv, and ProtT5nv with encoder frozen or trainable</li> <li>Downstream task example applications \u2013 secondary structure prediction for ESM-1nv and ProtT5nv, physchem prediction (lipophilicity, FreeSolv, ESOL) and retrosynthesis prediction for MegaMolBART</li> <li>Validation in loop to evaluate performance on downstream tasks during training: physchem prediction (MegaMolBART) and secondary structure prediction (ESM-1nv and ProtT5nv).</li> <li>Pipeline parallelism supported as a beta feature. Not fully tested.</li> <li>Example notebooks for pre-training, fine tuning, and downstream tasks</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_7","title":"Known Issues","text":"<ul> <li>Data preprocessing on DGX Cloud is slow. Faster to do it on a local machine.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-apis","title":"New APIs","text":"<ul> <li>BioNeMoDataModule - Encapsulates dataset instantiation in bionemo models so that many different datasets can be used with the same model</li> <li>EncoderFineTuning - Base class to facilitate implementation of downstream tasks built on embeddings from other models</li> </ul>"},{"location":"user-guide/background/SUMMARY/","title":"SUMMARY","text":"<ul> <li>NeMo2 Parallelism</li> <li>Megatron Dataset Considerations</li> </ul>"},{"location":"user-guide/background/megatron_datasets/","title":"Writing Megatron-LM Compatible Datamodules","text":"<p>Megatron-LM relies on determinism in the training dataset classes to ensure that input tensors are initialized correctly across model-parallel ranks (see NeMo2 Parallelism). As a consequence, new dataset classes must be careful to preserve the required determinism. Common operations such as data augmentation, masking, etc. can cause <code>dataset[i]</code> to return random results for a given index, breaking this megatron contract.</p>"},{"location":"user-guide/background/megatron_datasets/#multi-epoch-training","title":"Multi-Epoch Training","text":"<p>One training regime where this limitation is most apparent is is multi-epoch training, where standard training recipes would apply different random masks or different data augmentation strategies each time the data is encountered. BioNeMo provides a number of utilities that make multi-epoch training easier while still obeying the determinism requirements of megatron.</p> <p>The MultiEpochDatasetResampler class simplifies the process of multi-epoch training, where the data should both be re-shuffled each epoch with different random effects applied each time the data is seen. To be compatible with this resampler, the provided dataset class's <code>__getitem__</code> method should accept a EpochIndex tuple that contains both an epoch and index value. Random effects can then be performed by setting the torch random seed based on the epoch value:</p> <pre><code>class MyDataset:\n    def __getitem__(self, idx: EpochIndex):\n        rng = torch.Generator()\n        rng.manual_seed(idx.epoch)\n        ...\n</code></pre> <p>Avoid <code>torch.manual_seed</code></p> <p>Megatron-LM handles torch seeding internally. Calling <code>torch.cuda.manual_seed</code> inside the user-provided dataset can cause issues with model parallelism. See megatron/core/tensor_parallel/random.py#L198-L199 for more details.</p> <p>For deterministic datasets that still want to train for multiple epochs with epoch-level shuffling, the IdentityMultiEpochDatasetWrapper class can simplify this process by wrapping a dataset that accepts integer indices and passing along the EpochIndex index values from the resampled dataset.</p> <pre><code>class MyDeterministicDataset:\n    def __getitem__(self, index: int):\n        ...\n\ndataset = IdentityMultiEpochDatasetWrapper(MyDeterministicDataset())\nfor sample in MultiEpochDatasetResampler(dataset, num_epochs=3, shuffle=True):\n    ...\n</code></pre>"},{"location":"user-guide/background/megatron_datasets/#training-resumption","title":"Training Resumption","text":"<p>To ensure identical behavior with and without job interruption, BioNeMo provides MegatronDataModule to save and load state dict for training resumption, and provides [WrappedDataLoader][nemo.lightning.data.WrappedDataLoader] to add a <code>mode</code> attribute to [DataLoader][torch.utils.data.DataLoader].</p> <pre><code>class MyDataModule(MegatronDataModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ...\n\n    def train_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"train\",\n        )\n\n    def val_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"validation\",\n        )\n\n    def test_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"test\",\n        )\n</code></pre> <p>MegatronDataModule</p> <p>Users will see non-overlapping training curve if their datamodule is not inheritting from <code>MegatronDataModule</code>, unless similar logics are handled by the users. In <code>MegatronDataModule</code>, <code>self.update_init_global_step()</code> must be called right before the dataloaders are returned to ensure that training resumes with the correct sample index instead of restarting from 0 everytime. We recommend users to inherit from <code>MegatronDataModule</code> similar to the pattern above.</p> <p>WrappedDataLoader</p> <p>The <code>WrappedDataLoader</code> class is a wrapper around the PyTorch DataLoader class that adds the <code>mode</code> attribute to the dataloader. The dataloader will resume from the last sample index only when mode is 'train'. <code>val_dataloader</code> and <code>test_dataloader</code> are unaffected.</p> <p>WARNING: 'train' is the default value of <code>mode</code> in <code>WrappedDataLoader</code>. If not set, users might find their validation/test dataloader changes behavior by resuming from a non-zero sample index.</p>"},{"location":"user-guide/background/megatron_datasets/#testing-datasets-for-megatron-compatibility","title":"Testing Datasets For Megatron Compatibility","text":"<p>BioNeMo also provides utility functions for test suites to validate that datasets conform to the megatron data model. The [assert_dataset_compatible_with_megatron][bionemo.testing.data_utils.assert_dataset_compatible_with_megatron] function calls the dataset with identical indices and ensures the outputs are identical, while also checking to see if <code>torch.manual_seed</code> was used.</p> <p>Example datasets in BioNeMo</p> <p>The ESMMaskedResidueDataset demonstrates one approach for leveraging EpochIndex indices to perform epoch-level randomization within the confines of megatron's data model.</p>"},{"location":"user-guide/background/nemo2/","title":"NeMo2 Parallelism","text":"<p>NeMo2 represents tools and utilities to extend the capabilities of <code>pytorch-lightning</code> to support training and inference with megatron models. While pytorch-lightning supports parallel abstractions sufficient for LLMs that fit on single GPUs (distributed data parallel, aka DDP) and even somewhat larger architectures that need to be sharded across small clusters of GPUs (Fully Sharded Data Parallel, aka FSDP), when you get to very large architectures and want the most efficient pretraining and inference possible, megatron-supported parallelism is a great option.</p> <p>So in other words, NeMo2 adds the Megatron strategy in addition to the standard DDP and FSDP strategies.</p> <p>Many downstream constraints and conventions are driven by the underlying constraints of megatron.</p>"},{"location":"user-guide/background/nemo2/#deeper-background-on-megatron","title":"Deeper background on megatron","text":""},{"location":"user-guide/background/nemo2/#other-options-for-parallelizing-smaller-models","title":"Other options for parallelizing smaller models","text":"<p>Megatron is a system for supporting advanced varieties of model parallelism. While vanilla models can be executed in parallel with systems such as distributed data parallel (DDP) or moderately large models can be trained with Meta's Fully Sharded Data Parallel (FSDP/FSDP2), when you get to very large models and you want to train them with maximal efficiency, you want some variant of megatron.</p>"},{"location":"user-guide/background/nemo2/#ddp-background","title":"DDP background","text":"<p>DDP is the best option when you can fit the entire model on every GPU in your cluster. With DDP, you can parallelize your <code>global batch</code> across multiple GPUs by splitting it into smaller <code>mini-batches</code>, one for each GPU. Each GPU computes the forward and backward pass independently for its subset of data, allowing for maximal utilization. Synchronization of gradients occurs after the backward pass is complete for each batch, followed by a weight update that ensures all GPUs have synchronized parameters for the next iteration. Here is an example of how this might appear on your cluster with a small model:</p> <p></p>"},{"location":"user-guide/background/nemo2/#fsdp-background","title":"FSDP background","text":"<p>FSDP extends DDP by sharding (splitting) model weights across GPUs in your cluster to optimize memory usage. While data is still split across GPUs in the same way as DDP, FSDP strategically synchronizes and broadcasts the necessary shards of model weights to all GPUs just-in-time for computation during the forward pass.</p> <p>For example, when a layer is needed for computation, the owning GPU sends that shard of weights to the other GPUs, which then perform the forward computation on that layer. After the computation is complete, FSDP frees the memory for that layer on all GPUs except the one that owns the shard. This process continues iteratively for each layer until the entire model has been executed on the data.</p> <p>Note that this process parallelizes the storage in a way that enables too large models to be executed (assuming a single layer is not too large to fit on a GPU). Megatron (next) co-locates both storage and compute.</p> <p>The following two figures show two steps through the forward pass of a model that has been sharded with FSDP.  </p>"},{"location":"user-guide/background/nemo2/#model-parallelism","title":"Model Parallelism","text":"<p>Model parallelism is the catch-all term for the variety of different parallelism strategies that could be applied to parallelizing your model across a cluster. Below we explain several varieties of model parallelism that are implemented in megatron. As mentioned in the previous section, one key advantage to the megatron-specific parallelism types described next are that they co-locate storage and compute of the layers. Inefficiencies caused by naive scheduler implementations are also addressed (discussed in the section on schedulers).</p>"},{"location":"user-guide/background/nemo2/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Pipeline parallelism is similar to FSDP, but the model blocks that are sharded are also computed in parallel on the nodes that own the model weight in question. You can think of this as a larger simulated GPU that happens to be spread across several child GPUs. Examples of this include <code>parallel_state.is_pipeline_last_stage()</code> which is commonly used to tell if a particular node is on last pipeline stage, where you compute the final head outputs, loss, etc. . Similarly there are convenience environmental lookups for the first pipeline stage (where you compute the embedding for example) <code>parallel_state.is_pipeline_first_stage()</code>.</p>"},{"location":"user-guide/background/nemo2/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>Tensor parallelism represents splitting single layers across GPUs. This can also solve the problem where some individual layers could in theory be too large to fit on a single GPU, which would make FSDP not possible. This would still work since individual layer weights (and computations) are distributed. Examples of this in megatron include <code>RowParallelLinear</code> and <code>ColumnParallelLinear</code> layers. </p>"},{"location":"user-guide/background/nemo2/#sequence-parallelism","title":"Sequence Parallelism","text":"<p>In megatron, \"sequence parallelism\" refers to the parallelization of the dropout, and layernorm blocks of a transformer. The idea is roughly as follows. First, remember that in a typical transformer architecture, the <code>embedding_dimension</code> is the only dimension that <code>LayerNorm</code> is applied over. Similarly, Dropout (outside of the attention block) is an operation that is applied on the last embedding dimension. These two layers are independent over the sequence dimension, so they can be processed in blocks on separate GPUs. As can be seen in the following figure, the initial <code>LayerNorm</code> in a multi-headed transformer block is executed in parallel. Next the results are gathered for the self attention and linear layers (which are typically set up for tensor parallelism). Next the result from those layers is scattered back to sequence parallel nodes which execute dropout, do a residual connection from the previous sequence parallel output, and a layernorm. Next those results are again gathered for the final FFN and activation layers prior to a final scattering across sequence parallel GPUs for the output of that transformer block. </p> <p>As a user, if you know that your transformer is executed in parallel and you have custom losses or downstream layers, you need to make sure that the appropriate gather operations are occurring for your loss computation etc.</p>"},{"location":"user-guide/background/nemo2/#context-parallelism","title":"Context Parallelism","text":"<p>Context parallelism extends sequence parallelism by also parallelizing the attention mechanism itself, similar to Ring Attention. In general, if you are using a transformer, context parallelism is going to perform better than sequence parallelism for very long input sequences. That said, due to the necessity of all-gather and reduce scatter operations throughout the architecture, the general advice that you should avoid these kinds of parallelism if a micro-batch fits on a single device still holds. Splitting across elements in a global batch represent the fewest necessary communications between GPUs on your cluster, so standard DDP should run the fastest if you can get your training loop for a micro batch to fit on one GPU.</p>"},{"location":"user-guide/background/nemo2/#mixing-parallelism-strategies","title":"Mixing parallelism strategies","text":"<p>You can mix multiple kinds of parallelism together to achieve a more performant result. In general experimentation should be done to identify the optimal mix of parallelism. See this YouTube tutorial from Jared Casper for more background on megatron parallelism strategies.</p> <p>Below is a figure demonstrating how mixing strategies results in larger \"virtual GPUs\", which similarly means you have fewer distinct micro-batches in flight across your cluster. Also note that the number of virtual GPUs is multiplicative so if you have <code>TP=2</code> and <code>PP=2</code> then you are creating a larger virtual GPU out of <code>2*2=4</code> GPUs, so your cluster size needs to be a multiple of 4 in this case. </p>"},{"location":"user-guide/background/nemo2/#scheduling-model-parallelism","title":"Scheduling model parallelism","text":"<p>You can improve on naive schedules by splitting up micro-batches into smaller pieces, executing multiple stages of the model on single GPUs, and starting computing the backwards pass of one micro-batch while another is going through forward. These optimizations allow for better cluster GPU utilization to be achieved. For example the following figure shows how more advanced splitting techniques in megatron (eg the interleaved scheduler) provide better utilization when model parallelism is used. Again when you can get away without using model parallelism (DDP), that is generally the best approach. </p>"},{"location":"user-guide/contributing/code-review/","title":"Code Review","text":"<p>This document describes the process and etiquette for code review in the BioNeMo repo. You should read this document if you are a developer working in the BioNeMo repo.</p> <p>The purpose of these guidelines is to help reduce the friction between engineers writing code and those reviewing code. As with many rules, there are exceptions. These exceptions are not comprehensive, so if you find exceptions that should be listed, please raise them so they can be evaluated for their inclusion.</p>"},{"location":"user-guide/contributing/code-review/#code-review-process","title":"Code Review Process","text":"<p>The code review process is progressive:</p> <ol> <li>Review by your team</li> <li>Review by domain experts (CODEOWNERS)</li> <li>Approval by Approval-list users.</li> <li>(Optional) Coverage Check approval by Approval-list users.</li> </ol>"},{"location":"user-guide/contributing/code-review/#1-team-review","title":"1. Team Review","text":"<p>You should first ask contributors to review your change. The contributing team can provide the most contextualized feedback, and this review step is where most issues with the change should be caught and addressed.</p> <p>Proceed to the next step after you've addressed your team's comments and have received an approval. There is no actual requirement in gitlab to receive your team-based approval - it is simply best practice.</p>"},{"location":"user-guide/contributing/code-review/#2-owner-review","title":"2. Owner Review","text":"<p>Code owners are domain experts for a particular part of the repository. They are typically the original authors of a set of source files. Code ownership structures tend to mirror team structures; a single team is often responsible for a functional component.</p> <p>You must receive approval from at least one owner of every file you change. Unless the file(s) do not have any owners specified in the <code>CODEOWNERS</code> file.</p> <p>If your change only modifies files owned by your team, owner review happens implicitly in the team review step (i.e. in many cases you may already have this requirement satisfied after step 1 above).</p>"},{"location":"user-guide/contributing/code-review/#3-approval","title":"3. Approval","text":"<p>You must also receive approval for your change. Approval indicates that the change is ready to be merged, and is the final step in the review process. Self approval is strictly forbidden.</p> <p>Approval is granted by an approver, in the form of an approval stamp in gitlab. Approvers review an entire change, in contrast to owners, who focus on reviewing the files that they own.</p> <p>To find an approver, first ask your team if there is a preferred approver. You can also check in #clara-discovery-bionemo-dev.</p> <p>*If your team has an approver, approval will usually be granted shortly after your team has had a chance to review your change.</p> <p>*Often, a single review from one person will be sufficient to complete the entire review process. If the reviewer is on your team, is an approver, and is an owner of the files you modified, then the process may collapse to a single review.</p>"},{"location":"user-guide/contributing/code-review/#4-coverage-check","title":"4. Coverage Check","text":"<p>Our repository has automated checks to ensure test coverage has not regresed. The coverage check approvers will be the same as the Approval-list users. If codeline test coverage regresses, the Approvers must make a judgement call whether it is acceptible or not the merge the code. Occaisonally the coverage check algorithm has a false positive (i.e. code coverage doesn't regress, yet coverage check approval is flagged by gitlab), and in this case Approvers are i free to simply approve the false coverage regression.</p>"},{"location":"user-guide/contributing/code-review/#responsibilities","title":"Responsibilities","text":""},{"location":"user-guide/contributing/code-review/#all-commenters-reviewers-owners-approvers-etc","title":"All Commenters (Reviewers, Owners, Approvers, etc.)","text":"<p>If a comment thread is start by anyone, it is expected that the thread starter resolves the comment. Resolving a thread by the original thread starter indicates that the person who started the discussion is happy with the outcome.</p>"},{"location":"user-guide/contributing/code-review/#reviewers","title":"Reviewers","text":"<p>All developers can and should review changes. These reviews are fundamental to maintaining the quality of the codebase. Reviews are also an important way to stay aware of what people are working on and increase the bus factor for areas of the code.</p> <p>Reviewers should always do the following when reviewing code:</p> <ul> <li> <p>Be respectful.</p> </li> <li> <p>Assume best intentions.</p> </li> <li> <p>Review the code, not the person.</p> </li> <li> <p>Leave clear, actionable feedback. Use comments in gitlab to signal that you expect   changes to be made, and explicitly enumerate them. If you don't, it can leave the author of the patch wondering if   your comments are optional. Requesting code changes should not be interpreted as a   judgment of the change, but rather as an indicator that the   reviewer wants to engage with the author to turn it into an   approval.</p> </li> </ul> <p>A more detailed etiquette guide can be found later in this document.</p>"},{"location":"user-guide/contributing/code-review/#owners","title":"Owners","text":"<p>As an owner, you have the ability to delay code from being merged by withholding your signoff. This ability comes with important responsibilities:</p> <ul> <li> <p>A duty to respond to reviews in a timely manner. If you can't stay   on top of review requests, you should relinquish ownership.</p> </li> <li> <p>A bias to \"yes\". \"No\", by itself, is never an acceptable answer.   When you reject a change you must work with the change author to   arrive at a mutually agreeable solution.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#approvers","title":"Approvers","text":"<p>As an approver, you have additional responsibilities beyond that of an owner:</p> <ul> <li> <p>You are responsible for the change and the effect it has on the   codebase.</p> </li> <li> <p>You must ensure that reviews have been performed by the appropriate   reviewers, that these reviews were not rushed.</p> </li> <li> <p>If a change breaks something, you are expected to be actively   involved in the cleanup.</p> </li> </ul> <p>You should always decline to approve a change if you're unfamiliar with the areas of code it touches.</p>"},{"location":"user-guide/contributing/code-review/#becoming-an-owner","title":"Becoming an Owner","text":"<p>Code ownership information is stored in CODEOWNERS file. Since these CODEOWNERS file are stored in the repository, changing them follows the same process as changing code.</p> <p>To become an owner, add your github username to the CODEOWNERS file that you want to be a part of, and submit the change to Gitlab. The change to the CODEOWNERS file will follow the same review process as any other code change. The existing owners will decide whether to delegate ownership to you or not.</p>"},{"location":"user-guide/contributing/code-review/#becoming-an-approver","title":"Becoming an Approver","text":"<p>As an approver, you are responsible for ensuring the consistency and integrity of our code base. Before becoming an approver, study this document so that you are completely familiar with the responsibilities of reviewers and approvers. Additionally, make sure that you are intimately familiar with our coding style guides and best practices:</p> <ul> <li>Contributing</li> <li>In addition, make sure that you understand and can apply all elements of the     Google Python style guide, which we adhere     to for all Python code</li> </ul>"},{"location":"user-guide/contributing/code-review/#details-on-etiquette-for-good-citizens","title":"Details on Etiquette for Good Citizens","text":"<p>In the following sections, we provide deeper thoughts and recommendations for everyone contributing to the repo, in order to have a fruitful interaction across the team members.</p>"},{"location":"user-guide/contributing/code-review/#patch-changes-sizes-and-policies","title":"Patch changes, sizes, and policies","text":"<ul> <li> <p>It takes 30-45 minutes to review every 100-200 lines of code. Be   mindful of the size of changes you are introducing and try to keep   your patch under 500 lines of code whenever possible. As a change   grows in size (lines of codes) the reviewer's ability to find   issues diminishes.</p> </li> <li> <p>As a corollary, for a refactor-type change, consider separating     \"no-logic-change\" and \"logic-change\" into distinct reviews     (perhaps in a dependent review change) to ease the pattern     matching burden on your reviewers.</p> </li> <li> <p>All reviewers may request an PR is too large if it is larger than   500 lines of net code addition. The only exception are MRs into   the <code>bionemo2/contrib</code> directory, where larger MRs are permissible.   This includes lines of code, but not something such as dummy data or   a fake dataset that may contain thousands of lines of stuff that is not   actually functional code.</p> </li> <li> <p>Each patch should be kept to one logical change, which should be   described in the title of the patch. Unrelated changes should be   split out into separate patches. Fixing whitespace on a line   you're editing is reasonable. Fixing whitespace around the code   you're working on should be a separate 'cleanup' patch.</p> </li> <li> <p>Where possible, larger patches (&gt;500 LOC) should be split into   multiple smaller patches that are consistent individually. Test   your patches before submitting them to Gitlab via gitlab pipelines or locally.   The more you test before submitting your patch for review, the better. It's also   appreciated if you add a line to the commit message describing how   the patch was tested. This prevents people from having to ask   whether and how the patch was tested. Stating that the patch was   not tested is also fine, although you might be asked to do some   testing in cases where that would be reasonable.</p> </li> <li> <p>Abandon patches that are no longer useful or that you don't intend   to keep working on.</p> </li> <li> <p>Follow code styling and rules stated in the project's documents   (for example, contributing.md, of which the Google Python   Style Guide is a subet) as these define the   look and feel of the code which defines the most fundamentals of how the code should be   developed and allows reviewers to focus on the most important aspects of a new piece of code.   For bash scripting please follow the Google Shell Style Guide here</p> </li> <li> <p>We follow a revert + fix policy in the codebase for any showstopper   bug that might appear as a result of an PR introducing errors not   caught by sanity. In exceptional circumstances when an MR cannot be   reverted and there is a hotfix ready, leadership can consider   merging without revert. However, this should be an exception.   Failures policies are described in more depth here.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#review-timelines","title":"Review timelines","text":"<ul> <li> <p>In general, patches should remain open for review for at least 24   hours since the last significant modification to the change. The   purpose is to let developers around the world have a chance to   review. Complex reworks, even if they don't change the purpose of   the patch but the way it's implemented, should restart the waiting   period.</p> </li> <li> <p>Speedy changes: A change can go in without the waiting period if its   purpose is to fix a recently-introduced issue that has not been   possible to revert. In that case, the commit message has to   explain what change introduced the problem and the nature of the   problem so that the emergency need becomes apparent. The change   itself should be as limited in scope and impact as possible to   make it simple to assess the impact.</p> </li> <li> <p>Trivial changes that deal with minor issues like inconsistencies in   whitespace or spelling fixes that don't impact the final binary   output also don't need to wait for the round of the world reviews.   Such changes should point out in their commit messages how the   author verified that the binary output is identical. Note that   trivial fixes shouldn't necessarily be expedited: Just like   they're not critical enough for things to go wrong because of   them, they're not critical enough to require quick handling.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#reviewers_1","title":"Reviewers","text":"<ul> <li> <p>Do not approve if you have not reviewed the code you were asked to   review. Spend time reviewing or re-assign to someone else. Someone   that approves the change must review the entire change holistically   If you are a code owner of a particular file, it is appropriate to only reviews the files you own.</p> </li> <li> <p>If request an PR change their code, you are responsible for giving concrete   recommendations for what could be changed to resolve the issue the   patch addresses. If you feel strongly that a patch should NEVER be   merged, you are responsible for defending your position and   listening to other points of view. Asking for changes and walking away is   not acceptable, and may cause your approval status to be removed by the   leadership team.</p> </li> <li> <p>Include justification for critique: When you review code, you should   always try to include justification for your critique, unless that   critique is a nit, a style guide violation, or an obvious bug.   Nits and style guide violations tend to overlap, and you shouldn't   have to justify the use of a shared style guide or things like   proper spelling. Likewise, you shouldn't have to justify bug-free   code. However, when it comes to design choices you should always   include justification for when you might want to change certain   things</p> </li> <li> <p>If there have been comments or discussion on a patch, verify that   the comments have been addressed before giving an approval. If you feel   that a comment is invalid, please respond to that comment instead   of just ignoring it.</p> </li> <li> <p>Be conscientious when approving patches. As the arbiter, you need to make sure the MR   is complete, that proper reviews are done, that all the required   tests have passed, that API changes have been reviewed by the API   owners. Please make sure that the necessary convergence tests and unit tests   have passed and have not regressed. If KPI regression (convergence tests) is found, you may need to   consult with other stakeholders before approving the MR. In some   cases a Cl will require convergence testing. Owners/ Approvers   should make sure convergence testing is performed when necessary and   that no new issues are identified. And that overall, the code   changes are sound and integrate well with the rest of the modules   and systems. In the event that the patch breaks things, you are   expected to be actively involved in the cleanup effort and support   the authors by reverting and speeding the fixes. This means you   shouldn't approve a patch just because you trust the author of a   patch - Make sure you understand what the implications of a patch   might be, or leave the review to others. Partial reviews,   reviewing code style, for example, can be given a positive review or a LGTM.   This also applies if you think the patch looks good, but may   not have the experience to know if there may be unintended   consequences.</p> </li> <li> <p>Please make sure that the code changes are covered by the existing   unit tests and if necessary ask the contributor to add or update   tests.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#contributors","title":"Contributors","text":"<ul> <li> <p>Before providing a patch for review ask yourself these questions:</p> </li> <li> <p>Is this PR the right size? If it's too long break it up.</p> </li> <li> <p>Is this MR and all the changes included necessary? (all code has     to be maintained)</p> </li> <li> <p>Does this MR duplicate existing functionality? If yes, can I     extend what is there?</p> </li> <li> <p>Is the code readable? Am I using esoteric language constructs     that affect readability? Does the code follow the conventions     of the codebase?</p> </li> <li> <p>Is the MR production ready? In other words - does it have tests,     documentation, error handling, etc, etc.</p> </li> <li> <p>Bring attention to patches that you would like reviewed. Add   reviewers, ask for reviewers on slack or even just rebase it   against the current codebase to bring it to the top of the Gitlab   list. If you're not sure who would be a good reviewer, gitlab will suggest OWNERS based   on the CODEOWNERS file in the UI or look at the   git history of the files that you've changed, and add those   people. For NIM-API based changes there is a small team managing these   therefore seek for those people to review APIs.</p> </li> <li> <p>Try to coordinate with other significant contributors to the code   when making changes to areas you do not own. Before you type a   single line of code!. These people made the most significant   changes to that part of the code and therefore are knowledgeable   of any tradeoffs to be made. Coming with new code already written   will cause painful back and forth and will be less efficient for   all. Learn the design, propose changes and get an agreement before   making changes.</p> </li> <li> <p>Don't modify other people's branches unless you have coordinated this   with the owner of that branch. Not only is this considered rude,   but your changes could be unintentionally lost. An exception to   this would be for branches that have not been updated for more than   90 days and therefore can be considered orphaned.</p> </li> <li> <p>Respond to anyone who has taken the time to review your branches,   even if it's just to say that you disagree. While it may seem   annoying to address a request to fix spelling or 'trivial' issues,   it's generally easy to handle in Gitlab's built-in editor. If you   do use the built-in editor, remember to get that change to your   local copy before re-pushing. It's also acceptable to add fixes   for these sorts of comments to another branch, but it's recommended   that that branch be pushed to Gitlab before the initial branch gets   submitted.</p> </li> <li> <p>Check if there's documentation that needs to be updated to remain   current after your change. If there's no documentation for the   part you're working on, consider adding some.</p> </li> <li> <p>When contributing a significant change to core parts of the code   base, or when introducing a new way of doing something that you   think is worthwhile to apply across the tree, please bring up your   design doc to the commit, so reviewers can read it.</p> </li> <li> <p>Don't expect that people will review your patch unless you ask them   to. Adding other people as reviewers is the easiest way.   But also you can use Slack to actively ping the reviewers, which   is especially useful for urgent MRs.</p> </li> <li> <p>Don't expect people to drop all of what they are doing to review   your patch. Everyone has a day-time job, and while code reviews   are part of that job, they usually do not extend the full working   day.</p> </li> <li> <p>Do not resolve (ack/Done) any comments open by others so they can   find their comments easily and resolve them. Unless being told to   self-resolve.</p> </li> <li> <p>As a contributor you are responsible for the code you bring in and   any failures being caught during unit or convergence testing. Etc.   You should monitor that your code gets merged successfully and   that no errors have appeared after the code has been merged   (nightly testing / convergence testing) and respond promptly to address   any failures.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#general-etiquette","title":"General Etiquette","text":"<ul> <li> <p>We try to assume the best of each other in this community. It's okay   to discuss mistakes (e.g., isolated instances of non-trivial and   non-critical changes submitted early) but try to keep such   inquiries blameless. If a change leads to problems with the code,   the focus should be on fixing the issue, not on assigning blame.</p> </li> <li> <p>Be respectful to others when commenting on branches. Comments should   be kept to the code and should be kept in a polite tone. Assume   your colleagues are intelligent and do not intend any malice or   disrespect. Resist the urge to retaliate against perceived verbal   misconduct, such behavior is not conducive to getting branches   merged. Also, avoid absolute and aggressive language as this can   tend to escalate emotions. Comments are meant to be collaborative.   Very often, the commenter might also be incorrect since they may not have   the full story of the patch in mind since they were not the author. A comment   is not a demand, it's a suggestion towards a mutually acceptable solution   between the author and the reviewer.</p> </li> <li> <p>Don't submit code that you know will break other   platforms/dependencies. If your patch affects code that is used by   others, it should be compatible with those. While it would be nice   to update any other dependents, you must at least provide a path   that will allow other platforms to continue working.</p> </li> <li> <p>Don't write commit messages that are vague or wouldn't make sense to   partners that read the logs. For example, do not write \"[topic]   Bugfix\" as your header in the commit message. Keep links to videos   out of the commit message. Again, partners are going to see these   logs and it does not make sense to link to something they will not   have access to view. Keep your commit messages under 72 chars in   length per line. Good commits have an appropriate topic with a   nice one-liner that explains the change briefly. This is then   followed by a few sentences or more on a description of the   change. Be professional in your language and do not put things in   the message that a partner would not understand. Avoid the use of   acronyms, abbreviations, or codenames, especially those meaningful   only at nvidia. Also, use correct English (check your spelling   please). This pertains to the final commit message after the branch   is squashed. Intermediary commit messages do not matter at all. Commits   must be squashed on merge.</p> </li> <li> <p>Consider breaking up large individual patches into smaller patches   grouped by areas. This makes the patches easier to review but   increases the number of patches. The way this is handled is a   personal decision, as long as each patch is still one logical   change.</p> </li> <li> <p>Contributions made to the <code>bionemo/contrib</code> folder have more flexible requirements. All requirements not mentioned below still   apply to the contrib folder. These exceptions are</p> </li> <li> <p>Code line numbers limit is increased to 2200 lines.</p> </li> <li>Unit test coverage requirements are decreased to 65% coverage.</li> <li>Code line length of &lt;=120 character spaces is acceptable.</li> <li> <p>Commit messages do not need have as verbose of an explanation.     All other requirements pertaining to approver, contributor, and reviewer responsibility still apply.</p> </li> <li> <p>Reviews are about the code. It's easy to take it personally when   someone is criticizing your code, but the whole idea is to get   better code into our codebase. Again, this also applies in the   other direction: review code, critically think about and respond   to the code, but don't make it personal.</p> </li> <li> <p>Don't develop on a personal branch for a long time and then dump a   large number of files and lines of code into a review as a new   \"feature.\" Features should be developed off of the main trunk just   like any other change. It is even more important to follow   processes as the code becomes more critical. Features can   be broken into small working changes and don't need to be   developed all at once. Creating large changes like this leads to   less efficiency in the review process and can lead to more   mistakes.</p> </li> <li> <p>In BioNeMo, for features under development that are not ready for   production, we put them inside the <code>bionemo2/contrib</code> folder. This allows for teams to develop   faster (less strict reviews) while testing code. When a feature is   complete and well tested, we move it to <code>bionemo2/src</code> or   <code>bionemo2/core</code> and we complete all the requirements for productroduction.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#references","title":"References","text":"<ul> <li>Coreboot gerrit guidelines (heavily - inspired the etiquette portion of this document)</li> <li>Code Review Culture</li> <li>Proven practices for peer review</li> <li>https://confluence.nvidia.com/display/DS/SDK+Code+reviews</li> <li>https://confluence.nvidia.com/display/DS/SDK+Best+Practices (Review and source control sections)</li> </ul>"},{"location":"user-guide/contributing/contributing/","title":"Contributing Guidelines","text":"<p>Note</p> <p>For code review standards please see the Code Review page.</p> <p>For all PRs, an approved NVIDIA staff member must sign off and trigger the continuous integration (CI) tests. These are initiated by the member commenting <code>/build-ci</code> directly on the PR. All PRs must have successful CI runs and sufficient code review before being merged.</p>"},{"location":"user-guide/contributing/contributing/#developer-certificate-of-origin-dco","title":"Developer Certificate of Origin (DCO)","text":"<p>We require that all contributors \"sign-off\" on their commits (not GPG signing, just adding the <code>-s | --signoff</code> argument, or follow the instructions below for auto-signing). This sign-off certifies that you adhere to the  Developer Certificate of Origin (DCO) (full text); in short that the contribution is your original work, or you have rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not signed-off will not be accepted.</p> <p>To sign off on a commit, simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes:</p> <pre><code>git commit -s -m \"Add cool feature.\"\n</code></pre> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>If you would like this to happen automatically to all of your commits, you can modify your local <code>~/.git-config-template.txt</code> file. You can do this with a command like the following:</p> <pre><code>echo \"Signed-off-by: Your Name &lt;your@email.com&gt;\" &gt; ~/.git-commit-template.txt\ngit config --local commit.template ~/.git-commit-template.txt\n</code></pre> <p>If you have a commit that you want to retroactively sign, you can do that with:</p> <pre><code>git commit --amend --no-edit --signoff\n</code></pre>"},{"location":"user-guide/contributing/contributing/#python-coding-standards","title":"Python Coding Standards","text":"<p>This page contains the Python coding standards for the BioNeMo repository. They apply to all Python code in the repository (unless external constraints prevent it).</p>"},{"location":"user-guide/contributing/contributing/#coding-style","title":"Coding Style","text":"<ul> <li>We follow the Google Python Style Guide with a few tweaks.</li> <li>The most important parts of this style guide that our code must adhere to are:</li> <li>Docstring</li> <li>Mutable global state</li> <li>Do not use mutable values as default arguments</li> <li>Default iterators</li> <li>Bad naming / abbreviation</li> <li>The exceptions to this style guide are:</li> <li>Module imports. If a module is uniquely named, import     the module. Otherwise, import the value, type, or function directly.</li> <li>Linting and formatting of all code is required by using <code>ruff</code> with BioNeMo's configured options.</li> <li>Unit testing with <code>pytest</code>. See Unit Tests for more details.</li> <li>Add type annotations everywhere. In particular, new code should all be type-annotated as thoroughly as possible. This   also obviates the need for including type hints in the function docstring. It is ok to omit annotations for private   helper functions, but use your best judgement.</li> <li>Include docstrings for every class, function, and method exposed to the user.</li> <li>Docstrings should answer (a) what is the code doing and (b) why would someone use it.</li> <li>Never use wildcard imports.</li> <li>Define <code>__all__ = (,)</code> in modules: make explicit the API of each module, auto-documenting the most important definitions.</li> <li>Minimize the use of <code>**kwargs</code>.</li> <li><code>raise</code> an <code>Exception</code> instead of using an <code>assert</code> statement.</li> <li>F-strings are preferred to format strings.</li> <li>Loggers are preferred to print. In BioNeMo, you can use logger from <code>import logging</code>.</li> <li>Private functions (functions starting with <code>_</code>) shouldn't be called outside its host file.</li> </ul>"},{"location":"user-guide/contributing/contributing/#general-guidelines","title":"General Guidelines","text":"<ul> <li>User-oriented: make it easy for end users, even at the cost of writing more code in the background</li> <li>Robust: make it hard for users to make mistakes.</li> <li>Well-tested: please add simple, fast unit tests. See Unit Tests.</li> <li>Reusable: for every piece of code, think about how it can be reused in the future and make it easy to reuse.</li> <li>Readable: code should be easy to read and well documented (with comments and docstrings).</li> <li>Legal: if you copy even one line of code from the Internet, make sure that the code allows the license that   BioNeMo supports. Give credit and link back to the code.</li> <li>Sensible: code should make sense. If you think a piece of code might be confusing, write comments.</li> <li>Consistent: we work in a team. It is important to integrate changes with existing code.</li> <li>Readable: your code should be easy to read and understand by any other engineer, including outside NVIDIA. Some   tips:</li> <li>Document your code. Make all comments complete sentences, starting with a capitalized letter and ending with a     period.</li> <li>Avoid abbreviations: 'bn' is harder to understand than 'batch_norm'.</li> <li>Avoid baked-in constants throughout the code. Instead, specify them as parameters to your function. If you must have     a constant, follow the naming guideline (e.g., <code>GLOBAL_CONSTANT</code>).</li> <li>Avoid functions that span hundreds of lines. Large functions are more difficult to read and more difficult to test.     If &gt;120 lines, consider re-factoring it into smaller logical functions, each unit-tested and well-documented.</li> <li>Re-use code by importing. Do not copy and paste code.</li> <li>Usage of third-party code should be legally compatible and attributed.</li> </ul>"},{"location":"user-guide/contributing/contributing/#pull-request-pr-guidelines","title":"Pull Request (PR) Guidelines","text":""},{"location":"user-guide/contributing/contributing/#labeling-your-pr-as-external-contributor","title":"Labeling Your PR as External Contributor","text":"<p>If you are an external contributor (not an NVIDIA employee), please add the <code>contribution</code> label to your PR before submitting. Labels can be accessed in the right sidebar of the GitHub user interface when creating or editing a PR.</p>"},{"location":"user-guide/contributing/contributing/#ci-pipeline-configuration-controls","title":"CI Pipeline Configuration Controls","text":"<p>CI pipeline behavior can be controlled via checkboxes in PR descriptions to optimize test execution:</p> <p>Key behaviors:</p> <ul> <li>Controls processed automatically on PR submit/update</li> <li>Labels applied based on checkbox status</li> <li>Invalid combinations default to most restrictive option</li> </ul>"},{"location":"user-guide/contributing/contributing/#skip_ci","title":"SKIP_CI","text":"<ul> <li>Skips entire CI pipeline</li> <li>Use for documentation typos, README updates</li> </ul>"},{"location":"user-guide/contributing/contributing/#include_notebooks_tests","title":"INCLUDE_NOTEBOOKS_TESTS","text":"<ul> <li>Enables notebook validation tests</li> <li>Use when modifying notebooks or notebook-related code</li> <li>Disabled by default</li> </ul>"},{"location":"user-guide/contributing/contributing/#include_slow_tests","title":"INCLUDE_SLOW_TESTS","text":"<ul> <li>Enables unit tests labelled as slow ie CLI tests</li> <li>Use when modifying core functionalities and require extensive, end-2-end, testing</li> <li>Disabled by default</li> </ul>"},{"location":"user-guide/contributing/contributing/#developer-workflows","title":"Developer workflows","text":"<p>You should always carefully test your changes. Run <code>pytest ...</code> in your container locally. All tests are done via <code>pytest</code>.</p> <p>Changes that affect model training accuracy or compute performance should be tested on SLURM.</p> <p>Developer workflow for external code contributions is as follows:</p> <ol> <li> <p>External developers must first fork the upstream BioNeMo OSS repository and for BioNeMo2 (this branch) use the <code>main</code> branch as base.</p> </li> <li> <p>Clone the forked repository and push changes to the personal fork.</p> </li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/YOUR_FORK.git bionemo-framework\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin &lt;local-branch&gt;:&lt;remote-branch&gt;\n</code></pre> <p>Developer workflow for internal or those developers that have been granted push access to our repository is as follows:</p> <ol> <li>Clone this repository locally</li> <li>Create a branch which ideally should be of the form <code>username/branch_description</code></li> <li>Push branch up to our repository <code>git push -u origin HEAD</code></li> </ol> <p>For both internal and external developers, the next step is opening a PR:</p> <ol> <li>Once the code changes are staged on the fork and ready for review, a   Pull Request (PR) can be     requested to merge the changes from a branch of the     fork or branch into <code>main</code>.<ul> <li>Exercise caution when selecting the source and target branches for the PR. Note that versioned releases of TensorRT OSS are posted to <code>release/</code> branches of the upstream repo.</li> <li>Creation of a PR creation kicks off the code review process.</li> <li>At least one TensorRT engineer will be assigned for the review.</li> <li>While under review, mark your PRs as work-in-progress by prefixing the PR title with [WIP].</li> </ul> </li> <li>Once ready, CI can be started by a developer with permissions when they add a <code>/build-ci</code> comment. This must pass   prior to merging.</li> </ol>"},{"location":"user-guide/contributing/contributing/#general-guidelines_1","title":"General guidelines","text":"<p>Send your PRs to the <code>main</code> branch. Branch off from <code>main</code> when making your changes. Prefix your branches with your name or initials (for example, <code>your_name/branch_description</code>) if you have push access to our repository otherwise please create a fork with your branch and submit a PR with <code>main</code> as the target.</p> <ul> <li>Make sure your PR does one thing. Have a clear answer to \"What does this PR do?\"</li> <li>Make sure you have the linters enabled via pre-commit hooks (<code>pre-commit install</code>) (See also Pre-commit   validation)</li> <li>Follow the default PR template</li> <li>Make sure all unit tests finish successfully before running PR pipeline by invoking <code>pytest scripts sub-packages</code>.</li> <li>Make sure you added necessary tests and documentation changes (could be just comments in the config files) for the   feature in your PR</li> <li>Rebase your feature branch with the latest <code>main</code> to include any new changes that have been added. Resolve merge   conflicts, if any</li> <li>Send your PR and request a review</li> <li>If your PR is still a work in progress, mark it as \"Draft\"</li> <li>Your merge request must pass all pipelines and be peer-reviewed before it can be merged.</li> <li>Make sure to merge your PR when it is ready and pipeline is successful</li> </ul>"},{"location":"user-guide/contributing/contributing/#unit-tests","title":"Unit tests","text":"<p>Contributors to BioNeMo FW are expected to unit test their introduced changes.</p> <p>After testing your code locally, trigger tests in the PR's CI. Let a code-owner know that you are ready for the build to  run and they will leave a <code>/build-ci</code> comment on your PR which will run the CI test suite.</p>"},{"location":"user-guide/contributing/contributing/#adding-unit-tests","title":"Adding unit tests","text":"<p>Add unit tests under <code>tests</code> to examine use cases of new classes or methods that are being added to the codebase. Each test file must be for a particular file or module. For example if you have a file that is under <code>src/path/to/module/my_file_name.py</code> then your test should match the path at <code>tests/path/to/module/test_my_file_name.py</code>. Check the tests folders in the sub-modules of this repository for examples. If you are testing a module, such as integrating multiple examples of different files, then you can use the following pattern to test the module, say in the above example, if you wanted to test functions from several files together that all exist in the same <code>src/path/to/module</code> then you could create a <code>tests/path/to/test_module.py</code> file. The same is true for parents of that module and so on. Generally unit tests should exist at the level of the individual file however.</p>"},{"location":"user-guide/contributing/contributing/#pre-commit-validation","title":"Pre-commit validation","text":"<p>We use pre-commit for essential static checks. These checks are enforced on new PRs through the CI process, but should also be run locally. After following the installation instructions for pre-commit, run <code>pre-commit install</code> in the bionemo-framework repository to initialize the checks.</p> <p>To run pre-commit checks (and fix errors where possible), run <code>pre-commit run --all-files</code>. To ignore a pre-commit error locally, use <code>git commit -n ...</code> to allow the commit to proceed with some failing pre-commit checks.</p>"},{"location":"user-guide/contributing/contributing/#updating-license-header-on-python-files","title":"Updating License Header on Python Files","text":"<p>If you add new Python (<code>.py</code>) files, be sure to run our license-check. If you have not already done sone, please install the dev-requirements.txt. If you are working directly inside a release container, you may need to manually install these. We recommend using the developer container for contributions.</p> <pre><code>pip install -r dev-requirements.txt --user\npython ./scripts/license_check.py --modify --replace --license-header ./license_header -c sub-packages/ -c docs/ -c scripts/ -c ci/ -c internal/\n</code></pre>"},{"location":"user-guide/contributing/contributing/#updating-the-secrets-baseline-file","title":"Updating the secrets baseline file","text":"<p>If false-positives are raised by the detect-secrets pre-commit hook, they can be added to the baseline files by running the following commands:</p> <pre><code>detect-secrets scan --baseline .secrets.baseline --exclude-files '(.*\\.ipynb|.*\\.baseline)$'\ndetect-secrets scan --baseline .secrets-nb.baseline --exclude-files '^.(?!.*\\.ipynb)' --exclude-lines '\"(hash|id|image/\\w+)\":.*'\n</code></pre> <p>The resulting altered baseline files should then be committed.</p>"},{"location":"user-guide/contributing/sub-package_dependency_graph/","title":"Sub package dependency graph","text":""},{"location":"user-guide/contributing/sub-package_dependency_graph/#sub-package-dependency-graph","title":"Sub-Package Dependency Graph","text":"<p>The script in <code>sub-packages/bionemo/fw/src/dependency_graph.py</code> generates a dependency graph for the BioNeMo sub-packages and verifies that the pyproject.toml and tach.toml files align and capture the dependencies needed for imports in the python files. Additionally, it checks dependencies between BioNeMo sub-packages and creates visual representations of the dependencies in pyproject.toml files, in tach.toml, and in the source files.</p> <p>These are visualizations of the dependency graph from the pyproject.toml files:</p> <p></p> <p>Similarly from the tach.toml file:</p> <p></p> <p>And these are the dependencies from the file imports:</p> <p></p>"},{"location":"user-guide/contributing/Writing%20Documentation/","title":"Writing Good and Thorough Documentation","text":"<p>As a contributor to our codebase, writing high-quality documentation is an essential part of ensuring that others can understand and work with your code effectively. Good documentation helps to reduce confusion, facilitate collaboration, and streamline the development process. In this guide, we will outline the principles and best practices for writing thorough and readable documentation that adheres to the Chicago Manual of Style.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/#chicago-manual-of-style","title":"Chicago Manual of Style","text":"<p>Our documentation follows the Chicago Manual of Style, a widely accepted standard for writing and formatting. This style guide provides a consistent approach to writing, grammar, and punctuation, making it easier for readers to understand and navigate our documentation.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/#key-principles","title":"Key Principles","text":"<p>When writing documentation, keep the following principles in mind:</p> <ol> <li>Clarity: Use clear and concise language to convey your message. Avoid ambiguity and jargon that may confuse readers.</li> <li>Accuracy: Ensure that your documentation is accurate and up-to-date. Verify facts, details, and code snippets     before publishing.</li> <li>Completeness: Provide all necessary information to understand the code, including context, syntax, and examples.</li> <li>Consistency: Use a consistent tone, voice, and style throughout the documentation.</li> <li>Accessibility: Make your documentation easy to read and understand by using headings, bullet points, and short paragraphs.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#documentation-structure","title":"Documentation Structure","text":"<p>A well-structured documentation page should include the following elements:</p> <ol> <li>Header: A brief title that summarizes the content of the page.</li> <li>Introduction: A short overview of the topic, including its purpose and relevance.</li> <li>Syntax and Parameters: A detailed explanation of the code syntax, including parameters, data types, and return values.</li> <li>Examples: Concrete examples that illustrate how to use the code, including input and output.</li> <li>Tips and Variations: Additional information, such as best practices, common pitfalls, and alternative approaches.</li> <li>Related Resources: Links to relevant documentation, tutorials, and external resources.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#best-practices","title":"Best Practices","text":"<p>To ensure high-quality documentation, follow these best practices:</p> <ol> <li>Use headings and subheadings: Organize your content with clear headings and subheadings to facilitate scanning and navigation.</li> <li>Use bullet points and lists: Break up complex information into easy-to-read lists and bullet points.</li> <li>Provide context: Give readers a clear understanding of the code's purpose, history, and relationships to other components.</li> <li>Review and edit: Carefully review and edit your documentation to ensure accuracy, completeness, and consistency.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#resources","title":"Resources","text":"<p>For more information on the Chicago Manual of Style, refer to their online published version.</p> <p>By following these guidelines and principles, you wi ll be able to create high-quality documentation that helps others understand and work with your code effectively. Remember to always prioritize clarity, accuracy, and completeness, and to use the Chicago Style Guide as your reference for writing and formatting.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/","title":"Jupyter Notebook Support","text":"In\u00a0[1]: Copied! <pre>a = 1\nb = 2\na + b\n</pre> a = 1 b = 2 a + b Out[1]: <pre>3</pre> In\u00a0[2]: Copied! <pre>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nxs = np.linspace(0, 2*np.pi, 100)\nplt.plot(xs, np.sin(xs))\n</pre> %matplotlib inline import matplotlib.pyplot as plt import numpy as np  xs = np.linspace(0, 2*np.pi, 100) plt.plot(xs, np.sin(xs)) Out[2]: <pre>[&lt;matplotlib.lines.Line2D at 0x7672067fb190&gt;]</pre> In\u00a0[3]: Copied! <pre>#NBVAL_CHECK_OUTPUT\n\nimport numpy as np\n\n\nprint(np.arange(5))\n</pre> #NBVAL_CHECK_OUTPUT  import numpy as np   print(np.arange(5)) <pre>[0 1 2 3 4]\n</pre>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#jupyter-notebook-support","title":"Jupyter Notebook Support\u00b6","text":"<p>Jupyter notebooks can be rendered as part of the documentation build system as an alternative to markdown files. The docs site uses mkdocs-jupyter to build and render jupyter notebooks as markdown files.</p> <p>Note: There are some limitations to jupyter rendering.</p> <ol> <li>Notebooks are not executed as part of the docs publishing pipeline. CI tests to ensure notebook consistency are run separately (see Testing Jupyter Notebooks).</li> <li>Notebook markdown cells don't support the full range of mkdocs-material configuration, including things like admonitions, referencing automatically-generated API documentation via mkdocstrings etc. (more here).</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#example-code-block","title":"Example code block\u00b6","text":"<p>Markdown headings can be used to create a TOC similarly to traditional mkdocs pages.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#embedded-visualizations","title":"Embedded visualizations\u00b6","text":"<p>We can also embed images using standard approaches to embedding graphics in notebooks.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#testing-jupyter-notebooks","title":"Testing Jupyter Notebooks\u00b6","text":"<p>Jupyter notebooks are run as part of the CI build suite using <code>nbval</code>. To run these tests locally, run</p> <pre>pytest --nbval-lax docs/\n</pre> <p>from the repository root. By default, <code>nbval</code> will only check that the notebook executes successfully. To add additional checks to ensure the consistency of the output, add a <code>#NBVAL_CHECK_OUTPUT</code> marker comment, which will ensure that the output of the saved jupyter notebook matches the output when the notebook is executed in CI.</p> <p>For example:</p>"},{"location":"user-guide/contributing/Writing%20Documentation/mkdocs/","title":"MkDocs","text":""},{"location":"user-guide/contributing/Writing%20Documentation/mkdocs/#build-system","title":"Build system","text":"<p>BioNeMo 2 uses Material for MkDocs to build it's documentation. Docstrings are converted to automatically-generated API reference pages using <code>mkdocstrings</code>, and can be linked from markdown pages using paths.</p>"},{"location":"user-guide/developer-guide/bionemo-core/bionemo-core-Overview/","title":"bionemo-core","text":"<p>Common code that all BioNeMo framework packages depend on. Contains highly reusable, battle-tested abstractions and implementations that are valuable across a wide variety of domains and applications.</p> <p>Crucially, the <code>bionemo-core</code> Python package (namespace <code>bionemo.core</code>) depends on PyTorch and PyTorch Lightning. Other key BioNeMo component libraries, such as <code>bionemo-llm</code> and <code>bionemo-geometric</code>, obtain their PyTorch dependencies via <code>bionemo-core</code>.</p>"},{"location":"user-guide/developer-guide/bionemo-core/bionemo-core-Overview/#developer-setup","title":"Developer Setup","text":"<p>After following the setup specified in the README, you may install this project's code in your environment via executing: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests with code coverage, execute: <pre><code>pytest -v --cov=bionemo --cov-report=term .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-core/bionemo-core-Overview/#package-highlights","title":"Package Highlights","text":"<p>In <code>bionemo.core.model.config</code>: - <code>ModelOutput</code>: A Model's forward pass may produce a tensor, multiple tensors, or named tensors. - <code>LossType</code>: A generic type parameter for a loss function. - <code>Model</code>: An interface for any ML model that accepts and produces <code>torch.Tensor</code>s. - <code>ModelType</code>: A generic type parameter that is constrained to the <code>Model</code> interface. - <code>BionemoModelConfig</code>: An abstract class that enables parameterizable model instantiation that is compatible with Megatron. - <code>BionemoTrainableModelConfig</code>: An extension that includes the loss function to use with the model during training.</p> <p>In <code>bionemo.core.utils</code>: - the <code>batching_utils</code> module's <code>pad_token_ids</code>, which pads token ids with padding value &amp; returns a mask. - the <code>dtype</code> module's <code>get_autocast_dtype</code>, which converts from nemo/nvidia datatypes to their PyTorch equivalents. - the <code>random_utils</code> module, which includes functions for managing random seeds and performing sampling.</p> <p>In the <code>bionemo.data</code> package, there is: - <code>multi_epoch_dataset</code>: contains many dataset implements that are useful for mutli-epoch training. - <code>resamplers</code>: contains a P-RNG based Dataset implementation.</p> <p>There's a constant global value, <code>bionemo.core.BIONEMO_CACHE_DIR</code>, which is used as a local on-disk cache for resources.</p>"},{"location":"user-guide/developer-guide/bionemo-esm2/bionemo-esm2-Overview/","title":"bionemo-esm2","text":"<p>ESM-2 is a protein language model with BERT architecture trained on millions of protein sequences from UniProt. ESM-2 learns the patterns and dependencies between amino acids that ultimately give rise to a protein\u2019s structure. ESM-2 is pretrained on a masked language model (MLM) objective. During pretraining, 15% of the input sequence is perturbed, and within which 80% of the residues are replaced with a mask token, 10% are replaced with a random token, and 10% are left unchanged. The model is then trained to predict the original amino acids at the perturbed positions with the context of the surrounding amino acids.</p> <p>Despite pretraining on an MLM objective, the sequence representation learned by ESM-2 is highly transferable to downstream tasks. ESM-2 can be fine-tuned on a variety of tasks, including secondary structure prediction as, and whole-sequence prediction on cellular localization, thermostability, solubility, and other protein properties.</p>"},{"location":"user-guide/developer-guide/bionemo-esm2/bionemo-esm2-Overview/#setup","title":"Setup","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/","title":"bionemo-evo2","text":"<p><code>bionemo-evo2</code> is a <code>pip</code>-installable package that contains data preprocessing, training, and inferencing code for Evo2, a new <code>Hyena</code>-based foundation model for genome generation and understanding. Built upon <code>Megatron-LM</code> parallelism and <code>NeMo2</code> algorithms, <code>bionemo-evo2</code> provides the remaining tools necessary to effectively fine-tune the pre-trained Evo2 model checkpoint on user-provided sequences at scale, and generate state-of-the-art life-like DNA sequences from Evo2 for downstream metagenomic tasks.</p>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#quickstart-tutorials","title":"Quickstart tutorials","text":"<p>Please see</p>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#installation","title":"Installation","text":"<p>To install this package, execute the following command:</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests, execute the following command:</p> <pre><code>pytest -v .\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#preprocessing","title":"Preprocessing","text":"<p>To train or fine-tune Evo2 on a custom dataset, we need to preprocess and index sequence data for training from raw FASTA files into tokenized binaries compliant with <code>NeMo2</code> / <code>Megatron-LM</code>. For more information about how to configure your data for training, refer to data/README.md and utils.config.Evo2PreprocessingConfig.</p> <pre><code>preprocess_evo2 -c &lt;CONFIG_PATH&gt;\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#training","title":"Training","text":"<p>Given a preprocessed collection of preprocessed datasets, and optionally a pre-trained NeMo2 checkpoint for Evo2, training can be executed using the following command:</p> <pre><code>$ train_evo2 --help\nusage: train_evo2 [-h] (-d DATASET_CONFIG | --mock-data) [--dataset-dir DATASET_DIR] [--num-nodes NUM_NODES] [--devices DEVICES] [--seq-length SEQ_LENGTH]\n                  [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE] [--context-parallel-size CONTEXT_PARALLEL_SIZE]\n                  [--no-wandb] [--wandb-project WANDB_PROJECT] [--wandb-run-id WANDB_RUN_ID] [--wandb-group WANDB_GROUP] [--wandb-job-type WANDB_JOB_TYPE] [--wandb-offline]\n                  [--wandb-anonymous] [--sequence-parallel] [--fp8] [--micro-batch-size MICRO_BATCH_SIZE] [--global-batch-size GLOBAL_BATCH_SIZE]\n                  [--grad-acc-batches GRAD_ACC_BATCHES] [--max-steps MAX_STEPS] [--val-check-interval VAL_CHECK_INTERVAL] [--grad-reduce-in-fp32] [--fp8-wgrad]\n                  [--use-megatron-comm-overlap-llama3-8k] [--tp-comm-overlap-backend {nccl,mpi,gloo}] [--align-param-gather]\n                  [--model-size {1b,1b_nv,40b,40b_arc_longcontext,40b_nv,7b,7b_arc_longcontext,7b_nv,test,test_nv}] [--add-bias-output] --experiment-dir EXPERIMENT_DIR\n                  [--limit-val-batches LIMIT_VAL_BATCHES] [--log-every-n-steps LOG_EVERY_N_STEPS] [--ckpt-dir CKPT_DIR] [--wd WD] [--restore-optimizer-from-ckpt]\n                  [--no-average-in-collective] [--seed SEED] [--workers WORKERS] [--gc-interval GC_INTERVAL] [--enable-preemption] [--ckpt-async-save]\n                  [--ckpt-format {torch_dist,zarr}] [--eod-pad-in-loss-mask] [--cross-entropy-loss-fusion] [--no-fp32-residual-connection]\n                  [--debug-ddp-parity-freq DEBUG_DDP_PARITY_FREQ] [--hybrid-override-pattern HYBRID_OVERRIDE_PATTERN] [--num-layers NUM_LAYERS] [--tflops-callback]\n                  [--log-parameters-and-shapes] [--lr LR] [--min-lr MIN_LR] [--warmup-steps WARMUP_STEPS] [--nsys-profiling] [--nsys-start-step NSYS_START_STEP]\n                  [--nsys-end-step NSYS_END_STEP] [--no-renormalize-loss] [--nsys-ranks NSYS_RANKS [NSYS_RANKS ...]]\n                  [--activation-checkpoint-recompute-num-layers ACTIVATION_CHECKPOINT_RECOMPUTE_NUM_LAYERS] [--disable-checkpointing] [--clip-grad CLIP_GRAD]\n                  [--seq-len-interpolation-factor SEQ_LEN_INTERPOLATION_FACTOR] [--overlap-param-gather] [--overlap-grad-reduce] [--hidden-dropout HIDDEN_DROPOUT]\n                  [--attention-dropout ATTENTION_DROPOUT] [--no-activation-checkpointing | --selective-activation-checkpointing]\n\nTrain a Hyena model using NeMo 2.0.\n\noptions:\n  -h, --help            show this help message and exit\n  -d DATASET_CONFIG, --dataset-config DATASET_CONFIG\n                        Path to the blended / weighted training dataset configuration YAML. (default: None)\n  --mock-data           Train with Mock data (for testing/debugging), either set this or provide a dataset config. (default: False)\n  --dataset-dir DATASET_DIR\n                        Absolute path to the dataset directory. Defaults to using the absolute or relative paths (dataset_prefix) specified in the dataset config YAML.\n                        (default: None)\n  --num-nodes NUM_NODES\n                        Number of nodes to use for training, defaults to 1. (default: 1)\n  --devices DEVICES     Number of devices to use for training, defaults to 1. (default: 1)\n  --seq-length SEQ_LENGTH\n                        Training sequence length (default: 8192)\n  --tensor-parallel-size TENSOR_PARALLEL_SIZE\n                        Order of tensor parallelism. Defaults to 1. (default: 1)\n  --pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE\n                        Order of pipeline parallelism. Defaults to 1. (default: 1)\n  --context-parallel-size CONTEXT_PARALLEL_SIZE\n                        Order of context parallelism. Defaults to 1. (default: 1)\n  --no-wandb            Disable Wandb logging (default: False)\n  --wandb-project WANDB_PROJECT\n                        Wandb project name (default: bionemo_evo2)\n  --wandb-run-id WANDB_RUN_ID\n                        Wandb run identifier (default: None)\n  --wandb-group WANDB_GROUP\n                        A unique string shared by all runs in a given group (default: None)\n  --wandb-job-type WANDB_JOB_TYPE\n                        A unique string representing a type of run, which is useful when you're grouping runs together into larger experiments using group. (default: None)\n  --wandb-offline       Use wandb in offline mode (default: False)\n  --wandb-anonymous     Enable or explicitly disable anonymous logging (default: False)\n  --sequence-parallel   Set to enable sequence parallelism. (default: False)\n  --fp8                 Set to enable FP8 (default: False)\n  --micro-batch-size MICRO_BATCH_SIZE\n                        Micro-batch size for data-parallel training. (default: 1)\n  --global-batch-size GLOBAL_BATCH_SIZE\n                        Global batch size for training. If set to None, infer it from the TP, CP, and PP parameters. (default: None)\n  --grad-acc-batches GRAD_ACC_BATCHES\n                        Number of batches to accumulate gradients over. (default: 1)\n  --max-steps MAX_STEPS\n                        Number of training optimizer update steps. (default: None)\n  --val-check-interval VAL_CHECK_INTERVAL\n                        Number of steps between validation measurements and model checkpoints. (default: None)\n  --grad-reduce-in-fp32\n                        Gradient reduce in FP32. (default: False)\n  --fp8-wgrad           Faster option that is maybe less accurate (TBD) when using fp8. (default: False)\n  --use-megatron-comm-overlap-llama3-8k\n  --tp-comm-overlap-backend {nccl,mpi,gloo}\n                        TP communication backend to use. Defaults to 'nccl'. (default: nccl)\n  --align-param-gather\n  --model-size {1b,1b_nv,40b,40b_arc_longcontext,40b_nv,7b,7b_arc_longcontext,7b_nv,test,test_nv}\n                        Model architecture to use, choose between 7b, 40b, or test (a sub-model of 4 layers, less than 1B parameters). '_arc_1m' models have GLU / FFN\n                        dimensions that support 1M context length when trained with TP&lt;=8. (default: 7b)\n  --add-bias-output     Add bias to the output layer to enable learning a simple prior. (default: False)\n  --experiment-dir EXPERIMENT_DIR\n                        Directory to write model checkpoints and results to. (default: None)\n  --limit-val-batches LIMIT_VAL_BATCHES\n                        Number of validation steps (default: 20)\n  --log-every-n-steps LOG_EVERY_N_STEPS\n                        Number of steps between logging. (default: 1)\n  --ckpt-dir CKPT_DIR   Directory to restore an initial checkpoint from. Use this for supervised fine-tuning. (default: None)\n  --wd WD               Weight decay for optimizer. (default: 0.01)\n  --restore-optimizer-from-ckpt\n                        Restore optimizer state from initial checkpoint. Defaults to False. (default: False)\n  --no-average-in-collective\n                        Avaerage optimizer state in collective rather than dividing by dp size and summing. (default: False)\n  --seed SEED           Set random seed for training. (default: 1234)\n  --workers WORKERS     Number of workers to use for data loading. (default: 8)\n  --gc-interval GC_INTERVAL\n                        Set to a value &gt; 0 if you want to synchronize garbage collection, will do gc every gc-interval steps. (default: 0)\n  --enable-preemption   Enable preemption hooks. If enabled this will save a checkpoint whenver slurm exits. (default: False)\n  --ckpt-async-save\n  --ckpt-format {torch_dist,zarr}\n                        Specify checkpoint format to use. Defaults to 'torch_dist', as 'zarr' is deprecated. Only use if resuming training from a zarr checkpoint. (default:\n                        torch_dist)\n  --eod-pad-in-loss-mask\n                        Do not predict EOD/Pad tokens (typical default, but not default in original evo2). (default: False)\n  --cross-entropy-loss-fusion\n                        Use the faster, but maybe less accurate fused form of cross entropy, which also has bf16 grads internally. (default: False)\n  --no-fp32-residual-connection\n                        If set, turn off fp32 residual connections which may be faster but may impact accuracy. (default: False)\n  --debug-ddp-parity-freq DEBUG_DDP_PARITY_FREQ\n                        Set to value &gt; 0 to debug DDP weight parity between ranks. (default: 0)\n  --hybrid-override-pattern HYBRID_OVERRIDE_PATTERN\n                        Override the hybrid override pattern in the config (specifies hyena layer ordering and type). (default: None)\n  --num-layers NUM_LAYERS\n                        If set, override the number of layers specified in the requested config. (default: None)\n  --tflops-callback     Enable tflops calculation callback for Hyena / Evo2. Defaults to False. (default: False)\n  --log-parameters-and-shapes\n                        Log training parameters shapes and dtypes for debugging. (default: False)\n  --lr LR               Learning rate. (default: 0.0003)\n  --min-lr MIN_LR       Min learning rate in cosine annealing. (default: 3e-05)\n  --warmup-steps WARMUP_STEPS\n                        Number of warmup steps in cosine annealing (default: 2500)\n  --nsys-profiling      Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling output you must run the whole program with\n                        `nsys`. For example: `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true --capture-range=cudaProfilerApi --capture-range-\n                        end=stop [regular python command here]` (default: False)\n  --nsys-start-step NSYS_START_STEP\n                        Start nsys profiling after this step. (default: 0)\n  --nsys-end-step NSYS_END_STEP\n                        End nsys profiling after this step. (default: None)\n  --no-renormalize-loss\n                        Do not renormalize the loss weights. (default: False)\n  --nsys-ranks NSYS_RANKS [NSYS_RANKS ...]\n                        Enable nsys profiling for these ranks. (default: [0])\n  --activation-checkpoint-recompute-num-layers ACTIVATION_CHECKPOINT_RECOMPUTE_NUM_LAYERS\n                        If set, override the default value set in the config. (default: None)\n  --disable-checkpointing\n                        Disable creating a ModelCheckpoint callback. (default: True)\n  --clip-grad CLIP_GRAD\n                        Grad clip value. Note that when using DDP this may need to be inflated. (default: 1.0)\n  --seq-len-interpolation-factor SEQ_LEN_INTERPOLATION_FACTOR\n                        Adjusts the linear scaling of ROPE (Rotary Position Embedding) for context extension. Set this factor relative to your base context length e.g., for\n                        an original context length of 8192 and an extended context length of 524288, use 524288/8192 = 64. (default: None)\n  --overlap-param-gather\n                        Overlap the parameter gather with the optimizer step. This is currently disabled due to a NeMo bug when using DDP. Making this an option defaulting to\n                        False is a temporary solution until the bug is fixed. (default: False)\n  --overlap-grad-reduce\n                        Overlap the gradient reduce with the optimizer step. (default: False)\n  --hidden-dropout HIDDEN_DROPOUT\n                        Dropout probability for the hyena layers (default: 0.0)\n  --attention-dropout ATTENTION_DROPOUT\n                        Dropout probability for the attention layers. (default: 0.0)\n  --no-activation-checkpointing\n  --selective-activation-checkpointing\n</code></pre> <p>To supply a pre-trained checkpoint, pass the NeMo2 checkpoint directory to <code>--ckpt-dir</code>, and the script will dump newly trained checkpoints and logs to <code>--experiment-dir</code>. However, if there are existing well-defined checkpoints in the directory specified by <code>--experiment-dir</code>, the script will automatically resume training from the most recent checkpoint in the experiment directory instead of starting from the checkpoint specified by <code>--ckpt-dir</code>, which streamlines long training sessions. (To disable this behavior, supply a new or clean <code>--experiment-dir</code> when restarting from <code>--ckpt-dir</code>.)</p> <p>Training data and sampling weights can be specified using the <code>--dataset-config</code> argument as a YAML file adhering to the following schema: utils.config.Evo2BlendedDatasetConfig. For more information about dataset sampling and blending during training with Megatron-LM, refer to megatron/core/datasets/readme.md. For example:</p> <pre><code>- dataset_prefix: /workspace/bionemo2/data/metagenomics/pretraining_data_metagenomics/data_metagenomics_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.18\n- dataset_prefix: /workspace/bionemo2/data/gtdb_imgpr/pretraining_data_gtdb_imgpr/data_gtdb_imgpr_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.24\n- dataset_prefix: /workspace/bionemo2/data/imgvr_untagged/imgvr_untagged_data/data_imgvr_train_text_CharLevelTokenizer_document\n  dataset_split: train\n  dataset_weight: 0.03\n- dataset_prefix: /workspace/bionemo2/data/promoters/pretraining_data_promoters/data_promoters_valid_text_CharLevelTokenizer_document\n  dataset_split: validation\n  dataset_weight: 0.0003\n- dataset_prefix: /workspace/bionemo2/data/organelle/pretraining_data_organelle/data_organelle_valid_text_CharLevelTokenizer_document\n  dataset_split: validation\n  dataset_weight: 0.005\n- dataset_prefix: /workspace/bionemo2/data/metagenomics/pretraining_data_metagenomics/data_metagenomics_test_text_CharLevelTokenizer_document\n  dataset_split: test\n  dataset_weight: 0.18\n- dataset_prefix: /workspace/bionemo2/data/gtdb_v220/gtdb_v220_imgpr_merged_data/data_gtdb_imgpr_test_text_CharLevelTokenizer_document\n  dataset_split: test\n  dataset_weight: 0.24\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#inference","title":"Inference","text":"<p>Once you have a pre-trained or fine-tuned Evo2 checkpoint, you can also prompt the model to generate DNA sequences using the following command:</p> <pre><code>$ infer_evo2 --help\nusage: infer_evo2 [-h] [--prompt PROMPT] --ckpt-dir CKPT_DIR [--temperature TEMPERATURE] [--top-k TOP_K] [--top-p TOP_P] [--max-new-tokens MAX_NEW_TOKENS] [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE] [--context-parallel-size CONTEXT_PARALLEL_SIZE] [--output-file OUTPUT_FILE]\n\noptions:\n  -h, --help            show this help message and exit\n  --prompt PROMPT       Prompt to generate text from Evo2. Defaults to a phylogenetic lineage tag for E coli.\n  --ckpt-dir CKPT_DIR   Path to checkpoint directory containing pre-trained Evo2 model.\n  --temperature TEMPERATURE\n                        Temperature during sampling for generation.\n  --top-k TOP_K         Top K during sampling for generation.\n  --top-p TOP_P         Top P during sampling for generation.\n  --max-new-tokens MAX_NEW_TOKENS\n                        Maximum number of tokens to generate.\n  --tensor-parallel-size TENSOR_PARALLEL_SIZE\n                        Order of tensor parallelism. Defaults to 1.\n  --pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE\n                        Order of pipeline parallelism. Defaults to 1.\n  --context-parallel-size CONTEXT_PARALLEL_SIZE\n                        Order of context parallelism. Defaults to 1.\n  --output-file OUTPUT_FILE\n                        Output file containing the generated text produced by the Evo2 model. If not provided, the output will be logged.\n</code></pre> <p>As in <code>train_evo2</code>, <code>--ckpt-dir</code> points to the NeMo2 checkpoint directory for Evo2 that you want to load for inference. <code>--output-file</code> can be used to dump the output into a <code>.txt</code> file, and if not specified the output will be logged in the terminal.</p> <pre><code>[NeMo I 2025-01-06 17:22:22 infer:102] ['CTCTTCTGGTATTTGG']\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#prediction","title":"Prediction","text":"<p>To run a forward pass of the Evo2 model, you can call <code>predict_evo2</code>, which processes a batch of sequences and returns either raw token logits or, if <code>--output-log-prob-seqs</code> is set, log-probability scores.</p> <p>For example, to predict the log-probability scores of a batch of sequences saved to <code>fasta_path</code>, you can run the following command:</p> <pre><code>predict_evo2 \\\n  --fasta &lt;fasta_path&gt; \\\n  --ckpt-dir &lt;PATH_TO_CHECKPOINT&gt; \\\n  --output-dir &lt;PATH_TO_OUTPUT_FILE&gt; \\\n  --model-size 1b \\\n  --tensor-parallel-size 1 \\\n  ----pipeline-model-parallel-size 1 \\\n  --context-parallel-size 1 \\\n  --output-log-prob-seqs\n</code></pre> <p>An example of using <code>predict_evo2</code> for variant effect prediction can be found in our Evo 2 Zeroshot BRCA1 Notebook. This notebook demonstrates how to use Evo2 to predict whether single nucleotide variants (SNVs) in the BRCA1 gene are likely to be harmful to protein function and potentially increase cancer risk, by comparing the model's log probability scores between the reference and variant sequences.</p>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#checkpoint-conversion-from-hugging-face-to-nemo2","title":"Checkpoint conversion from hugging face to NeMo2","text":"<p>The following conversion script should work on any savanna formatted arc evo2 checkpoint. Make sure you match up the model size with the checkpoint you are converting. The pyproject.toml makes the conversion script available as a command line tool <code>evo2_convert_to_nemo2</code>, so you can try replacing:</p> <pre><code>evo2_convert_to_nemo2 \\\n  ...\n</code></pre> <p>with the following if you want to run with <code>-m pdb</code> or something:</p> <pre><code>python \\\n  sub-packages/bionemo-evo2/src/bionemo/evo2/utils/checkpoint/convert_to_nemo.py \\\n  ...\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#1b-8k","title":"1b-8k","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_1b_base \\\n  --model-size 1b --output-dir nemo2_evo2_1b_8k\n</code></pre> <p>This new checkpoint <code>nemo2_evo2_1b_8k</code> is ready to go in nemo2 format in downstream pretraining or prediction workflows.</p>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#optional-steps-if-you-want-to-register-the-model-with-ngc","title":"Optional steps if you want to register the model with NGC","text":"<p>If you want to register the checkpoint with NGC (typically only NVIDIA employees) then you can do the following.</p> <p>To create the checkpoint for distribution in NGC, first cd into the checkpiont directory:</p> <pre><code>cd nemo2_evo2_1b_8k\n</code></pre> <p>Then run the following command to make a tar of the full directory that gets unpacked into the current directory which our NGC loader expects:</p> <pre><code>tar -czvf ../nemo2_evo2_1b_8k.tar.gz .\n</code></pre> <p>Finally <code>sha256sum</code> the tar file to get the checksum:</p> <pre><code>sha256sum nemo2_evo2_1b_8k.tar.gz\n</code></pre> <p>Then register it into the loader for testing purposes by editing <code>sub-packages/bionemo-core/src/bionemo/core/data/resources/evo2.yaml</code>.</p>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#7b-8k","title":"7b-8k","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_7b_base \\\n  --model-size 7b --output-dir nemo2_evo2_7b_8k\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#7b-1m","title":"7b-1M","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_7b \\\n  --model-size 7b_arc_longcontext --output-dir nemo2_evo2_7b_1m\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#40b-8k","title":"40b-8k","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_40b_base \\\n  --model-size 40b --output-dir nemo2_evo2_40b_8k\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-evo2/bionemo-evo2-Overview/#40b-1m","title":"40b-1M","text":"<pre><code>evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_40b \\\n  --model-size 40b_arc_longcontext --output-dir nemo2_evo2_40b_1m\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/","title":"bionemo example model Overview","text":""},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#bionemo-example_model","title":"bionemo-example_model","text":""},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#introduction","title":"Introduction","text":"<p>This is a minimalist package containing an example model that makes use of bionemo2 and nemo conventions. It contains the necessary models, dataloaders, datasets, and custom loss functions. The referenced classes and functions are in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p>This tutorial demonstrates the creation of a simple MNIST model. This should be run in a BioNeMo container. The BioNeMo Framework container can run in a brev.dev launchable: . It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credit. Notebooks and a shell interface can be launched by clicking <code>Open Notebook</code>. (Note: This links to the nightly release and may be out of sync with these docs.)</p> <p>For this tutorial, we will reuse elements from the BioNeMo example_model package.</p> <p><code>Megatron</code>/<code>NeMo</code> modules and datasets are special derivatives of PyTorch modules and datasets that extend and accelerate the distributed training and inference capabilities of PyTorch.</p> <p>Some distinctions of Megatron/NeMo are:</p> <ul> <li><code>torch.nn.Module</code>/<code>LightningModule</code> changes into <code>MegatronModule</code>.</li> <li>Loss functions should extend the <code>MegatronLossReduction</code> module and implement a <code>reduce</code> method for aggregating loss across multiple micro-batches.</li> <li>Megatron configuration classes (for example <code>megatron.core.transformer.TransformerConfig</code>) are extended with a <code>configure_model</code> method that defines how model weights are initialized and loaded in a way that is compliant with training via NeMo2.</li> <li>Various modifications and extensions to common PyTorch classes, such as adding a <code>MegatronDataSampler</code> (and re-sampler such as <code>PRNGResampleDataset</code> or <code>MultiEpochDatasetResampler</code>) to your <code>LightningDataModule</code>.</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#loss-functions","title":"Loss Functions","text":"<p>First, we define a simple loss function in <code>bionemo.example_model.lightning.lightning_basic</code>. These should extend the <code>MegatronLossReduction</code> class. The output of forward and backward passes happen in parallel. There should be a forward function that calculates the loss defined. The reduce function is required.</p> <p>Loss functions used here are <code>MSELossReduction</code> and <code>ClassifierLossReduction</code>. These functions return a Tensor, which contain the losses for the microbatches, and a <code>SameSizeLossDict</code> containing the average loss. This is a Typed Dictionary that is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#datasets-and-datamodules","title":"Datasets and Datamodules","text":"<p>Datasets used for model training must be compatible with Megatron datasets. To enable this, the output of a given index and epoch must be deterministic. However, we may wish to have a different ordering in every epoch. To enable this, the items in the dataset should be accessible by both the epoch and the index. This can be done by accessing elements of the dataset with <code>EpochIndex</code> from <code>bionemo.core.data.multi_epoch_dataset</code>. A simple way of doing this is to wrap a dataset with <code>IdentityMultiEpochDatasetWrapper</code> imported from <code>bionemo.core.data.multi_epoch_dataset</code>. In this example, in in <code>bionemo.example_model.lightning.lightning_basic</code>, we use a custom dataset <code>MNISTCustomDataset</code> that wraps the <code>__getitem__</code> method of the MNIST dataset such that it returns a dict instead of a Tuple or tensor. The <code>MNISTCustomDataset</code> returns elements of type <code>MnistItem</code>, which is a <code>TypedDict</code>.</p> <p>In the data module/data loader class, it is necessary to have a data_sampler attribute to shuffle the data and that allows the sampler to be used with Megatron. This is a nemo2 peculiarity. A <code>nemo.lightning.pytorch.plugins.MegatronDataSampler</code> is the best choice. It sets up the capability to utilize micro-batching and gradient accumulation. It is also the place where the global batch size is constructed.</p> <p>Also the sampler will not shuffle your data. So you need to wrap your dataset in a dataset shuffler that maps sequential IDs to random IDs in your dataset. This can be done with <code>MultiEpochDatasetResampler</code> from <code>bionemo.core.data.multi_epoch_dataset</code>.</p> <p>This is implemented in the <code>MNISTDataModule</code>. In the setup method of the dataloader, the train, test and validation sets are <code>MNISTCustomDataset</code> are wrapped in the <code>IdentityMultiEpochDatasetWrapper</code>. These are then wrapped in the <code>MultiEpochDatasetResampler</code>. More information about <code>MegatronCompatability</code> and how to set up more complicated datasets can be found in <code>docs.user-guide.background.megatron_datasets.md</code>.</p> <p>We also define a <code>train_dataloader</code>, <code>val_dataloader</code>, and <code>predict_dataloader</code> methods that return the corresponding dataloaders.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#models","title":"Models","text":"<p>Models need to be Megatron modules. At the most basic level this just means:</p> <ol> <li>They extend <code>MegatronModule</code> from megatron.core.transformer.module.</li> <li>They need a config argument of type <code>megatron.core.ModelParallelConfig</code>. An easy way of implementing this is to inherit from <code>bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig</code>. This is a class for BioNeMo that supports usage with Megatron models, as NeMo2 requires. This class also inherits <code>ModelParallelConfig</code>.</li> <li>They need a self.<code>model_type:megatron.core.transformer.enums.ModelType</code> enum defined (<code>ModelType.encoder_or_decoder</code> is a good option.)</li> <li><code>def set_input_tensor(self, input_tensor)</code> needs to be present. This is used in model parallelism. This function can be a stub/placeholder function.</li> </ol> <p>The following models are implemented in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p><code>ExampleModelTrunk</code> is a base model. This returns a tensor. <code>ExampleModel</code> is a model that extends the base model with a few linear layers and it is used for pretraining. This returns the output of the base model and of the full model.</p> <p><code>ExampleFineTuneModel</code> extends the <code>ExampleModelTrunk</code> by adding a classification layer. This returns a tensor of logits over the 10 potential digits.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#model-configs","title":"Model Configs","text":"<p>The model config class is used to instantiate the model. These configs must have: 1. A <code>configure_model</code> method which allows the Megatron strategy to lazily initialize the model after the parallel computing environment has been setup. These also handle loading starting weights for fine-tuning cases. Additionally these configs tell the trainer which loss you want to use with a matched model. 2. A <code>get_loss_reduction_class</code> method that defines the loss function.</p> <p>The following configs are implemented in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p>Here, a base generic config <code>ExampleGenericConfig</code> is defined.  <code>PretrainConfig</code> extends this class. This defines the model class and the loss class in: <pre><code>class PretrainConfig(ExampleGenericConfig[\"PretrainModel\", \"MSELossReduction\"], iom.IOMixinWithGettersSetters):\n\n    model_cls: Type[PretrainModel] = PretrainModel\n    loss_cls: Type[MSELossReduction] = MSELossReduction\n</code></pre></p> <p>Similarly, <code>ExampleFineTuneConfig</code> extends <code>ExampleGenericConfig</code> for finetuning.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#training-module","title":"Training Module","text":"<p>It is helpful to have a training module that inherits from <code>lightning.pytorch.LightningModule</code> which organizes the model architecture, training, validation, and testing logic while abstracting away boilerplate code, enabling easier and more scalable training. This wrapper can be used for all model and loss combinations specified in the config. In <code>bionemo.example_model.lightning.lightning_basic</code>, we define <code>BionemoLightningModule</code>.</p> <p>In this example, <code>training_step</code>, <code>validation_step</code>, and <code>predict_step</code> define the training, validation, and prediction loops are independent of the forward method. In nemo:</p> <ol> <li>NeMo's Strategy overrides the <code>train_step</code>, <code>validation_step</code> and <code>prediction_step</code> methods.</li> <li>The strategies' training step will call the forward method of the model.</li> <li>That forward method then calls the wrapped forward step of <code>MegatronParallel</code> which wraps the forward method of the model.</li> <li>That wrapped forward step is then executed inside the <code>MegatronCore</code> scheduler, which calls the <code>_forward_step</code> method from the <code>MegatronParallel</code> class.</li> <li>Which then calls the <code>training_step</code>, <code>validation_step</code> and <code>prediction_step</code> function here.</li> </ol> <p>Additionally, during these steps, we log the validation, testing, and training loss. This is done similarly to https://lightning.ai/docs/torchmetrics/stable/pages/lightning.html. These logs can then be exported to wandb, or other metric viewers. For more complicated tracking, it may be necessary to use pytorch callbacks: https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html.</p> <p>Further <code>loss_reduction_class()</code>, <code>training_loss_reduction()</code>, <code>validation_loss_reduction(),</code> and<code>test_loss_reduction()</code> are defined based on what's in the config. Additionally,  <code>configure_model()</code> is defined based on the config.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#training-the-models","title":"Training the models","text":"<p>In <code>bionemo.example_model.lightning.lightning_basic</code> a checkpoint_callback variable is defined. This enables .nemo file-like checkpointing.</p> <p>The remaining functions are defined in the training scripts: <code>pretrain_mnist.py</code>, <code>finetune_mnist.py</code>, and <code>predict_mnist.py</code>.</p> <p>We specify a training strategy of type <code>nemo.lightning.MegatronStrategy</code>. This strategy implements model parallelism using NVIDIA's Megatron-LM framework. It supports various forms of parallelism including tensor model parallelism, pipeline model parallelism, sequence parallelism, and expert parallelism for efficient training of large language models.</p> <p>We specify a trainer of type <code>nemo.lightning.Trainer</code>, which is an extension of the pytorch lightning trainer. This is where the devices, validation intervals, maximal steps, maximal number of epochs, and how frequently to log are specified.</p> <p>We specify a nemo-logger. We can set TensorBoard and WandB logging, along with extra loggers. Here, we specify a <code>CSVLogger</code> from lightning.pytorch.loggers.</p> <p>We can now proceed to training. The first pre-training scripts is <code>bionemo/example_model/training_scripts/pretrain_mnist.py</code></p> <p>Then, we train the model with the <code>BionemoLightningModule</code>, <code>MNISTDataModule</code>, trainer and nemo_logger.</p> <p>This script will print out the location of the final model:  <p>Then we can run a finetuning-script: <pre><code>python src/bionemo/example_model/training_scripts/finetune_mnist.py ---pretrain_ckpt_dirpath &lt;pretrain_directory&gt;\n</code></pre></p> <p>A nuance here is that in the config file, we specify the initial checkpoint path, along with which keys to skip. In the previous model checkpoint, we did not have a head labelled \"digit_classifier\", so we specify it as a head to be skipped. This script will print the location of the finetuned directory: . <p>Finally, we can run a classification task with <pre><code>python src/bionemo/example_model/training_scripts/predict_mnist.py  --finetune_dir &lt;finetune_dir&gt;.\n</code></pre></p> <p>The results can be viewed with TensorBoardLogger if that is configured, or as a CSV file created by the <code>CSVLogger</code>.</p>"},{"location":"user-guide/developer-guide/bionemo-fw/bionemo-fw-Overview/","title":"bionemo-fw","text":"<p>The BioNeMo Framework (FW): a production grade framework for AI-enabled Drug Discovery.</p> <p>The <code>bionemo-fw</code> Python package contains framework-spanning code under the <code>bionemo.fw</code> namespace. All other namespaces of the BioNeMo Framework (<code>bionemo.*</code>) are dependencies of this package.</p>"},{"location":"user-guide/developer-guide/bionemo-fw/bionemo-fw-Overview/#developer-setup","title":"Developer Setup","text":"<p>After following the setup specified in the README, you may install this project's code in your environment via executing: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests with code coverage, execute: <pre><code>pytest -v --cov=bionemo --cov-report=term .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/","title":"bionemo-geneformer","text":"<p>Geneformer is a foundational single-cell RNA (scRNA) language model using a BERT architecture trained on millions of single-cell RNA sequences. It captures gene co-expression patterns to learn cellular representations, enabling predictive tasks across biology and medicine. Geneformer is trained on a masked language model (MLM) objective, where expression rank-ordered \"gene tokens\" in single-cell RNA sequences are masked, replaced, or left unchanged, and the model learns to predict these masked genes based on context. This module provides Dataset classes, collators for expression rank ordering, and Config objects for constructing Geneformer-style models.</p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#setup","title":"Setup","text":"<p>To install, execute the following from this directory (or point the install to this directory):</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#acquiring-data","title":"Acquiring Data","text":"<p>Datasets are expected to be in the form of AnnData (.h5ad) objects such as those downloaded from Cell x Gene | CZI. They are then pre-processed with <code>sub-packages/bionemo-scdl/src/bionemo/scdl/scripts/convert_h5ad_to_scdl.py</code>.</p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#geneformer-nv-10m-and-106m","title":"Geneformer-nv 10M and 106M","text":"<p>Refer to the Dataset cards and Model cards to learn more about the pre-trained checkpoints provided for both 10M and 106M of Geneformer-nv.</p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#see-also","title":"See Also","text":"<ul> <li>sc-DL pypi</li> <li>sc-DL github</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-geometric/bionemo-geometric-Overview/","title":"bionemo-geometric","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-llm/bionemo-llm-Overview/","title":"bionemo-llm","text":"<p>The Bionemo Large Language Model (LLM) submodule contains common code used in submodules that train LLMs on biological datasets (currently <code>bionemo-esm2</code> and <code>bionemo-geneformer</code>). This includes data masking and collate functions, the bio-BERT common architecture code, loss functions, and other NeMo / Megatron-LM compatibility functions. Sub-packages should only depend on <code>bionemo-llm</code> if they need access to NeMo and Megatron-LM.</p>"},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/","title":"Modular Co-Design (MoCo) Interpolants","text":""},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/#description","title":"Description","text":"<p>MoCo enables abstracted interpolants for building and sampling from a variety of popular generative model frameworks. Specifically, MoCo supports interpolants for both continuous and discrete data types.</p>"},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/#continuous-data-interpolants","title":"Continuous Data Interpolants","text":"<p>MoCo currently supports the following continuous data interpolants: - DDPM (Denoising Diffusion Probabilistic Models) - VDM (Variational Diffusion Models) - CFM (Conditional Flow Matching)</p>"},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/#discrete-data-interpolants","title":"Discrete Data Interpolants","text":"<p>MoCo also supports the following discrete data interpolants: - D3PM (Discrete Denoising Diffusion Probabilistic Models) - MDLM (Masked Diffusion Language Models) - DFM (Discrete Flow Matching)</p>"},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/#useful-abstractions","title":"Useful Abstractions","text":"<p>MoCo also provides useful wrappers for customizable time distributions and inference time schedules.</p>"},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/#extendible","title":"Extendible","text":"<p>If the desired interpolant or sampling method is not already supported, MoCo was designed to be easily extended.</p>"},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/#installation","title":"Installation","text":"<p>For Conda environment setup, please refer to the <code>environment</code> directory for specific instructions.</p> <p>Once your environment is set up, you can install this project by running the following command:</p> <p><pre><code>pip install -e .\n</code></pre> This will install the project in editable mode, allowing you to make changes and see them reflected immediately.</p>"},{"location":"user-guide/developer-guide/bionemo-moco/bionemo-moco-Overview/#examples","title":"Examples","text":"<p>Please see examples of all interpolants in the examples directory.</p>"},{"location":"user-guide/developer-guide/bionemo-noodles/bionemo-noodles-Overview/","title":"bionemo-noodles","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/","title":"BioNeMo-SCDL: Single Cell Data Loading for Scalable Training of Single Cell Foundation Models.","text":""},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#package-overview","title":"Package Overview","text":"<p>BioNeMo-SCDL provides an independent pytorch-compatible dataset class for single cell data with a consistent API. BioNeMo-SCDL is developed and maintained by NVIDIA. This package can be run independently from BioNeMo. It improves upon simple AnnData-based dataset classes in the following ways:</p> <ul> <li>A consistent API across input formats that is promised to be consistent across package versions.</li> <li>Improved performance when loading large datasets. It allows for loading and fast iteration of large datasets.</li> <li>Ability to use datasets that are much, much larger than memory. This is because the datasets are stored in a numpy memory-mapped format.</li> <li>Additionally, conversion of large (significantly larger than memory) AnnData files into the SCDL format.</li> <li>[Future] Full support for ragged arrays (i.e., datasets with different feature counts; currently only a subset of the API functionality is supported for ragged arrays).</li> <li>[Future] Support for improved compression.</li> </ul> <p>BioNeMo-SCDL's API resembles that of AnnData, so code changes are minimal. In most places a simple swap from an attribute to a function is sufficient (i.e., swapping <code>data.n_obs</code> for <code>data.number_of_rows()</code>).</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#installation","title":"Installation","text":"<p>This package can be installed with <pre><code>pip install bionemo-scdl\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#usage","title":"Usage","text":""},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#getting-example-data","title":"Getting example data","text":"<p>Here is how to process an example dataset from CellxGene with ~25,000 cells:</p> <p>Download \"https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\" to hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-a-single-cell-dataset-from-an-h5ad-file","title":"Loading a single cell dataset from an H5AD file","text":"<pre><code>from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n\ndata = SingleCellMemMapDataset(\"97e_scmm\", \"hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\")\n</code></pre> <p>This creates a <code>SingleCellMemMapDataset</code> that is stored at 97e_scmm in large, memory-mapped arrays that enables fast access of datasets larger than the available amount of RAM on a system.</p> <p>If the dataset is large, the AnnData file can be lazy-loaded and then read in based on chunks of rows in a paginated manner. This can be done by setting the parameters when instantiating the <code>SingleCellMemMapDataset</code>: - <code>paginated_load_cutoff</code>, which sets the minimal file size in megabytes at which an AnnData file will be read in in a paginated manner. - <code>load_block_row_size</code>, which is the number of rows that are read into memory at a given time.</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#interrogating-single-cell-datasets-and-exploring-the-api","title":"Interrogating single cell datasets and exploring the API","text":"<pre><code>data.number_of_rows()\n## 25382\n\ndata.number_of_variables()\n## [34455]\n\ndata.number_of_values()\n## 874536810\n\ndata.number_nonzero_values()\n## 26947275\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#saving-scdl-single-cell-dataloader-datasets-to-disk","title":"Saving SCDL (Single Cell Dataloader) datasets to disk","text":"<p>When you open a SCDL dataset, you must choose a path where the backing data structures are stored. However, these structures are not guaranteed to be in a valid serialized state during runtime.</p> <p>Calling the <code>save</code> method guarantees the on-disk object is in a valid serialized state, at which point the current python process can exit, and the object can be loaded by another process later.</p> <pre><code>data.save()\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-scdl-datasets-from-a-scdl-archive","title":"Loading SCDL datasets from a SCDL archive","text":"<p>When you're ready to reload a SCDL dataset, just pass the path to the serialized data:</p> <pre><code>reloaded_data = SingleCellMemMapDataset(\"97e_scmm\")\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#using-scdl-datasets-in-model-training","title":"Using SCDL datasets in model training","text":"<p>SCDL implements the required functions of the PyTorch Dataset abstract class. You can use PyTorch-compatible DataLoaders to load batches of data from a SCDL class. With a batch size of 1 this can be run without a collating function. With a batch size greater than 1, there is a collation function (<code>collate_sparse_matrix_batch</code>), that will collate several sparse arrays into the CSR (Compressed Sparse Row) torch tensor format.</p> <pre><code>from torch.utils.data import DataLoader\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n\n## Mock model: you can remove this and pass the batch to your own model in actual code.\nmodel = lambda x : x\n\ndataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 2\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</code></pre> <p>For some applications, we might want to also use the features. These can be specified with get_row(index, return_features = True). By default, all features are returned, but the features can be specified with the feature_vars argument in get_row, which corresponds to a list of the feature names to return.</p> <pre><code>for index in range(len(data)):\n    model(data.get_row(index,return_features = True))\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#examples","title":"Examples","text":"<p>The examples directory contains various examples for utilizing SCDL.</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#converting-existing-cell-x-gene-data-to-scdl","title":"Converting existing Cell x Gene data to SCDL","text":"<p>If there are multiple AnnData files, they can be converted into a single <code>SingleCellMemMapDataset</code>. If the hdf5 directory has one or more AnnData files, the <code>SingleCellCollection</code> class crawls the filesystem to recursively find AnnData files (with the h5ad extension).</p> <p>To convert existing AnnData files, you can either write your own script using the SCDL API or utilize the convenience script <code>convert_h5ad_to_scdl</code>.</p> <p>Here's an example:</p> <pre><code>convert_h5ad_to_scdl --data-path hdf5s --save-path example_dataset\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#runtimes-with-scdl","title":"Runtimes with SCDL","text":"<p>The runtime and memory usage are examined on a CellXGene Dataset with ~1.5 million rows and a size of 24 GB. On this dataset, there is a 4.9x memory speed up.</p> <p></p> <p>Additionally, the peak memory usage when iterating over the datasets with the SCDL dataloader is only 36.5 MB, since the whole dataset is never loaded into memory due to the numpy memomory-mapped backing.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#future-work-and-roadmap","title":"Future Work and Roadmap","text":"<p>SCDL is currently in public beta. In the future, expect improvements in data compression and data loading performance.</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#license","title":"LICENSE","text":"<p>BioNeMo-SCDL has an Apache 2.0 license, as found in the LICENSE file.</p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/","title":"bionemo-size-aware-batching","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#summary-of-usage","title":"Summary of Usage","text":"<p>This package provides a simple way to create mini-batches in a memory consumption-aware (or size-aware) manner, making it useful for tasks like training models on datasets with varying memory requirements. The usage typically consists of the following steps:</p> <ol> <li>Use the <code>collect_cuda_peak_alloc</code> function to collect CUDA peak memory    allocation statistics for a user-defined workflow. It's expected that the    user-defined workflow will return a list of features extracted from the data    so that the memory model in the following step can use these features to    predict the memory allocation.</li> <li>User defines and trains a memory model using the features and memory allocation    data from previous step. This memory model can then be used to predict memory    consumption.</li> <li>Use <code>SizeAwareBatchSampler</code> or <code>size_aware_batching</code> with the memory model    prediction (from the previous step) to build batch of data so that the    resulting mini-batches do not exceed a specified maximum total memory size.</li> </ol> <p>In addition, this package provides one solution to create homogeneous mini-batches, which can be useful to reduce the padding when aligning the shape of inputs when training or evaluating the models. This <code>BucketBatchSampler</code> can be used in conjunction with <code>torch.utils.data.BatchSampler</code>, <code>SizeAwareBatchSampler</code> or other user-defined batch samplers. This usage can leverage the <code>create_buckets</code> function and follow the steps below:</p> <ol> <li>Gather the tensor sizes of elements in the dataset, which are the shapes of    tensors in a specific dimension where you want to reduce the padding.</li> <li>Provide your own bucket boundaries based on the tensor sizes, or create bucket    boundaries with <code>create_buckets</code> function with the tensor sizes and bucket    maximal width and the minimal bucket count. The <code>create_buckets</code> function    will try to create buckets with smallest widths and element counts &gt;= minimal    bucket count, unless the maximal width or the boundary is reached.</li> <li>Use <code>BucketBatchSampler</code> with base batch sampler like <code>torch.utils.data.BatchSampler</code> or    <code>SizeAwareBatchSampler</code> for each bucket. The <code>BucketBatchSampler</code> will select a bucket each time    and generate a mini-batch from this bucket using the base batch sampler for this bucket.    As such, the padding necessary for the generated mini-batches will be always smaller    than the width of buckets.</li> </ol> <p>Refer to the later sections for the API documentation and examples on how to achieve each of the steps above.</p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#utils-module","title":"utils Module","text":"<ul> <li> <p>collect_cuda_peak_alloc: A function that     collects CUDA peak memory allocation statistics and features to be used for     memory usage prediction for a given workflow.</p> </li> <li> <p>create_buckets: A function to create buckets for a     list of integers with pre-defined maximal width of ranges and minimal     bucket sizes.</p> </li> </ul>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sampler-module","title":"sampler Module","text":"<ul> <li>size_aware_batching: A generator that batches elements from an iterable while     ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>SizeAwareBatchSampler: A class that batches elements of varying sizes while     ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>BucketBatchSampler: A class that groups elements of varying sizes based on predefined     bucket ranges, and create batches with elements from each bucket to ensure that each batch has elements with     homogeneous sizes.</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#api-reference-and-examples","title":"API reference and examples","text":""},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#utils","title":"utils","text":""},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#collect_cuda_peak_alloc","title":"collect_cuda_peak_alloc","text":"<pre><code>def collect_cuda_peak_alloc(\n    dataset: Iterable[Data],\n    work: Callable[[Data], Feature],\n    device: torch.device,\n    cleanup: Optional[Callable[[], None]] = None\n) -&gt; Tuple[List[Feature], List[int]]\n</code></pre> <p>Collects CUDA peak memory allocation statistics for a given workflow.</p> <p>This function iterates through the provided dataset, applies the given feature function to each data point, and records the peak CUDA memory allocation during this process. The features extracted from the data points are collected along with their corresponding memory usage statistics.</p> <p>Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.</p> <p>Arguments:</p> <ul> <li><code>dataset</code> - An iterable containing the input data.</li> <li><code>work</code> - A function that takes a data point and returns its corresponding feature. This is where     the main computation happens and memory allocations are tracked.</li> <li><code>device</code> - The target Torch CUDA device.</li> <li><code>cleanup</code> - A function that is called after each iteration to perform any necessary cleanup.</li> </ul> <p>Returns:</p> <p>A tuple containing the collected features and their corresponding memory usage statistics.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If the provided device is not a CUDA device.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n&gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n&gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n&gt;&gt;&gt; dataset, model, optimizer = ...\n&gt;&gt;&gt; # Set the target Torch CUDA device.\n&gt;&gt;&gt; device = torch.device(\"cuda:0\")\n&gt;&gt;&gt; model = model.to(device)\n\n&gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n&gt;&gt;&gt; # do a training step\n&gt;&gt;&gt; def work(data):\n...     # example body of a training loop\n...     optimizer.zero_grad()\n...     output = model(data.to(device))\n...     loss = compute_loss(output)\n...     loss.backward()\n...     optimizer.step()\n...     # extract the feature for later to be modeled or analyzed\n...     return featurize(data)\n\n&gt;&gt;&gt; # can optionally use a cleanup function to release the references\n&gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n&gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n&gt;&gt;&gt; def cleanup():\n...     model.zero_grad(set_to_none=True)\n\n&gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n&gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n...     dataset=batches,\n...     work=work,\n...     device=device,\n...     cleanup=cleanup,\n... )\n\n\n&gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n&gt;&gt;&gt; # that can use these statistics to predict memory usage\n&gt;&gt;&gt; memory_model = ...\n&gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#create_buckets","title":"create_buckets","text":"<pre><code>def create_buckets(sizes: torch.Tensor, max_width: int,\n                   min_bucket_count: int) -&gt; Buckets\n</code></pre> <p>Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.</p> <p>It will return a named tuple containing the bucket boundaries and the actual bucket sizes. e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.</p> <p>Arguments:</p> <ul> <li><code>sizes</code> - An 1D tensor of integers.</li> <li><code>max_width</code> - The maximum width of a bucket, should be a positive integer.</li> <li><code>min_bucket_count</code> - The minimum count of a bucket, should be a positive integer.   Bucket size may be smaller than min_bucket_count if its width reaches max_width.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code> - If the provided sizes is empty, or not integers.</li> <li><code>ValueError</code> - If max_width is not a positive integer or min_bucket_count is not a positive integer.</li> </ul> <p>Returns:</p> <p>A named tuple containing bucket boundaries in ascending order and the number of elements in each bucket.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n&gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n&gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 1,  6, 11, 16, 21, 23])\n\n&gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([12,  0,  0,  0,  4])\n\n&gt;&gt;&gt; sizes = torch.arange(20)\n&gt;&gt;&gt; # min_bucket_count is used to control bucket size\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 0,  5, 10, 15, 20])\n\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([5, 5, 5, 5])\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sampler","title":"sampler","text":""},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#size_aware_batching","title":"size_aware_batching","text":"<pre><code>def size_aware_batching(\n    dataset: Iterable[Data],\n    sizeof: Callable[[Data], Real],\n    max_total_size: Real,\n    collate_fn: Optional[Callable[[Iterable[Data]], BatchCollated]] = None,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None\n) -&gt; Iterator[Union[List[Data], BatchCollated]]\n</code></pre> <p>A generator that batches elements from an iterable while ensuring that the total size of each batch does not exceed a specified maximum. Here the size can be a measurement of memory consumption of the elements in the batch. This can be useful for both indexible data or non-indexible but iterable data.</p> <p>Arguments:</p> <ul> <li><code>dataset</code> - The input iterable.</li> <li><code>sizeof</code> - A function or mapping that returns the \"size\" of each element in <code>dataset</code>.   E.g., this can be used to determine how much memory an element consumes. Its return   type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</li> <li><code>max_total_size</code> - The maximum total \"size\" of each batch. The semantics of \"size\"   is defined by the <code>sizeof</code> argument. The type of this value must be comparable   with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</li> <li><code>collate_fn</code> - An optional function to collate batches. Defaults to None, in which case   each batch is a list of elements from the input dataset</li> <li><code>info_logger</code> - A function to log info. Defaults to None.</li> <li><code>warn_logger</code> - A function to log warnings. Defaults to None.</li> </ul> <p>Yields:</p> <p>A generator that yields batches from <code>dataset</code>.</p> <p>Assumptions   1. Linear complexity. This function consumes the given Iterable of data (<code>dataset</code>) once,   by going over the data item one by one to build a batch and yield it as soon as the   addition of the next data item to the batch would exceed <code>max_total_size</code> or if the   batch is the last one (end of iteration)   2. Additive size measurement. For the general usage case of building mini-batches with   a threshold of the batch's memory consumption, it assumes that the size of the batch is   the sum of all elements in the batch (additive property).   3. Comparable type of <code>max_total_size</code> and <code>sizeof</code>'s return. <code>sizeof</code>'s return values   must be compared with <code>max_total_size</code> to threshold the size of batches</p> <p>Caveat - <code>1</code> - The generated batch sizes may have large variance   - how to workaround: filter the output of this generator using a batch size threshold - <code>2</code> - The number of batches may vary a lot across different epochs.   - how to workaround: increase the number of steps that compose an epoch,   e.g., in the Lightning training/validation loop, which effectively increases the input   dataset size per epoch</p> <p>Example</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import default_collate\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n&gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n&gt;&gt;&gt; def sizeof(x):\n...     return x.numel()\n\n&gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n&gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n&gt;&gt;&gt; batches = list(gen)\n&gt;&gt;&gt; print(batches)\n    [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sizeawarebatchsampler-objects","title":"SizeAwareBatchSampler Objects","text":"<pre><code>class SizeAwareBatchSampler(Sampler[List[int]])\n</code></pre> <p>A sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</p> <p>This is useful when dealing with datasets where each element has a different size, such as graphs or sequences of varying lengths. The sampler uses a provided <code>sizeof</code> function to determine the size of each element in the dataset and ensures that the total size of each batch does not exceed the specified <code>max_total_size</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n&gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n&gt;&gt;&gt; def sizeof(index):\n...     return dataset[index].numel()\n\n\n&gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n&gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n...     sampler=torch.utils.data.SequentialSampler(dataset),\n...     sizeof=sizeof,\n...     max_total_size=4\n... )\n\n\n&gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n&gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4]]\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__init__","title":"__init__","text":"<pre><code>def __init__(sampler: Union[Sampler[List[int]], Iterable[int]],\n             sizeof: Callable[[int], Real],\n             max_total_size: Real,\n             info_logger: Optional[Callable[[str], None]] = None,\n             warn_logger: Optional[Callable[[str], None]] = None) -&gt; None\n</code></pre> <p>Initializes the SizeAwareBatchSampler.</p> <p>Arguments:</p> <ul> <li><code>sampler</code> - The underlying sampler.</li> <li><code>sizeof</code> - A function that returns the size at each index. E.g., this can used to   determine how much memory an element consumes. Its return type must be   comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</li> <li><code>max_total_size</code> - The maximum total size of a mini-batch. The semantics of \"size\"   is defined by the <code>sizeof</code> argument. The type of this value must be comparable   with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</li> <li><code>info_logger</code> - A function to log info. Defaults to None.</li> <li><code>warn_logger</code> - A function to log warnings. Defaults None.</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code> - If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.</li> <li><code>ValueError</code> - If max_total_size is not a positive number.</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__iter__","title":"__iter__","text":"<pre><code>def __iter__() -&gt; Iterator[List[int]]\n</code></pre> <p>Iterate over batches of indices.</p> <p>This function yields batches of indices that do not exceed the maximum total size.</p> <p>Yields:</p> <p>A batch of indices that do not exceed the maximum total size.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#bucketbatchsampler-objects","title":"BucketBatchSampler Objects","text":"<pre><code>class BucketBatchSampler(Sampler[List[int]])\n</code></pre> <p>A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.</p> <p>Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements. Then, a base batch sampler is used for each bucket to create mini-batches.</p> <p>The bucket ranges are specified by <code>bucket_boundaries</code>, which will be first sorted internally and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals. e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created with ranges: [0, 5), [5, 10), [10, 16).</p> <p>The base batch sampler will be created by passing the element indices in each bucket as the data source, and <code>base_batch_sampler_shared_kwargs</code> and <code>base_batch_sampler_individual_kwargs</code> to the constructor of the base batch sampler class specified as <code>base_batch_sampler_class</code>. e.g. <code>base_batch_sampler_shared_kwargs = {'drop_last': True}</code> and <code>base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}</code> will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like <code>base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)</code>.</p> <p>In the <code>__iter__</code> method, if <code>shuffle</code> is <code>True</code>, the element indices in each bucket will be shuffled, and a bucket is randomly selected each time to create a mini-batch. If <code>shuffle</code> is <code>False</code>, there is no shuffle on element indices, and the bucket is selected in ascending order of its interval boundaries.</p> <p>This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.</p> <p>Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n&gt;&gt;&gt; # Define the sizes for a dataset\n&gt;&gt;&gt; sizes = torch.arange(25)\n&gt;&gt;&gt; # Define bucket ranges\n&gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n&gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n&gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=False,\n    )\n\n&gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n&gt;&gt;&gt; print(list(batch_sampler))\n[[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n&gt;&gt;&gt; # randomize the dataset and buckets\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(batch_sampler))\n[[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n&gt;&gt;&gt; print(list(batch_sampler))\n[[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n&gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n&gt;&gt;&gt; item_costs = sizes.tolist()\n&gt;&gt;&gt; def cost_of_element(index):\n        return item_costs[index]\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=SizeAwareBatchSampler,\n        base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n        base_batch_sampler_individual_kwargs={},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(iter(batch_sampler)))\n[[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__init___1","title":"__init__","text":"<pre><code>def __init__(sizes: torch.Tensor,\n             bucket_boundaries: torch.Tensor,\n             base_batch_sampler_class: Type[Sampler],\n             base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n             base_batch_sampler_individual_kwargs: Optional[Dict[\n                 str, Iterable]] = None,\n             shuffle: Optional[bool] = True,\n             generator: Optional[torch.Generator] = None) -&gt; None\n</code></pre> <p>Initializes the BucketBatchSampler.</p> <p>Arguments:</p> <ul> <li><code>sizes</code> - A 1D tensor of real numbers representing the size of each element in the dataset.</li> <li><code>bucket_boundaries</code> - A 1D tensor of real numbers representing the boundaries of the bucket ranges.   It will be first sorted and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals as bucket ranges.   It should not contain any duplicate values.</li> <li><code>base_batch_sampler_class</code> - Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,   <code>base_batch_sampler_shared_kwargs</code> and the corresponding <code>base_batch_sampler_individual_kwargs</code>.</li> <li><code>base_batch_sampler_shared_kwargs</code> - Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.   Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_individual_kwargs</code>. Default to  {}.</li> <li><code>base_batch_sampler_individual_kwargs</code> - Keyword argument dictionary used to initialize   each bucket batch sampler with the corresponding key value pairs.   Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).   Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_shared_kwargs</code>.   Default to  {}.</li> <li><code>shuffle</code> - A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.</li> <li><code>generator</code> - Generator used in sampling. Defaults to None.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code> - If <code>sizes</code> is not a 1D tensor of real numbers.</li> <li><code>ValueError</code> - If <code>bucket_boundaries</code> is not a 1D tensor of real numbers.</li> <li><code>ValueError</code> - If <code>base_batch_sampler_individual_kwargs</code> or <code>base_batch_sampler_individual_kwargs</code> is not a keyword argument dictionary.</li> <li><code>ValueError</code> - If the length of values in the dict of <code>base_batch_sampler_individual_kwargs</code> must be equal to len(bucket_boundaries) - 1.</li> <li><code>RuntimeError</code> - If there is no elements with sizes inside the ranges specified by <code>bucket_boundaries</code>.</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__len__","title":"__len__","text":"<pre><code>def __len__() -&gt; int\n</code></pre> <p>Get the number of batches.</p> <p>Can only be called if the <code>base_batch_sampler_class</code> has len() implemented</p> <p>Returns:</p> <ul> <li><code>int</code> - Number of batches</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__iter___1","title":"__iter__","text":"<pre><code>def __iter__() -&gt; Iterator[List[int]]\n</code></pre> <p>Iterate over batches of indices.</p> <p>This function yields batches of indices of elements with sizes from each bucket range.</p> <p>Yields:</p> <ul> <li><code>List[int]</code> - A batch of indices of elements with sizes from each bucket range.</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-testing/bionemo-testing-Overview/","title":"bionemo-testing","text":"<p>A package of test-time requirements and utilities for bionemo sub-packages. In particular, the <code>bionemo-testing</code> package handles downloading and caching data and other assets for running unit tests and example notebooks. For more information on test data handling, see BioNeMo test data management</p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/","title":"bionemo-webdatamodule","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#webdatamodule","title":"WebDataModule","text":"<pre><code>class WebDataModule(L.LightningDataModule)\n</code></pre> <p>A LightningDataModule for using webdataset tar files.</p> <p><code>WebDataModule</code> is a <code>LightningDataModule</code> for using webdataset tar files to setup PyTorch datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file directory and vaiours webdataset config settings. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p> <p>Examples:</p> <ol> <li> <p>create the data module with input directory to webdataset tar files.   Depending on which of the downstream Lightning.Trainer methods are called,   e.g., <code>Trainer.fit()</code>, <code>Trainer.validate()</code>, <code>Trainer.test()</code> or   <code>Trainer.predict()</code>, only a subset of the train, val and test splits need to   be specified in the various input options to the data module:</p> </li> <li> <p><code>Trainer.fit()</code> requires the <code>train</code> and <code>val</code> splits</p> </li> <li><code>Trainer.validate()</code> requires the <code>val</code> split</li> <li><code>Trainer.test()</code> requires the <code>test</code> splits</li> <li><code>Trainer.predict()</code> requires the <code>test</code> splits</li> </ol> <p>Here is an example of constructing the data module for <code>Trainer.fit()</code>: <pre><code>&gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; tar_file_prefix = \"shards\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; dirs_of_tar_files = {\n&gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n&gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; n_samples {\n&gt;&gt;&gt;     Split.train: 1000,\n&gt;&gt;&gt;     Split.val: 100,\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n&gt;&gt;&gt; # webdataset file (see\n&gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n&gt;&gt;&gt; # for details)\n&gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; seed = 27193781\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n&gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n&gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n&gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n&gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n&gt;&gt;&gt; # for details.\n&gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n&gt;&gt;&gt; # file parsing rule.\n&gt;&gt;&gt;\n&gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n&gt;&gt;&gt;\n&gt;&gt;&gt; from webdatast import shuffle\n&gt;&gt;&gt; pipeline_wds = {\n&gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n&gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n&gt;&gt;&gt;     Split.val: untuple\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n&gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n&gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n&gt;&gt;&gt; # user can customize their batching routines here\n&gt;&gt;&gt;\n&gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                    list_samples : torch.vstack(list_samples))\n&gt;&gt;&gt; pipeline_prebatch_wld = {\n        Split.train: [shuffle(n_samples[Split.train],\n                              rng=random.Random(seed_rng_shfl)), batch],\n        Split.val : batch,\n        Split.test : batch\n    }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n&gt;&gt;&gt; # WebLoader\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wds = {\n&gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n&gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n&gt;&gt;&gt;              'seed' : seed_rng_shfl}\n&gt;&gt;&gt;     for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wld = {\n&gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wds = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5})] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; invoke_wld = {\n&gt;&gt;&gt;     split: [(\"with_epoch\", {\"nbatches\" : 5}] for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # construct the data module\n&gt;&gt;&gt; data_module = WebDataModule(suffix_keys_wds,\n                                dirs_of_tar_files,\n                                prefix_tars_wds=tar_file_prefix,\n                                pipeline_wds=pipeline_wds,\n                                pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                kwargs_wds=kwargs_wds,\n                                kwargs_wld=kwargs_wld,\n                                invoke_wds=invoke_wds,\n                                invoke_wld=invoke_wld,\n                                )\n</code></pre></p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#__init__","title":"__init__","text":"<pre><code>def __init__(\n    suffix_keys_wds: Union[str, Iterable[str]],\n    dirs_tars_wds: Dict[Split, str],\n    prefix_tars_wds: str = \"wdshards\",\n    pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]],\n                                             Iterable[Any]]]] = None,\n    pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]],\n                                                      Iterable[Any]]]] = None,\n    kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n    kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n    invoke_wds: Optional[Dict[Split, List[Tuple[str, Dict[str, Any]]]]] = None,\n    invoke_wld: Optional[Dict[Split, List[Tuple[str, Dict[str,\n                                                          Any]]]]] = None)\n</code></pre> <p>Constructor.</p> <p>Arguments:</p> <ul> <li><code>suffix_keys_wds</code> - a set of keys each   corresponding to a data object in the webdataset tar file   dictionary. The data objects of these keys will be extracted and   tupled for each sample in the tar files</li> <li><code>dirs_tars_wds</code> - input dictionary: Split -&gt; tar file   directory that contains the webdataset tar files for each split   Kwargs:</li> <li><code>prefix_tars_wds</code> - name prefix of the input webdataset tar   files. The input tar files are globbed by   \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"</li> <li><code>pipeline_wds</code> - a dictionary of webdatast composable, i.e.,   functor that maps a iterator to another iterator that   transforms the data sample yield from the dataset object, for   different splits, or an iterable to such a sequence of such   iterators. For example, this can be used to transform the   sample in the worker before sending it to the main process of   the dataloader</li> <li><code>pipeline_prebatch_wld</code> - a dictionary   of webloader composable, i.e., functor that maps a iterator to   another iterator that transforms the data sample yield from the   WebLoader object, for different splits, or an iterable to a   seuqnence of such iterators. For example, this can be used for   batching the samples. NOTE: this is applied before batching is   yield from the WebLoader</li> <li><code>kwargs_wds</code> - kwargs for the WebDataset.init()   kwargs_wld : kwargs for the WebLoader.init(), e.g., num_workers, of each split</li> <li><code>invoke_wds</code> - a dictionary of WebDataset methods to be called upon WebDataset   construction. These methods must return the WebDataset object itself. Examples   are .with_length() and .with_epoch(). These methods will be applied towards   the end of returning the WebDataset object, i.e., after the pipline_wds   have been applied. The inner list of tuples each has its first element as the   method name and the second element as the corresponding method's kwargs.</li> <li><code>invoke_wld</code> - a dictionary of WebLoader methods to be called upon WebLoader   construction. These methods must return the WebLoader object itself. Examples   are .with_length() and .with_epoch(). These methods will be applied towards   the end of returning the WebLoader object, i.e., after the pipelin_prebatch_wld   have been applied. The inner list of tuples each has its first element as the   method name and the second element as the corresponding method's kwargs.</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#prepare_data","title":"prepare_data","text":"<pre><code>def prepare_data() -&gt; None\n</code></pre> <p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. Is a no-op.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#setup","title":"setup","text":"<pre><code>def setup(stage: str) -&gt; None\n</code></pre> <p>This is called on all Lightning-managed nodes in a multi-node training session.</p> <p>Arguments:</p> <ul> <li><code>stage</code> - \"fit\", \"test\" or \"predict\"</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#train_dataloader","title":"train_dataloader","text":"<pre><code>def train_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Webdataset for the training data.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#val_dataloader","title":"val_dataloader","text":"<pre><code>def val_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Webdataset for the validation data.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#test_dataloader","title":"test_dataloader","text":"<pre><code>def test_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Webdataset for the test data.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#predict_dataloader","title":"predict_dataloader","text":"<pre><code>def predict_dataloader() -&gt; wds.WebLoader\n</code></pre> <p>Alias for :func:<code>test_dataloader</code>.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#pickleddatawds-objects","title":"PickledDataWDS Objects","text":"<pre><code>class PickledDataWDS(WebDataModule)\n</code></pre> <p>A LightningDataModule to process pickled data into webdataset tar files.</p> <p><code>PickledDataWDS</code> is a LightningDataModule to process pickled data into webdataset tar files and setup dataset and dataloader. This inherits the webdataset setup from its parent module <code>WebDataModule</code>. This data module takes a directory of pickled data files, data filename prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files by globbing the specific pickle data files <code>{dir_pickles}/{name_subset[split]}.{suffix_pickles}</code> and outputing to webdataset tar file with the dict structure: NOTE: this assumes only one pickled file is processed for each sample. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p> <pre><code>    {\"__key__\" : name.replace(\".\", \"-\"),\n     suffix_pickles : pickled.dumps(data) }\n</code></pre> <p>Examples:</p> <ol> <li>create the data module with a directory of pickle files and the file name   prefix thereof for different splits to used by <code>Lightning.Trainer.fit()</code></li> </ol> <pre><code>&gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n&gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n&gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n&gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n&gt;&gt;&gt; # validation dataset\n\n&gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n&gt;&gt;&gt; names_subset = {\n&gt;&gt;&gt;     Split.train: [sample1, sample2],\n&gt;&gt;&gt;     Split.val: [sample4, sample5],\n&gt;&gt;&gt; }\n\n&gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n&gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n&gt;&gt;&gt; n_tars_wds = 5\n&gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n&gt;&gt;&gt; output_dir_tar_files = {\n        Split.train : \"/path/to/output/tars/dir-train\",\n        Split.val : \"/path/to/output/tars/dir-val\",\n        Split.test : \"/path/to/output/tars/dir-test\",\n    }\n\n&gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n&gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n&gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n&gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n&gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; invoke_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; # create the data module\n&gt;&gt;&gt; data_module = PickledDataWDS(\n&gt;&gt;&gt;     dir_pickles,\n&gt;&gt;&gt;     names_subset,\n&gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n&gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n&gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n&gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wds=invoke_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     invoke_wld=invoke_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt; )\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#__init___1","title":"__init__","text":"<pre><code>def __init__(dir_pickles: str,\n             names_subset: Dict[Split, List[str]],\n             *args,\n             n_tars_wds: Optional[int] = None,\n             **kwargs) -&gt; None\n</code></pre> <p>Constructor.</p> <p>Arguments:</p> <ul> <li><code>dir_pickles</code> - input directory of pickled data files</li> <li><code>names_subset</code> - list of filename prefix of   the data samples to be loaded in the dataset and dataloader for   each of the split</li> <li><code>*args</code> - arguments passed to the parent WebDataModule</li> <li><code>n_tars_wds</code> - attempt to create at least this number of   webdataset shards</li> <li><code>**kwargs</code> - arguments passed to the parent WebDataModule</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#prepare_data_1","title":"prepare_data","text":"<pre><code>def prepare_data() -&gt; None\n</code></pre> <p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. The nesting <code>pickles_to_tars</code> function goes through the data name prefixes in the different splits, read the corresponding pickled file and output a webdataset tar archive with the dict structure: {\"key\" : name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.</p>"},{"location":"user-guide/examples/conftest/","title":"Conftest","text":"<p>SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. SPDX-License-Identifier: LicenseRef-Apache2</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> In\u00a0[\u00a0]: Copied! <pre>def pytest_collectstart(collector):\n    if collector.fspath and collector.fspath.ext == \".ipynb\":\n        collector.skip_compare += (\n            \"text/html\",\n            \"application/javascript\",\n            \"stderr\",\n        )\n</pre> def pytest_collectstart(collector):     if collector.fspath and collector.fspath.ext == \".ipynb\":         collector.skip_compare += (             \"text/html\",             \"application/javascript\",             \"stderr\",         )"},{"location":"user-guide/examples/bionemo-esm2/finetune/","title":"ESM-2 Fine-tuning","text":"NOTE It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits.  <p>The ESM-2 model is a transformer-based protein language model that has achieved state-of-the-art results in various protein-related tasks. When fine-tuning ESM2, the task-head plays a crucial role. A task head refers to the additional layer or set of layers added on top of a pre-trained model, like the ESM-2 transformer-based protein language model, to adapt it for a specific downstream task. As a part of transfer learning, a pre-trained model is often utilized to learn generic features from a large-scale dataset. However, these features might not be directly applicable to the specific task at hand. By incorporating a task head, which consists of learnable parameters, the model can adapt and specialize to the target task. The task head serves as a flexible and adaptable component that learns task-specific representations by leveraging the pre-trained features as a foundation. Through fine-tuning, the task head enables the model to learn and extract task-specific patterns, improving performance and addressing the nuances of the downstream task. It acts as a critical bridge between the pre-trained model and the specific task, enabling efficient and effective transfer of knowledge.</p> NOTE We divided the fine-tuning use cases to *sequence-level* and *token-level* tasks where a target value is expected per each protein sequence and each token respectively. The first part of this tutorial will guide you through the steps for creating a sequence-level regression fine-tuning task for simplicity. The techniques demonstrated here can be adapted for sequence-level classification and token-level tasks.  <p>The utilities described in this tutorial are available in:</p> <pre>bionemo.esm2.model.finetune</pre> <p>In the second part of the tutorial, we will cover loading a pre-trained model, fine-tuning it sequence-level regression/classification and token-level classification, and using the fine-tuned models for inference. For instructions on pre-training the ESM-2 model, please refer to the ESM-2 Pretraining tutorial.</p> <p>We need to define some key classes to successfully build a fine-tuning module in BioNeMo framework:</p> <ol> <li>Loss Reduction Class - To compute the supervised fine-tuning loss.</li> <li>Fine-Tuned Model Head - Downstream task head model.</li> <li>Fine-Tuned Model - Model that combines ESM-2 with the task head model.</li> <li>Fine-Tuning Config - Configures the fine-tuning model and loss to use in the training and inference framework.</li> <li>Dataset - Training and inference datasets for ESM2 fine-tuning.</li> </ol> <p>A class for calculating the supervised loss of the fine-tune model from targets. We inherit from Megatron Bert Masked Language Model Loss (<code>BERTMLMLossWithReduction</code>) and override the <code>forward()</code> pass to compute MSE loss of the regression head within a micro-batch. The <code>reduce()</code> method is used for computing the average over the micro-batches and is only used for logging.</p> <pre>class RegressorLossReduction(BERTMLMLossWithReduction):\n    def forward(\n        self, batch: Dict[str, torch.Tensor], forward_out: Dict[str, torch.Tensor]\n    ) -&gt; Tuple[torch.Tensor, Union[PerTokenLossDict, SameSizeLossDict]]:\n\n        regression_output = forward_out[\"regression_output\"]\n        targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n\n        loss = torch.nn.functional.mse_loss(regression_output, targets)\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[ReductionT]) -&gt; torch.Tensor:\n        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return losses.mean()\n</pre> <p>An MLP class for sequence-level regression. This class inherits <code>MegatronModule</code> and uses the fine-tune config (<code>TransformerConfig</code>) to configure the regression head for the fine-tuned ESM-2 model.</p> <pre>class MegatronMLPHead(MegatronModule):\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        layer_sizes = [config.hidden_size, 256, 1]\n        self.linear_layers = torch.nn.ModuleList(\n            [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]\n        )\n        self.act = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=config.ft_dropout)\n\n    def forward(self, hidden_states: torch.Tensor) -&gt; List[torch.Tensor]:\n        ...\n</pre> <p>A fine-tuned ESM-2 model class for token classification tasks. This class inherits from the <code>ESM2Model</code> class and adds the custom regression head <code>MegatronMLPHead</code> the we created in the previous step. Optionally one can freeze all or parts of the encoder by parsing through the model parameters in the model constructor.</p> <pre>class ESM2FineTuneSeqModel(ESM2Model):\n    def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n        super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n\n        # freeze encoder parameters\n        if config.encoder_frozen:\n            for _, param in self.named_parameters():\n                param.requires_grad = False\n\n        if post_process:\n            self.regression_head = MegatronMLPHead(config)\n\n    def forward(self, *args, **kwargs,):\n        output = super().forward(*args, **kwargs)\n        ...\n        output[\"regression_output\"] = self.regression_head(embeddings)\n        return output\n</pre> <p>A <code>dataclass</code> that configures the fine-tuned ESM-2 model. In this example <code>ESM2FineTuneSeqConfig</code> inherits from <code>ESM2GenericConfig</code> and adds custom arguments to setup the fine-tuned model. The <code>configure_model()</code> method of this <code>dataclass</code> is called within the <code>Lightning</code> module to call the model constructor with the <code>dataclass</code> arguments.</p> <p>The common arguments among different fine-tuning tasks are</p> <ul> <li><code>model_cls</code>: The fine-tune model class defined in previous step (<code>ESM2FineTuneSeqModel</code>)</li> <li><code>initial_ckpt_path</code>: BioNeMo 2.0 ESM-2 pre-trained checkpoint</li> <li><code>initial_ckpt_skip_keys_with_these_prefixes</code>: skips keys when loading parameters from a checkpoint. For example, we should not look for <code>regression_head</code> in the pre-trained checkpoint.</li> <li><code>get_loss_reduction_class()</code>: Implements selection of the appropriate <code>MegatronLossReduction</code> class that we defined in the first step of this tutorial.</li> </ul> <pre>@dataclass\nclass ESM2FineTuneSeqConfig(\n    ESM2GenericConfig[ESM2FineTuneSeqModel, RegressorLossReduction], iom.IOMixinWithGettersSetters\n):\n    model_cls: Type[ESM2FineTuneSeqModel] = ESM2FineTuneSeqModel\n    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n    # self.override_parent_fields will be loaded from the checkpoint and override those values here.\n    initial_ckpt_path: str | None = None\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    encoder_frozen: bool = True  # freeze encoder parameters\n    ft_dropout: float = 0.25  # MLP layer dropout\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n        return RegressorLossReduction\n</pre> <p>We will use a sample dataset for demonstration purposes. Create a dataset class by extending <code>bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset</code>. The <code>InMemoryProteinDataset</code> has a <code>classmethod</code> (<code>from_csv</code>) that reads data from a CSV file that has <code>sequences</code> and optionally <code>labels</code> columns. It is important to override the <code>transform_label()</code> method that returns a <code>torch.Tensor</code> containing the label in correct format. As an example we can use this method to add custom tokenization if <code>label</code> is a string.</p> <p>The custom dataset class will be appropriate (found in <code>bionemo.esm2.model.finetune.dataset.InMemorySingleValueDataset</code>) as it facilitates predicting on a single value. An excerpt from the class is shown below. This example dataset has a class method <code>from_csv()</code> that expects a <code>data_path</code> to a CSV file that has <code>sequences</code>, and <code>labels</code> columns.</p> <pre>class InMemorySingleValueDataset(InMemoryProteinDataset):\n    def __init__(\n        self,\n        labels: pd.Series,\n        task_type: str = \"regression\",\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,\n    ):\n        super().__init__(sequences, labels, task_type, tokenizer, seed)\n\n    def transform_label(self, label: float) -&gt; Tensor:\n        return torch.tensor([label], dtype=torch.float)\n</pre> <p>The <code>transform_label</code> method allows for custom transformation of raw labels by casting or tokenization and need to be adjusted based on the data. Here we use this method to create a <code>float</code> tensor of the regression value.</p> <p>To coordinate the creation of training, validation and testing datasets from your data, we need to use a <code>datamodule</code> class. To do this we can directly use or extend the <code>ESM2FineTuneDataModule</code> class (located at <code>bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule</code>) which defines helpful abstract methods that use your dataset class.</p> <pre>dataset = InMemorySingleValueDataset.from_csv(data_path)\ndata_module = ESM2FineTuneDataModule(\n    train_dataset=dataset,\n    valid_dataset=dataset\n    micro_batch_size=4,   # size of a batch to be processed in a device\n    global_batch_size=8,  # size of batch across all devices. Should be multiple of micro_batch_size\n)\n</pre> <p>In the next part of this tutorial we will prepare the input needed to run sequence-level regression/classification and token-level classification fine-tuning examples.</p> <p>All commands should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. For more information on how to build or pull the BioNeMo2 container, refer to the Initialization Guide.</p> NOTE Some of the cells below generate long text output. We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output. Comment or delete this line in the cells below to restore full output. In\u00a0[1]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\nimport os\nimport shutil\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n</pre> %%capture --no-display --no-stderr cell_output  import os import shutil import pandas as pd  import warnings warnings.filterwarnings('ignore') warnings.simplefilter('ignore') <p>Set the work directory to store data and results:</p> NOTE We set the following to clean up the work directory created by this notebook  <pre>cleanup : bool = True</pre> In\u00a0[2]: Copied! <pre>cleanup : bool = True\n</pre> cleanup : bool = True In\u00a0[3]: Copied! <pre>work_dir=\"/workspace/bionemo2/esm2_finetune_tutorial\"\n\nif cleanup and os.path.exists(work_dir):\n    shutil.rmtree(work_dir)\n\nif not os.path.exists(work_dir):\n    os.makedirs(work_dir)\n    print(f\"Directory '{work_dir}' created.\")\nelse:\n    print(f\"Directory '{work_dir}' already exists.\")\n</pre> work_dir=\"/workspace/bionemo2/esm2_finetune_tutorial\"  if cleanup and os.path.exists(work_dir):     shutil.rmtree(work_dir)  if not os.path.exists(work_dir):     os.makedirs(work_dir)     print(f\"Directory '{work_dir}' created.\") else:     print(f\"Directory '{work_dir}' already exists.\") <pre>Directory '/workspace/bionemo2/esm2_finetune_tutorial' created.\n</pre> <p>The following code will download the internally pre-trained model, <code>esm2/8m:2.0</code>, from the NGC registry. Please refer to ESM-2 Model Overview for a list of available checkpoints.</p> In\u00a0[4]: Copied! <pre>from bionemo.core.data.load import load\n\npretrain_checkpoint_path = load(\"esm2/8m:2.0\")\nprint(pretrain_checkpoint_path)\n</pre> from bionemo.core.data.load import load  pretrain_checkpoint_path = load(\"esm2/8m:2.0\") print(pretrain_checkpoint_path) <pre>/home/ubuntu/.cache/bionemo/2957b2c36d5978d0f595d6f1b72104b312621cf0329209086537b613c1c96d16-esm2_hf_converted_8m_checkpoint.tar.gz.untar\n</pre> <p>The above example is downloading an internally trained 8M ESM-2 model. The pre-trained checkpoints can be downloaded from NGC resources using either the following bash command or the <code>load</code> function in <code>bionemo.core.data.load</code> as shown above.</p> <pre>download_bionemo_data esm2/650m:2.0\n</pre> <p>which returns the checkpoint path (e.g. <code>.../.cache/bionemo/975d29ee980fcb08c97401bbdfdcf8ce-esm2_650M_nemo2.tar.gz.untar</code>)</p> <p>We can take advantage of the ESM2 fine-tuning script in <code>bionemo.esm2.scripts.finetune_esm2</code> or use the <code>finetune_esm2</code> executable the fine-tuning process given:</p> <ul> <li>Pre-trained checkpoint of ESM2</li> <li>Finetune config class name that configures the finetune model and loss reduction</li> <li>Path to train and validation CSV data files</li> <li>Dataset class name</li> </ul> <p>To get the full list of arguments to tune a finetuning run use:</p> <pre>finetune_esm2 --help \n</pre> <p>For a detailed description of training loop and the arguments please refer to the ESM-2 Pretraining tutorial.</p> NOTE <p>Due to Megatron limitations, the log produced by the training run iterates on steps/iterations and not epochs. Therefore, <code>Training epoch</code> counter stays at value zero while <code>iteration</code> and <code>global_step</code> increase during the course of training (example in the following).</p> <pre>\nTraining epoch 0, iteration  | ... | global_step:  | reduced_train_loss: ... | val_loss: ...\n</pre> <p>to achieve the same epoch-based effect while training, please choose the number of training steps (<code>num_steps</code>) so that:</p> <pre>\nnum_steps * global_batch_size = len(dataset) * desired_num_epochs\n</pre> <p>For the purposes of this demo, we'll assume dataset consists of small set of protein sequences with a target value of <code>len(sequence) / 100.0</code> as their labels.</p> In\u00a0[5]: Copied! <pre>import pandas as pd\n\nartificial_sequence_data = [\n    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n]\n\ndata = [(seq, len(seq)/100.0) for seq in artificial_sequence_data]\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"regression_data.csv\")\ndf.to_csv(data_path, index=False)\n</pre> import pandas as pd  artificial_sequence_data = [     \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",     \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",     \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",     \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",     \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",     \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\", ]  data = [(seq, len(seq)/100.0) for seq in artificial_sequence_data]  # Create a DataFrame df = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])  # Save the DataFrame to a CSV file data_path = os.path.join(work_dir, \"regression_data.csv\") df.to_csv(data_path, index=False) <p>We will use the sequence-level fine-tune model config <code>ESM2FineTuneSeqConfig</code> and single-value dataset <code>InMemorySingleValueDataset</code> and set the task-type to <code>regression</code>. In addition to model and dataset configuration we can define the MLP task head and specify the number of hidden parameters (<code>mlp-hidden-size</code>), output layer size (<code>mlp-target-size</code>) and dropout (<code>mlp-ft-dropout</code>) in the following CLI call.</p> In\u00a0[6]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\n! finetune_esm2 \\\n    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n    --train-data-path {data_path} \\\n    --valid-data-path {data_path} \\\n    --config-class ESM2FineTuneSeqConfig \\\n    --dataset-class InMemorySingleValueDataset \\\n    --task-type \"regression\" \\\n    --mlp-ft-dropout 0.25 \\\n    --mlp-hidden-size 256 \\\n    --mlp-target-size 1 \\\n    --experiment-name \"sequence-level-regression\" \\\n    --num-steps 50 \\\n    --num-gpus 1 \\\n    --val-check-interval 10 \\\n    --log-every-n-steps 10 \\\n    --encoder-frozen \\\n    --lr 5e-3 \\\n    --lr-multiplier 1e2 \\\n    --scale-lr-layer \"regression_head\" \\\n    --result-dir {work_dir}  \\\n    --micro-batch-size 2 \\\n    --num-gpus 1 \\\n    --precision \"bf16-mixed\"\n</pre> %%capture --no-display --no-stderr cell_output  ! finetune_esm2 \\     --restore-from-checkpoint-path {pretrain_checkpoint_path} \\     --train-data-path {data_path} \\     --valid-data-path {data_path} \\     --config-class ESM2FineTuneSeqConfig \\     --dataset-class InMemorySingleValueDataset \\     --task-type \"regression\" \\     --mlp-ft-dropout 0.25 \\     --mlp-hidden-size 256 \\     --mlp-target-size 1 \\     --experiment-name \"sequence-level-regression\" \\     --num-steps 50 \\     --num-gpus 1 \\     --val-check-interval 10 \\     --log-every-n-steps 10 \\     --encoder-frozen \\     --lr 5e-3 \\     --lr-multiplier 1e2 \\     --scale-lr-layer \"regression_head\" \\     --result-dir {work_dir}  \\     --micro-batch-size 2 \\     --num-gpus 1 \\     --precision \"bf16-mixed\"  <p>The previous cell executes the finetuning and saves the checkpoints at the end of the run. The checkpoint path is logged at the end of the finetuning log file:</p> <pre><code>[NeMo I $$$$-$$-$$ 22:04:28 nemo_logging:393] Async checkpoint save for step 50 (/workspace/bionemo2/esm2_finetune_tutorial/sequence-level-regression/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last-v1.ckpt) finalized successfully.\n</code></pre> <p>To avoid long text output from the previous cell, the log is captured and stored into the <code>cell_output</code> variable. To visualize the log file uncomment and execute the next cell:</p> In\u00a0[7]: Copied! <pre># print(cell_output.stdout)\n</pre> # print(cell_output.stdout) <p>We can now use the checkpoint stored in the previous step to run inference. We will drop the <code>.ckpt</code> from the checkpoint path and provide that to the <code>--checkpoint-path</code> argument of <code>infer_esm2</code> executable.</p> <p>The input <code>--data-path</code> for inference is a CSV file with <code>sequences</code> column. It is also required to provide the appropriate <code>--config-class</code> name to load the model from the checkpoint. For a detailed description of inference arguments please refer to the ESM-2 Inference tutorial.</p> In\u00a0[8]: Copied! <pre># Create a DataFrame\ndf = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n\ncheckpoint_path = f\"{work_dir}/sequence-level-regression/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\"\nresults_path = f\"{work_dir}/sequence-level-regression/infer/\"\n</pre> # Create a DataFrame df = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])  # Save the DataFrame to a CSV file data_path = os.path.join(work_dir, \"sequences.csv\") df.to_csv(data_path, index=False)  checkpoint_path = f\"{work_dir}/sequence-level-regression/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\" results_path = f\"{work_dir}/sequence-level-regression/infer/\" In\u00a0[9]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --config-class ESM2FineTuneSeqConfig \\\n             --data-path {data_path} \\\n             --results-path {results_path} \\\n             --micro-batch-size 3 \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-embeddings \\\n             --include-input-ids\n</pre> %%capture --no-display --no-stderr cell_output  ! infer_esm2 --checkpoint-path {checkpoint_path} \\              --config-class ESM2FineTuneSeqConfig \\              --data-path {data_path} \\              --results-path {results_path} \\              --micro-batch-size 3 \\              --num-gpus 1 \\              --precision \"bf16-mixed\" \\              --include-embeddings \\              --include-input-ids <p>The inference results are written into a <code>.pt</code> file which can be loaded using PyTorch library:</p> In\u00a0[10]: Copied! <pre>import torch\nresults = torch.load(f\"{results_path}/predictions__rank_0.pt\")\n\nfor key, val in results.items():\n    if val is not None:\n        print(f'{key}\\t{val.shape}')\n</pre> import torch results = torch.load(f\"{results_path}/predictions__rank_0.pt\")  for key, val in results.items():     if val is not None:         print(f'{key}\\t{val.shape}') <pre>input_ids\ttorch.Size([10, 1024])\nembeddings\ttorch.Size([10, 320])\nregression_output\ttorch.Size([10, 1])\n</pre> <p>Similarly for a sequence-level classification task we can create a dataset by labeling our sequences with arbitrary class names and take advantage of <code>Label2IDTokenizer</code> in the <code>transform_label()</code> method of <code>InMemorySingleValueDataset</code>.</p> In\u00a0[11]: Copied! <pre>class_labels = [\n    \"E_class\",\n    \"C_class\",\n    \"H_class\",\n    \"H_class\",\n    \"C_class\",\n    \"H_class\",\n    \"H_class\",\n    \"C_class\",\n    \"H_class\",\n    \"C_class\",\n]\n\ndata = [(seq, label) for seq, label in zip(artificial_sequence_data, class_labels)]\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"classification_data.csv\")\ndf.to_csv(data_path, index=False)\n</pre> class_labels = [     \"E_class\",     \"C_class\",     \"H_class\",     \"H_class\",     \"C_class\",     \"H_class\",     \"H_class\",     \"C_class\",     \"H_class\",     \"C_class\", ]  data = [(seq, label) for seq, label in zip(artificial_sequence_data, class_labels)]  # Create a DataFrame df = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])  # Save the DataFrame to a CSV file data_path = os.path.join(work_dir, \"classification_data.csv\") df.to_csv(data_path, index=False) <p>Since this task is also a sequence-level fine-tuning, we will use <code>ESM2FineTuneSeqConfig</code> and single-value dataset <code>InMemorySingleValueDataset</code> but set the task-type to <code>classification</code>. For this classification task the MLP output layer size (<code>mlp-target-size</code>) should be set to number of classes in the dataset (3 in this example).</p> In\u00a0[12]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\n! finetune_esm2 \\\n    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n    --train-data-path {data_path} \\\n    --valid-data-path {data_path} \\\n    --config-class ESM2FineTuneSeqConfig \\\n    --dataset-class InMemorySingleValueDataset \\\n    --task-type \"classification\" \\\n    --mlp-ft-dropout 0.25 \\\n    --mlp-hidden-size 256 \\\n    --mlp-target-size 3 \\\n    --experiment-name \"sequence-level-classification\" \\\n    --num-steps 50 \\\n    --num-gpus 1 \\\n    --val-check-interval 10 \\\n    --log-every-n-steps 10 \\\n    --encoder-frozen \\\n    --lr 5e-3 \\\n    --lr-multiplier 1e2 \\\n    --scale-lr-layer \"classification_head\" \\\n    --result-dir {work_dir}  \\\n    --micro-batch-size 2 \\\n    --num-gpus 1 \\\n    --precision \"bf16-mixed\"\n</pre> %%capture --no-display --no-stderr cell_output  ! finetune_esm2 \\     --restore-from-checkpoint-path {pretrain_checkpoint_path} \\     --train-data-path {data_path} \\     --valid-data-path {data_path} \\     --config-class ESM2FineTuneSeqConfig \\     --dataset-class InMemorySingleValueDataset \\     --task-type \"classification\" \\     --mlp-ft-dropout 0.25 \\     --mlp-hidden-size 256 \\     --mlp-target-size 3 \\     --experiment-name \"sequence-level-classification\" \\     --num-steps 50 \\     --num-gpus 1 \\     --val-check-interval 10 \\     --log-every-n-steps 10 \\     --encoder-frozen \\     --lr 5e-3 \\     --lr-multiplier 1e2 \\     --scale-lr-layer \"classification_head\" \\     --result-dir {work_dir}  \\     --micro-batch-size 2 \\     --num-gpus 1 \\     --precision \"bf16-mixed\" <p>For this task we assign secondary structure label to each token in the sequence:</p> In\u00a0[13]: Copied! <pre>secondary_structure_labels = [\n    \"EEEECCCCCHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",\n    \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"HHHHHCCCCCHHHHHHHHHHHHHHCCCHHHHHHHHHH\",\n    \"HHHHHHHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"CHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",\n    \"HHHHHHHHHHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",\n    \"HHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n    \"HHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",\n    \"CCCCCCCCCCCCCCCCCCCCCCCCCCEEECCCCEEECHHHHHHHHHCCCCCCCCEEECCCCCC\",\n]\n\ndata = [(seq, label) for (seq, label) in zip(artificial_sequence_data, secondary_structure_labels)]\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"token_classification_data.csv\")\ndf.to_csv(data_path, index=False)\n</pre> secondary_structure_labels = [     \"EEEECCCCCHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",     \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",     \"HHHHHCCCCCHHHHHHHHHHHHHHCCCHHHHHHHHHH\",     \"HHHHHHHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",     \"CHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",     \"HHHHHHHHHHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",     \"HHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",     \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",     \"HHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",     \"CCCCCCCCCCCCCCCCCCCCCCCCCCEEECCCCEEECHHHHHHHHHCCCCCCCCEEECCCCCC\", ]  data = [(seq, label) for (seq, label) in zip(artificial_sequence_data, secondary_structure_labels)]  # Create a DataFrame df = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])  # Save the DataFrame to a CSV file data_path = os.path.join(work_dir, \"token_classification_data.csv\") df.to_csv(data_path, index=False) In\u00a0[14]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\n! finetune_esm2 \\\n    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n    --train-data-path {data_path} \\\n    --valid-data-path {data_path} \\\n    --config-class ESM2FineTuneTokenConfig \\\n    --dataset-class InMemoryPerTokenValueDataset \\\n    --task-type \"classification\" \\\n    --cnn-dropout 0.25 \\\n    --cnn-hidden-size 32 \\\n    --cnn-num-classes 3 \\\n    --experiment-name \"token-level-classification\" \\\n    --num-steps 50 \\\n    --num-gpus 1 \\\n    --val-check-interval 10 \\\n    --log-every-n-steps 10 \\\n    --encoder-frozen \\\n    --lr 5e-3 \\\n    --lr-multiplier 1e2 \\\n    --scale-lr-layer \"classification_head\" \\\n    --result-dir {work_dir}  \\\n    --micro-batch-size 2 \\\n    --num-gpus 1 \\\n    --precision \"bf16-mixed\"\n</pre> %%capture --no-display --no-stderr cell_output  ! finetune_esm2 \\     --restore-from-checkpoint-path {pretrain_checkpoint_path} \\     --train-data-path {data_path} \\     --valid-data-path {data_path} \\     --config-class ESM2FineTuneTokenConfig \\     --dataset-class InMemoryPerTokenValueDataset \\     --task-type \"classification\" \\     --cnn-dropout 0.25 \\     --cnn-hidden-size 32 \\     --cnn-num-classes 3 \\     --experiment-name \"token-level-classification\" \\     --num-steps 50 \\     --num-gpus 1 \\     --val-check-interval 10 \\     --log-every-n-steps 10 \\     --encoder-frozen \\     --lr 5e-3 \\     --lr-multiplier 1e2 \\     --scale-lr-layer \"classification_head\" \\     --result-dir {work_dir}  \\     --micro-batch-size 2 \\     --num-gpus 1 \\     --precision \"bf16-mixed\"  <p>The previous cell executes the finetuning and saves the checkpoints at the end of the run. The checkpoint path is logged at the end of the finetuning log file:</p> <pre><code>[NeMo I $$$$-$$-$$ 22:16:46 nemo_logging:393] Async checkpoint save for step 50 (/workspace/bionemo2/esm2_finetune_tutorial/token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last.ckpt) finalized successfully.\n</code></pre> <p>To avoid long text output from the previous cell, the log is captured and stored into the <code>cell_output</code> variable. To visualize the log file uncomment and execute the next cell:</p> In\u00a0[15]: Copied! <pre># print(cell_output.stdout)\n</pre> # print(cell_output.stdout) <p>We can now use the checkpoint stored in the previous step to run inference. We will drop the <code>.ckpt</code> from the checkpoint path and provide that to the <code>--checkpoint-path</code> argument of <code>infer_esm2</code> executable.</p> <p>The input <code>--data-path</code> for inference is a CSV file with <code>sequences</code> column. It is also required to provide the appropriate <code>--config-class</code> name to load the model from the checkpoint. For a detailed description of inference arguments please refer to the ESM-2 Inference tutorial.</p> In\u00a0[16]: Copied! <pre># Create a DataFrame\ndf = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n\ncheckpoint_path = f\"{work_dir}/token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\"\nresults_path = f\"{work_dir}/token-level-classification/infer/\"\n</pre> # Create a DataFrame df = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])  # Save the DataFrame to a CSV file data_path = os.path.join(work_dir, \"sequences.csv\") df.to_csv(data_path, index=False)  checkpoint_path = f\"{work_dir}/token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\" results_path = f\"{work_dir}/token-level-classification/infer/\" In\u00a0[17]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --config-class ESM2FineTuneTokenConfig \\\n             --data-path {data_path} \\\n             --results-path {results_path} \\\n             --micro-batch-size 3 \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-embeddings \\\n             --include-hiddens \\\n             --include-input-ids\n</pre> %%capture --no-display --no-stderr cell_output  ! infer_esm2 --checkpoint-path {checkpoint_path} \\              --config-class ESM2FineTuneTokenConfig \\              --data-path {data_path} \\              --results-path {results_path} \\              --micro-batch-size 3 \\              --num-gpus 1 \\              --precision \"bf16-mixed\" \\              --include-embeddings \\              --include-hiddens \\              --include-input-ids In\u00a0[18]: Copied! <pre># print(cell_output)\n</pre> # print(cell_output) <p>The inference results are written into a <code>.pt</code> file which can be loaded using PyTorch library:</p> In\u00a0[19]: Copied! <pre>import torch\nresults = torch.load(f\"{results_path}/predictions__rank_0.pt\")\n\nfor key, val in results.items():\n    if val is not None:\n        print(f'{key}\\t{val.shape}')\n</pre> import torch results = torch.load(f\"{results_path}/predictions__rank_0.pt\")  for key, val in results.items():     if val is not None:         print(f'{key}\\t{val.shape}') <pre>hidden_states\ttorch.Size([10, 1024, 320])\ninput_ids\ttorch.Size([10, 1024])\nembeddings\ttorch.Size([10, 320])\nclassification_output\ttorch.Size([10, 1024, 3])\n</pre> <p>We can use the label tokenizer to convert the classification output to class names. Note that for demonstration purposes we are using a small dataset of artificial sequences in this example. You may experience over-fitting and observe no change in the validation metrics. This amount of data and the short training run does not result in accurate predictions.</p> In\u00a0[20]: Copied! <pre>from bionemo.esm2.data.tokenizer import get_tokenizer\n\n\ntokenizer = get_tokenizer()\ntokens = tokenizer.all_tokens\naa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']\naa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens]\nextra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]\n\ninput_ids = results['input_ids'] # b, s\n# mask where non-amino acid tokens are True\nmask = ~torch.isin(input_ids, torch.tensor(extra_indices))\n</pre> from bionemo.esm2.data.tokenizer import get_tokenizer   tokenizer = get_tokenizer() tokens = tokenizer.all_tokens aa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C'] aa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens] extra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]  input_ids = results['input_ids'] # b, s # mask where non-amino acid tokens are True mask = ~torch.isin(input_ids, torch.tensor(extra_indices)) In\u00a0[21]: Copied! <pre>from bionemo.llm.data.label2id_tokenizer import Label2IDTokenizer\n\nlabel_tokenizer = Label2IDTokenizer()\nlabel_tokenizer = label_tokenizer.build_vocab(secondary_structure_labels)\n</pre> from bionemo.llm.data.label2id_tokenizer import Label2IDTokenizer  label_tokenizer = Label2IDTokenizer() label_tokenizer = label_tokenizer.build_vocab(secondary_structure_labels) In\u00a0[22]: Copied! <pre>output_ids = torch.argmax(results[\"classification_output\"], dim=-1)\n\nprint(\"Predicted Secondary Structures:\")\nfor i in range(output_ids.shape[0]):\n    ss_ids = output_ids[i][mask[i]]\n    print(label_tokenizer.ids_to_text(ss_ids.tolist()))\n</pre> output_ids = torch.argmax(results[\"classification_output\"], dim=-1)  print(\"Predicted Secondary Structures:\") for i in range(output_ids.shape[0]):     ss_ids = output_ids[i][mask[i]]     print(label_tokenizer.ids_to_text(ss_ids.tolist())) <pre>Predicted Secondary Structures:\nEEEECCCCCHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\nCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\nHHHHHCCCCCHHHHHHHHHHHHHHCCCHHHHHHHHHH\nHHHHHHHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\nCHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\nHHHHHHHHHHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\nHHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\nCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\nHHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\nCCCCCCCCCCCCCCCCCCCCCCCCCCEEECCCCEEECHHHHHHHHHCCCCCCCCEECCCCCCC\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#esm-2-fine-tuning","title":"ESM-2 Fine-tuning\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#building-a-regression-fine-tune-module","title":"Building a Regression Fine-tune Module\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#1-loss-reduction-class","title":"1 - Loss Reduction Class\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#2-fine-tuned-model-head","title":"2 - Fine-Tuned Model Head\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#3-fine-tuned-model","title":"3 - Fine-Tuned Model\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#4-fine-tuning-config","title":"4 - Fine-Tuning Config\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#5-dataset","title":"5 - Dataset\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#datamodule","title":"DataModule\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#setup-and-assumptions","title":"Setup and Assumptions\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#import-required-libraries","title":"Import Required Libraries\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#work-directory","title":"Work Directory\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#download-pre-trained-model-checkpoints","title":"Download Pre-trained Model Checkpoints\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#fine-tuning","title":"Fine-tuning\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#scaled-lr-for-fine-tune-head-parameters","title":"Scaled LR for fine-tune head parameters\u00b6","text":"<p>We can assign a different LR for specific layers (e.g. task head) during fine-tuning by making it possible to specify the name of the target layer as well as the LR multiplier.</p> <ul> <li><code>--lr-multiplier</code>: is a float that scales <code>--lr</code></li> <li><code>--sclae-lr-layer</code>: is the name of the layers for which we scale the LR</li> </ul>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#sequence-level-regression","title":"Sequence-level Regression\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#sequence-level-classification","title":"Sequence-level Classification\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/finetune/#toke-level-classification-data","title":"Toke-level Classification data\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/","title":"ESM-2 Inference","text":"NOTE It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits.  NOTE Some of the cells below generate long text output. We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output. Comment or delete this line in the cells below to restore full output. <p>In this tutorial, we will demonstrate how to download ESM2 checkpoint, create a CSV file with protein sequences, and infer a ESM-2 model.</p> <p>All commands should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. For more information on how to build or pull the BioNeMo2 container, refer to the Initialization Guide.</p> In\u00a0[1]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\nimport os\nimport torch\nimport shutil\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n</pre> %%capture --no-display --no-stderr cell_output  import os import torch import shutil import numpy as np import pandas as pd  import warnings warnings.filterwarnings('ignore') warnings.simplefilter('ignore')  <p>Set the work directory to store data and results:</p> NOTE We set the following to clean up the work directory created by this notebook  <pre>cleanup : bool = True</pre> In\u00a0[2]: Copied! <pre>cleanup : bool = True\n</pre> cleanup : bool = True In\u00a0[3]: Copied! <pre>work_dir=\"/workspace/bionemo2/esm2_inference_tutorial\"\n\nif cleanup and os.path.exists(work_dir):\n    shutil.rmtree(work_dir)\n\nif not os.path.exists(work_dir):\n    os.makedirs(work_dir)\n    print(f\"Directory '{work_dir}' created.\")\nelse:\n    print(f\"Directory '{work_dir}' already exists.\")\n</pre> work_dir=\"/workspace/bionemo2/esm2_inference_tutorial\"  if cleanup and os.path.exists(work_dir):     shutil.rmtree(work_dir)  if not os.path.exists(work_dir):     os.makedirs(work_dir)     print(f\"Directory '{work_dir}' created.\") else:     print(f\"Directory '{work_dir}' already exists.\") <pre>Directory '/workspace/bionemo2/esm2_inference_tutorial' created.\n</pre> <p>The following code will download the pre-trained model, <code>esm2n/650m:2.0</code>, from the NGC registry:</p> In\u00a0[4]: Copied! <pre>from bionemo.core.data.load import load\n\ncheckpoint_path = load(\"esm2/650m:2.0\")\nprint(checkpoint_path)\n</pre> from bionemo.core.data.load import load  checkpoint_path = load(\"esm2/650m:2.0\") print(checkpoint_path) <pre>Downloading data from 'nvidia/clara/esm2nv650m:2.0' to file '/home/ubuntu/.cache/bionemo/0798767e843e3d54315aef91934d28ae7d8e93c2849d5fcfbdf5fac242013997-esm2_650M_nemo2.tar.gz'.\n</pre> <pre>{\n    \"download_end\": \"2025-01-14 22:01:24\",\n    \"download_start\": \"2025-01-14 22:01:05\",\n    \"download_time\": \"18s\",\n    \"files_downloaded\": 1,\n    \"local_path\": \"/home/ubuntu/.cache/bionemo/tmpfj1e52vw/esm2nv650m_v2.0\",\n    \"size_downloaded\": \"1.12 GB\",\n    \"status\": \"COMPLETED\"\n}\n</pre> <pre>Untarring contents of '/home/ubuntu/.cache/bionemo/0798767e843e3d54315aef91934d28ae7d8e93c2849d5fcfbdf5fac242013997-esm2_650M_nemo2.tar.gz' to '/home/ubuntu/.cache/bionemo/0798767e843e3d54315aef91934d28ae7d8e93c2849d5fcfbdf5fac242013997-esm2_650M_nemo2.tar.gz.untar'\n</pre> <pre>/home/ubuntu/.cache/bionemo/0798767e843e3d54315aef91934d28ae7d8e93c2849d5fcfbdf5fac242013997-esm2_650M_nemo2.tar.gz.untar\n</pre> <p>We use the <code>InMemoryProteinDataset</code> class to load the protein sequence data from a <code>.csv</code> file. This data file should at least have a <code>sequences</code> column and can optionally have a <code>labels</code> column used for fine-tuning applications. Here is an example of how to create your own inference input data using a list of sequences in python:</p> In\u00a0[5]: Copied! <pre>import pandas as pd\n\nartificial_sequence_data = [\n    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n]\n\n# Create a DataFrame\ndf = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n</pre> import pandas as pd  artificial_sequence_data = [     \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",     \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",     \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",     \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",     \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",     \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",     \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\", ]  # Create a DataFrame df = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])  # Save the DataFrame to a CSV file data_path = os.path.join(work_dir, \"sequences.csv\") df.to_csv(data_path, index=False) <p>Similar to PyTorch Lightning, ESM-2 Inference takes advantage of some key classes:</p> <ol> <li><code>MegatronStrategy</code> - To launch and setup parallelism for NeMo and Megatron-LM.</li> <li><code>Trainer</code> - To configure training configurations and logging.</li> <li><code>ESMFineTuneDataModule</code> - To load sequence data for both fine-tuning and inference.</li> <li><code>ESM2Config</code> - To configure the ESM-2 model as <code>BionemoLightningModule</code>.</li> </ol> <p>Please refer to ESM-2 Pretraining and ESM-2 Fine-Tuning tutorials for detailed description of these classes.</p> <p>To run inference on the data created in the previous step, we can use the <code>infer_esm2</code> executable which calls <code>bionemo-framework/sub-packages/bionemo-esm2/src/bionemo/esm2/scripts/infer_esm2.py</code>. We can get a full description of inference arguments by providing <code>--help</code> in the following command:</p> In\u00a0[6]: Copied! <pre>! infer_esm2 --help\n</pre> ! infer_esm2 --help <pre>2025-01-14 22:01:45 - faiss.loader - INFO - Loading faiss with AVX512 support.\n2025-01-14 22:01:45 - faiss.loader - INFO - Successfully loaded faiss with AVX512 support.\n[NeMo W 2025-01-14 22:01:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[NeMo W 2025-01-14 22:01:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n      cm = get_cmap(\"Set1\")\n    \nusage: infer_esm2 [-h] --checkpoint-path CHECKPOINT_PATH --data-path DATA_PATH\n                  --results-path RESULTS_PATH\n                  [--precision {fp16,bf16,fp32,bf16-mixed,fp32-mixed,16-mixed,fp16-mixed,16,32}]\n                  [--num-gpus NUM_GPUS] [--num-nodes NUM_NODES]\n                  [--micro-batch-size MICRO_BATCH_SIZE]\n                  [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]\n                  [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]\n                  [--prediction-interval {epoch,batch}] [--include-hiddens]\n                  [--include-input-ids] [--include-embeddings]\n                  [--include-logits] [--config-class CONFIG_CLASS]\n\nInfer ESM2.\n\noptions:\n  -h, --help            show this help message and exit\n  --checkpoint-path CHECKPOINT_PATH\n                        Path to the ESM2 pretrained checkpoint\n  --data-path DATA_PATH\n                        Path to the CSV file containing sequences and label\n                        columns\n  --results-path RESULTS_PATH\n                        Path to the results directory.\n  --precision {fp16,bf16,fp32,bf16-mixed,fp32-mixed,16-mixed,fp16-mixed,16,32}\n                        Precision type to use for training.\n  --num-gpus NUM_GPUS   Number of GPUs to use for training. Default is 1.\n  --num-nodes NUM_NODES\n                        Number of nodes to use for training. Default is 1.\n  --micro-batch-size MICRO_BATCH_SIZE\n                        Micro-batch size. Global batch size is inferred from\n                        this.\n  --pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE\n                        Pipeline model parallel size. Default is 1.\n  --tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE\n                        Tensor model parallel size. Default is 1.\n  --prediction-interval {epoch,batch}\n                        Intervals to write DDP predictions into disk\n  --include-hiddens     Include hiddens in output of inference\n  --include-input-ids   Include input_ids in output of inference\n  --include-embeddings  Include embeddings in output of inference\n  --include-logits      Include per-token logits in output.\n  --config-class CONFIG_CLASS\n                        Model configs link model classes with losses, and\n                        handle model initialization (including from a prior\n                        checkpoint). This is how you can fine-tune a model.\n                        First train with one config class that points to one\n                        model class and loss, then implement and provide an\n                        alternative config class that points to a variant of\n                        that model and alternative loss. In the future this\n                        script should also provide similar support for picking\n                        different data modules for fine-tuning with different\n                        data types. Choices: dict_keys(['ESM2Config',\n                        'ESM2FineTuneSeqConfig', 'ESM2FineTuneTokenConfig'])\n</pre> <p>The hidden states (which are usually the output of each layer in a neural network) can be obtained by using <code>--include-hiddens</code> argument when calling the inference function of ESM-2 in BioNeMo Framework.</p> <p>The hidden states can be converted into fixed-size vector embeddings. This is done by removing the hidden state vectors corresponding to padding tokens, then averaging across the rest. This process is often used when the goal is to create a single vector representation from the hidden states of a model, which can be used for various sequence-level downstream tasks such as classification (e.g. subcellular localization) or regression (e.g. melting temperature prediction). To obtain the embedding results we can use <code>--include-embeddings</code> argument.</p> <p>By passing the hidden state of an amino acid sequence through the BERT language model head, we can obtain output logits at each position and transform them into probabilities. This can happen by using <code>--include-logits</code> argument. Logits here are the raw, unnormalized scores that represent the likelihood of each class and are not probabilities themselves; they can be any real number, including negative values.</p> <p>Now lets call <code>infer_esm2</code> executable with relevant arguments to compute and optionally return embeddings, hiddens and logits.</p> In\u00a0[7]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --data-path {data_path} \\\n             --results-path {work_dir} \\\n             --micro-batch-size 3 \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-hiddens \\\n             --include-embeddings \\\n             --include-logits \\\n             --include-input-ids\n</pre> %%capture --no-display --no-stderr cell_output  ! infer_esm2 --checkpoint-path {checkpoint_path} \\              --data-path {data_path} \\              --results-path {work_dir} \\              --micro-batch-size 3 \\              --num-gpus 1 \\              --precision \"bf16-mixed\" \\              --include-hiddens \\              --include-embeddings \\              --include-logits \\              --include-input-ids <p>Inference predictions are stored into <code>.pt</code> files for each device. Since we only used one device to run the inference (<code>--num-gpus 1</code>) in the previous step, the results were written to <code>{work_dir}/predictions__rank_0.pt</code> under the work directory of this notebook (defined above). The <code>.pt</code> file containes a dictionary of <code>{'result_key': torch.Tensor}</code> that be loaded with PyTorch:</p> In\u00a0[8]: Copied! <pre>import torch\nresults = torch.load(f\"{work_dir}/predictions__rank_0.pt\")\n\nfor key, val in results.items():\n    if val is not None:\n        print(f'{key}\\t{val.shape}')\n</pre> import torch results = torch.load(f\"{work_dir}/predictions__rank_0.pt\")  for key, val in results.items():     if val is not None:         print(f'{key}\\t{val.shape}') <pre>token_logits\ttorch.Size([1024, 10, 128])\nhidden_states\ttorch.Size([10, 1024, 1280])\ninput_ids\ttorch.Size([10, 1024])\nembeddings\ttorch.Size([10, 1280])\n</pre> <p>In this example <code>data</code> a python dict with the following keys <code>['token_logits', 'hidden_states', 'input_ids', 'embeddings']</code>. Logits (<code>token_logits</code>) tensor has a dimension of <code>[sequence, batch, hidden]</code> to improve the training performance. We will transpose the first two dimension in the following to have batch-first shape like the rest of the output tensors.</p> In\u00a0[9]: Copied! <pre>logits = results['token_logits'].transpose(0, 1)  # s, b, h  -&gt; b, s, h\nprint(logits.shape)\n</pre> logits = results['token_logits'].transpose(0, 1)  # s, b, h  -&gt; b, s, h print(logits.shape) <pre>torch.Size([10, 1024, 128])\n</pre> <p>The last dimension of <code>token_logits</code> is 128, with the first 33 positions corresponding to the amino acid vocabulary, followed by 95 paddings. We use the <code>tokenizer.vocab_size</code> to filter out the paddings and only keep the 33 vocab positions.</p> In\u00a0[10]: Copied! <pre>from bionemo.esm2.data.tokenizer import get_tokenizer\ntokenizer = get_tokenizer()\n\ntokens = tokenizer.all_tokens\nprint(f\"There are {tokenizer.vocab_size} unique tokens: {tokens}.\")\n\naa_logits = logits[..., :tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions\nprint(f\"Logits shape after removing the paddings in hidden dimension: {aa_logits.shape}\")\n</pre> from bionemo.esm2.data.tokenizer import get_tokenizer tokenizer = get_tokenizer()  tokens = tokenizer.all_tokens print(f\"There are {tokenizer.vocab_size} unique tokens: {tokens}.\")  aa_logits = logits[..., :tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions print(f\"Logits shape after removing the paddings in hidden dimension: {aa_logits.shape}\") <pre>There are 33 unique tokens: ['&lt;cls&gt;', '&lt;pad&gt;', '&lt;eos&gt;', '&lt;unk&gt;', 'L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U', 'Z', 'O', '.', '-', '&lt;null_1&gt;', '&lt;mask&gt;'].\nLogits shape after removing the paddings in hidden dimension: torch.Size([10, 1024, 33])\n</pre> <p>Let's set aside the tokens corresponding to the 20 known amino acids.</p> In\u00a0[11]: Copied! <pre>aa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']\n\naa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens]\nextra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]\n</pre> aa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']  aa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens] extra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens] <p>The sequence dimension in this example (1024) is representing the max sequence length wich includes paddings, EOS, and BOS. To filter the relevant amino acid information we can use the input sequence IDs in the results to create a mask that can be used to extract the relevant information in <code>aa_logits</code></p> In\u00a0[12]: Copied! <pre>input_ids = results['input_ids'] # b, s\n# mask where non-amino acid tokens are True\nmask = torch.isin(input_ids, torch.tensor(extra_indices))\n</pre> input_ids = results['input_ids'] # b, s # mask where non-amino acid tokens are True mask = torch.isin(input_ids, torch.tensor(extra_indices)) <p>The following snippet can be used to load and collate the predictions into a single dictionary.</p> <pre>import glob\nfrom bionemo.llm.lightning import batch_collator\n\ncollated_preditions = batch_collator([torch.load(path) for path in glob.glob(f\"{work_dir}/predictions__rank_*.pt\")])\nfor key, val in collated_preditions.items():\n    if val is not None:\n        print(f'{key}\\t{val.shape}')\n\n# token_logits\ttorch.Size([1024, 10, 128])\n# hidden_states\ttorch.Size([10, 1024, 1280])\n# input_ids     torch.Size([10, 1024])\n# embeddings\ttorch.Size([10, 1280])\n</pre> <p>For more in-depth example of inference and converting logits to probabilities please refer to ESM-2 Mutant Design Tutorial</p>"},{"location":"user-guide/examples/bionemo-esm2/inference/#esm-2-inference","title":"ESM-2 Inference\u00b6","text":"<p>This tutorial serves as a demo for ESM2 Inference using a CSV file with <code>sequences</code> column. To pre-train the ESM2 model please refer to ESM-2 Pretraining tutorial.</p>"},{"location":"user-guide/examples/bionemo-esm2/inference/#setup-and-assumptions","title":"Setup and Assumptions\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/#import-required-libraries","title":"Import Required Libraries\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/#work-directory","title":"Work Directory\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/#download-model-checkpoints","title":"Download Model Checkpoints\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/#data","title":"Data\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/#run-inference","title":"Run Inference\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/#inference-results","title":"Inference Results\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/inference/#ddp-inference-support","title":"DDP Inference Support\u00b6","text":"<p>Although this tutorial is utilizing one devive to run the inference, distributed inference is supported for ESM2 in BioNeMo Framework. One can simply set the the <code>--num-gpus n</code> to run distributed inference on <code>n</code> devices. The output predictions will be written into <code>predictions__rank_&lt;0...n-1&gt;.pt</code> under the <code>--results-path</code> provided. Moreover, by optionally including input token IDs with <code>--include-input-ids</code> we can snure 1:1 mapping between input sequences and output predictions.</p>"},{"location":"user-guide/examples/bionemo-esm2/mutant-design/","title":"Zero-Shot Protein Design Using ESM-2","text":"NOTE It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits. (Note: This links to the nightly release and may be out of sync with these docs.)  <p>We thank Adrian Lange from A-Alpha Bio for originally contributing this recipe. This notebook has since been modified by NVIDIA.</p> <ol> <li>ESM-2nv Inference Functionality<ul> <li>Objective: Perform inference on the pre-trained ESM-2 model.</li> <li>Steps: Download model checkpoints, create CSV data file of protein sequences, and generate hidden state representations and sequence embeddings from input protein sequences.</li> </ul> </li> <li>Logit and Probability Extraction<ul> <li>Objective: Obtain probability values of all possible tokens at each position in the amino acid sequence.</li> <li>Steps: Generate logits from hidden states, and transform them into probabilities.</li> </ul> </li> <li>Protein Mutant Design<ul> <li>Objective: Optimize an input protein sequence to align it more closely with naturally occurring protein variants.</li> <li>Steps: Sequentially mask amino acids, extract per-position probabilities (and create a heatmap), analyze positions where single-point mutants have higher likelihood than wild-type, and develop new candidates.</li> </ul> </li> </ol> <p>ESM-2 is a large-scale protein language model (PLM) trained on millions of protein sequences. It can capture complex patterns and relationships in protein sequences, allowing it to be used to predict likely amino acid substitutions at different positions. By leveraging ESM-2's masked language modeling (MLM) capabilities, we can identify potential mutations that may enhance a protein's properties or align it more closely with naturally occurring variants. ESM-2 has 650M and 3B parameter versions - for this demo, we will be using ESM-2 3B.</p> <p>This notebbok should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at <code>/workspace/bionemo2</code>. For more information on how to build or pull the BioNeMo2 container, refer to the Initialization Guide.</p> NOTE Some of the cells below generate long text output. We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output. Comment or delete this line in the cells below to restore full output. In\u00a0[1]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\nimport os\nimport torch\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n</pre> %%capture --no-display --no-stderr cell_output  import os import torch import shutil import numpy as np import pandas as pd import matplotlib.pyplot as plt  import warnings warnings.filterwarnings('ignore') warnings.simplefilter('ignore')  <p>Set the work directory to store data and results:</p> NOTE We set the following to clean up the work directory created by this notebook  <pre>cleanup : bool = True</pre> In\u00a0[2]: Copied! <pre>cleanup : bool = True\n</pre> cleanup : bool = True In\u00a0[3]: Copied! <pre>work_dir=\"/workspace/bionemo2/esm2_mutant_design_tutorial\"\n\nif cleanup and os.path.exists(work_dir):\n    shutil.rmtree(work_dir)\n\nif not os.path.exists(work_dir):\n    os.makedirs(work_dir)\n    print(f\"Directory '{work_dir}' created.\")\nelse:\n    print(f\"Directory '{work_dir}' already exists.\")\n</pre> work_dir=\"/workspace/bionemo2/esm2_mutant_design_tutorial\"  if cleanup and os.path.exists(work_dir):     shutil.rmtree(work_dir)  if not os.path.exists(work_dir):     os.makedirs(work_dir)     print(f\"Directory '{work_dir}' created.\") else:     print(f\"Directory '{work_dir}' already exists.\") <pre>Directory '/workspace/bionemo2/esm2_mutant_design_tutorial' created.\n</pre> NOTE The experiments in this notebook were run by using an ESM-2 3B model. Here we downsize to 650M model that allows execution on a larger set of NVIDIA GPUs (when the memory is insufficient for 3B). To reproduce the original experiment download the 3B checkpoint by adding this change to the next cell:  <pre>checkpoint = \"esm2/3b:2.0\"</pre> In\u00a0[4]: Copied! <pre>from bionemo.core.data.load import load\n\ncheckpoint = \"esm2/650m:2.0\"  # change to \"esm2/3b:2.0\" to use the ESM-2 3B model\ncheckpoint_path = load(checkpoint, source=\"ngc\")\n</pre> from bionemo.core.data.load import load  checkpoint = \"esm2/650m:2.0\"  # change to \"esm2/3b:2.0\" to use the ESM-2 3B model checkpoint_path = load(checkpoint, source=\"ngc\") <p>In this section, we will explore the key inference functionalities of the pre-trained model.</p> <p>In the first step we prepare the data by creating a CSV file with <code>sequences</code> column that holds the protein sequences that we use as inference input.</p> In\u00a0[5]: Copied! <pre>import pandas as pd\n\nsequences = [\n    'MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL', # length: 41\n    'MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA', # length: 39\n]\n# Create a DataFrame\ndf = pd.DataFrame(sequences, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\ndata_path = os.path.join(work_dir, \"sequences.csv\")\ndf.to_csv(data_path, index=False)\n</pre> import pandas as pd  sequences = [     'MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL', # length: 41     'MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA', # length: 39 ] # Create a DataFrame df = pd.DataFrame(sequences, columns=[\"sequences\"])  # Save the DataFrame to a CSV file data_path = os.path.join(work_dir, \"sequences.csv\") df.to_csv(data_path, index=False) <p>Let's also check the tokenizer vocabulary.</p> In\u00a0[6]: Copied! <pre>from bionemo.esm2.data.tokenizer import get_tokenizer, BioNeMoESMTokenizer\ntokenizer = get_tokenizer()\n\ntokens = tokenizer.all_tokens\nprint(f\"There are {tokenizer.vocab_size} unique tokens: {tokens}.\")\n</pre> from bionemo.esm2.data.tokenizer import get_tokenizer, BioNeMoESMTokenizer tokenizer = get_tokenizer()  tokens = tokenizer.all_tokens print(f\"There are {tokenizer.vocab_size} unique tokens: {tokens}.\") <pre>There are 33 unique tokens: ['&lt;cls&gt;', '&lt;pad&gt;', '&lt;eos&gt;', '&lt;unk&gt;', 'L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U', 'Z', 'O', '.', '-', '&lt;null_1&gt;', '&lt;mask&gt;'].\n</pre> <p>Let's set aside the tokens corresponding to the 20 known amino acids.</p> In\u00a0[7]: Copied! <pre>aa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']\n\naa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens]\nextra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]\n</pre> aa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']  aa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens] extra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens] <p>ESM-2nv was trained with a Masked Language Modeling (MLM) objective. Thus, we are able to mask a position in an amino acid sequence and obtain values for the most probable amino acids at that position, based on the surrounding context. Let's sequentially obtain these values for every position in the sequence.</p> <p>The hidden states (which are usually the output of each layer in a neural network) can be obtained by using <code>--include-hiddens</code> argument when calling the inference function of ESM-2 in BioNeMo Framework.</p> <p>The hidden states can be converted into fixed-size vector embeddings. This is done by removing the hidden state vectors corresponding to padding tokens, then averaging across the rest. This process is often used when the goal is to create a single vector representation from the hidden states of a model, which can be used for various sequence-level downstream tasks such as classification (e.g. subcellular localization) or regression (e.g. melting temperature prediction). To obtain the embedding results we can use <code>--include-embeddings</code> argument.</p> <p>By passing the hidden state of an amino acid sequence through the BERT language model head, we can obtain output logits at each position and transform them into probabilities. This can happen by using <code>--include-logits</code> argument. Logits here are the raw, unnormalized scores that represent the likelihood of each class and are not probabilities themselves; they can be any real number, including negative values.</p> <p>When we apply the softmax function to logits, it converts them into a probability distribution over the classes, where the sum of probabilities equals 1.</p> <p>Now lets call <code>infer_esm2</code> executable with relevant arguments to compute and optionally return embeddings, hiddens and logits.</p> In\u00a0[8]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\nexample_dir = os.path.join(work_dir, \"inference_example\")\nos.makedirs(example_dir, exist_ok=True)\n\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --data-path {data_path} \\\n             --results-path {example_dir} \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-hiddens \\\n             --include-embeddings \\\n             --include-logits \\\n             --include-input-ids\n</pre> %%capture --no-display --no-stderr cell_output  example_dir = os.path.join(work_dir, \"inference_example\") os.makedirs(example_dir, exist_ok=True)  ! infer_esm2 --checkpoint-path {checkpoint_path} \\              --data-path {data_path} \\              --results-path {example_dir} \\              --num-gpus 1 \\              --precision \"bf16-mixed\" \\              --include-hiddens \\              --include-embeddings \\              --include-logits \\              --include-input-ids <p>This will write the output of ESM-2 inference into a python dictionary and save that into <code>predictions__rank_0.pt</code> which can be loaded via PyTorch. DDP inference is supported in BioNeMo Framework and can be utilized by setting <code>--num-gpus n</code> to use <code>n</code> devices. The output predictions are then written to n distinct files <code>predictions__rank_&lt;0...n-1&gt;.pt</code>. Please refer to ESM-2 Inference Tutorial for more information regarding the DDP support and how to interpret the prediction outputs.</p> In\u00a0[9]: Copied! <pre>results = torch.load(f\"{example_dir}/predictions__rank_0.pt\")\n\nfor key, val in results.items():\n    if val is not None:\n        print(f'{key}\\t{val.shape}')\n</pre> results = torch.load(f\"{example_dir}/predictions__rank_0.pt\")  for key, val in results.items():     if val is not None:         print(f'{key}\\t{val.shape}') <pre>token_logits\ttorch.Size([1024, 2, 128])\nhidden_states\ttorch.Size([2, 1024, 1280])\ninput_ids\ttorch.Size([2, 1024])\nembeddings\ttorch.Size([2, 1280])\n</pre> <p>Logits (<code>token_logits</code>) tensor has a dimension of <code>[sequence, batch, hidden]</code> to improve the training performance. We will transpose the first two dimension in the following to have batch-first shape like the rest of the output tensors.</p> In\u00a0[10]: Copied! <pre>logits = results['token_logits'].transpose(0, 1)  # s, b, h  -&gt; b, s, h\nprint(logits.shape)\n</pre> logits = results['token_logits'].transpose(0, 1)  # s, b, h  -&gt; b, s, h print(logits.shape) <pre>torch.Size([2, 1024, 128])\n</pre> <p>The sequnce dimension of <code>toke_logits</code> is 1024, which includes begining-of-sequence, end-of-sequence (eos/bos) and padding. The last dimension of <code>token_logits</code> is 128, with the first 33 positions corresponding to the amino acid vocabulary, followed by 95 paddings. We use the <code>tokenizer.vocab_size</code> to filter out the paddings and only keep the 33 vocab positions.</p> In\u00a0[11]: Copied! <pre>aa_logits = logits[..., :tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions\nprint(aa_logits.shape)\n</pre> aa_logits = logits[..., :tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions print(aa_logits.shape) <pre>torch.Size([2, 1024, 33])\n</pre> <p>We will force the probabilities of non-amino acid tokens to become zero by calling softmax on <code>-inf</code>. These tokens IDs are listed as <code>extra_indices</code> and we set the logits values to <code>-inf</code>.</p> <p>Now we can convert the logits to probabilities using PyTorch Softmax function.</p> In\u00a0[12]: Copied! <pre>aa_logits[..., extra_indices] = - torch.inf  # force non-amino acid token probs to zero\n\nprobs = torch.softmax(aa_logits, dim=-1)\n\n# check that rows sum to 1\n# probs.sum(dim=-1)\n</pre> aa_logits[..., extra_indices] = - torch.inf  # force non-amino acid token probs to zero  probs = torch.softmax(aa_logits, dim=-1)  # check that rows sum to 1 # probs.sum(dim=-1) <p>These steps are summerized in the <code>logits_to_probs()</code> function below:</p> In\u00a0[13]: Copied! <pre>def logits_to_probs(\n        logits: torch.Tensor, tokenizer: BioNeMoESMTokenizer = get_tokenizer()\n) -&gt; torch.Tensor:\n    \"\"\"Convert token logits to probabilities\n\n    Args:\n        logits (torch.Tensor): logits tensor with the [batch, sequence, hidden] dimensions\n        tokenizer (BioNeMoESMTokenizer): ESM2 tokenizer\n\n    Returns:\n        probabilities (torch.Tensor): probability tensor with [batch, sequence, tokenizer.vocab_size]\n    \"\"\"\n    aa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']\n    extra_indices = [i for i, token in enumerate(tokenizer.all_tokens) if token not in aa_tokens]\n\n    aa_logits = logits[..., :tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions\n    aa_logits[..., extra_indices] = - torch.inf  # force non-amino acid token probs to zero\n    return torch.softmax(aa_logits, dim=-1)\n</pre> def logits_to_probs(         logits: torch.Tensor, tokenizer: BioNeMoESMTokenizer = get_tokenizer() ) -&gt; torch.Tensor:     \"\"\"Convert token logits to probabilities      Args:         logits (torch.Tensor): logits tensor with the [batch, sequence, hidden] dimensions         tokenizer (BioNeMoESMTokenizer): ESM2 tokenizer      Returns:         probabilities (torch.Tensor): probability tensor with [batch, sequence, tokenizer.vocab_size]     \"\"\"     aa_tokens = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C']     extra_indices = [i for i, token in enumerate(tokenizer.all_tokens) if token not in aa_tokens]      aa_logits = logits[..., :tokenizer.vocab_size]  # filter out the 95 paddings and only keep 33 vocab positions     aa_logits[..., extra_indices] = - torch.inf  # force non-amino acid token probs to zero     return torch.softmax(aa_logits, dim=-1)  <p>In this section, we aim to optimize an input protein sequence by introducing single-point mutations that align it more closely with naturally occurring protein variants. These mutants may present properties that enhance the protein's functionality, such as improved stability or increased catalytic activity. By leveraging ESM-2's masked language modeling capabilities, we can identify amino acid substitutions with higher likelihood than the wild-type residues. This approach allows us to explore the protein sequence space efficiently, potentially discovering variants with superior characteristics.</p> <p>Let's take a starting sequence and scan through the positions, iteratively placing a <code>&lt;mask&gt;</code> token in place of the existing amino acid at each position. We will then predict probabilities at each masked location. If you only want to analyze substitutions within a predefined portion of the sequence (e.g. a specific alpha helix), you can set <code>start_pos</code> and <code>end_pos</code> accordingly, below.</p> In\u00a0[14]: Copied! <pre>seq = 'MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL' # length: 41\n\nstart_pos = 0\nend_pos = len(seq)\n</pre> seq = 'MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL' # length: 41  start_pos = 0 end_pos = len(seq) In\u00a0[15]: Copied! <pre>positions = np.arange(start_pos, end_pos)\n\nsequentially_masked = list()\nfor index in positions:\n    masked = seq[:index] + \"&lt;mask&gt;\" + seq[index+1:]\n    sequentially_masked.append(masked)\n</pre> positions = np.arange(start_pos, end_pos)  sequentially_masked = list() for index in positions:     masked = seq[:index] + \"\" + seq[index+1:]     sequentially_masked.append(masked) <p>Let's save the masked sequences into a CSV file and look at the first few elements of <code>sequentially_masked_sequences</code>:</p> In\u00a0[16]: Copied! <pre># Create a DataFrame\ndf = pd.DataFrame(sequentially_masked, columns=[\"sequences\"])\n\n# Save the DataFrame to a CSV file\nmasked_data_path = os.path.join(work_dir, \"sequentially_masked_sequences.csv\")\ndf.to_csv(masked_data_path, index=False)\n\n\ndf.head(n=5)\n</pre> # Create a DataFrame df = pd.DataFrame(sequentially_masked, columns=[\"sequences\"])  # Save the DataFrame to a CSV file masked_data_path = os.path.join(work_dir, \"sequentially_masked_sequences.csv\") df.to_csv(masked_data_path, index=False)   df.head(n=5) Out[16]: sequences 0 &lt;mask&gt;SLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL 1 M&lt;mask&gt;LKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL 2 MS&lt;mask&gt;KRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL 3 MSL&lt;mask&gt;RKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL 4 MSLK&lt;mask&gt;KNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL <p>We now extract the logits and convert them to probability matrix for each element of <code>sequentially_masked</code>. This can easily be done by calling the inference function above with <code>--include-logits</code> and using softmax to convert the logits to probabilities. We can then select the probability vectors corresponding to the masked positions, and combine them into a final probability matrix.</p> In\u00a0[17]: Copied! <pre>%%capture --no-display --no-stderr cell_output\n\n! infer_esm2 --checkpoint-path {checkpoint_path} \\\n             --data-path {masked_data_path} \\\n             --results-path {work_dir} \\\n             --num-gpus 1 \\\n             --precision \"bf16-mixed\" \\\n             --include-logits \\\n             --include-input-ids\n</pre> %%capture --no-display --no-stderr cell_output  ! infer_esm2 --checkpoint-path {checkpoint_path} \\              --data-path {masked_data_path} \\              --results-path {work_dir} \\              --num-gpus 1 \\              --precision \"bf16-mixed\" \\              --include-logits \\              --include-input-ids In\u00a0[18]: Copied! <pre>results = torch.load(f\"{work_dir}/predictions__rank_0.pt\")\n\n# cast to FP32 since BFloat16 is an unsupported ScalarType in numpy\nlogits = results['token_logits'].transpose(0, 1).to(dtype=torch.float32)  # s, b, h  -&gt; b, s, h\n\nprobs = logits_to_probs(logits)\nprint(probs.shape)\n</pre> results = torch.load(f\"{work_dir}/predictions__rank_0.pt\")  # cast to FP32 since BFloat16 is an unsupported ScalarType in numpy logits = results['token_logits'].transpose(0, 1).to(dtype=torch.float32)  # s, b, h  -&gt; b, s, h  probs = logits_to_probs(logits) print(probs.shape) <pre>torch.Size([41, 1024, 33])\n</pre> <p>We are only interested in the probabilities associate with the amino acid tokens. So we need to ignore padding, and eos/bos tokens. Since all the sequence have the same length we can use that to filter them:</p> In\u00a0[19]: Copied! <pre>probas_final = probs[:, 1:positions.size+1, :]\nprobas_final.shape\n</pre> probas_final = probs[:, 1:positions.size+1, :] probas_final.shape Out[19]: <pre>torch.Size([41, 41, 33])</pre> <p>Select and combine probabilities corresponding to each mask</p> In\u00a0[20]: Copied! <pre>probas_final = probas_final[np.arange(probas_final.shape[0]), positions, :]\nprint(probas_final.shape)\n</pre> probas_final = probas_final[np.arange(probas_final.shape[0]), positions, :] print(probas_final.shape) <pre>torch.Size([41, 33])\n</pre> <p>Let's visualize the results. We can plot the predicted probabilities of each token across all positions of interest.</p> In\u00a0[21]: Copied! <pre># Create heatmap\ndat = probas_final[:, aa_indices]\n\nplt.figure(figsize=(11, 5))\nim = plt.imshow(dat.T, cmap='viridis', aspect='auto')\n\n# Add color scale\ncbar = plt.colorbar(im)\ncbar.set_label('Probability', rotation=270, labelpad=15)\n\n# Set y-axis labels (amino acid tokens) and x-axis labels (position in sequence)\nplt.yticks(ticks=np.arange(len(aa_tokens)), labels=aa_tokens)\nplt.xticks(ticks=np.arange(dat.shape[0]), labels=list(seq))\nplt.gca().xaxis.set_ticks_position('bottom')\n\n# Add axes titles and main title\nplt.xlabel('Position in Sequence')\nplt.ylabel('Token Labels')\nplt.title('Positional Token Probabilities')\n\n# Adjust layout to prevent clipping of labels\nplt.tight_layout()\n\nplt.show()\n</pre> # Create heatmap dat = probas_final[:, aa_indices]  plt.figure(figsize=(11, 5)) im = plt.imshow(dat.T, cmap='viridis', aspect='auto')  # Add color scale cbar = plt.colorbar(im) cbar.set_label('Probability', rotation=270, labelpad=15)  # Set y-axis labels (amino acid tokens) and x-axis labels (position in sequence) plt.yticks(ticks=np.arange(len(aa_tokens)), labels=aa_tokens) plt.xticks(ticks=np.arange(dat.shape[0]), labels=list(seq)) plt.gca().xaxis.set_ticks_position('bottom')  # Add axes titles and main title plt.xlabel('Position in Sequence') plt.ylabel('Token Labels') plt.title('Positional Token Probabilities')  # Adjust layout to prevent clipping of labels plt.tight_layout()  plt.show() <p>We can now translate the logits/probabilities back into the sequence space, by mapping the highest probability in each position to the corresponding amino acid.</p> In\u00a0[22]: Copied! <pre># Predicted seq (Argmax --&gt; Collect token IDs of predicted seq --&gt; Convert to amino acids)\npred_idx_list = np.argmax(probas_final, axis=-1).tolist()\npred_seq = \"\".join([tokenizer.id_to_token(id) for id in pred_idx_list])\n\n# Original seq\ntrue_idx_list = [tokenizer.token_to_id(seq[i]) for i in positions]\ntrue_seq = \"\".join([tokenizer.id_to_token(id) for id in true_idx_list])\n</pre> # Predicted seq (Argmax --&gt; Collect token IDs of predicted seq --&gt; Convert to amino acids) pred_idx_list = np.argmax(probas_final, axis=-1).tolist() pred_seq = \"\".join([tokenizer.id_to_token(id) for id in pred_idx_list])  # Original seq true_idx_list = [tokenizer.token_to_id(seq[i]) for i in positions] true_seq = \"\".join([tokenizer.id_to_token(id) for id in true_idx_list]) <p>Let's compare the sequences and visually inspect the positions where a mutant is suggested over the wild-type. Note that the predicted sequence is displayed on the top, and the original sequence is on the bottom.</p> In\u00a0[23]: Copied! <pre># Compare prediction (reconstruction) to true (input sequence)\ndisplay(pred_seq + \" (Predicted Sequence)\")\ndisplay(\n    \"\".join(\n        [\".\" if a == b else \"|\" for a, b in zip(pred_seq, true_seq)]\n    )\n)\ndisplay(true_seq + \" (Input Sequence)\")\n</pre> # Compare prediction (reconstruction) to true (input sequence) display(pred_seq + \" (Predicted Sequence)\") display(     \"\".join(         [\".\" if a == b else \"|\" for a, b in zip(pred_seq, true_seq)]     ) ) display(true_seq + \" (Input Sequence)\") <pre>'MSEKKKVVALILAAGKGSRLGAGRPKQFLKIGGKTILERTL (Predicted Sequence)'</pre> <pre>'..|.|.||...|...|.|.|..||...|||..|..|..||.'</pre> <pre>'MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL (Input Sequence)'</pre> <p>Amongst the mismatches, we can:</p> <ol> <li>Collect all positions where a mutant is suggested over the wild-type amino acid.</li> <li>At these positions, find the mutant with the highest probability.</li> </ol> In\u00a0[24]: Copied! <pre># Collect indices where a mutant is suggested over the wild-type\nmatches = [c1 == c2 for c1, c2 in zip(pred_seq, true_seq)]\nmismatch_index = [i for i, value in enumerate(matches) if not value]\n\n# Filter probability matrix to mismatches-only\nprobas_mismatch = probas_final[mismatch_index, :]\n\n# Find index of mutant with highest likelihood\nindex_flat = np.argmax(probas_mismatch)\nindex_2d = np.unravel_index(index_flat, probas_mismatch.shape)\nindex_of_interest = mismatch_index[index_2d[0]]\nposition_of_interest = positions[index_of_interest]\nprint(\"Position:\", position_of_interest)\nprint(\"Mutation:\", true_seq[position_of_interest] + str(position_of_interest) + pred_seq[position_of_interest])\n</pre> # Collect indices where a mutant is suggested over the wild-type matches = [c1 == c2 for c1, c2 in zip(pred_seq, true_seq)] mismatch_index = [i for i, value in enumerate(matches) if not value]  # Filter probability matrix to mismatches-only probas_mismatch = probas_final[mismatch_index, :]  # Find index of mutant with highest likelihood index_flat = np.argmax(probas_mismatch) index_2d = np.unravel_index(index_flat, probas_mismatch.shape) index_of_interest = mismatch_index[index_2d[0]] position_of_interest = positions[index_of_interest] print(\"Position:\", position_of_interest) print(\"Mutation:\", true_seq[position_of_interest] + str(position_of_interest) + pred_seq[position_of_interest]) <pre>Position: 32\nMutation: S32G\n</pre> <p>Let's check the probability associated to mutations at this position.</p> In\u00a0[25]: Copied! <pre># Sort tokens by probability\ntoken_ids_sort = sorted(enumerate(probas_final[index_of_interest]), key=lambda x: x[1], reverse=True)\n\ntokens_sort = [(tokenizer.all_tokens[i], i, p.item()) for i, p in token_ids_sort]\n\ntokens_sort_df = pd.DataFrame(tokens_sort, columns=['Token', 'Token ID', 'Probability'])\ntokens_sort_df.head()\n</pre> # Sort tokens by probability token_ids_sort = sorted(enumerate(probas_final[index_of_interest]), key=lambda x: x[1], reverse=True)  tokens_sort = [(tokenizer.all_tokens[i], i, p.item()) for i, p in token_ids_sort]  tokens_sort_df = pd.DataFrame(tokens_sort, columns=['Token', 'Token ID', 'Probability']) tokens_sort_df.head() Out[25]: Token Token ID Probability 0 G 6 0.827384 1 D 13 0.055431 2 E 9 0.032586 3 N 17 0.030137 4 S 8 0.018279 <p>It's clear that for this position, the amino acid Glycine (G) has a higher likelihood than the wild-type, Serine (S). In this way, we can use ESM-2nv to design novel mutant candidates for downstream testing.</p> <p>There are many ways that we can engineer candidates from ESM-2nv outputs. We can continue finding the top n single-point mutants, find the top n double- or multi-point mutants, randomly sample over the probability space generated by the input sequence, sample only within certain positions of interest (e.g. known active sites), etc. Through this process, a set of mutants can be developed for further in silico or wet lab testing.</p>"},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#zero-shot-protein-design-using-esm-2","title":"Zero-Shot Protein Design Using ESM-2\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#demo-objectives","title":"Demo Objectives\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#background","title":"Background\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#setup","title":"Setup\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#import-required-libraries","title":"Import Required Libraries\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#work-directory","title":"Work Directory\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#download-model-checkpoints","title":"Download Model Checkpoints\u00b6","text":"<p>The following code will download the pre-trained model from the NGC registry:</p>"},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#esm-2-inference","title":"ESM-2 Inference\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#data","title":"Data\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#tokenizer","title":"Tokenizer\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#obtaining-model-outputs","title":"Obtaining Model Outputs\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#note","title":"Note\u00b6","text":"<p>The sequence dimension in this example (1024) is representing the max sequence length wich includes paddings, EOS, and BOS. To filter the relevant amino acid information we can use the input sequence IDs in the results to create a mask:</p> <pre>    input_ids = results['input_ids'] # b, s\n    # mask where non-amino acid tokens are True\n    mask = torch.isin(input_ids, torch.tensor(extra_indices))\n</pre>"},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#mutant-design-through-esm-2nv","title":"Mutant Design through ESM-2nv\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#sequential-masking","title":"Sequential Masking\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#extraction-of-probabilities","title":"Extraction of Probabilities\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#amino-acid-heatmap","title":"Amino Acid Heatmap\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/mutant-design/#mutant-discovery","title":"Mutant Discovery\u00b6","text":""},{"location":"user-guide/examples/bionemo-esm2/pretrain/","title":"ESM-2 Pretraining","text":"<p>This tutorial serves as a demo for pretraining ESM2 from scratch with UniProt sequences.</p> <p>The ESM-2 model is a transformer-based protein language model that was pretrained on masked language model (MLM) task. The objective is to recover the original amino acid types of the perturbed locations from the rest of the protein sequences. Through pretraining, ESM-2 learns the evolutionary information in protein sequences similar to conservation analysis and Pott's model, and predicts the optimal mutations on any given protein sequence.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#setup-and-assumptions","title":"Setup and Assumptions","text":"<p>In this tutorial, we will demonstrate how to create an ESM-2 pretraining data module, and create and train a ESM-2 model.</p> <p>All commands should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. The BioNeMo Framework container can run in a brev.dev launchable: . It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credit. After launching the instance, launch a Terminal session in the Jupyter Lab UI. (Note: This links to the nightly release and may be out of sync with these docs.)</p> <p>Alternatively,  more information on how to build or pull the BioNeMo2 container locally, refer to the Initialization Guide.</p> <p>This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at <code>/workspace/bionemo2</code>.</p> <p>Note</p> <p>This <code>WORKDIR</code> may be <code>/workspaces/bionemo-framework</code> if you are using the VSCode Dev Container.</p> <p>Similar to PyTorch Lightning, we have to define some key classes:</p> <ol> <li><code>MegatronStrategy</code> - To launch and setup parallelism for NeMo and Megatron-LM.</li> <li><code>Trainer</code> - To configure training configurations and logging.</li> <li><code>ESMDataModule</code> - To load pretraining training and validation data with mapped UniRef90 sequences to UniRef50 clusters.</li> <li><code>ESM2Config</code> - To configure the ESM-2 model as <code>BionemoLightningModule</code>.</li> </ol>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#1-megatronstrategy","title":"1 - MegatronStrategy","text":"<p>BioNeMo2 supports data parallel (DP), tensor parallel (TP) and pipeline parallel (PP) for training large models. Instead of <code>DDPStrategy</code> in PyTorch Lightning, we use <code>MegatronStrategy</code> to launch and setup parallelism for NeMo and Megatron-LM.</p> <pre><code>from nemo import lightning as nl\nfrom bionemo.llm.utils.datamodule_utils import infer_global_batch_size\n\nmicro_batch_size = 2\nnum_nodes = 1\ndevices = 2\naccumulate_grad_batches = 1\ntensor_model_parallel_size = 2\npipeline_model_parallel_size = 1\n\nglobal_batch_size = infer_global_batch_size(\n    micro_batch_size=micro_batch_size,\n    num_nodes=num_nodes,\n    devices=devices,\n    accumulate_grad_batches=accumulate_grad_batches,\n    tensor_model_parallel_size=tensor_model_parallel_size,\n    pipeline_model_parallel_size=pipeline_model_parallel_size,\n)\n\nstrategy = nl.MegatronStrategy(\n    tensor_model_parallel_size=tensor_model_parallel_size,\n    pipeline_model_parallel_size=pipeline_model_parallel_size,\n    ddp=\"megatron\",\n    find_unused_parameters=True,\n    ckpt_include_optimizer=True,\n)\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#2-trainer","title":"2 - Trainer","text":"<p>BioNeMo2 trainer is very similar to PyTorch Lightning trainer. We can configure the training configurations and logging.</p> <pre><code>from lightning.pytorch.callbacks import LearningRateMonitor, RichModelSummary\nfrom bionemo.llm.lightning import PerplexityLoggingCallback\n\nnum_steps = 20\nlimit_val_batches = 2  # limit the validation epoch to 2 batches\nval_check_interval = 10  # validation epoch every 10 steps\nprecision = \"bf16-mixed\"  # use bf16-mixed precision\n\ntrainer = nl.Trainer(\n    devices=devices,\n    max_steps=num_steps,\n    accelerator=\"gpu\",\n    strategy=strategy,\n    limit_val_batches=limit_val_batches,\n    val_check_interval=val_check_interval,\n    num_nodes=num_nodes,\n    callbacks=[\n        PerplexityLoggingCallback(),\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n    ],\n    plugins=nl.MegatronMixedPrecision(precision=precision),  # precision is handled through plugins in BioNeMo2\n)\n</code></pre> <p>Here are examples of other possible configurations. <pre><code>from bionemo.core.utils.dtypes import PrecisionTypes\n\nlimit_val_batches_all_data = 1.  # validate on 100% of the validation dataset\nlimit_val_batches_half_data = 0.5  # validate on 50% of the validation dataset\nlimit_val_batches_one_batch = 1  # validate on 1 batch\n\nprint(PrecisionTypes)  # show all possible precision types\n</code></pre></p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#3-esmdatamodule","title":"3 - ESMDataModule","text":"<p>Before instantiating with data module, we can first download the testing ESM-2 pretraining data with <code>download_bionemo_data</code>. The command line will download the data if we haven't yet, and will return the path to the testing data, which is needed to instantiate <code>ESMDataModule</code>.</p> <pre><code>download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source ngc  # test data\n# download_bionemo_data esm2/fulldata_esm2_pretrain:2.0 --source ngc  # full data (~80GB)\n</code></pre> <p>On top of the path to the data directory, BioNeMo2 data module requires global and micro batch sizes to ensure that the input tensors are initialized correctly across model-parallel ranks (see Megatron Dataset Considerations).</p> <pre><code>from bionemo.esm2.data.datamodule import ESMDataModule\nfrom bionemo.esm2.data.dataset import RandomMaskStrategy\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\n\ndata_path = __your_downloaded_test_data_path__  # fill your path from the command line output\n\ntrain_cluster_path = f\"{data_path}/2024_03_sanity/train_clusters_sanity.parquet\"\ntrain_database_path = f\"{data_path}/2024_03_sanity/train_sanity.db\"\nvalid_cluster_path = f\"{data_path}/2024_03_sanity/valid_clusters.parquet\"\nvalid_database_path = f\"{data_path}/2024_03_sanity/validation.db\"\n\nmin_seq_length = None  # optional; filter sequences by minimum length if given\nmax_seq_length = 128  # required; default to 1024\n\nnum_dataset_workers = 1\nrandom_mask_strategy = RandomMaskStrategy.ALL_TOKENS  # default in BioNemo2 and HuggingFace implementation\n\ndata = ESMDataModule(\n    train_cluster_path=train_cluster_path,  # UniRef50 training cluster centers\n    train_database_path=train_database_path,  # UniRef90 training sequences\n    valid_cluster_path=valid_cluster_path,  # UniRef50 validation cluster centers\n    valid_database_path=valid_database_path,  # UniRef90 validation sequences\n    global_batch_size=global_batch_size,\n    micro_batch_size=micro_batch_size,\n    min_seq_length=min_seq_length,\n    max_seq_length=max_seq_length,\n    num_workers=num_dataset_workers,\n    random_mask_strategy=random_mask_strategy,\n)\n</code></pre> <p><code>RandomMaskStrategy</code></p> <p>When trained on MLM objective, the loss function randomly includes 15% of the tokens, within which 80% are masked, 10% are replaced with a random token, and 10% are kept unchanged. Since the vocabulary includes amino acids as well as special tokens, part of the protein sequence may be replaced by a special token. This is the default in both BioNeMo2 and HuggingFace ESM-2 implementation.</p> <p>To enforce amino-acid-only replacement, users can pass <code>random_mask_strategy=RandomMaskStrategy.AMINO_ACID_ONLY</code> to <code>ESMDataModule</code>.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#4-esm2config","title":"4. ESM2Config","text":"<p>Instead of initializing the whole model on each rank, sharded models are lazily created on the target rank with the help of a configuration object. <code>ESM2Config</code> is a dataclass that envelopes architecture parameters (such as <code>num_layers</code>) and the specification of each torch module (<code>ModuleSpec</code>) in the transformer, which are accelerated with flash and fused attentions in TransformerEngine. While we can initialize a model from <code>ESM2Config</code>, its setup is only completed in under <code>trainer.setup</code>, which is called on individual devices.</p> <pre><code>from megatron.core.optimizer import OptimizerConfig\nfrom nemo.lightning.pytorch.optim import MegatronOptimizerModule\n\nfrom bionemo.core.utils.dtypes import get_autocast_dtype\nfrom bionemo.esm2.api import ESM2Config\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\nfrom bionemo.esm2.model.lr_scheduler import WarmupAnnealDecayHoldScheduler\nfrom bionemo.llm.lightning import BionemoLightningModule\nfrom bionemo.llm.model.biobert.lightning import biobert_lightning_module\nfrom bionemo.llm.model.biobert.model import BiobertSpecOption\n\n# ESM-2 650M config\nnum_layers = 33\nhidden_size = 1280\nnum_attention_heads = 20\nffn_hidden_size = 4 * hidden_size\n\nnemo1_init_path = None  # initialize from nemo1 checkpoint\nrestore_from_checkpoint_path = None  # initialize from nemo2 checkpoint\nneed_megatron_variable_seq_lengths_reductions: bool = (\n    pipeline_model_parallel_size * tensor_model_parallel_size &gt; 1 and min_seq_length != max_seq_length\n)  # essential for pipeline/tensor parallel\nbiobert_spec_option = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec  # accelerated esm2 with transformer engine\n\nwarmup_steps = 2000\nlr = 1e-4\n\n# Create model config\nesm2_config = ESM2Config(\n    seq_length=max_seq_length,\n    num_layers=num_layers,\n    hidden_size=hidden_size,\n    num_attention_heads=num_attention_heads,\n    ffn_hidden_size=ffn_hidden_size,\n    params_dtype=get_autocast_dtype(precision),\n    pipeline_dtype=get_autocast_dtype(precision),\n    autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n    biobert_spec_option=biobert_spec_option,\n    nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n    initial_ckpt_path=str(restore_from_checkpoint_path) if restore_from_checkpoint_path is not None else None,\n    variable_seq_lengths=need_megatron_variable_seq_lengths_reductions,\n)\n\n# Create model instance\ntokenizer = get_tokenizer()\n\nmodel: BionemoLightningModule = biobert_lightning_module(\n    esm2_config,\n    tokenizer=tokenizer,\n    optimizer=MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=lr,\n            optimizer=\"adam\",\n            use_distributed_optimizer=True,\n            weight_decay=0.01,\n            adam_beta1=0.9,\n            adam_beta2=0.98,\n        ),\n        lr_scheduler=WarmupAnnealDecayHoldScheduler(\n            warmup_steps=warmup_steps, max_steps=num_steps, max_lr=lr, min_lr=lr / 10.0, anneal_percentage=0.10\n        ),\n    ),\n)\n</code></pre> <p><code>ModuleSpec</code></p> <p><code>ModelSpec</code> decides what torch modules are used in the transformer layers. By default, BioNeMo2 accelerates ESM-2 architecture with TransformerEngine layers. Users can define their own <code>ModelSpec</code> for customized transformer layers. See <code>get_biobert_spec</code>.</p> <p><code>BionemoLightningModule</code></p> <p>Since the model is lazily initialized in the target rank, breakpoints for debugging purposes should be added after <code>trainer.setup</code>.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#model-pretraining","title":"Model Pretraining","text":"<p>To close the loop, users can make use of <code>llm.train</code> from NeMo to begin training.</p> <pre><code>from typing import Optional\n\nfrom nemo.collections import llm\nfrom nemo.lightning import resume\nfrom nemo.lightning.pytorch import callbacks as nl_callbacks\n\nfrom bionemo.llm.utils.logger_utils import WandbLoggerOptions, setup_nemo_lightning_logger\n\n\n# WANDB logging\nwandb_options: Optional[WandbLoggerOptions] = (\n    None\n    if wandb_project is None\n    else WandbLoggerOptions(\n        offline=False,\n        project=__your_wandb_project__,\n        entity=__your_wandb_entity__,\n        tags=None,\n        group=None,\n        id=None,\n        anonymous=False,\n        log_model=False,\n    )\n)\n\ncheckpoint_callback = nl_callbacks.ModelCheckpoint(\n    save_last=True,\n    monitor=\"val_loss\",\n    save_top_k=1,\n    always_save_context=True,\n)\n\nnemo_logger = setup_nemo_lightning_logger(\n    root_dir=__your_result_dir__,\n    name=__your_experiment_name__,\n    initialize_tensorboard_logger=True,\n    wandb_kwargs=wandb_options,\n    ckpt_callback=checkpoint_callback,\n)\n\nllm.train(\n    model=model,\n    data=data,\n    trainer=trainer,\n    log=nemo_logger,\n    resume=resume.AutoResume(\n        resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n        resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n    ),\n)\n</code></pre> <p>Or simply use the ESM2 pretrain located in <code>$WORKDIR/sub-packages/bionemo-esm2/src/bionemo/esm2/scripts/train_esm2.py</code>. This script can be called either by directly using python or the installed executable <code>train_esm2</code>:</p> <pre><code># Enable fused attention in transformer engine for speed-up\nDATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source ngc)\n\ntrain_esm2 \\\n    --train-cluster-path ${DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet \\\n    --train-database-path ${DATA_DIR}/2024_03_sanity/train_sanity.db \\\n    --valid-cluster-path ${DATA_DIR}/2024_03_sanity/valid_clusters.parquet \\\n    --valid-database-path ${DATA_DIR}/2024_03_sanity/validation.db \\\n    --precision=\"bf16-mixed\" \\\n    --num-gpus 1 \\\n    --num-nodes 1 \\\n    --num-steps 100 \\\n    --val-check-interval 25 \\\n    --max-seq-length 1024 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --num-layers 33 \\\n    --hidden-size 1280 \\\n    --num-attention-head 20 \\\n    --ffn-hidden-size 5120 \\\n    --tensor-model-parallel-size 1 \\\n    --create-tensorboard-logger \\\n    --wandb_project=__your_wandb_project__ \\\n    --experiment-name=__your_wandb_experiment_name\n</code></pre> <p>This script will automatically create <code>./results</code> and store the checkpoints under <code>esm2</code>. Automatic pretraining resumption is handled automatically when <code>--resume-if-exists</code> set to True, and <code>--restore-from-checkpoint-path</code> is available if users want to restore from a specific path.</p> <p>Weight And Biases</p> <p>If intend to use <code>--wandb_project</code>, users should log in Weight and Biases or alternatively export the environment variable <code>WANDB_API_KEY</code>. If not provided, the logger will be disabled.</p> <p>Non-critical Warnings from Command Line Runs</p> <p>Users might experience <code>torch._dynamo.convert_frame</code> warning messages and depreciation warning on <code>async_grad_allreduce</code> from Megatron-LM. Users can safely ignore them and is non-critical to pretraining.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#recommended-pretraining-configuration","title":"Recommended Pretraining Configuration","text":"<p>We benchmark our implementation on the following model sizes<sup>1</sup>. These parameters are handled by</p> Model Size # Layers Hidden Size # Attention Heads FFN Hidden Size 8M 8 320 20 1280 650M 33 1280 20 5120 3B 36 2560 40 10240 15B 48 5120 40 20480 <p>In our current benchmark, we recommend the following trainiing and device configurations on A100 80GB GPUs to match with the published 2M token global batch size.</p> Model Size # GPUs Micro Batch Size Tensor Model Parallel Size 8M 32 64 1 650M 64 32 1 3B 128 16 1 15B 3120 2 2 <p>Additional Notes on Micro Batch Size</p> <p>While the above micro batch sizes are selected in 2^n to arrive at 2,097,152 tokens global batch size, users should observe performance boost by fitting the largest possible micro batch size onto the device without OOM. The currently largest batch sizes are listed below.</p> Model Size Max. micro batch size Tensor Model Parallel Size 8M 70 1 650M 48 1 3B 16 1 15B 3 2 <p>The only exception is 15B model where the authors reported 3.2M tokens global batch size. We arrived at 3,194,880 tokens on 390 A100 nodes.</p> <p>Maximum micro batch sizes for these model sizes are tested on 2 nodes of A100 80GB GPUs.</p> <p>Memory Allocation from Distributed Optimizer</p> <p>Distributed optimizer is enabled by default for improved memory allocation. Users might observe that the same micro batch size used on multi-device pretraining results in OOM on a single device. If additional optimization is necessary, we recommend running short benchmark on the same number of devices as in the production run.</p> <ol> <li> <p>Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, et al. \u201cEvolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model.\u201d Science 379, no. 6637 (March 17, 2023): 1123\u201330. https://doi.org/10.1126/science.ade2574\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/examples/bionemo-evo2/evo2_zeroshot_brca/","title":"Zero-shot prediction of BRCA1 variant effects with Evo 2","text":"In\u00a0[1]: Copied! <pre>!pip install biopython openpyxl\n</pre> !pip install biopython openpyxl <pre>DEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\nDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\nDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\nDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\nDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\nDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nRequirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.85)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.0.1\n[notice] To update, run: python -m pip install --upgrade pip\n</pre> In\u00a0[2]: Copied! <pre>import glob\nimport gzip\nimport json\nimport math\nimport os\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport torch\nfrom Bio import SeqIO\nfrom pathlib import Path\nfrom sklearn.metrics import roc_auc_score\n</pre> import glob import gzip import json import math import os  import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import torch from Bio import SeqIO from pathlib import Path from sklearn.metrics import roc_auc_score  <p>We start by loading a dataset from\u00a0Findlay et al. (2018), which contains experimentally measured function scores of 3,893\u00a0BRCA1\u00a0SNVs. These function scores reflect the extent by which the genetic variant has disrupted the protein's function, with lower scores indicating greater disruption. In this dataset, the SNVs are classified into three categories based on their function scores:\u00a0<code>LOF</code>\u00a0(loss-of-function),\u00a0<code>INT</code>\u00a0(intermediate), and\u00a0<code>FUNC</code>\u00a0(functional). We start by reading in this dataset.</p> In\u00a0[3]: Copied! <pre># Download the data if not present\nif not os.path.exists('brca1'):\n    os.makedirs('brca1')\n\ncommit_hash = \"3819474bee6c24938016614411f1fa025e542bbe\"\n\nif not os.path.exists(os.path.join('brca1', '41586_2018_461_MOESM3_ESM.xlsx')):\n    !wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/41586_2018_461_MOESM3_ESM.xlsx -O brca1/41586_2018_461_MOESM3_ESM.xlsx\n\nif not os.path.exists(os.path.join('brca1', 'GRCh37.p13_chr17.fna.gz')):\n    !wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/GRCh37.p13_chr17.fna.gz -O brca1/GRCh37.p13_chr17.fna.gz\n</pre> # Download the data if not present if not os.path.exists('brca1'):     os.makedirs('brca1')  commit_hash = \"3819474bee6c24938016614411f1fa025e542bbe\"  if not os.path.exists(os.path.join('brca1', '41586_2018_461_MOESM3_ESM.xlsx')):     !wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/41586_2018_461_MOESM3_ESM.xlsx -O brca1/41586_2018_461_MOESM3_ESM.xlsx  if not os.path.exists(os.path.join('brca1', 'GRCh37.p13_chr17.fna.gz')):     !wget https://github.com/ArcInstitute/evo2/raw/{commit_hash}/notebooks/brca1/GRCh37.p13_chr17.fna.gz -O brca1/GRCh37.p13_chr17.fna.gz  <p>We then group the <code>FUNC</code> and <code>INT</code> classes of SNVs together into a single category (<code>FUNC/INT</code>).</p> In\u00a0[4]: Copied! <pre>brca1_df = pd.read_excel(\n    os.path.join('brca1', '41586_2018_461_MOESM3_ESM.xlsx'),\n    header=2,\n)\nbrca1_df = brca1_df[[\n    'chromosome', 'position (hg19)', 'reference', 'alt', 'function.score.mean', 'func.class',\n]]\n\n# Rename columns\nbrca1_df.rename(columns={\n    'chromosome': 'chrom',\n    'position (hg19)': 'pos',\n    'reference': 'ref',\n    'alt': 'alt',\n    'function.score.mean': 'score',\n    'func.class': 'class',\n}, inplace=True)\n\n# Convert to two-class system\nbrca1_df['class'] = brca1_df['class'].replace(['FUNC', 'INT'], 'FUNC/INT')\n\nbrca1_df.head(10)\n</pre> brca1_df = pd.read_excel(     os.path.join('brca1', '41586_2018_461_MOESM3_ESM.xlsx'),     header=2, ) brca1_df = brca1_df[[     'chromosome', 'position (hg19)', 'reference', 'alt', 'function.score.mean', 'func.class', ]]  # Rename columns brca1_df.rename(columns={     'chromosome': 'chrom',     'position (hg19)': 'pos',     'reference': 'ref',     'alt': 'alt',     'function.score.mean': 'score',     'func.class': 'class', }, inplace=True)  # Convert to two-class system brca1_df['class'] = brca1_df['class'].replace(['FUNC', 'INT'], 'FUNC/INT')  brca1_df.head(10) Out[4]: chrom pos ref alt score class 0 17 41276135 T G -0.372611 FUNC/INT 1 17 41276135 T C -0.045313 FUNC/INT 2 17 41276135 T A -0.108254 FUNC/INT 3 17 41276134 T G -0.277963 FUNC/INT 4 17 41276134 T C -0.388414 FUNC/INT 5 17 41276134 T A -0.280973 FUNC/INT 6 17 41276133 C T -0.973683 FUNC/INT 7 17 41276133 C G -0.373489 FUNC/INT 8 17 41276133 C A 0.006314 FUNC/INT 9 17 41276132 A T -0.207552 FUNC/INT <p>We build a function to parse the reference and variant sequences of a 8,192-bp window around the genomic position of each SNV, using the reference sequence of human chromosome 17 where BRCA1 is located.</p> In\u00a0[5]: Copied! <pre>WINDOW_SIZE = 8192\n\n# Read the reference genome sequence of chromosome 17\nwith gzip.open(os.path.join('brca1', 'GRCh37.p13_chr17.fna.gz'), \"rt\") as handle:\n    for record in SeqIO.parse(handle, \"fasta\"):\n        seq_chr17 = str(record.seq)\n        break\n\ndef parse_sequences(pos, ref, alt):\n    \"\"\"\n    Parse reference and variant sequences from the reference genome sequence.\n    \"\"\"\n    p = pos - 1 # Convert to 0-indexed position\n    full_seq = seq_chr17\n\n    ref_seq_start = max(0, p - WINDOW_SIZE//2)\n    ref_seq_end = min(len(full_seq), p + WINDOW_SIZE//2)\n    ref_seq = seq_chr17[ref_seq_start:ref_seq_end]\n    snv_pos_in_ref = min(WINDOW_SIZE//2, p)\n    var_seq = ref_seq[:snv_pos_in_ref] + alt + ref_seq[snv_pos_in_ref+1:]\n\n    # Sanity checks\n    assert len(var_seq) == len(ref_seq)\n    assert ref_seq[snv_pos_in_ref] == ref\n    assert var_seq[snv_pos_in_ref] == alt\n\n    return ref_seq, var_seq\n</pre> WINDOW_SIZE = 8192  # Read the reference genome sequence of chromosome 17 with gzip.open(os.path.join('brca1', 'GRCh37.p13_chr17.fna.gz'), \"rt\") as handle:     for record in SeqIO.parse(handle, \"fasta\"):         seq_chr17 = str(record.seq)         break  def parse_sequences(pos, ref, alt):     \"\"\"     Parse reference and variant sequences from the reference genome sequence.     \"\"\"     p = pos - 1 # Convert to 0-indexed position     full_seq = seq_chr17      ref_seq_start = max(0, p - WINDOW_SIZE//2)     ref_seq_end = min(len(full_seq), p + WINDOW_SIZE//2)     ref_seq = seq_chr17[ref_seq_start:ref_seq_end]     snv_pos_in_ref = min(WINDOW_SIZE//2, p)     var_seq = ref_seq[:snv_pos_in_ref] + alt + ref_seq[snv_pos_in_ref+1:]      # Sanity checks     assert len(var_seq) == len(ref_seq)     assert ref_seq[snv_pos_in_ref] == ref     assert var_seq[snv_pos_in_ref] == alt      return ref_seq, var_seq <p>To make things run faster, we'll just look at a balanced sample of our data. If you want to run on the full dataset, set <code>disable_sample=True</code></p> In\u00a0[6]: Copied! <pre>disable_sample = False\nSAMPLE_FRAC = 0.2\nbalanced_sample = True\n\nrandom_state = 42\nif not disable_sample:\n    if balanced_sample:\n        # Get the number of rows in the dataframe\n        num_rows_minor_class = math.ceil(len(brca1_df[brca1_df['class'] == 'LOF']) * SAMPLE_FRAC)\n        brca1_df = pd.concat([\n            brca1_df[brca1_df['class'] == 'LOF'].sample(n=num_rows_minor_class, random_state=random_state),\n            brca1_df[brca1_df['class'] == 'FUNC/INT'].sample(n=num_rows_minor_class, random_state=random_state)\n        ]).sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n    else:\n        # Calculate the number of rows to sample\n        num_rows_to_sample = int(len(brca1_df) * SAMPLE_FRAC)\n        brca1_df = brca1_df.sample(frac=SAMPLE_FRAC, random_state=random_state).reset_index(drop=True)\nbrca1_df.shape\n</pre> disable_sample = False SAMPLE_FRAC = 0.2 balanced_sample = True  random_state = 42 if not disable_sample:     if balanced_sample:         # Get the number of rows in the dataframe         num_rows_minor_class = math.ceil(len(brca1_df[brca1_df['class'] == 'LOF']) * SAMPLE_FRAC)         brca1_df = pd.concat([             brca1_df[brca1_df['class'] == 'LOF'].sample(n=num_rows_minor_class, random_state=random_state),             brca1_df[brca1_df['class'] == 'FUNC/INT'].sample(n=num_rows_minor_class, random_state=random_state)         ]).sample(frac=1.0, random_state=random_state).reset_index(drop=True)     else:         # Calculate the number of rows to sample         num_rows_to_sample = int(len(brca1_df) * SAMPLE_FRAC)         brca1_df = brca1_df.sample(frac=SAMPLE_FRAC, random_state=random_state).reset_index(drop=True) brca1_df.shape  Out[6]: <pre>(330, 6)</pre> <p>Next, we'll write these to local <code>.fasta</code> files so we can use them for prediction below.</p> In\u00a0[7]: Copied! <pre># Create output directory\noutput_dir = Path(\"brca1_fasta_files\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Save reference and variant sequences to FASTA\nref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\"\nvar_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"\n\n# Track unique sequences\nref_sequences = set()\nvar_sequences = set()\nref_seq_to_name = {}\n# Store unique sequences with metadata for writing\nref_entries = []\nvar_entries = []\nref_names = []\nvar_names = []\n# Collect unique reference and variant sequences\nfor idx, row in brca1_df.iterrows():\n    ref_seq, var_seq = parse_sequences(row['pos'], row['ref'], row['alt'])\n\n    # Add to sets to ensure uniqueness\n    if ref_seq not in ref_sequences:\n        ref_sequences.add(ref_seq)\n        ref_name = f\"BRCA1_ref_pos_{row['pos']}_{row['ref']}_class_{row['class']}\"\n\n        ref_entries.append(\n            f\"&gt;{ref_name}\\n{ref_seq}\\n\"\n        )\n        ref_names.append(ref_name)\n        ref_seq_to_name[ref_seq] = ref_name\n    else:\n        ref_name = ref_seq_to_name[ref_seq]\n        ref_names.append(ref_name)\n    if var_seq not in var_sequences:\n        var_sequences.add(var_seq)\n        var_name = f\"BRCA1_var_pos_{row['pos']}_{row['ref']}to{row['alt']}_class_{row['class']}\"\n\n        var_entries.append(\n            f\"&gt;{var_name}\\n{var_seq}\\n\"\n        )\n        var_names.append(var_name)\n    else:\n        assert False, \"Duplicate variant sequence\"\n\n# Write unique sequences to FASTA files\nwith open(ref_fasta_path, \"w\") as f:\n    f.writelines(ref_entries)\n\nwith open(var_fasta_path, \"w\") as f:\n    f.writelines(var_entries)\n\n\nbrca1_df['ref_fasta_name'] = ref_names\nbrca1_df['var_fasta_name'] = var_names\n\n# Print counts\nprint(f\"Total unique reference sequences: {len(ref_sequences)}\")\nprint(f\"Total unique variant sequences: {len(var_sequences)}\")\n</pre> # Create output directory output_dir = Path(\"brca1_fasta_files\") output_dir.mkdir(parents=True, exist_ok=True)  # Save reference and variant sequences to FASTA ref_fasta_path = output_dir / \"brca1_reference_sequences.fasta\" var_fasta_path = output_dir / \"brca1_variant_sequences.fasta\"  # Track unique sequences ref_sequences = set() var_sequences = set() ref_seq_to_name = {} # Store unique sequences with metadata for writing ref_entries = [] var_entries = [] ref_names = [] var_names = [] # Collect unique reference and variant sequences for idx, row in brca1_df.iterrows():     ref_seq, var_seq = parse_sequences(row['pos'], row['ref'], row['alt'])      # Add to sets to ensure uniqueness     if ref_seq not in ref_sequences:         ref_sequences.add(ref_seq)         ref_name = f\"BRCA1_ref_pos_{row['pos']}_{row['ref']}_class_{row['class']}\"          ref_entries.append(             f\"&gt;{ref_name}\\n{ref_seq}\\n\"         )         ref_names.append(ref_name)         ref_seq_to_name[ref_seq] = ref_name     else:         ref_name = ref_seq_to_name[ref_seq]         ref_names.append(ref_name)     if var_seq not in var_sequences:         var_sequences.add(var_seq)         var_name = f\"BRCA1_var_pos_{row['pos']}_{row['ref']}to{row['alt']}_class_{row['class']}\"          var_entries.append(             f\"&gt;{var_name}\\n{var_seq}\\n\"         )         var_names.append(var_name)     else:         assert False, \"Duplicate variant sequence\"  # Write unique sequences to FASTA files with open(ref_fasta_path, \"w\") as f:     f.writelines(ref_entries)  with open(var_fasta_path, \"w\") as f:     f.writelines(var_entries)   brca1_df['ref_fasta_name'] = ref_names brca1_df['var_fasta_name'] = var_names  # Print counts print(f\"Total unique reference sequences: {len(ref_sequences)}\") print(f\"Total unique variant sequences: {len(var_sequences)}\")   <pre>Total unique reference sequences: 296\nTotal unique variant sequences: 330\n</pre> <p>Then, we load Evo 2 1B model, loading the Evo 2 weights from hugging face.</p> <p>Note - for better performance, load the 7b model by setting <code>MODEL_SIZE=\"7b\"</code> which also works well GPUs that do not support FP8.</p> In\u00a0[8]: Copied! <pre>MODEL_SIZE = \"1b\"  # also try 7b if you have a GPU with more than 32GB of memory\n# Define checkpoint path\ncheckpoint_path = Path(f\"nemo2_evo2_{MODEL_SIZE}_8k\")\n\n# Check if the directory does not exist or is empty\nif not checkpoint_path.exists() or not any(checkpoint_path.iterdir()):\n    !evo2_convert_to_nemo2 --model-path hf://arcinstitute/savanna_evo2_1b_base --model-size {MODEL_SIZE} --output-dir nemo2_evo2_{MODEL_SIZE}_8k\nelse:\n    print(\"Checkpoint directory is not empty. Skipping command.\")\n</pre> MODEL_SIZE = \"1b\"  # also try 7b if you have a GPU with more than 32GB of memory # Define checkpoint path checkpoint_path = Path(f\"nemo2_evo2_{MODEL_SIZE}_8k\")  # Check if the directory does not exist or is empty if not checkpoint_path.exists() or not any(checkpoint_path.iterdir()):     !evo2_convert_to_nemo2 --model-path hf://arcinstitute/savanna_evo2_1b_base --model-size {MODEL_SIZE} --output-dir nemo2_evo2_{MODEL_SIZE}_8k else:     print(\"Checkpoint directory is not empty. Skipping command.\")  <pre>Checkpoint directory is not empty. Skipping command.\n</pre> <p>Next, we score the likelihoods of the reference and variant sequences of each SNV.</p> In\u00a0[9]: Copied! <pre># Define output directories for prediction results\npredict_ref_dir = output_dir / \"reference_predictions\"\npredict_var_dir = output_dir / \"variant_predictions\"\npredict_ref_dir.mkdir(parents=True, exist_ok=True)\npredict_var_dir.mkdir(parents=True, exist_ok=True)\n# Check if FP8 is supported on the current GPU\nimport torch\n\ndef check_fp8_support():\n    \"\"\"\n    Check if FP8 is supported on the current GPU.\n    FP8 requires compute capability 8.9+ (Ada Lovelace/Hopper architecture or newer).\n    \"\"\"\n    if not torch.cuda.is_available():\n        return False, \"CUDA not available\"\n    \n    device_props = torch.cuda.get_device_properties(0)\n    compute_capability = f\"{device_props.major}.{device_props.minor}\"\n    device_name = device_props.name\n    \n    # FP8 is supported on compute capability 8.9+ (Ada Lovelace/Hopper architecture)\n    is_supported = (device_props.major &gt; 8) or (device_props.major == 8 and device_props.minor &gt;= 9)\n    \n    return is_supported, f\"Device: {device_name}, Compute Capability: {compute_capability}\"\n\nfp8_supported, gpu_info = check_fp8_support()\nprint(f\"FP8 Support: {fp8_supported}\")\nprint(gpu_info)\n\n# Note: If FP8 is not supported, you may want to disable it in the model config\n# The Evo2 config has 'use_fp8_input_projections: True' by default\n\nfp8_option = \"--fp8\" if fp8_supported else \"\"\n\n# Update predict commands to run on the full dataset\npredict_ref_command = (\n    f\"predict_evo2 --fasta {ref_fasta_path} --ckpt-dir {checkpoint_path} \"\n    f\"--output-dir {predict_ref_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1 \"\n    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\"\n)\n\npredict_var_command = (\n    f\"predict_evo2 --fasta {var_fasta_path} --ckpt-dir {checkpoint_path} \"\n    f\"--output-dir {predict_var_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1 \"\n    f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\"\n)\n</pre> # Define output directories for prediction results predict_ref_dir = output_dir / \"reference_predictions\" predict_var_dir = output_dir / \"variant_predictions\" predict_ref_dir.mkdir(parents=True, exist_ok=True) predict_var_dir.mkdir(parents=True, exist_ok=True) # Check if FP8 is supported on the current GPU import torch  def check_fp8_support():     \"\"\"     Check if FP8 is supported on the current GPU.     FP8 requires compute capability 8.9+ (Ada Lovelace/Hopper architecture or newer).     \"\"\"     if not torch.cuda.is_available():         return False, \"CUDA not available\"          device_props = torch.cuda.get_device_properties(0)     compute_capability = f\"{device_props.major}.{device_props.minor}\"     device_name = device_props.name          # FP8 is supported on compute capability 8.9+ (Ada Lovelace/Hopper architecture)     is_supported = (device_props.major &gt; 8) or (device_props.major == 8 and device_props.minor &gt;= 9)          return is_supported, f\"Device: {device_name}, Compute Capability: {compute_capability}\"  fp8_supported, gpu_info = check_fp8_support() print(f\"FP8 Support: {fp8_supported}\") print(gpu_info)  # Note: If FP8 is not supported, you may want to disable it in the model config # The Evo2 config has 'use_fp8_input_projections: True' by default  fp8_option = \"--fp8\" if fp8_supported else \"\"  # Update predict commands to run on the full dataset predict_ref_command = (     f\"predict_evo2 --fasta {ref_fasta_path} --ckpt-dir {checkpoint_path} \"     f\"--output-dir {predict_ref_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1 \"     f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\" )  predict_var_command = (     f\"predict_evo2 --fasta {var_fasta_path} --ckpt-dir {checkpoint_path} \"     f\"--output-dir {predict_var_dir} --model-size {MODEL_SIZE} --tensor-parallel-size 1 \"     f\"--pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs {fp8_option}\" ) <pre>FP8 Support: True\nDevice: NVIDIA RTX 6000 Ada Generation, Compute Capability: 8.9\n</pre> In\u00a0[10]: Copied! <pre>print(f\"Running command: {predict_ref_command}\")\n!{predict_ref_command}\n</pre> print(f\"Running command: {predict_ref_command}\") !{predict_ref_command} <pre>Running command: predict_evo2 --fasta brca1_fasta_files/brca1_reference_sequences.fasta --ckpt-dir nemo2_evo2_1b_8k --output-dir brca1_fasta_files/reference_predictions --model-size 1b --tensor-parallel-size 1 --pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs --fp8\n[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n[NeMo W 2025-03-04 01:01:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n[NeMo W 2025-03-04 01:01:11 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Experiments will be logged at /tmp/tmpupzx4lk1/default\n[NeMo W 2025-03-04 01:01:11 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/tmpupzx4lk1\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Using byte-level tokenization\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has data parallel group : [0]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Ranks 0 has data parallel rank: 0\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has context parallel group: [0]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] All context parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Ranks 0 has context parallel rank: 0\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has model parallel group: [0]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] All model parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has embedding group: [0]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] All embedding group ranks: [[0]]\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[NeMo I 2025-03-04 01:01:11 num_microbatches_calculator:228] setting number of microbatches to constant 1\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:01:11 random:220] CPU RNG state changed within GPU RNG context\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n[NeMo W 2025-03-04 01:01:11 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1108204800\n[NeMo I 2025-03-04 01:01:11 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n[NeMo I 2025-03-04 01:01:11 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n    Params for bucket 1 (1108204800 elements):\n    \tmodule.decoder.layers.22.mixer.dense.bias\n    \tmodule.decoder.layers.19.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.16.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.9.mixer.dense.bias\n    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.0.mixer.dense_projection.weight\n    \tmodule.decoder.layers.23.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.20.mixer.dense.weight\n    \tmodule.decoder.layers.15.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.dense.weight\n    \tmodule.decoder.layers.2.mixer.mixer.filter.R\n    \tmodule.decoder.layers.1.mixer.dense_projection.weight\n    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.22.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.18.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.11.mixer.dense.weight\n    \tmodule.decoder.layers.10.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.22.mixer.dense.weight\n    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mixer.dense.weight\n    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.dense_projection.weight\n    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.13.mixer.dense.bias\n    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.8.mixer.mixer.filter.h\n    \tmodule.decoder.layers.6.mixer.mixer.filter.R\n    \tmodule.decoder.layers.3.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.1.mixer.dense.weight\n    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.22.mixer.dense_projection.weight\n    \tmodule.decoder.layers.19.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.p\n    \tmodule.decoder.layers.14.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.23.mixer.mixer.filter.R\n    \tmodule.decoder.layers.21.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.15.mixer.dense.bias\n    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.8.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.6.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.2.mixer.mixer.filter.p\n    \tmodule.decoder.final_norm.weight\n    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.13.mixer.dense.weight\n    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.2.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.24.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.23.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.20.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.15.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.12.mixer.mixer.filter.h\n    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.7.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.5.mixer.dense.bias\n    \tmodule.decoder.layers.21.mixer.dense_projection.weight\n    \tmodule.decoder.layers.18.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.15.mixer.dense.weight\n    \tmodule.decoder.layers.6.mixer.dense_projection.weight\n    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.2.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.20.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.dense.bias\n    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.12.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.9.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.5.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.4.mixer.dense.bias\n    \tmodule.decoder.layers.2.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.23.mixer.dense_projection.weight\n    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.15.mixer.dense_projection.weight\n    \tmodule.decoder.layers.13.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.8.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.19.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.R\n    \tmodule.decoder.layers.14.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.11.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.0.mixer.dense.bias\n    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.20.mixer.mixer.filter.p\n    \tmodule.decoder.layers.19.mixer.dense.weight\n    \tmodule.decoder.layers.18.mixer.dense.bias\n    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.4.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.1.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.21.mixer.dense.weight\n    \tmodule.decoder.layers.16.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.13.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.6.mixer.dense.bias\n    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.21.mixer.dense.bias\n    \tmodule.decoder.layers.19.mixer.dense_projection.weight\n    \tmodule.decoder.layers.14.mixer.dense_projection.weight\n    \tmodule.decoder.layers.12.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.p\n    \tmodule.decoder.layers.7.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.0.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.23.mixer.dense.bias\n    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.18.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.14.mixer.dense.bias\n    \tmodule.decoder.layers.8.mixer.dense.bias\n    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.1.mixer.mixer.filter.h\n    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mixer.dense_projection.weight\n    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.6.mixer.dense.weight\n    \tmodule.decoder.layers.2.mixer.dense_projection.weight\n    \tmodule.decoder.layers.0.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.20.mixer.dense_projection.weight\n    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.8.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.mixer.filter.h\n    \tmodule.decoder.layers.23.mixer.dense.weight\n    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.20.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.18.mixer.dense_projection.weight\n    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.13.mixer.mixer.filter.p\n    \tmodule.decoder.layers.11.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.8.mixer.dense.weight\n    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.22.mixer.mixer.filter.h\n    \tmodule.decoder.layers.20.mixer.mixer.filter.R\n    \tmodule.decoder.layers.14.mixer.dense.weight\n    \tmodule.decoder.layers.12.mixer.dense.bias\n    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.4.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.17.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.13.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.8.mixer.dense_projection.weight\n    \tmodule.decoder.layers.6.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.22.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.16.mixer.dense.bias\n    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.12.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.R\n    \tmodule.decoder.layers.7.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.0.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.23.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.12.mixer.dense.weight\n    \tmodule.decoder.layers.11.mixer.dense.bias\n    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.1.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.21.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.18.mixer.dense.weight\n    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.9.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.6.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.20.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.16.mixer.dense.weight\n    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.12.mixer.dense_projection.weight\n    \tmodule.decoder.layers.7.mixer.dense_projection.weight\n    \tmodule.decoder.layers.5.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.4.mixer.dense.weight\n    \tmodule.decoder.layers.23.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.15.mixer.mixer.filter.h\n    \tmodule.decoder.layers.13.mixer.mixer.filter.R\n    \tmodule.decoder.layers.11.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.7.mixer.dense.bias\n    \tmodule.decoder.layers.2.mixer.dense.weight\n    \tmodule.embedding.word_embeddings.weight\n    \tmodule.decoder.layers.22.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mixer.dense_projection.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.4.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.0.mixer.dense.weight\n    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.15.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.13.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.2.mixer.dense.bias\n    \tmodule.decoder.layers.1.mixer.dense.bias\n    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.11.mixer.dense_projection.weight\n    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.6.mixer.mixer.filter.p\n    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.2.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.20.mixer.dense.bias\n    \tmodule.decoder.layers.19.mixer.mixer.filter.h\n    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.14.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.7.mixer.dense.weight\n    \tmodule.decoder.layers.4.mixer.dense_projection.weight\n    \tmodule.decoder.layers.1.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.23.mixer.mixer.filter.p\n    \tmodule.decoder.layers.21.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.13.mixer.dense_projection.weight\n    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.6.mixer.hyena_proj_conv.short_conv_weight\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Doing selective restore from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n[NeMo I 2025-03-04 01:01:11 nemo_logging:393] Using &lt;megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7330b1b9bec0&gt; dist-ckpt load strategy.\n[WARNING  | py.warnings        ]: /workspaces/bionemo-framework/3rdparty/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n  checkpoint.load_state_dict(\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  device = getattr(value, \"device\", None)\n\n[NeMo I 2025-03-04 01:01:12 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1741050071.495s : Time spent in load_checkpoint: 0.932s\n[NeMo I 2025-03-04 01:01:12 nemo_logging:393] Restoring model weights from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n[NeMo I 2025-03-04 01:01:12 nemo_logging:393] Finished restoring from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False), cleaning up.\n</pre> In\u00a0[11]: Copied! <pre>print(f\"Running command: {predict_var_command}\")\n!{predict_var_command}\n</pre> print(f\"Running command: {predict_var_command}\") !{predict_var_command} <pre>Running command: predict_evo2 --fasta brca1_fasta_files/brca1_variant_sequences.fasta --ckpt-dir nemo2_evo2_1b_8k --output-dir brca1_fasta_files/variant_predictions --model-size 1b --tensor-parallel-size 1 --pipeline-model-parallel-size 1 --context-parallel-size 1 --output-log-prob-seqs --fp8\n[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n[NeMo W 2025-03-04 01:02:34 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n[NeMo W 2025-03-04 01:02:35 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Experiments will be logged at /tmp/tmpf9avvfzw/default\n[NeMo W 2025-03-04 01:02:35 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/tmpf9avvfzw\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Using byte-level tokenization\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has data parallel group : [0]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Ranks 0 has data parallel rank: 0\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has context parallel group: [0]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] All context parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Ranks 0 has context parallel rank: 0\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has model parallel group: [0]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] All model parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has embedding group: [0]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] All embedding group ranks: [[0]]\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[NeMo I 2025-03-04 01:02:35 num_microbatches_calculator:228] setting number of microbatches to constant 1\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-03-04 01:02:35 random:220] CPU RNG state changed within GPU RNG context\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n[NeMo W 2025-03-04 01:02:35 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1108204800\n[NeMo I 2025-03-04 01:02:35 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n[NeMo I 2025-03-04 01:02:35 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n    Params for bucket 1 (1108204800 elements):\n    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.22.mixer.dense_projection.weight\n    \tmodule.decoder.layers.20.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.15.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.12.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.9.mixer.dense.bias\n    \tmodule.decoder.layers.7.mixer.dense.bias\n    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.2.mixer.dense.bias\n    \tmodule.decoder.layers.1.mixer.dense.bias\n    \tmodule.decoder.layers.23.mixer.mixer.filter.R\n    \tmodule.decoder.layers.21.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.18.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.6.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.2.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.1.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.final_norm.weight\n    \tmodule.decoder.layers.23.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.20.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.13.mixer.dense.bias\n    \tmodule.decoder.layers.12.mixer.mixer.filter.h\n    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.7.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.5.mixer.dense.bias\n    \tmodule.decoder.layers.21.mixer.dense_projection.weight\n    \tmodule.decoder.layers.19.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.p\n    \tmodule.decoder.layers.14.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.11.mixer.dense.bias\n    \tmodule.decoder.layers.10.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.6.mixer.dense_projection.weight\n    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.0.mixer.dense_projection.weight\n    \tmodule.decoder.layers.21.mixer.dense.bias\n    \tmodule.decoder.layers.15.mixer.dense.bias\n    \tmodule.decoder.layers.13.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.2.mixer.mixer.filter.R\n    \tmodule.decoder.layers.1.mixer.dense_projection.weight\n    \tmodule.decoder.layers.23.mixer.dense_projection.weight\n    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.13.mixer.dense.weight\n    \tmodule.decoder.layers.8.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.24.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.15.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.11.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.6.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.20.mixer.mixer.filter.p\n    \tmodule.decoder.layers.18.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.15.mixer.dense.weight\n    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.1.mixer.dense.weight\n    \tmodule.decoder.layers.21.mixer.dense.weight\n    \tmodule.decoder.layers.19.mixer.dense.bias\n    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.4.mixer.dense.bias\n    \tmodule.decoder.layers.3.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.20.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.15.mixer.dense_projection.weight\n    \tmodule.decoder.layers.12.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.p\n    \tmodule.decoder.layers.7.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.5.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.2.mixer.mixer.filter.p\n    \tmodule.decoder.layers.23.mixer.dense.bias\n    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.19.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.R\n    \tmodule.decoder.layers.14.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.13.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.2.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.dense.weight\n    \tmodule.decoder.layers.18.mixer.dense.bias\n    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.6.mixer.dense.weight\n    \tmodule.decoder.layers.4.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.13.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.8.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.mixer.filter.h\n    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.2.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.23.mixer.dense.weight\n    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.dense_projection.weight\n    \tmodule.decoder.layers.14.mixer.dense_projection.weight\n    \tmodule.decoder.layers.11.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.8.mixer.dense.weight\n    \tmodule.decoder.layers.6.mixer.dense.bias\n    \tmodule.decoder.layers.2.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.22.mixer.mixer.filter.h\n    \tmodule.decoder.layers.20.mixer.mixer.filter.R\n    \tmodule.decoder.layers.18.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.14.mixer.dense.bias\n    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mixer.dense_projection.weight\n    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.8.mixer.dense_projection.weight\n    \tmodule.decoder.layers.6.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.0.mixer.dense.bias\n    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.22.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.20.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.12.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.R\n    \tmodule.decoder.layers.7.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.1.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.23.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.18.mixer.dense_projection.weight\n    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.13.mixer.mixer.filter.p\n    \tmodule.decoder.layers.12.mixer.dense.weight\n    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n    \tmodule.embedding.word_embeddings.weight\n    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.21.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.14.mixer.dense.weight\n    \tmodule.decoder.layers.9.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.4.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.0.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.20.mixer.dense_projection.weight\n    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.13.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.12.mixer.dense_projection.weight\n    \tmodule.decoder.layers.7.mixer.dense_projection.weight\n    \tmodule.decoder.layers.5.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.1.mixer.mixer.filter.h\n    \tmodule.decoder.layers.23.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.16.mixer.dense.bias\n    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.11.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.8.mixer.dense.bias\n    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.2.mixer.dense_projection.weight\n    \tmodule.decoder.layers.0.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.22.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mixer.dense_projection.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.18.mixer.dense.weight\n    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.17.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.16.mixer.dense.weight\n    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.11.mixer.dense_projection.weight\n    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.6.mixer.mixer.filter.p\n    \tmodule.decoder.layers.4.mixer.dense.weight\n    \tmodule.decoder.layers.20.mixer.dense.bias\n    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.15.mixer.mixer.filter.h\n    \tmodule.decoder.layers.13.mixer.mixer.filter.R\n    \tmodule.decoder.layers.12.mixer.dense.bias\n    \tmodule.decoder.layers.9.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.7.mixer.dense.weight\n    \tmodule.decoder.layers.23.mixer.mixer.filter.p\n    \tmodule.decoder.layers.21.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.6.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.4.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.0.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.22.mixer.dense.bias\n    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.15.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.1.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.23.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.20.mixer.dense.weight\n    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.8.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.5.mixer.dense.weight\n    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.22.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.mixer.filter.h\n    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.14.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.11.mixer.dense.weight\n    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.4.mixer.dense_projection.weight\n    \tmodule.decoder.layers.22.mixer.dense.weight\n    \tmodule.decoder.layers.13.mixer.dense_projection.weight\n    \tmodule.decoder.layers.9.mixer.dense.weight\n    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.dense_projection.weight\n    \tmodule.decoder.layers.2.mixer.dense.weight\n    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.16.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.8.mixer.mixer.filter.h\n    \tmodule.decoder.layers.6.mixer.mixer.filter.R\n    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.0.mixer.dense.weight\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Doing selective restore from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n[NeMo I 2025-03-04 01:02:35 nemo_logging:393] Using &lt;megatron.core.dist_checkpointing.strategies.fully_parallel.FullyParallelLoadStrategyWrapper object at 0x7bffabfab6e0&gt; dist-ckpt load strategy.\n[WARNING  | py.warnings        ]: /workspaces/bionemo-framework/3rdparty/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n  checkpoint.load_state_dict(\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  device = getattr(value, \"device\", None)\n\n[NeMo I 2025-03-04 01:02:36 nemo_logging:393] Global Checkpoint Load : Rank : 0 : Start time : 1741050155.807s : Time spent in load_checkpoint: 0.618s\n[NeMo I 2025-03-04 01:02:36 nemo_logging:393] Restoring model weights from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False)\n[NeMo I 2025-03-04 01:02:36 nemo_logging:393] Finished restoring from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=False), cleaning up.\n</pre> <p>We calculate the change in likelihoods for each variant relative to the likelihood of their respective wild-type sequence.</p> <p>First, we load the prediction files and sequence id maps:</p> In\u00a0[12]: Copied! <pre># Find and load prediction files\nref_pred_files = glob.glob(os.path.join(predict_ref_dir, \"predictions__rank_*.pt\"))\nvar_pred_files = glob.glob(os.path.join(predict_var_dir, \"predictions__rank_*.pt\"))\n\n# Load sequence ID maps (maps sequence ID -&gt; prediction index)\nwith open(os.path.join(predict_ref_dir, \"seq_idx_map.json\"), \"r\") as f:\n    ref_seq_idx_map = json.load(f)\nwith open(os.path.join(predict_var_dir, \"seq_idx_map.json\"), \"r\") as f:\n    var_seq_idx_map = json.load(f)\n\n# Load predictions\nref_preds = torch.load(ref_pred_files[0])\nvar_preds = torch.load(var_pred_files[0])\n</pre> # Find and load prediction files ref_pred_files = glob.glob(os.path.join(predict_ref_dir, \"predictions__rank_*.pt\")) var_pred_files = glob.glob(os.path.join(predict_var_dir, \"predictions__rank_*.pt\"))  # Load sequence ID maps (maps sequence ID -&gt; prediction index) with open(os.path.join(predict_ref_dir, \"seq_idx_map.json\"), \"r\") as f:     ref_seq_idx_map = json.load(f) with open(os.path.join(predict_var_dir, \"seq_idx_map.json\"), \"r\") as f:     var_seq_idx_map = json.load(f)  # Load predictions ref_preds = torch.load(ref_pred_files[0]) var_preds = torch.load(var_pred_files[0]) <p>Then, calculate the delta score:</p> In\u00a0[13]: Copied! <pre># next, calculate change in likelihoods\nref_log_probs = []\nvar_log_probs = []\nfor _, row in brca1_df.iterrows():\n    ref_name = row['ref_fasta_name']\n    var_name = row['var_fasta_name']\n    ref_log_probs.append(ref_preds['log_probs_seqs'][ref_seq_idx_map[ref_name]].item())\n    var_log_probs.append(var_preds['log_probs_seqs'][var_seq_idx_map[var_name]].item())\nbrca1_df['ref_log_probs'] = ref_log_probs\nbrca1_df['var_log_probs'] = var_log_probs\n# ideally probability of a broken variant is lower than a good one. So a bad var - good ref is negative.\nbrca1_df['evo2_delta_score'] = brca1_df['var_log_probs'] - brca1_df['ref_log_probs']\nbrca1_df.head()\n</pre> # next, calculate change in likelihoods ref_log_probs = [] var_log_probs = [] for _, row in brca1_df.iterrows():     ref_name = row['ref_fasta_name']     var_name = row['var_fasta_name']     ref_log_probs.append(ref_preds['log_probs_seqs'][ref_seq_idx_map[ref_name]].item())     var_log_probs.append(var_preds['log_probs_seqs'][var_seq_idx_map[var_name]].item()) brca1_df['ref_log_probs'] = ref_log_probs brca1_df['var_log_probs'] = var_log_probs # ideally probability of a broken variant is lower than a good one. So a bad var - good ref is negative. brca1_df['evo2_delta_score'] = brca1_df['var_log_probs'] - brca1_df['ref_log_probs'] brca1_df.head()  Out[13]: chrom pos ref alt score class ref_fasta_name var_fasta_name ref_log_probs var_log_probs evo2_delta_score 0 17 41199729 C T -2.646816 LOF BRCA1_ref_pos_41199729_C_class_LOF BRCA1_var_pos_41199729_CtoT_class_LOF -0.952360 -0.953044 -0.000684 1 17 41215381 T G -2.352741 LOF BRCA1_ref_pos_41215381_T_class_LOF BRCA1_var_pos_41215381_TtoG_class_LOF -0.848368 -0.848730 -0.000361 2 17 41215390 C A -1.371155 LOF BRCA1_ref_pos_41215390_C_class_LOF BRCA1_var_pos_41215390_CtoA_class_LOF -0.848341 -0.847456 0.000885 3 17 41219688 T A -2.053136 LOF BRCA1_ref_pos_41219688_T_class_LOF BRCA1_var_pos_41219688_TtoA_class_LOF -1.027623 -1.028068 -0.000445 4 17 41219652 C G -2.026390 LOF BRCA1_ref_pos_41219652_C_class_LOF BRCA1_var_pos_41219652_CtoG_class_LOF -1.032667 -1.032678 -0.000011 <p>This delta likelihood should be predictive of how disruptive the SNV is to the protein's function: the lower the delta, the more likely that the SNV is disruptive. We can show this by comparing the distributions of delta likelihoods for the two classes of SNVs (functional/intermediate vs loss-of-function).</p> In\u00a0[14]: Copied! <pre>plt.figure(figsize=(4, 2))\n\n# Plot stripplot of distributions\np = sns.stripplot(\n    data=brca1_df,\n    x='evo2_delta_score',\n    y='class',\n    hue='class',\n    order=['FUNC/INT', 'LOF'],\n    palette=['#777777', 'C3'],\n    size=2,\n    jitter=0.3,\n)\n\n# Mark medians from each distribution\nsns.boxplot(showmeans=True,\n            meanline=True,\n            meanprops={'visible': False},\n            medianprops={'color': 'k', 'ls': '-', 'lw': 2},\n            whiskerprops={'visible': False},\n            zorder=10,\n            x=\"evo2_delta_score\",\n            y=\"class\",\n            data=brca1_df,\n            showfliers=False,\n            showbox=False,\n            showcaps=False,\n            ax=p)\nplt.xlabel('Delta likelihood score, Evo 2')\nplt.ylabel('BRCA1 SNV class')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(4, 2))  # Plot stripplot of distributions p = sns.stripplot(     data=brca1_df,     x='evo2_delta_score',     y='class',     hue='class',     order=['FUNC/INT', 'LOF'],     palette=['#777777', 'C3'],     size=2,     jitter=0.3, )  # Mark medians from each distribution sns.boxplot(showmeans=True,             meanline=True,             meanprops={'visible': False},             medianprops={'color': 'k', 'ls': '-', 'lw': 2},             whiskerprops={'visible': False},             zorder=10,             x=\"evo2_delta_score\",             y=\"class\",             data=brca1_df,             showfliers=False,             showbox=False,             showcaps=False,             ax=p) plt.xlabel('Delta likelihood score, Evo 2') plt.ylabel('BRCA1 SNV class') plt.tight_layout() plt.show() <p>We can also calculate the area under the receiver operating characteristic curve (AUROC) of this zero-shot prediction method. Note that the results are nearly random unless you are on one of the following configurations:</p> <ul> <li><code>--fp8</code> on an fp8 enabled GPU with either the 1b or 7b models. The 40b likely works as well.</li> <li>the 7b model uniquely seems to work well without <code>--fp8</code> so if you are on an older device, the 7b model should produce robust results. Change the <code>MODEL_SIZE</code> earlier in this tutorial and rerun for good results in that case.</li> </ul> In\u00a0[15]: Copied! <pre># Calculate AUROC of zero-shot predictions\n#  class 1 is LOF which is the bad thing. That means we expect this to be more negative.\ny_true = (brca1_df['class'] == 'LOF')\nauroc = roc_auc_score(y_true, -brca1_df['evo2_delta_score'])\nprint(f'Zero-shot prediction AUROC: {auroc:.2}')\n</pre> # Calculate AUROC of zero-shot predictions #  class 1 is LOF which is the bad thing. That means we expect this to be more negative. y_true = (brca1_df['class'] == 'LOF') auroc = roc_auc_score(y_true, -brca1_df['evo2_delta_score']) print(f'Zero-shot prediction AUROC: {auroc:.2}') <pre>Zero-shot prediction AUROC: 0.77\n</pre>"},{"location":"user-guide/examples/bionemo-evo2/evo2_zeroshot_brca/#zero-shot-prediction-of-brca1-variant-effects-with-evo-2","title":"Zero-shot prediction of BRCA1 variant effects with Evo 2\u00b6","text":"<p>Note - this notebook is a reproduction of The Arc Institute\u2019s same-titled notebook here, using the BioNeMo 2 implementation of Evo2.</p> <p>The human\u00a0BRCA1\u00a0gene encodes for a protein that repairs damaged DNA (Moynahan et al., 1999). Certain variants of this gene have been associated with an increased risk of breast and ovarian cancers (Miki et al., 1994). Using Evo 2, we can predict whether a particular single nucleotide variant (SNV) of the\u00a0BRCA1\u00a0gene is likely to be harmful to the protein's function, and thus potentially increase the risk of cancer for the patient with the genetic variant.</p>"},{"location":"user-guide/examples/bionemo-evo2/evo2_zeroshot_brca/#predict-variant-seqs-sample","title":"Predict variant seqs (sample)\u00b6","text":""},{"location":"user-guide/examples/bionemo-evo2/fine-tuning-tutorial/","title":"Fine tuning tutorial","text":"In\u00a0[9]: Copied! <pre># Clean up any prior runs\n!rm -rf preprocessed_data\n!rm -rf preatraining_demo\n!rm -rf nemo2_evo2_1b_8k\n!rm -rf pretraining_demo\n!rm -rf training_data_config.yaml\n!rm -rf preprocess_config.yaml\n!rm -f chr17.fa.gz\n!rm -f chr18.fa.gz\n!rm -f chr21.fa.gz\n!rm -f chr17.fa\n!rm -f chr18.fa\n!rm -f chr21.fa\n!rm -f chr17_18_21.fa\n</pre> # Clean up any prior runs !rm -rf preprocessed_data !rm -rf preatraining_demo !rm -rf nemo2_evo2_1b_8k !rm -rf pretraining_demo !rm -rf training_data_config.yaml !rm -rf preprocess_config.yaml !rm -f chr17.fa.gz !rm -f chr18.fa.gz !rm -f chr21.fa.gz !rm -f chr17.fa !rm -f chr18.fa !rm -f chr21.fa !rm -f chr17_18_21.fa  In\u00a0[2]: Copied! <pre>import os\nconcat_path = \"chr17_18_21.fa\"\nif not os.path.exists(concat_path):\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr17.fa.gz\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr18.fa.gz\n    !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr21.fa.gz\n    !zcat chr17.fa.gz &gt; chr17.fa\n    !zcat chr18.fa.gz &gt; chr18.fa\n    !zcat chr21.fa.gz &gt; chr21.fa\n    !cat chr17.fa chr18.fa chr21.fa &gt; chr17_18_21.fa\n</pre> import os concat_path = \"chr17_18_21.fa\" if not os.path.exists(concat_path):     !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr17.fa.gz     !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr18.fa.gz     !wget https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr21.fa.gz     !zcat chr17.fa.gz &gt; chr17.fa     !zcat chr18.fa.gz &gt; chr18.fa     !zcat chr21.fa.gz &gt; chr21.fa     !cat chr17.fa chr18.fa chr21.fa &gt; chr17_18_21.fa  <pre>--2025-02-25 01:11:46--  https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr17.fa.gz\nResolving hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)... 128.114.119.163\nConnecting to hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)|128.114.119.163|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 25930986 (25M) [application/x-gzip]\nSaving to: \u2018chr17.fa.gz.2\u2019\n\nchr17.fa.gz.2       100%[===================&gt;]  24.73M  82.3MB/s    in 0.3s    \n\n2025-02-25 01:11:49 (82.3 MB/s) - \u2018chr17.fa.gz.2\u2019 saved [25930986/25930986]\n\n--2025-02-25 01:11:49--  https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr18.fa.gz\nResolving hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)... 128.114.119.163\nConnecting to hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)|128.114.119.163|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 25154367 (24M) [application/x-gzip]\nSaving to: \u2018chr18.fa.gz.1\u2019\n\nchr18.fa.gz.1       100%[===================&gt;]  23.99M  54.6MB/s    in 0.4s    \n\n2025-02-25 01:11:50 (54.6 MB/s) - \u2018chr18.fa.gz.1\u2019 saved [25154367/25154367]\n\n--2025-02-25 01:11:50--  https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr21.fa.gz\nResolving hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)... 128.114.119.163\nConnecting to hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)|128.114.119.163|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12709705 (12M) [application/x-gzip]\nSaving to: \u2018chr21.fa.gz.1\u2019\n\nchr21.fa.gz.1       100%[===================&gt;]  12.12M  67.5MB/s    in 0.2s    \n\n2025-02-25 01:11:50 (67.5 MB/s) - \u2018chr21.fa.gz.1\u2019 saved [12709705/12709705]\n\n</pre> In\u00a0[3]: Copied! <pre>full_fasta_path = os.path.abspath(concat_path)\noutput_dir = os.path.abspath(\"preprocessed_data\")\noutput_yaml = f\"\"\"\n- datapaths: [\"{full_fasta_path}\"]\n  output_dir: \"{output_dir}\"\n  output_prefix: chr17_18_21_uint8_distinct\n  train_split: 0.9\n  valid_split: 0.05\n  test_split: 0.05\n  overwrite: True\n  embed_reverse_complement: true\n  random_reverse_complement: 0.0\n  random_lineage_dropout: 0.0\n  include_sequence_id: false\n  transcribe: \"back_transcribe\"\n  force_uppercase: false\n  indexed_dataset_dtype: \"uint8\"\n  tokenizer_type: \"Byte-Level\"\n  vocab_file: null\n  vocab_size: null\n  merges_file: null\n  pretrained_tokenizer_model: null\n  special_tokens: null\n  fast_hf_tokenizer: true\n  append_eod: true\n  enforce_sample_length: null\n  ftfy: false\n  workers: 1\n  preproc_concurrency: 100000\n  chunksize: 25\n  drop_empty_sequences: true\n  nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.\n  seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout.\n\"\"\"\nwith open(\"preprocess_config.yaml\", \"w\") as f:\n    print(output_yaml, file=f)\n</pre> full_fasta_path = os.path.abspath(concat_path) output_dir = os.path.abspath(\"preprocessed_data\") output_yaml = f\"\"\" - datapaths: [\"{full_fasta_path}\"]   output_dir: \"{output_dir}\"   output_prefix: chr17_18_21_uint8_distinct   train_split: 0.9   valid_split: 0.05   test_split: 0.05   overwrite: True   embed_reverse_complement: true   random_reverse_complement: 0.0   random_lineage_dropout: 0.0   include_sequence_id: false   transcribe: \"back_transcribe\"   force_uppercase: false   indexed_dataset_dtype: \"uint8\"   tokenizer_type: \"Byte-Level\"   vocab_file: null   vocab_size: null   merges_file: null   pretrained_tokenizer_model: null   special_tokens: null   fast_hf_tokenizer: true   append_eod: true   enforce_sample_length: null   ftfy: false   workers: 1   preproc_concurrency: 100000   chunksize: 25   drop_empty_sequences: true   nnn_filter: false  # If you split your fasta on NNN (in human these are contigs), then you should set this to true.   seed: 12342  # Not relevant because we are not using random reverse complement or lineage dropout. \"\"\" with open(\"preprocess_config.yaml\", \"w\") as f:     print(output_yaml, file=f)  In\u00a0[4]: Copied! <pre>!preprocess_evo2 --config preprocess_config.yaml\n</pre> !preprocess_evo2 --config preprocess_config.yaml <pre>[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n\n[NeMo I 2025-02-25 01:12:03 nemo_logging:393] Using byte-level tokenization\n[NeMo I 2025-02-25 01:12:03 nemo_logging:393] Created temporary binary datasets: /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_train.bin.tmp /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_val.bin.tmp /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_test.bin.tmp\n[NeMo I 2025-02-25 01:12:32 nemo_logging:393] Average preprocessing time per sequence: 1.337763786315918\n[NeMo I 2025-02-25 01:12:32 nemo_logging:393] Average indexing time per sequence: 3.9368359645207724\n[NeMo I 2025-02-25 01:12:32 nemo_logging:393] Number of sequences processed: 6\n[NeMo I 2025-02-25 01:12:32 nemo_logging:393] Finished preprocessing chr17_18_21_uint8_distinct ([PosixPath('/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/chr17_18_21.fa')]) in 28.605 seconds with 1 workers.\n</pre> In\u00a0[5]: Copied! <pre>!ls -lh preprocessed_data/\n</pre> !ls -lh preprocessed_data/ <pre>total 402M\n-rw-r--r-- 1 ubuntu ubuntu 159M Feb 25 01:12 chr17_18_21_uint8_distinct_byte-level_test.bin\n-rw-r--r-- 1 ubuntu ubuntu   82 Feb 25 01:12 chr17_18_21_uint8_distinct_byte-level_test.idx\n-rw-r--r-- 1 ubuntu ubuntu 154M Feb 25 01:12 chr17_18_21_uint8_distinct_byte-level_train.bin\n-rw-r--r-- 1 ubuntu ubuntu   82 Feb 25 01:12 chr17_18_21_uint8_distinct_byte-level_train.idx\n-rw-r--r-- 1 ubuntu ubuntu  90M Feb 25 01:12 chr17_18_21_uint8_distinct_byte-level_val.bin\n-rw-r--r-- 1 ubuntu ubuntu   82 Feb 25 01:12 chr17_18_21_uint8_distinct_byte-level_val.idx\n</pre> In\u00a0[6]: Copied! <pre>!evo2_convert_to_nemo2 \\\n  --model-path hf://arcinstitute/savanna_evo2_1b_base \\\n  --model-size 1b --output-dir nemo2_evo2_1b_8k\n</pre> !evo2_convert_to_nemo2 \\   --model-path hf://arcinstitute/savanna_evo2_1b_base \\   --model-size 1b --output-dir nemo2_evo2_1b_8k <pre>[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n[NeMo W 2025-02-25 01:12:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Using byte-level tokenization\n[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: False\n[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has data parallel group : [0]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Ranks 0 has data parallel rank: 0\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has context parallel group: [0]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] All context parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Ranks 0 has context parallel rank: 0\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has model parallel group: [0]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] All model parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has embedding group: [0]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] All embedding group ranks: [[0]]\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\ndistributed_backend=gloo\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n\n[NeMo I 2025-02-25 01:12:48 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[WARNING  | megatron.core.tensor_parallel.random]: CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:12:49 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n[NeMo W 2025-02-25 01:12:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[NeMo I 2025-02-25 01:13:03 nemo_logging:393] Converted Hyena model to Nemo, model saved to nemo2_evo2_1b_8k\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1074: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n  return isinstance(obj, torch.Tensor)\n\n</pre> In\u00a0[7]: Copied! <pre>from pathlib import Path\noutput_pfx = str(Path(os.path.abspath(\"preprocessed_data\"))/\"chr17_18_21_uint8_distinct_byte-level\")\noutput_yaml = f\"\"\"\n- dataset_prefix: {output_pfx}_train\n  dataset_split: train\n  dataset_weight: 1.0\n- dataset_prefix: {output_pfx}_val\n  dataset_split: validation\n  dataset_weight: 1.0\n- dataset_prefix: {output_pfx}_test\n  dataset_split: test\n  dataset_weight: 1.0\n\"\"\"\nwith open(\"training_data_config.yaml\", \"w\") as f:\n    print(output_yaml, file=f)\n</pre> from pathlib import Path output_pfx = str(Path(os.path.abspath(\"preprocessed_data\"))/\"chr17_18_21_uint8_distinct_byte-level\") output_yaml = f\"\"\" - dataset_prefix: {output_pfx}_train   dataset_split: train   dataset_weight: 1.0 - dataset_prefix: {output_pfx}_val   dataset_split: validation   dataset_weight: 1.0 - dataset_prefix: {output_pfx}_test   dataset_split: test   dataset_weight: 1.0 \"\"\" with open(\"training_data_config.yaml\", \"w\") as f:     print(output_yaml, file=f) In\u00a0[8]: Copied! <pre># For evo2 training and fine-tuning follow the same set of steps, so we use the same train_evo2 command.\n#  the big difference is the --ckpt-dir argument which points to a pre-existing checkpoint from some other training run.\n!train_evo2 \\\n    -d training_data_config.yaml \\\n    --dataset-dir {preprocessed_data} \\\n    --experiment-dir pretraining_demo \\\n    --model-size 1b \\\n    --devices 1 \\\n    --num-nodes 1 \\\n    --seq-length 1024 \\\n    --micro-batch-size 2 \\\n    --lr 0.0001 \\\n    --warmup-steps 5 \\\n    --max-steps 100 \\\n    --ckpt-dir nemo2_evo2_1b_8k \\\n    --clip-grad 1 \\\n    --wd 0.01 \\\n    --activation-checkpoint-recompute-num-layers 1 \\\n    --val-check-interval 50 \\\n    --ckpt-async-save \\\n    --no-wandb\n</pre> # For evo2 training and fine-tuning follow the same set of steps, so we use the same train_evo2 command. #  the big difference is the --ckpt-dir argument which points to a pre-existing checkpoint from some other training run. !train_evo2 \\     -d training_data_config.yaml \\     --dataset-dir {preprocessed_data} \\     --experiment-dir pretraining_demo \\     --model-size 1b \\     --devices 1 \\     --num-nodes 1 \\     --seq-length 1024 \\     --micro-batch-size 2 \\     --lr 0.0001 \\     --warmup-steps 5 \\     --max-steps 100 \\     --ckpt-dir nemo2_evo2_1b_8k \\     --clip-grad 1 \\     --wd 0.01 \\     --activation-checkpoint-recompute-num-layers 1 \\     --val-check-interval 50 \\     --ckpt-async-save \\     --no-wandb <pre>[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n[NeMo W 2025-02-25 01:13:17 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Using byte-level tokenization\n[WARNING  | py.warnings        ]: /workspaces/bionemo-framework/3rdparty/NeMo/nemo/collections/llm/gpt/data/pre_training.py:190: UserWarning: split='900,50,50' will be ignored since datasets are being created from 3 separate distributions.\n  warnings.warn(\n\n[INFO     | pytorch_lightning.utilities.rank_zero]: Trainer already configured with model summary callbacks: [&lt;class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'&gt;]. Skipping setting a default `ModelSummary` callback.\n[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n[NeMo W 2025-02-25 01:13:19 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Experiments will be logged at pretraining_demo/default\n[NeMo W 2025-02-25 01:13:19 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to pretraining_demo/dummy\n[NeMo W 2025-02-25 01:13:19 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :pretraining_demo. Training from scratch.\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has data parallel group : [0]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Ranks 0 has data parallel rank: 0\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has context parallel group: [0]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] All context parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Ranks 0 has context parallel rank: 0\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has model parallel group: [0]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] All model parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has embedding group: [0]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] All embedding group ranks: [[0]]\n[NeMo I 2025-02-25 01:13:19 nemo_logging:393] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[NeMo I 2025-02-25 01:13:20 utils:302] Building Evo2Dataset splits with sizes=[200, 120, 2] and config=GPTDatasetConfig(random_seed=1234, sequence_length=1024, blend=None, blend_per_split=[(['/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_train'], [1.0]), (['/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_val'], [1.0]), (['/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_test'], [1.0])], split=None, split_matrix=None, num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=&lt;nemo.collections.common.tokenizers.bytelevel_tokenizers.ByteLevelTokenizer object at 0x7e5db038ffe0&gt;, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)\n[NeMo I 2025-02-25 01:13:20 utils:302] Load the _IndexReader from /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_train.idx\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the sequence lengths\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the sequence pointers\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the document indices\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of sequences: 2\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of documents: 2\n[NeMo I 2025-02-25 01:13:20 utils:302] Build and save the Evo2Dataset train indices\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of samples: 156979\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of epochs: 1\n[NeMo I 2025-02-25 01:13:20 utils:302] Load the _IndexReader from /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_val.idx\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the sequence lengths\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the sequence pointers\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the document indices\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of sequences: 2\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of documents: 2\n[NeMo I 2025-02-25 01:13:20 utils:302] Build and save the Evo2Dataset valid indices\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of samples: 91230\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of epochs: 1\n[NeMo I 2025-02-25 01:13:20 utils:302] Load the _IndexReader from /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/preprocessed_data/chr17_18_21_uint8_distinct_byte-level_test.idx\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the sequence lengths\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the sequence pointers\n[NeMo I 2025-02-25 01:13:20 utils:302] \tExtract the document indices\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of sequences: 2\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of documents: 2\n[NeMo I 2025-02-25 01:13:20 utils:302] Build and save the Evo2Dataset test indices\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of samples: 162612\n[NeMo I 2025-02-25 01:13:20 utils:302] &gt; total number of epochs: 1\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo exists and is not empty.\n\n[NeMo I 2025-02-25 01:13:20 nemo_logging:393] Padded vocab_size: 512, original vocab_size: 512, dummy tokens: 0.\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\n[NeMo W 2025-02-25 01:13:20 random:220] CPU RNG state changed within GPU RNG context\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo I 2025-02-25 01:13:20 nemo_logging:393] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.\n[NeMo I 2025-02-25 01:13:20 num_microbatches_calculator:228] setting number of microbatches to constant 1\n[NeMo I 2025-02-25 01:13:20 nemo_logging:393]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1108204800\n[NeMo I 2025-02-25 01:13:20 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, average_in_collective=True, fp8_param_gather=False)\n[NeMo I 2025-02-25 01:13:20 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n    Params for bucket 1 (1108204800 elements):\n    \tmodule.decoder.layers.23.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.15.mixer.mixer.filter.h\n    \tmodule.decoder.layers.13.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.10.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.7.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.1.mixer.dense.bias\n    \tmodule.decoder.layers.23.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.21.mixer.dense.weight\n    \tmodule.decoder.layers.19.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.16.mixer.dense.weight\n    \tmodule.decoder.layers.13.mixer.dense.bias\n    \tmodule.decoder.layers.11.mixer.dense_projection.weight\n    \tmodule.decoder.layers.8.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.6.mixer.mixer.filter.p\n    \tmodule.decoder.layers.4.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.17.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.12.mixer.dense.bias\n    \tmodule.decoder.layers.2.mixer.dense.bias\n    \tmodule.decoder.layers.23.mixer.mixer.filter.p\n    \tmodule.decoder.layers.21.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.13.mixer.dense_projection.weight\n    \tmodule.decoder.layers.10.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.8.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.6.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.3.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.1.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.19.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.14.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.11.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.5.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.2.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.24.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.23.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.20.mixer.dense.weight\n    \tmodule.decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.15.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.12.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.dense.weight\n    \tmodule.decoder.layers.17.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.23.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.22.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.8.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.7.mixer.dense.bias\n    \tmodule.decoder.layers.0.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.22.mixer.dense.weight\n    \tmodule.decoder.layers.14.mixer.dense.weight\n    \tmodule.decoder.layers.12.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.9.mixer.dense.weight\n    \tmodule.decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.dense_projection.weight\n    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.mixer.filter.h\n    \tmodule.decoder.layers.16.mixer.mixer.filter.R\n    \tmodule.decoder.layers.15.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.10.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.4.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.2.mixer.dense_projection.weight\n    \tmodule.decoder.layers.24.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.22.mixer.dense_projection.weight\n    \tmodule.decoder.layers.20.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.14.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.4.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.0.mixer.dense.bias\n    \tmodule.decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.21.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.12.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.8.mixer.mixer.filter.h\n    \tmodule.decoder.layers.6.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.3.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.1.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.21.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.13.mixer.dense.weight\n    \tmodule.decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.6.mixer.dense.bias\n    \tmodule.decoder.layers.4.mixer.dense_projection.weight\n    \tmodule.decoder.layers.23.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.20.mixer.mixer.filter.R\n    \tmodule.decoder.layers.18.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.15.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.10.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.5.mixer.dense.bias\n    \tmodule.decoder.layers.2.mixer.dense.weight\n    \tmodule.decoder.layers.0.mixer.dense.weight\n    \tmodule.decoder.layers.23.mixer.dense.bias\n    \tmodule.decoder.layers.21.mixer.dense_projection.weight\n    \tmodule.decoder.layers.19.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.16.mixer.mixer.filter.p\n    \tmodule.decoder.layers.15.mixer.dense.weight\n    \tmodule.decoder.layers.6.mixer.dense_projection.weight\n    \tmodule.decoder.layers.3.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.3.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.22.mixer.dense.bias\n    \tmodule.decoder.layers.20.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.12.mixer.mixer.filter.h\n    \tmodule.decoder.layers.9.mixer.mixer.filter.R\n    \tmodule.decoder.layers.7.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.4.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.0.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.23.mixer.dense_projection.weight\n    \tmodule.decoder.layers.20.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.18.mixer.dense.weight\n    \tmodule.decoder.layers.15.mixer.dense_projection.weight\n    \tmodule.decoder.layers.13.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.8.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.5.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.2.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.21.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.16.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.14.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.9.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.22.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.20.mixer.mixer.filter.p\n    \tmodule.decoder.layers.18.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.14.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.7.mixer.dense.weight\n    \tmodule.decoder.layers.5.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.4.mixer.dense.bias\n    \tmodule.decoder.layers.1.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.16.mixer.dense_projection.weight\n    \tmodule.decoder.layers.13.mixer.mixer.filter.R\n    \tmodule.decoder.layers.11.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.3.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.17.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.24.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.22.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.20.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.14.mixer.dense_projection.weight\n    \tmodule.decoder.layers.12.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.9.mixer.mixer.filter.p\n    \tmodule.decoder.layers.7.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.2.mixer.mixer.filter.R\n    \tmodule.decoder.layers.20.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.19.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.15.mixer.dense.bias\n    \tmodule.decoder.layers.13.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.5.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.1.mixer.dense_projection.weight\n    \tmodule.decoder.layers.18.mixer.dense.bias\n    \tmodule.decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.dense.weight\n    \tmodule.decoder.layers.13.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.11.mixer.dense.weight\n    \tmodule.decoder.layers.9.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.6.mixer.dense.weight\n    \tmodule.decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.22.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.21.mixer.dense.bias\n    \tmodule.decoder.layers.14.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.9.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.8.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.2.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n    \tmodule.embedding.word_embeddings.weight\n    \tmodule.decoder.layers.23.mixer.dense.weight\n    \tmodule.decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.19.mixer.dense_projection.weight\n    \tmodule.decoder.layers.16.mixer.dense.bias\n    \tmodule.decoder.layers.13.mixer.mixer.filter.p\n    \tmodule.decoder.layers.11.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.8.mixer.dense.weight\n    \tmodule.decoder.layers.1.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.1.mixer.dense.weight\n    \tmodule.decoder.layers.1.mixer.mixer.filter.h\n    \tmodule.decoder.final_norm.weight\n    \tmodule.decoder.layers.24.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.18.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.5.mixer.mixer.filter.h\n    \tmodule.decoder.layers.2.mixer.mixer.filter.p\n    \tmodule.decoder.layers.18.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.16.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.15.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.13.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.10.self_attention.linear_qkv.weight\n    \tmodule.decoder.layers.8.mixer.dense_projection.weight\n    \tmodule.decoder.layers.6.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n    \tmodule.decoder.layers.22.mixer.mixer.filter.h\n    \tmodule.decoder.layers.20.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.13.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.12.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.7.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.23.mixer.mixer.filter.gamma\n    \tmodule.decoder.layers.20.mixer.dense.bias\n    \tmodule.decoder.layers.18.mixer.dense_projection.weight\n    \tmodule.decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.12.mixer.dense.weight\n    \tmodule.decoder.layers.11.mixer.dense.bias\n    \tmodule.decoder.layers.7.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.2.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.0.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.24.self_attention.linear_proj.weight\n    \tmodule.decoder.layers.19.mixer.dense.bias\n    \tmodule.decoder.layers.15.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.14.mixer.dense.bias\n    \tmodule.decoder.layers.9.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.6.mixer.mixer.filter.R\n    \tmodule.decoder.layers.4.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.1.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.17.self_attention.linear_proj.bias\n    \tmodule.decoder.layers.17.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.20.mixer.dense_projection.weight\n    \tmodule.decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.12.mixer.dense_projection.weight\n    \tmodule.decoder.layers.9.mixer.dense.bias\n    \tmodule.decoder.layers.7.mixer.dense_projection.weight\n    \tmodule.decoder.layers.5.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.0.mixer.dense_projection.weight\n    \tmodule.decoder.layers.23.mixer.mixer.filter.R\n    \tmodule.decoder.layers.21.mixer.mixer.short_conv.short_conv_weight\n    \tmodule.decoder.layers.18.mlp.linear_fc1.weight\n    \tmodule.decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.11.mixer.dense_projection.layer_norm_weight\n    \tmodule.decoder.layers.8.mixer.dense.bias\n    \tmodule.decoder.layers.6.mixer.mixer.conv_bias\n    \tmodule.decoder.layers.2.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.22.mixer.hyena_proj_conv.short_conv_weight\n    \tmodule.decoder.layers.19.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n    \tmodule.decoder.layers.11.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.9.mixer.dense_projection.weight\n    \tmodule.decoder.layers.6.mlp.linear_fc2.weight\n    \tmodule.decoder.layers.4.mixer.dense.weight\n    \tmodule.decoder.layers.2.mixer.mixer.filter.gamma\n[NeMo I 2025-02-25 01:13:20 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n[NeMo I 2025-02-25 01:13:20 nemo_logging:393] Doing selective restore from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n[WARNING  | py.warnings        ]: /workspaces/bionemo-framework/3rdparty/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n  checkpoint.load_state_dict(\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/torch/distributed/checkpoint/planner_helpers.py:316: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  device = getattr(value, \"device\", None)\n\n[NeMo I 2025-02-25 01:13:21 nemo_logging:393] Restoring model weights from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True)\n[NeMo I 2025-02-25 01:13:21 nemo_logging:393] Finished restoring from RestoreConfig(path='nemo2_evo2_1b_8k', adapter_path=None, load_model_state=True, load_optim_state=False, load_artifacts=True), cleaning up.\n\u250f\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   \u2503 Name                                \u2503 Type              \u2503 Params \u2503 Mode  \u2503\n\u2521\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0 \u2502 module                              \u2502 DDP               \u2502  1.1 B \u2502 train \u2502\n\u2502 1 \u2502 module.module                       \u2502 Float16Module     \u2502  1.1 B \u2502 train \u2502\n\u2502 2 \u2502 module.module.module                \u2502 HyenaModel        \u2502  1.1 B \u2502 train \u2502\n\u2502 3 \u2502 module.module.module.embedding      \u2502 LanguageModelEmb\u2026 \u2502  983 K \u2502 train \u2502\n\u2502 4 \u2502 module.module.module.rotary_pos_emb \u2502 RotaryEmbedding   \u2502      0 \u2502 train \u2502\n\u2502 5 \u2502 module.module.module.decoder        \u2502 HyenaStack        \u2502  1.1 B \u2502 train \u2502\n\u2502 6 \u2502 module.module.module.output_layer   \u2502 ColumnParallelLi\u2026 \u2502      0 \u2502 train \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTrainable params: 1.1 B                                                         \nNon-trainable params: 0                                                         \nTotal params: 1.1 B                                                             \nTotal estimated model params size (MB): 4.4 K                                   \nModules in train mode: 356                                                      \nModules in eval mode: 0                                                         \n[NeMo W 2025-02-25 01:13:30 rerun_state_machine:1264] Implicit initialization of Rerun State Machine!\n[NeMo W 2025-02-25 01:13:30 rerun_state_machine:239] RerunStateMachine initialized in mode RerunMode.DISABLED\nTraining epoch 0, iteration 0/99 | lr: 0 | global_batch_size: 2 | global_step: 0 | reduced_train_loss: 1.246 | train_step_timing in s: 9.091\nTraining epoch 0, iteration 1/99 | lr: 2e-05 | global_batch_size: 2 | global_step: 1 | reduced_train_loss: 1.322 | train_step_timing in s: 1.682 | consumed_samples: 4\nTraining epoch 0, iteration 2/99 | lr: 4e-05 | global_batch_size: 2 | global_step: 2 | reduced_train_loss: 1.217 | train_step_timing in s: 0.4297 | consumed_samples: 6\nTraining epoch 0, iteration 3/99 | lr: 6e-05 | global_batch_size: 2 | global_step: 3 | reduced_train_loss: 1.277 | train_step_timing in s: 0.4295 | consumed_samples: 8\nTraining epoch 0, iteration 4/99 | lr: 8e-05 | global_batch_size: 2 | global_step: 4 | reduced_train_loss: 1.3 | train_step_timing in s: 0.4304 | consumed_samples: 10\nTraining epoch 0, iteration 5/99 | lr: 0.0001 | global_batch_size: 2 | global_step: 5 | reduced_train_loss: 1.309 | train_step_timing in s: 0.4296 | consumed_samples: 12\nTraining epoch 0, iteration 6/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 6 | reduced_train_loss: 1.062 | train_step_timing in s: 0.4301 | consumed_samples: 14\nTraining epoch 0, iteration 7/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 7 | reduced_train_loss: 1.287 | train_step_timing in s: 0.4293 | consumed_samples: 16\nTraining epoch 0, iteration 8/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 8 | reduced_train_loss: 1.292 | train_step_timing in s: 0.4287 | consumed_samples: 18\nTraining epoch 0, iteration 9/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 9 | reduced_train_loss: 1.274 | train_step_timing in s: 0.4288 | consumed_samples: 20\nTraining epoch 0, iteration 10/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 10 | reduced_train_loss: 1.131 | train_step_timing in s: 0.4289 | consumed_samples: 22\nTraining epoch 0, iteration 11/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 11 | reduced_train_loss: 1.243 | train_step_timing in s: 0.4298 | consumed_samples: 24\nTraining epoch 0, iteration 12/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 12 | reduced_train_loss: 1.226 | train_step_timing in s: 0.4305 | consumed_samples: 26\nTraining epoch 0, iteration 13/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 13 | reduced_train_loss: 1.316 | train_step_timing in s: 0.429 | consumed_samples: 28\nTraining epoch 0, iteration 14/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 14 | reduced_train_loss: 1.263 | train_step_timing in s: 0.4286 | consumed_samples: 30\nTraining epoch 0, iteration 15/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 15 | reduced_train_loss: 1.305 | train_step_timing in s: 0.43 | consumed_samples: 32\nTraining epoch 0, iteration 16/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 16 | reduced_train_loss: 1.286 | train_step_timing in s: 0.4297 | consumed_samples: 34\nTraining epoch 0, iteration 17/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 17 | reduced_train_loss: 1.272 | train_step_timing in s: 0.4298 | consumed_samples: 36\nTraining epoch 0, iteration 18/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 18 | reduced_train_loss: 1.289 | train_step_timing in s: 0.4294 | consumed_samples: 38\nTraining epoch 0, iteration 19/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 19 | reduced_train_loss: 1.273 | train_step_timing in s: 0.4304 | consumed_samples: 40\nTraining epoch 0, iteration 20/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 20 | reduced_train_loss: 0.6654 | train_step_timing in s: 0.4304 | consumed_samples: 42\nTraining epoch 0, iteration 21/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 21 | reduced_train_loss: 1.213 | train_step_timing in s: 0.4297 | consumed_samples: 44\nTraining epoch 0, iteration 22/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 22 | reduced_train_loss: 1.289 | train_step_timing in s: 0.4305 | consumed_samples: 46\nTraining epoch 0, iteration 23/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 23 | reduced_train_loss: 1.304 | train_step_timing in s: 0.4312 | consumed_samples: 48\nTraining epoch 0, iteration 24/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 24 | reduced_train_loss: 1.264 | train_step_timing in s: 0.4316 | consumed_samples: 50\nTraining epoch 0, iteration 25/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 25 | reduced_train_loss: 1.257 | train_step_timing in s: 0.4316 | consumed_samples: 52\nTraining epoch 0, iteration 26/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 26 | reduced_train_loss: 1.295 | train_step_timing in s: 0.4309 | consumed_samples: 54\nTraining epoch 0, iteration 27/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 27 | reduced_train_loss: 1.305 | train_step_timing in s: 0.4309 | consumed_samples: 56\nTraining epoch 0, iteration 28/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 28 | reduced_train_loss: 1.324 | train_step_timing in s: 0.4322 | consumed_samples: 58\nTraining epoch 0, iteration 29/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 29 | reduced_train_loss: 1.311 | train_step_timing in s: 0.4309 | consumed_samples: 60\nTraining epoch 0, iteration 30/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 30 | reduced_train_loss: 1.334 | train_step_timing in s: 0.4308 | consumed_samples: 62\nTraining epoch 0, iteration 31/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 31 | reduced_train_loss: 0.709 | train_step_timing in s: 0.4315 | consumed_samples: 64\nTraining epoch 0, iteration 32/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 32 | reduced_train_loss: 1.262 | train_step_timing in s: 0.4312 | consumed_samples: 66\nTraining epoch 0, iteration 33/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 33 | reduced_train_loss: 1.332 | train_step_timing in s: 0.4318 | consumed_samples: 68\nTraining epoch 0, iteration 34/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 34 | reduced_train_loss: 1.272 | train_step_timing in s: 0.4318 | consumed_samples: 70\nTraining epoch 0, iteration 35/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 35 | reduced_train_loss: 1.249 | train_step_timing in s: 0.4322 | consumed_samples: 72\nTraining epoch 0, iteration 36/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 36 | reduced_train_loss: 1.28 | train_step_timing in s: 0.4311 | consumed_samples: 74\nTraining epoch 0, iteration 37/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 37 | reduced_train_loss: 1.321 | train_step_timing in s: 0.4313 | consumed_samples: 76\nTraining epoch 0, iteration 38/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 38 | reduced_train_loss: 1.293 | train_step_timing in s: 0.4321 | consumed_samples: 78\nTraining epoch 0, iteration 39/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 39 | reduced_train_loss: 1.279 | train_step_timing in s: 0.4316 | consumed_samples: 80\nTraining epoch 0, iteration 40/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 40 | reduced_train_loss: 1.081 | train_step_timing in s: 0.4306 | consumed_samples: 82\nTraining epoch 0, iteration 41/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 41 | reduced_train_loss: 1.284 | train_step_timing in s: 0.4313 | consumed_samples: 84\nTraining epoch 0, iteration 42/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 42 | reduced_train_loss: 1.305 | train_step_timing in s: 0.4307 | consumed_samples: 86\nTraining epoch 0, iteration 43/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 43 | reduced_train_loss: 1.265 | train_step_timing in s: 0.4307 | consumed_samples: 88\nTraining epoch 0, iteration 44/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 44 | reduced_train_loss: 1.296 | train_step_timing in s: 0.4335 | consumed_samples: 90\nTraining epoch 0, iteration 45/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 45 | reduced_train_loss: 1.313 | train_step_timing in s: 0.4335 | consumed_samples: 92\nTraining epoch 0, iteration 46/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 46 | reduced_train_loss: 1.304 | train_step_timing in s: 0.4326 | consumed_samples: 94\nTraining epoch 0, iteration 47/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 47 | reduced_train_loss: 1.299 | train_step_timing in s: 0.4329 | consumed_samples: 96\nTraining epoch 0, iteration 48/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 48 | reduced_train_loss: 1.321 | train_step_timing in s: 0.4335 | consumed_samples: 98\nTraining epoch 0, iteration 49/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 49 | reduced_train_loss: 1.281 | train_step_timing in s: 0.4338 | consumed_samples: 100\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['lr-McoreOpt/pg1', 'lr-McoreOpt/pg2', 'lr', 'global_batch_size', 'global_step', 'step', 'reduced_train_loss', 'grad_norm', 'num_zeros_in_grad', 'train_step_timing in s', 'consumed_samples', 'epoch']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n\n[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 49: 'val_loss' was not in top 5\n[WARNING  | bitsandbytes.cextension]: Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so')\n[WARNING  | bitsandbytes.cextension]: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n[NeMo W 2025-02-25 01:14:02 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n\n[NeMo I 2025-02-25 01:14:17 nemo_logging:393] Scheduled async checkpoint save for /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo/default--val_loss=0.0000-epoch=0-consumed_samples=100.0-last.ckpt\nValidation: iteration 1/20\nValidation: iteration 2/20\nValidation: iteration 3/20\nValidation: iteration 4/20\nValidation: iteration 5/20\nValidation: iteration 6/20\nValidation: iteration 7/20\nValidation: iteration 8/20\nValidation: iteration 9/20\nValidation: iteration 10/20\nValidation: iteration 11/20\nValidation: iteration 12/20\nValidation: iteration 13/20\nValidation: iteration 14/20\nValidation: iteration 15/20\nValidation: iteration 16/20\nValidation: iteration 17/20\nValidation: iteration 18/20\nValidation: iteration 19/20\nValidation: iteration 20/20\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('global_batch_size', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n\n[WARNING  | py.warnings        ]: /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n\nTraining epoch 0, iteration 50/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 50 | reduced_train_loss: 1.316 | train_step_timing in s: 0.4343 | consumed_samples: 102 | val_loss: 1.049\nTraining epoch 0, iteration 51/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 51 | reduced_train_loss: 1.151 | train_step_timing in s: 0.4323 | consumed_samples: 104 | val_loss: 1.049\nTraining epoch 0, iteration 52/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 52 | reduced_train_loss: 1.255 | train_step_timing in s: 0.432 | consumed_samples: 106 | val_loss: 1.049\nTraining epoch 0, iteration 53/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 53 | reduced_train_loss: 1.302 | train_step_timing in s: 0.4316 | consumed_samples: 108 | val_loss: 1.049\nTraining epoch 0, iteration 54/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 54 | reduced_train_loss: 1.315 | train_step_timing in s: 0.4319 | consumed_samples: 110 | val_loss: 1.049\nTraining epoch 0, iteration 55/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 55 | reduced_train_loss: 1.315 | train_step_timing in s: 0.4194 | consumed_samples: 112 | val_loss: 1.049\nTraining epoch 0, iteration 56/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 56 | reduced_train_loss: 1.302 | train_step_timing in s: 0.4328 | consumed_samples: 114 | val_loss: 1.049\nTraining epoch 0, iteration 57/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 57 | reduced_train_loss: 1.239 | train_step_timing in s: 0.4334 | consumed_samples: 116 | val_loss: 1.049\nTraining epoch 0, iteration 58/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 58 | reduced_train_loss: 1.325 | train_step_timing in s: 0.4343 | consumed_samples: 118 | val_loss: 1.049\nTraining epoch 0, iteration 59/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 59 | reduced_train_loss: 0.7567 | train_step_timing in s: 0.4317 | consumed_samples: 120 | val_loss: 1.049\nTraining epoch 0, iteration 60/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 60 | reduced_train_loss: 1.289 | train_step_timing in s: 0.432 | consumed_samples: 122 | val_loss: 1.049\nTraining epoch 0, iteration 61/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 61 | reduced_train_loss: 1.31 | train_step_timing in s: 0.4225 | consumed_samples: 124 | val_loss: 1.049\nTraining epoch 0, iteration 62/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 62 | reduced_train_loss: 1.255 | train_step_timing in s: 0.4342 | consumed_samples: 126 | val_loss: 1.049\nTraining epoch 0, iteration 63/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 63 | reduced_train_loss: 1.328 | train_step_timing in s: 0.4246 | consumed_samples: 128 | val_loss: 1.049\nTraining epoch 0, iteration 64/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 64 | reduced_train_loss: 1.222 | train_step_timing in s: 0.4377 | consumed_samples: 130 | val_loss: 1.049\nTraining epoch 0, iteration 65/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 65 | reduced_train_loss: 1.252 | train_step_timing in s: 0.4324 | consumed_samples: 132 | val_loss: 1.049\nTraining epoch 0, iteration 66/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 66 | reduced_train_loss: 1.288 | train_step_timing in s: 0.4327 | consumed_samples: 134 | val_loss: 1.049\nTraining epoch 0, iteration 67/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 67 | reduced_train_loss: 1.307 | train_step_timing in s: 0.4338 | consumed_samples: 136 | val_loss: 1.049\n[NeMo I 2025-02-25 01:14:27 nemo_logging:393] Async checkpoint save for step 50 (/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo/default--val_loss=0.0000-epoch=0-consumed_samples=100.0-last.ckpt) finalized successfully.\nTraining epoch 0, iteration 68/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 68 | reduced_train_loss: 1.286 | train_step_timing in s: 0.4343 | consumed_samples: 138 | val_loss: 1.049\nTraining epoch 0, iteration 69/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 69 | reduced_train_loss: 1.321 | train_step_timing in s: 0.433 | consumed_samples: 140 | val_loss: 1.049\nTraining epoch 0, iteration 70/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 70 | reduced_train_loss: 1.286 | train_step_timing in s: 0.4332 | consumed_samples: 142 | val_loss: 1.049\nTraining epoch 0, iteration 71/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 71 | reduced_train_loss: 1.285 | train_step_timing in s: 0.4348 | consumed_samples: 144 | val_loss: 1.049\nTraining epoch 0, iteration 72/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 72 | reduced_train_loss: 0.7515 | train_step_timing in s: 0.4342 | consumed_samples: 146 | val_loss: 1.049\nTraining epoch 0, iteration 73/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 73 | reduced_train_loss: 1.365 | train_step_timing in s: 0.4333 | consumed_samples: 148 | val_loss: 1.049\nTraining epoch 0, iteration 74/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 74 | reduced_train_loss: 1.252 | train_step_timing in s: 0.4332 | consumed_samples: 150 | val_loss: 1.049\nTraining epoch 0, iteration 75/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 75 | reduced_train_loss: 1.265 | train_step_timing in s: 0.4338 | consumed_samples: 152 | val_loss: 1.049\nTraining epoch 0, iteration 76/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 76 | reduced_train_loss: 1.314 | train_step_timing in s: 0.4333 | consumed_samples: 154 | val_loss: 1.049\nTraining epoch 0, iteration 77/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 77 | reduced_train_loss: 1.298 | train_step_timing in s: 0.4341 | consumed_samples: 156 | val_loss: 1.049\nTraining epoch 0, iteration 78/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 78 | reduced_train_loss: 1.333 | train_step_timing in s: 0.4339 | consumed_samples: 158 | val_loss: 1.049\nTraining epoch 0, iteration 79/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 79 | reduced_train_loss: 1.291 | train_step_timing in s: 0.4348 | consumed_samples: 160 | val_loss: 1.049\nTraining epoch 0, iteration 80/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 80 | reduced_train_loss: 1.316 | train_step_timing in s: 0.4219 | consumed_samples: 162 | val_loss: 1.049\nTraining epoch 0, iteration 81/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 81 | reduced_train_loss: 1.335 | train_step_timing in s: 0.4347 | consumed_samples: 164 | val_loss: 1.049\nTraining epoch 0, iteration 82/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 82 | reduced_train_loss: 1.319 | train_step_timing in s: 0.434 | consumed_samples: 166 | val_loss: 1.049\nTraining epoch 0, iteration 83/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 83 | reduced_train_loss: 1.23 | train_step_timing in s: 0.434 | consumed_samples: 168 | val_loss: 1.049\nTraining epoch 0, iteration 84/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 84 | reduced_train_loss: 1.33 | train_step_timing in s: 0.4342 | consumed_samples: 170 | val_loss: 1.049\nTraining epoch 0, iteration 85/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 85 | reduced_train_loss: 1.316 | train_step_timing in s: 0.4351 | consumed_samples: 172 | val_loss: 1.049\nTraining epoch 0, iteration 86/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 86 | reduced_train_loss: 1.309 | train_step_timing in s: 0.4353 | consumed_samples: 174 | val_loss: 1.049\nTraining epoch 0, iteration 87/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 87 | reduced_train_loss: 1.19 | train_step_timing in s: 0.4353 | consumed_samples: 176 | val_loss: 1.049\nTraining epoch 0, iteration 88/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 88 | reduced_train_loss: 1.301 | train_step_timing in s: 0.4223 | consumed_samples: 178 | val_loss: 1.049\nTraining epoch 0, iteration 89/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 89 | reduced_train_loss: 1.327 | train_step_timing in s: 0.4385 | consumed_samples: 180 | val_loss: 1.049\nTraining epoch 0, iteration 90/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 90 | reduced_train_loss: 1.3 | train_step_timing in s: 0.4235 | consumed_samples: 182 | val_loss: 1.049\nTraining epoch 0, iteration 91/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 91 | reduced_train_loss: 1.278 | train_step_timing in s: 0.4357 | consumed_samples: 184 | val_loss: 1.049\nTraining epoch 0, iteration 92/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 92 | reduced_train_loss: 1.302 | train_step_timing in s: 0.4364 | consumed_samples: 186 | val_loss: 1.049\nTraining epoch 0, iteration 93/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 93 | reduced_train_loss: 1.094 | train_step_timing in s: 0.4364 | consumed_samples: 188 | val_loss: 1.049\nTraining epoch 0, iteration 94/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 94 | reduced_train_loss: 1.326 | train_step_timing in s: 0.4234 | consumed_samples: 190 | val_loss: 1.049\nTraining epoch 0, iteration 95/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 95 | reduced_train_loss: 1.176 | train_step_timing in s: 0.4366 | consumed_samples: 192 | val_loss: 1.049\nTraining epoch 0, iteration 96/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 96 | reduced_train_loss: 1.282 | train_step_timing in s: 0.4364 | consumed_samples: 194 | val_loss: 1.049\nTraining epoch 0, iteration 97/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 97 | reduced_train_loss: 1.293 | train_step_timing in s: 0.437 | consumed_samples: 196 | val_loss: 1.049\nTraining epoch 0, iteration 98/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 98 | reduced_train_loss: 1.313 | train_step_timing in s: 0.4363 | consumed_samples: 198 | val_loss: 1.049\nTraining epoch 0, iteration 99/99 | lr: 3e-05 | global_batch_size: 2 | global_step: 99 | reduced_train_loss: 1.309 | train_step_timing in s: 0.4345 | consumed_samples: 200 | val_loss: 1.049\n[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 99: 'val_loss' reached 1.04856 (best 1.04856), saving model to '/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo/default--val_loss=1.0486-epoch=0-consumed_samples=200.0.ckpt' as top 5\n[NeMo I 2025-02-25 01:14:42 nemo_logging:393] Scheduled async checkpoint save for /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo/default--val_loss=1.0486-epoch=0-consumed_samples=200.0.ckpt\n[NeMo I 2025-02-25 01:14:43 nemo_logging:393] Scheduled async checkpoint save for /workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo/default--val_loss=1.0486-epoch=0-consumed_samples=200.0-last.ckpt\nValidation: iteration 1/20\nValidation: iteration 2/20\nValidation: iteration 3/20\nValidation: iteration 4/20\nValidation: iteration 5/20\nValidation: iteration 6/20\nValidation: iteration 7/20\nValidation: iteration 8/20\nValidation: iteration 9/20\nValidation: iteration 10/20\nValidation: iteration 11/20\nValidation: iteration 12/20\nValidation: iteration 13/20\nValidation: iteration 14/20\nValidation: iteration 15/20\nValidation: iteration 16/20\nValidation: iteration 17/20\nValidation: iteration 18/20\nValidation: iteration 19/20\nValidation: iteration 20/20\n[INFO     | pytorch_lightning.utilities.rank_zero]: `Trainer.fit` stopped: `max_steps=100` reached.\n[NeMo I 2025-02-25 01:14:45 nemo_logging:393] Pending async checkpoint saves. Finalizing them synchronously now\n[NeMo I 2025-02-25 01:14:54 nemo_logging:393] Async checkpoint save for step 100 (/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo/default--val_loss=1.0486-epoch=0-consumed_samples=200.0.ckpt) finalized successfully.\n[NeMo I 2025-02-25 01:14:54 nemo_logging:393] Async checkpoint save for step 100 (/workspaces/bionemo-framework/docs/docs/user-guide/examples/bionemo-evo2/pretraining_demo/default--val_loss=1.0486-epoch=0-consumed_samples=200.0-last.ckpt) finalized successfully.\n</pre>"},{"location":"user-guide/examples/bionemo-evo2/fine-tuning-tutorial/#fine-tuning-tutorial-for-evo2","title":"Fine-tuning tutorial for Evo2\u00b6","text":"<p>This tutorial goes through a toy fine-tuning example end to end starting with a fasta and continuing training a hugging face checkpoint on this user defined dataset.</p>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/","title":"Geneformer Cell Type Classification Benchmark","text":"<p>NOTE: it takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits.</p> In\u00a0[1]: Copied! <pre># NBVAL_CHECK_OUTPUT\nimport cellxgene_census\n\n\nCENSUS_VERSION = \"2023-12-15\"\nwith cellxgene_census.open_soma(census_version=CENSUS_VERSION) as census:\n    adata = cellxgene_census.get_anndata(\n        census,\n        \"Homo sapiens\",\n        obs_value_filter='dataset_id==\"8e47ed12-c658-4252-b126-381df8d52a3d\"',\n    )\nuq_cells = sorted(adata.obs[\"cell_type\"].unique().tolist())\nuq_cells\n</pre> # NBVAL_CHECK_OUTPUT import cellxgene_census   CENSUS_VERSION = \"2023-12-15\" with cellxgene_census.open_soma(census_version=CENSUS_VERSION) as census:     adata = cellxgene_census.get_anndata(         census,         \"Homo sapiens\",         obs_value_filter='dataset_id==\"8e47ed12-c658-4252-b126-381df8d52a3d\"',     ) uq_cells = sorted(adata.obs[\"cell_type\"].unique().tolist()) uq_cells <pre>/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from optuna import progress_bar as pbar_module\n</pre> Out[1]: <pre>['B cell',\n 'CD4-positive, alpha-beta T cell',\n 'CD8-positive, alpha-beta T cell',\n 'IgA plasma cell',\n 'IgG plasma cell',\n 'M cell of gut',\n 'T follicular helper cell',\n 'activated CD4-positive, alpha-beta T cell, human',\n 'conventional dendritic cell',\n 'dendritic cell, human',\n 'endothelial cell of artery',\n 'endothelial cell of lymphatic vessel',\n 'enterocyte',\n 'enteroendocrine cell',\n 'fibroblast',\n 'gamma-delta T cell',\n 'glial cell',\n 'intestinal crypt stem cell',\n 'intestinal tuft cell',\n 'intestine goblet cell',\n 'mast cell',\n 'memory B cell',\n 'monocyte',\n 'myeloid cell',\n 'myofibroblast cell',\n 'pericyte',\n 'plasma cell',\n 'plasmacytoid dendritic cell',\n 'regulatory T cell',\n 'transit amplifying cell',\n 'vein endothelial cell']</pre> In\u00a0[2]: Copied! <pre># NBVAL_CHECK_OUTPUT\nimport random\nfrom contextlib import contextmanager\n\n\n@contextmanager\ndef random_seed(seed: int):\n    state = random.getstate()\n    random.seed(seed)\n    try:\n        yield\n    finally:\n        # Go back to previous state\n        random.setstate(state)\n\n\nwith random_seed(32):\n    indices = list(range(len(adata)))\n    random.shuffle(indices)\n\nmicro_batch_size: int = 32\nnum_steps: int = 256\nselection = sorted(indices[: micro_batch_size * num_steps])\n# NOTE: there's a current constraint that predict_step needs to be a function of micro-batch-size.\n#  this is something we are working on fixing. A quick hack is to set micro-batch-size=1, but this is\n#  slow. In this notebook we are going to use mbs=32 and subsample the anndata.\nadata = adata[selection].copy()  # so it's not a view\nadata.shape\n</pre> # NBVAL_CHECK_OUTPUT import random from contextlib import contextmanager   @contextmanager def random_seed(seed: int):     state = random.getstate()     random.seed(seed)     try:         yield     finally:         # Go back to previous state         random.setstate(state)   with random_seed(32):     indices = list(range(len(adata)))     random.shuffle(indices)  micro_batch_size: int = 32 num_steps: int = 256 selection = sorted(indices[: micro_batch_size * num_steps]) # NOTE: there's a current constraint that predict_step needs to be a function of micro-batch-size. #  this is something we are working on fixing. A quick hack is to set micro-batch-size=1, but this is #  slow. In this notebook we are going to use mbs=32 and subsample the anndata. adata = adata[selection].copy()  # so it's not a view adata.shape Out[2]: <pre>(8192, 60664)</pre> In\u00a0[3]: Copied! <pre>import shutil\n\nfrom bionemo.core import BIONEMO_CACHE_DIR\n\n\ncleanup: bool = True\nnotebook_workdir = BIONEMO_CACHE_DIR / \"notebook_tutorials\" / \"geneformer_celltype_classification\"\nif cleanup and notebook_workdir.exists():\n    shutil.rmtree(notebook_workdir)\nnotebook_workdir.mkdir(parents=True, exist_ok=True)\ninput_dir = notebook_workdir / \"celltype-bench-dataset-input\"\ndata_dir = notebook_workdir / \"celltype-bench-dataset\"\ninput_dir.mkdir(parents=True, exist_ok=True)\nh5ad_outfile = input_dir / \"hs-celltype-bench.h5ad\"\nadata.write_h5ad(h5ad_outfile)\n</pre> import shutil  from bionemo.core import BIONEMO_CACHE_DIR   cleanup: bool = True notebook_workdir = BIONEMO_CACHE_DIR / \"notebook_tutorials\" / \"geneformer_celltype_classification\" if cleanup and notebook_workdir.exists():     shutil.rmtree(notebook_workdir) notebook_workdir.mkdir(parents=True, exist_ok=True) input_dir = notebook_workdir / \"celltype-bench-dataset-input\" data_dir = notebook_workdir / \"celltype-bench-dataset\" input_dir.mkdir(parents=True, exist_ok=True) h5ad_outfile = input_dir / \"hs-celltype-bench.h5ad\" adata.write_h5ad(h5ad_outfile) In\u00a0[4]: Copied! <pre>!convert_h5ad_to_scdl --data-path {input_dir} --save-path {data_dir}\n</pre> !convert_h5ad_to_scdl --data-path {input_dir} --save-path {data_dir} <p>Importantly, the .npy files are used by BioNeMo dataset object. features.csv contains the metadata requested, in this case cell_type. It's important that the output of our model has the same order as features.csv, as this contains the labels used in the following benchmark.</p> In\u00a0[5]: Copied! <pre># NBVAL_CHECK_OUTPUT\nfrom glob import glob\n\n\nfiles = sorted(\n    [f.split(\"/\")[-1] for f in glob(str(data_dir / \"*\"))]\n)  # strip off the directory name and sort for the test\nfiles\n</pre> # NBVAL_CHECK_OUTPUT from glob import glob   files = sorted(     [f.split(\"/\")[-1] for f in glob(str(data_dir / \"*\"))] )  # strip off the directory name and sort for the test files Out[5]: <pre>['col_ptr.npy',\n 'data.npy',\n 'features',\n 'metadata.json',\n 'row_ptr.npy',\n 'version.json']</pre> In\u00a0[6]: Copied! <pre>from bionemo.core.data.load import load\n\n\n# 106m checkpoint\ngeneformer_106m = load(\"geneformer/106M_240530:2.0\")\n# 10m checkpoint\ngeneformer_10m = load(\"geneformer/10M_240530:2.0\")\n# 10m bionemo2 trained checkpoint\ngeneformer_10m_bnmo2 = load(\"geneformer/10M_241113:2.0\")\n</pre> from bionemo.core.data.load import load   # 106m checkpoint geneformer_106m = load(\"geneformer/106M_240530:2.0\") # 10m checkpoint geneformer_10m = load(\"geneformer/10M_240530:2.0\") # 10m bionemo2 trained checkpoint geneformer_10m_bnmo2 = load(\"geneformer/10M_241113:2.0\") In\u00a0[7]: Copied! <pre>result_path_10m = notebook_workdir / \"results_10m.pt\"\nresult_path_10m_bnmo2 = notebook_workdir / \"results_10m_bnmo2.pt\"\nresults_path_10m_random = notebook_workdir / \"results_10m_randomweights.pt\"\nresult_path_106m = notebook_workdir / \"results_106m.pt\"\n</pre> result_path_10m = notebook_workdir / \"results_10m.pt\" result_path_10m_bnmo2 = notebook_workdir / \"results_10m_bnmo2.pt\" results_path_10m_random = notebook_workdir / \"results_10m_randomweights.pt\" result_path_106m = notebook_workdir / \"results_106m.pt\" In\u00a0[8]: Copied! <pre>!infer_geneformer \\\n    --data-dir {data_dir} \\\n    --checkpoint-path {geneformer_10m} \\\n    --results-path {result_path_10m} \\\n    --micro-batch-size {micro_batch_size} \\\n    --seq-len 2048 \\\n    --num-dataset-workers 10 \\\n    --num-gpus 1 \\\n    --include-input-ids\n</pre> !infer_geneformer \\     --data-dir {data_dir} \\     --checkpoint-path {geneformer_10m} \\     --results-path {result_path_10m} \\     --micro-batch-size {micro_batch_size} \\     --seq-len 2048 \\     --num-dataset-workers 10 \\     --num-gpus 1 \\     --include-input-ids <pre>[NeMo W 2025-01-23 16:25:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n      cm = get_cmap(\"Set1\")\n    \n[NeMo W 2025-01-23 16:26:03 nemo_logging:405] Tokenizer vocab file: /root/.cache/bionemo/d8e3ea569bc43768c24aa651aff77722df202078415528497c22394046b08cc3-singlecell-scdltestdata-20241203.tar.gz.untar/cellxgene_2023-12-15_small_processed_scdl/train/geneformer.vocab already exists. Overwriting...\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] *************** Preprocessing Finished ************\n[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has data parallel group : [0]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Ranks 0 has data parallel rank: 0\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has context parallel group: [0]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] All context parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Ranks 0 has context parallel rank: 0\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has model parallel group: [0]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] All model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has embedding group: [0]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] All embedding group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:03 nemo_logging:393] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[WARNING  | /usr/local/lib/python3.12/dist-packages/bionemo/llm/model/config.py]: Loading /root/.cache/bionemo/a27061ee347f453b1bf175e288df31e9813903ebcb4924a77ac50dccc730889d-geneformer_10M_240530_nemo2.tar.gz.untar\n[NeMo I 2025-01-23 16:26:04 nemo_logging:393] Padded vocab_size: 25472, original vocab_size: 25429, dummy tokens: 43.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo W 2025-01-23 16:26:04 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n[NeMo I 2025-01-23 16:26:04 nemo_logging:393]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 10300032\n</pre> In\u00a0[9]: Copied! <pre># !infer_geneformer \\\n#     --data-dir {data_dir} \\\n#     --checkpoint-path {geneformer_10m_bnmo2} \\\n#     --results-path {result_path_10m_bnmo2} \\\n#     --micro-batch-size {micro_batch_size} \\\n#     --seq-len 2048 \\\n#     --num-dataset-workers 10 \\\n#     --num-gpus 1 \\\n#     --include-input-ids\n</pre> # !infer_geneformer \\ #     --data-dir {data_dir} \\ #     --checkpoint-path {geneformer_10m_bnmo2} \\ #     --results-path {result_path_10m_bnmo2} \\ #     --micro-batch-size {micro_batch_size} \\ #     --seq-len 2048 \\ #     --num-dataset-workers 10 \\ #     --num-gpus 1 \\ #     --include-input-ids In\u00a0[10]: Copied! <pre>!infer_geneformer \\\n    --data-dir {data_dir} \\\n    --results-path {results_path_10m_random} \\\n    --micro-batch-size {micro_batch_size} \\\n    --seq-len 2048  \\\n    --num-dataset-workers 10 \\\n    --num-gpus 1 \\\n    --include-input-ids\n</pre> !infer_geneformer \\     --data-dir {data_dir} \\     --results-path {results_path_10m_random} \\     --micro-batch-size {micro_batch_size} \\     --seq-len 2048  \\     --num-dataset-workers 10 \\     --num-gpus 1 \\     --include-input-ids <pre>[NeMo W 2025-01-23 16:26:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n      cm = get_cmap(\"Set1\")\n    \n[NeMo W 2025-01-23 16:26:45 nemo_logging:405] Tokenizer vocab file: /root/.cache/bionemo/d8e3ea569bc43768c24aa651aff77722df202078415528497c22394046b08cc3-singlecell-scdltestdata-20241203.tar.gz.untar/cellxgene_2023-12-15_small_processed_scdl/train/geneformer.vocab already exists. Overwriting...\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] *************** Preprocessing Finished ************\n[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has data parallel group : [0]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Ranks 0 has data parallel rank: 0\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has context parallel group: [0]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] All context parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Ranks 0 has context parallel rank: 0\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has model parallel group: [0]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] All model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has embedding group: [0]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] All embedding group ranks: [[0]]\n[NeMo I 2025-01-23 16:26:45 nemo_logging:393] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[NeMo I 2025-01-23 16:26:46 nemo_logging:393] Padded vocab_size: 25472, original vocab_size: 25429, dummy tokens: 43.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo W 2025-01-23 16:26:46 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n[NeMo I 2025-01-23 16:26:46 nemo_logging:393]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 10300032\n</pre> In\u00a0[11]: Copied! <pre>!infer_geneformer \\\n    --data-dir {data_dir} \\\n    --checkpoint-path {geneformer_106m} \\\n    --results-path {result_path_106m} \\\n    --micro-batch-size {micro_batch_size} \\\n    --seq-len 2048 \\\n    --num-dataset-workers 10 \\\n    --num-gpus 1 \\\n    --include-input-ids\n</pre> !infer_geneformer \\     --data-dir {data_dir} \\     --checkpoint-path {geneformer_106m} \\     --results-path {result_path_106m} \\     --micro-batch-size {micro_batch_size} \\     --seq-len 2048 \\     --num-dataset-workers 10 \\     --num-gpus 1 \\     --include-input-ids <pre>[NeMo W 2025-01-23 16:27:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n      cm = get_cmap(\"Set1\")\n    \n[NeMo W 2025-01-23 16:27:26 nemo_logging:405] Tokenizer vocab file: /root/.cache/bionemo/d8e3ea569bc43768c24aa651aff77722df202078415528497c22394046b08cc3-singlecell-scdltestdata-20241203.tar.gz.untar/cellxgene_2023-12-15_small_processed_scdl/train/geneformer.vocab already exists. Overwriting...\n[NeMo I 2025-01-23 16:27:26 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:27:26 nemo_logging:393] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\n[NeMo I 2025-01-23 16:27:26 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:27:26 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:27:26 nemo_logging:393] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\n[NeMo I 2025-01-23 16:27:26 nemo_logging:393] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] *************** Preprocessing Finished ************\n[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has data parallel group : [0]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Ranks 0 has data parallel rank: 0\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has context parallel group: [0]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] All context parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Ranks 0 has context parallel rank: 0\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has model parallel group: [0]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] All model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has embedding group: [0]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] All embedding group ranks: [[0]]\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[WARNING  | /usr/local/lib/python3.12/dist-packages/bionemo/llm/model/config.py]: Loading /root/.cache/bionemo/7d67a526379eb8581f2aaaf03425ae9ec81a38570b24ddc8b22818e5d26ea772-geneformer_106M_240530_nemo2.tar.gz.untar\n[NeMo I 2025-01-23 16:27:27 nemo_logging:393] Padded vocab_size: 25472, original vocab_size: 25429, dummy tokens: 43.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo W 2025-01-23 16:27:28 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n[NeMo I 2025-01-23 16:27:28 nemo_logging:393]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 106808960\n</pre> In\u00a0[12]: Copied! <pre>def run_benchmark(data, labels, use_pca=True):\n    '''\n    data - contains the single cell expression (or whatever feature) in each row.\n    labels - contains the string label for each cell\n\n    data_shape (R, C)\n    labels_shape (R,)\n    '''\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.pipeline import Pipeline\n    from sklearn.model_selection import StratifiedKFold, cross_validate\n    from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n    from sklearn.decomposition import PCA\n    from sklearn.model_selection import cross_val_predict\n\n    np.random.seed(1337)\n    # Define the target dimension 'n_components'\n    n_components = 10  # for example, adjust based on your specific needs\n\n    # Create a pipeline that includes Gaussian random projection and RandomForestClassifier\n    if use_pca:\n        pipeline = Pipeline([\n            ('projection', PCA(n_components=n_components)),\n            ('classifier', RandomForestClassifier(class_weight='balanced'))\n        ])\n    else:\n        pipeline = Pipeline([\n            ('classifier', RandomForestClassifier(class_weight='balanced'))\n        ])\n\n    # Set up StratifiedKFold to ensure each fold reflects the overall distribution of labels\n    cv = StratifiedKFold(n_splits=5)\n\n    # Define the scoring functions\n    scoring = {\n        'accuracy': make_scorer(accuracy_score),\n        'precision': make_scorer(precision_score, average='macro'),  # 'macro' averages over classes\n        'recall': make_scorer(recall_score, average='macro'),\n        'f1_score': make_scorer(f1_score, average='macro'),\n        # 'roc_auc' requires probability or decision function; hence use multi_class if applicable\n        'roc_auc': make_scorer(roc_auc_score, multi_class='ovr'),\n    }\n\n    # Perform stratified cross-validation with multiple metrics using the pipeline\n    results = cross_validate(pipeline, data, labels, cv=cv, scoring=scoring, return_train_score=False)\n\n    # Print the cross-validation results\n    print(\"Cross-validation metrics:\")\n    results_out = {}\n    for metric, scores in results.items():\n        if metric.startswith('test_'):\n            results_out[metric] = (scores.mean(), scores.std())\n            print(f\"{metric[5:]}: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n\n    predictions = cross_val_predict(pipeline, data, labels, cv=cv)\n\n    # v Return confusion matrix and metrics.\n    conf_matrix = confusion_matrix(labels, predictions)\n\n    return results_out, conf_matrix\n</pre> def run_benchmark(data, labels, use_pca=True):     '''     data - contains the single cell expression (or whatever feature) in each row.     labels - contains the string label for each cell      data_shape (R, C)     labels_shape (R,)     '''     import numpy as np     from sklearn.ensemble import RandomForestClassifier     from sklearn.pipeline import Pipeline     from sklearn.model_selection import StratifiedKFold, cross_validate     from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix     from sklearn.decomposition import PCA     from sklearn.model_selection import cross_val_predict      np.random.seed(1337)     # Define the target dimension 'n_components'     n_components = 10  # for example, adjust based on your specific needs      # Create a pipeline that includes Gaussian random projection and RandomForestClassifier     if use_pca:         pipeline = Pipeline([             ('projection', PCA(n_components=n_components)),             ('classifier', RandomForestClassifier(class_weight='balanced'))         ])     else:         pipeline = Pipeline([             ('classifier', RandomForestClassifier(class_weight='balanced'))         ])      # Set up StratifiedKFold to ensure each fold reflects the overall distribution of labels     cv = StratifiedKFold(n_splits=5)      # Define the scoring functions     scoring = {         'accuracy': make_scorer(accuracy_score),         'precision': make_scorer(precision_score, average='macro'),  # 'macro' averages over classes         'recall': make_scorer(recall_score, average='macro'),         'f1_score': make_scorer(f1_score, average='macro'),         # 'roc_auc' requires probability or decision function; hence use multi_class if applicable         'roc_auc': make_scorer(roc_auc_score, multi_class='ovr'),     }      # Perform stratified cross-validation with multiple metrics using the pipeline     results = cross_validate(pipeline, data, labels, cv=cv, scoring=scoring, return_train_score=False)      # Print the cross-validation results     print(\"Cross-validation metrics:\")     results_out = {}     for metric, scores in results.items():         if metric.startswith('test_'):             results_out[metric] = (scores.mean(), scores.std())             print(f\"{metric[5:]}: {scores.mean():.3f} (+/- {scores.std():.3f})\")      predictions = cross_val_predict(pipeline, data, labels, cv=cv)      # v Return confusion matrix and metrics.     conf_matrix = confusion_matrix(labels, predictions)      return results_out, conf_matrix In\u00a0[13]: Copied! <pre>import torch\n\n\ninfer_Xs_10m = torch.load(result_path_10m / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy()\nassert len(adata) == len(infer_Xs_10m), (len(adata), len(infer_Xs_10m))\nassert infer_Xs_10m.shape == (8192, 256)\n</pre> import torch   infer_Xs_10m = torch.load(result_path_10m / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy() assert len(adata) == len(infer_Xs_10m), (len(adata), len(infer_Xs_10m)) assert infer_Xs_10m.shape == (8192, 256)  <pre>/tmp/ipykernel_5543/2637469332.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  infer_Xs_10m = torch.load(result_path_10m / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy()\n</pre> In\u00a0[14]: Copied! <pre># infer_Xs_10m_bnmo2 = torch.load(result_path_10m_bnmo2 / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy()\n# assert len(adata) == len(infer_Xs_10m_bnmo2), (len(adata), len(infer_Xs_10m))\n# assert infer_Xs_10m_bnmo2.shape == (8192, 256)\n</pre> # infer_Xs_10m_bnmo2 = torch.load(result_path_10m_bnmo2 / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy() # assert len(adata) == len(infer_Xs_10m_bnmo2), (len(adata), len(infer_Xs_10m)) # assert infer_Xs_10m_bnmo2.shape == (8192, 256) In\u00a0[15]: Copied! <pre>infer_Xs_106m = torch.load(result_path_106m / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy()\nassert len(adata) == len(infer_Xs_106m), (len(adata), len(infer_Xs_106m))\nassert infer_Xs_106m.shape == (8192, 768)\n</pre> infer_Xs_106m = torch.load(result_path_106m / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy() assert len(adata) == len(infer_Xs_106m), (len(adata), len(infer_Xs_106m)) assert infer_Xs_106m.shape == (8192, 768) <pre>/tmp/ipykernel_5543/4058871012.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  infer_Xs_106m = torch.load(result_path_106m / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy()\n</pre> In\u00a0[16]: Copied! <pre>infer_Xs_10m_random = torch.load(results_path_10m_random / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy()\nassert len(adata) == len(infer_Xs_10m_random), (len(adata), len(infer_Xs_10m_random))\nassert infer_Xs_10m_random.shape == (8192, 256)\n</pre> infer_Xs_10m_random = torch.load(results_path_10m_random / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy() assert len(adata) == len(infer_Xs_10m_random), (len(adata), len(infer_Xs_10m_random)) assert infer_Xs_10m_random.shape == (8192, 256) <pre>/tmp/ipykernel_5543/3286066556.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  infer_Xs_10m_random = torch.load(results_path_10m_random / \"predictions__rank_0.pt\")['embeddings'].float().cpu().numpy()\n</pre> In\u00a0[17]: Copied! <pre>import pandas as pd\nimport numpy as np\n# Now fetch the class labels and raw expression for the same dataset. These are used as labels in classification and as one of our baselines.\n\ninfer_metadata = adata.obs\nraw_Xs = np.asarray(adata.X.todense())\n# Here we perform a norm over the total counts for each cell, adding a pseudocount to assist with the following logarithm.\nnormed_Xs = (raw_Xs + 1) / raw_Xs.sum(axis=1, keepdims=True)\nlogp1_Xs = np.log( normed_Xs )\n</pre> import pandas as pd import numpy as np # Now fetch the class labels and raw expression for the same dataset. These are used as labels in classification and as one of our baselines.  infer_metadata = adata.obs raw_Xs = np.asarray(adata.X.todense()) # Here we perform a norm over the total counts for each cell, adding a pseudocount to assist with the following logarithm. normed_Xs = (raw_Xs + 1) / raw_Xs.sum(axis=1, keepdims=True) logp1_Xs = np.log( normed_Xs ) In\u00a0[18]: Copied! <pre># Now we look at our dataset, how is the distribution of cell counts? Its clear that certain celltypes dominate the dataset, this is good to keep in mind when investigating models.\n#  we expect the macro averages and F1-score to be the most reliable metrics for overall performance.\nfrom collections import Counter\nimport seaborn as sb\n\nlabels = infer_metadata['cell_type'].values\nlabel_counts = Counter(labels)\n\nax = sb.barplot(x=label_counts.keys(), y=label_counts.values())\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\nax.set_title(\"Cell type counts for classification dataset\")\n</pre> # Now we look at our dataset, how is the distribution of cell counts? Its clear that certain celltypes dominate the dataset, this is good to keep in mind when investigating models. #  we expect the macro averages and F1-score to be the most reliable metrics for overall performance. from collections import Counter import seaborn as sb  labels = infer_metadata['cell_type'].values label_counts = Counter(labels)  ax = sb.barplot(x=label_counts.keys(), y=label_counts.values()) ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right') ax.set_title(\"Cell type counts for classification dataset\") <pre>/tmp/ipykernel_5543/771671311.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n</pre> Out[18]: <pre>Text(0.5, 1.0, 'Cell type counts for classification dataset')</pre> In\u00a0[19]: Copied! <pre># Now we assign integer labels to each of our strings. These do not need to be transformed into one-hot vectors as Random Forest is non-parametric.\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ninteger_labels = label_encoder.fit_transform(labels)\nprint(integer_labels)\n</pre> # Now we assign integer labels to each of our strings. These do not need to be transformed into one-hot vectors as Random Forest is non-parametric. from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() integer_labels = label_encoder.fit_transform(labels) print(integer_labels) <pre>[ 1  1 19 ... 17 14 14]\n</pre> In\u00a0[20]: Copied! <pre># Distribution of log transforms, looks decent.\nfrom matplotlib import pyplot\npyplot.hist(logp1_Xs.flatten());\n</pre> # Distribution of log transforms, looks decent. from matplotlib import pyplot pyplot.hist(logp1_Xs.flatten()); In\u00a0[21]: Copied! <pre>def plot_cm(cm, labels=label_encoder.classes_):\n    '''\n    Helper function for visualizing accuracy across labels.\n    '''\n    from matplotlib.colors import BoundaryNorm, ListedColormap\n\n    # Example confusion matrix (replace with your actual data)\n    conf_matrix = np.random.rand(31, 31)\n\n    # Define the bins and the color map\n    #bounds = np.arange(0.0, 1.1, 0.1)\n    #cmap = ListedColormap(sb.color_palette(\"RdYlBu_r\", len(bounds) - 1))\n    #norm = BoundaryNorm(boundaries=bounds, ncolors=len(bounds) - 1, clip=True)\n\n    #_ = sb.heatmap(cm / cm.sum(axis=0),cmap=cmap, norm=norm, cbar_kws={\"ticks\": bounds}, linewidths=0.5, linecolor='black', xticklabels=labels, yticklabels=labels)\n    _ = sb.heatmap(cm / cm.sum(axis=0), cmap=sb.color_palette(\"Blues\", as_cmap=True), vmin=0, vmax=1, linewidth=0.1, linecolor='lightgrey', xticklabels=labels, yticklabels=labels)\n    pyplot.xticks(rotation=45, ha='right')\n    _ = pyplot.yticks(rotation=0)\n</pre> def plot_cm(cm, labels=label_encoder.classes_):     '''     Helper function for visualizing accuracy across labels.     '''     from matplotlib.colors import BoundaryNorm, ListedColormap      # Example confusion matrix (replace with your actual data)     conf_matrix = np.random.rand(31, 31)      # Define the bins and the color map     #bounds = np.arange(0.0, 1.1, 0.1)     #cmap = ListedColormap(sb.color_palette(\"RdYlBu_r\", len(bounds) - 1))     #norm = BoundaryNorm(boundaries=bounds, ncolors=len(bounds) - 1, clip=True)      #_ = sb.heatmap(cm / cm.sum(axis=0),cmap=cmap, norm=norm, cbar_kws={\"ticks\": bounds}, linewidths=0.5, linecolor='black', xticklabels=labels, yticklabels=labels)     _ = sb.heatmap(cm / cm.sum(axis=0), cmap=sb.color_palette(\"Blues\", as_cmap=True), vmin=0, vmax=1, linewidth=0.1, linecolor='lightgrey', xticklabels=labels, yticklabels=labels)     pyplot.xticks(rotation=45, ha='right')     _ = pyplot.yticks(rotation=0) In\u00a0[22]: Copied! <pre>logp1_results, logp1_cm = run_benchmark(logp1_Xs, integer_labels)\n</pre> logp1_results, logp1_cm = run_benchmark(logp1_Xs, integer_labels) <pre>/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.775 (+/- 0.035)\nprecision: 0.635 (+/- 0.044)\nrecall: 0.546 (+/- 0.029)\nf1_score: 0.561 (+/- 0.035)\nroc_auc: nan (+/- nan)\n</pre> In\u00a0[23]: Copied! <pre>plot_cm(logp1_cm)\n</pre> plot_cm(logp1_cm) In\u00a0[24]: Copied! <pre>results_10m_random, cm_10m_random = run_benchmark(infer_Xs_10m_random, integer_labels, use_pca=False)\n</pre> results_10m_random, cm_10m_random = run_benchmark(infer_Xs_10m_random, integer_labels, use_pca=False) <pre>/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.399 (+/- 0.008)\nprecision: 0.143 (+/- 0.017)\nrecall: 0.092 (+/- 0.006)\nf1_score: 0.079 (+/- 0.007)\nroc_auc: nan (+/- nan)\n</pre> In\u00a0[25]: Copied! <pre>plot_cm(cm_10m_random)\n</pre> plot_cm(cm_10m_random) <pre>/tmp/ipykernel_5543/3742577664.py:16: RuntimeWarning: invalid value encountered in divide\n  _ = sb.heatmap(cm / cm.sum(axis=0), cmap=sb.color_palette(\"Blues\", as_cmap=True), vmin=0, vmax=1, linewidth=0.1, linecolor='lightgrey', xticklabels=labels, yticklabels=labels)\n</pre> In\u00a0[26]: Copied! <pre>results_10m, cm_10m = run_benchmark(infer_Xs_10m, integer_labels, use_pca=False)\n</pre> results_10m, cm_10m = run_benchmark(infer_Xs_10m, integer_labels, use_pca=False) <pre>/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.839 (+/- 0.016)\nprecision: 0.788 (+/- 0.029)\nrecall: 0.677 (+/- 0.015)\nf1_score: 0.702 (+/- 0.017)\nroc_auc: nan (+/- nan)\n</pre> In\u00a0[27]: Copied! <pre>plot_cm(cm_10m)\n</pre> plot_cm(cm_10m) In\u00a0[28]: Copied! <pre># results_10m_bnmo2, cm_10m_bnmo2 = run_benchmark(infer_Xs_10m_bnmo2, integer_labels, use_pca=False)\n</pre> # results_10m_bnmo2, cm_10m_bnmo2 = run_benchmark(infer_Xs_10m_bnmo2, integer_labels, use_pca=False) In\u00a0[29]: Copied! <pre># plot_cm(cm_10m_bnmo2)\n</pre> # plot_cm(cm_10m_bnmo2) In\u00a0[30]: Copied! <pre>results_106M, cm_106M = run_benchmark(infer_Xs_106m, integer_labels, use_pca=False)\n</pre> results_106M, cm_106M = run_benchmark(infer_Xs_106m, integer_labels, use_pca=False) <pre>/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n    score = scorer._score(\n            ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 388, in _score\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 635, in roc_auc_score\n    return _multiclass_roc_auc_score(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 707, in _multiclass_roc_auc_score\n    if not np.allclose(1, y_score.sum(axis=1)):\n                          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py\", line 49, in _sum\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1\n\n  warnings.warn(\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.905 (+/- 0.015)\nprecision: 0.912 (+/- 0.025)\nrecall: 0.819 (+/- 0.015)\nf1_score: 0.843 (+/- 0.016)\nroc_auc: nan (+/- nan)\n</pre> In\u00a0[31]: Copied! <pre>plot_cm(cm_106M)\n</pre> plot_cm(cm_106M) In\u00a0[32]: Copied! <pre>data = {\n    'model': [\n        'Baseline Logp1 PCA+RF',\n        '10M RandomWeights',\n        '10M parameters',\n        # '10M parameters BioNeMo2 re-trained',\n        '106M parameters'],\n    'f1_score_mean': [\n        logp1_results['test_f1_score'][0],\n        results_10m_random['test_f1_score'][0],\n        results_10m['test_f1_score'][0],\n        # results_10m_bnmo2['test_f1_score'][0],\n        results_106M['test_f1_score'][0]\n    ],\n    'f1_score_std': [\n        logp1_results['test_f1_score'][1],\n        results_10m_random['test_f1_score'][1],\n        results_10m['test_f1_score'][1],\n        # results_10m_bnmo2['test_f1_score'][1],\n        results_106M['test_f1_score'][1]\n    ],\n    'accuracy_mean': [\n        logp1_results['test_accuracy'][0],\n        results_10m_random['test_accuracy'][0],\n        results_10m['test_accuracy'][0],\n        # results_10m_bnmo2['test_accuracy'][0],\n        results_106M['test_accuracy'][0]\n    ],\n    'accuracy_std': [\n        logp1_results['test_accuracy'][1],\n        results_10m_random['test_accuracy'][1],\n        results_10m['test_accuracy'][1],\n        # results_10m_bnmo2['test_accuracy'][1],\n        results_106M['test_accuracy'][1]\n    ]\n}\n\ndf = pd.DataFrame(data)\n\nfig, ax = pyplot.subplots(figsize=(10, 10))\n# F1 Score plot\nsb.barplot(x='model', y='f1_score_mean', data=df, capsize=0.2, palette='viridis', ax=ax)\nax.set_title('F1 Score Comparison')\nax.set_xlabel('Model')\nax.set_ylabel('F1 Score')\nax.set_yticks(np.arange(.0, 1.05, .05))\nax.set_ylim(.0, 1.0)\npyplot.xticks(rotation=45, ha='right')\npyplot.savefig(\"F1-score-models.png\")\n\n# Accuracy plot\nfig, ax = pyplot.subplots(figsize=(10, 10))\nsb.barplot(x='model', y='accuracy_mean', data=df, ax=ax, capsize=0.2, palette='viridis')\nax.set_title('Accuracy Comparison')\nax.set_xlabel('Model')\nax.set_ylabel('Accuracy')\nax.set_yticks(np.arange(.0, 1.05, .05))\nax.set_ylim(.0, 1.0)\npyplot.xticks(rotation=45, ha='right')\npyplot.savefig(\"average-accuracy-models.png\")\n</pre> data = {     'model': [         'Baseline Logp1 PCA+RF',         '10M RandomWeights',         '10M parameters',         # '10M parameters BioNeMo2 re-trained',         '106M parameters'],     'f1_score_mean': [         logp1_results['test_f1_score'][0],         results_10m_random['test_f1_score'][0],         results_10m['test_f1_score'][0],         # results_10m_bnmo2['test_f1_score'][0],         results_106M['test_f1_score'][0]     ],     'f1_score_std': [         logp1_results['test_f1_score'][1],         results_10m_random['test_f1_score'][1],         results_10m['test_f1_score'][1],         # results_10m_bnmo2['test_f1_score'][1],         results_106M['test_f1_score'][1]     ],     'accuracy_mean': [         logp1_results['test_accuracy'][0],         results_10m_random['test_accuracy'][0],         results_10m['test_accuracy'][0],         # results_10m_bnmo2['test_accuracy'][0],         results_106M['test_accuracy'][0]     ],     'accuracy_std': [         logp1_results['test_accuracy'][1],         results_10m_random['test_accuracy'][1],         results_10m['test_accuracy'][1],         # results_10m_bnmo2['test_accuracy'][1],         results_106M['test_accuracy'][1]     ] }  df = pd.DataFrame(data)  fig, ax = pyplot.subplots(figsize=(10, 10)) # F1 Score plot sb.barplot(x='model', y='f1_score_mean', data=df, capsize=0.2, palette='viridis', ax=ax) ax.set_title('F1 Score Comparison') ax.set_xlabel('Model') ax.set_ylabel('F1 Score') ax.set_yticks(np.arange(.0, 1.05, .05)) ax.set_ylim(.0, 1.0) pyplot.xticks(rotation=45, ha='right') pyplot.savefig(\"F1-score-models.png\")  # Accuracy plot fig, ax = pyplot.subplots(figsize=(10, 10)) sb.barplot(x='model', y='accuracy_mean', data=df, ax=ax, capsize=0.2, palette='viridis') ax.set_title('Accuracy Comparison') ax.set_xlabel('Model') ax.set_ylabel('Accuracy') ax.set_yticks(np.arange(.0, 1.05, .05)) ax.set_ylim(.0, 1.0) pyplot.xticks(rotation=45, ha='right') pyplot.savefig(\"average-accuracy-models.png\") <pre>/tmp/ipykernel_5543/808009756.py:42: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sb.barplot(x='model', y='f1_score_mean', data=df, capsize=0.2, palette='viridis', ax=ax)\n/tmp/ipykernel_5543/808009756.py:53: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sb.barplot(x='model', y='accuracy_mean', data=df, ax=ax, capsize=0.2, palette='viridis')\n</pre>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#geneformer-cell-type-classification-benchmark","title":"Geneformer Cell Type Classification Benchmark\u00b6","text":"<p>Here we benchmark four models, with two baselines. These models are tasked with cell type classification, using the Crohn's disease small intestine dataset from Elmentaite et al. (2020), Developmental Cell. This dataset contains approximately 22,500 single cells from both healthy children aged 4-13 and children with Crohn's disease. This dataset contains 31 unique cell types which we assume to be annotated accurately. This dataset was held out of our pre-training dataset as all diseased samples were removed.</p> <ul> <li>Baseline (1) scRNA workflow: this model uses PCA with 10 components and random forest on normalized and log transformed expression counts to produce a result.</li> <li>Baseline (2) geneformer with random weight initialization. Some performance can come from large random projections, but we want to do better than that.</li> <li>geneformer-10M + geneformer106M as described in the model cards.</li> </ul> <p>First, we download the dataset from czi that we are interested in, and then create the requisite sc_memmap dataset object.</p>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#create-the-scmemmap-object-check-outputs","title":"Create the scmemmap object, check outputs\u00b6","text":""},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#execute-inference","title":"Execute inference\u00b6","text":"<p>We run inference on all there of our models, which are downloaded by <code>load(...)</code> function in a previous cell. We have a one-off inference script for geneformer that is installed as part of the <code>bionemo-geneformer</code> package. See the <code>pyproject.toml</code> in the source directory if you are curious or want to use this as a template to make your own inference scripts. This script should work for any <code>sc_memmap</code> converted geneformer dataset, and geneformer bionemo2 model checkpoint though.</p>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#benchmarking","title":"Benchmarking\u00b6","text":"<p>see below the benchmarking snippet. We take in a datavector, and a set of labels. We optionally fit PCA and then a RF model inside cross validation. Metrics are using the <code>macro</code> (average over each class) for handling multi-class labels. Additionally, we return the confusion matrix for further investigation.</p>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#execute-benchmarks","title":"Execute benchmarks\u00b6","text":"<p>Finally we execute our benchmarks, and collect results and confusion matrix. You can see in the figures below, we plot the performance by cell type for each model (confusion matrix heatmap). Perhaps unsurprisingly, we see that the most frequent cell type (enterocyte) has the highest accuracy across all models. This suggests bias in the model due to unbalanced data, however, further investigation is beyond the scope of this tutorial. Furthermore, we see continually improved performance as we move through the models, from baselines, to our provided pretrained model.</p> <p>Perhaps most interesting is the 106M parameter model, which clearly outperforms all other models by all metrics, but especially by F1-score. This suggests that training larger models based on geneformer perform well, and that more work may be done.</p>"},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/","title":"Building Generative Models for Continuous Data via Continuous Interpolants","text":"<p>NOTE: it takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits.\"</p> In\u00a0[1]: Copied! <pre>import math\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom sklearn.datasets import make_moons\n</pre> import math import os import time  import matplotlib.pyplot as plt import numpy as np import torch  from sklearn.datasets import make_moons In\u00a0[2]: Copied! <pre>def sample_moons(n, normalize = False):\n    x1, _ = make_moons(n_samples=n, noise=0.08)\n    x1 = torch.Tensor(x1)\n    x1 =  x1 * 3 - 1\n    if normalize:\n        x1 = (x1 - x1.mean(0))/x1.std(0) * 2\n    return x1\n</pre> def sample_moons(n, normalize = False):     x1, _ = make_moons(n_samples=n, noise=0.08)     x1 = torch.Tensor(x1)     x1 =  x1 * 3 - 1     if normalize:         x1 = (x1 - x1.mean(0))/x1.std(0) * 2     return x1 In\u00a0[3]: Copied! <pre>x1 = sample_moons(1000)\nplt.scatter(x1[:, 0], x1[:, 1])\n</pre> x1 = sample_moons(1000) plt.scatter(x1[:, 0], x1[:, 1]) Out[3]: <pre>&lt;matplotlib.collections.PathCollection at 0x71fad37d11e0&gt;</pre> In\u00a0[4]: Copied! <pre>dim = 2\nhidden_size = 64\nbatch_size = 256\nmodel = torch.nn.Sequential(\n            torch.nn.Linear(dim + 1, hidden_size),\n            torch.nn.SELU(),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.SELU(),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.SELU(),\n            torch.nn.Linear(hidden_size, dim),\n        )\noptimizer = torch.optim.Adam(model.parameters())\n</pre> dim = 2 hidden_size = 64 batch_size = 256 model = torch.nn.Sequential(             torch.nn.Linear(dim + 1, hidden_size),             torch.nn.SELU(),             torch.nn.Linear(hidden_size, hidden_size),             torch.nn.SELU(),             torch.nn.Linear(hidden_size, hidden_size),             torch.nn.SELU(),             torch.nn.Linear(hidden_size, dim),         ) optimizer = torch.optim.Adam(model.parameters()) In\u00a0[5]: Copied! <pre>from bionemo.moco.interpolants import ContinuousFlowMatcher\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.distributions.prior import GaussianPrior\n\nuniform_time = UniformTimeDistribution()\nsimple_prior = GaussianPrior()\nsigma = 0.1\ninterpolant = ContinuousFlowMatcher(time_distribution=uniform_time, \n                            prior_distribution=simple_prior, \n                            sigma=sigma, \n                            prediction_type=\"velocity\")\n# Place both the model and the interpolant on the same device\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\ninterpolant = interpolant.to_device(DEVICE)\n</pre> from bionemo.moco.interpolants import ContinuousFlowMatcher from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.distributions.prior import GaussianPrior  uniform_time = UniformTimeDistribution() simple_prior = GaussianPrior() sigma = 0.1 interpolant = ContinuousFlowMatcher(time_distribution=uniform_time,                              prior_distribution=simple_prior,                              sigma=sigma,                              prediction_type=\"velocity\") # Place both the model and the interpolant on the same device DEVICE = \"cuda\" model = model.to(DEVICE) interpolant = interpolant.to_device(DEVICE) In\u00a0[6]: Copied! <pre>for k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = interpolant.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = interpolant.sample_time(batch_size)\n    xt = interpolant.interpolate(x1, t, x0)\n    ut = interpolant.calculate_target(x1, x0)\n\n    vt = model(torch.cat([xt, t[:, None]], dim=-1))\n    loss = interpolant.loss(vt, ut, target_type=\"velocity\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 5000 == 0:\n        print(f\"{k+1}: loss {loss.item():0.3f}\") \n</pre> for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = interpolant.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = interpolant.sample_time(batch_size)     xt = interpolant.interpolate(x1, t, x0)     ut = interpolant.calculate_target(x1, x0)      vt = model(torch.cat([xt, t[:, None]], dim=-1))     loss = interpolant.loss(vt, ut, target_type=\"velocity\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 5000 == 0:         print(f\"{k+1}: loss {loss.item():0.3f}\")  <pre>5000: loss 2.766\n10000: loss 2.730\n15000: loss 3.084\n20000: loss 2.839\n</pre> In\u00a0[7]: Copied! <pre>from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\ninference_sched = LinearInferenceSchedule(nsteps = 100)\nschedule = inference_sched.generate_schedule().to(DEVICE)\ndts = inference_sched.discretize().to(DEVICE)\nschedule, dts\n</pre> from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule  inference_sched = LinearInferenceSchedule(nsteps = 100) schedule = inference_sched.generate_schedule().to(DEVICE) dts = inference_sched.discretize().to(DEVICE) schedule, dts Out[7]: <pre>(tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n         0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n         0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n         0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n         0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n         0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n         0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n         0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n         0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n         0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n         0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n         0.9900], device='cuda:0'),\n tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,\n         0.0100], device='cuda:0'))</pre> In\u00a0[8]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample]\nfor dt, t in zip(dts, schedule):\n    full_t = inference_sched.pad_time(inf_size, t, DEVICE)\n    vt = model(torch.cat([sample, full_t[:, None]], dim=-1)) # calculate the vector field based on the definition of the model\n    sample = interpolant.step(vt, sample, dt, full_t)\n    trajectory.append(sample) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample] for dt, t in zip(dts, schedule):     full_t = inference_sched.pad_time(inf_size, t, DEVICE)     vt = model(torch.cat([sample, full_t[:, None]], dim=-1)) # calculate the vector field based on the definition of the model     sample = interpolant.step(vt, sample, dt, full_t)     trajectory.append(sample) # save the trajectory for plotting purposes  In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024 plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[14]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE)\ntrajectory = [sample.detach().cpu()]\nfor dt, t in zip(dts, schedule):\n    time  = inference_sched.pad_time(inf_size, t, DEVICE) #torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        vt = model(torch.cat([sample, time[:, None]], dim=-1))\n    sample = interpolant.step_score_stochastic(vt, sample, dt, time, noise_temperature=1.0, gt_mode = \"tan\")\n    trajectory.append(sample.detach().cpu())\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) trajectory = [sample.detach().cpu()] for dt, t in zip(dts, schedule):     time  = inference_sched.pad_time(inf_size, t, DEVICE) #torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         vt = model(torch.cat([sample, time[:, None]], dim=-1))     sample = interpolant.step_score_stochastic(vt, sample, dt, time, noise_temperature=1.0, gt_mode = \"tan\")     trajectory.append(sample.detach().cpu()) In\u00a0[15]: Copied! <pre>traj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(0)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(1)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.title(\"Stochastic score sampling Temperature = 1.0\")\nplt.show()\n</pre>  traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024 plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(0)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\") # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(1)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.title(\"Stochastic score sampling Temperature = 1.0\") plt.show() In\u00a0[16]: Copied! <pre>model = torch.nn.Sequential(\n            torch.nn.Linear(dim + 1, hidden_size),\n            torch.nn.SELU(),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.SELU(),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.SELU(),\n            torch.nn.Linear(hidden_size, dim),\n        ).to(DEVICE)\ninf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE)\ntrajectory = [sample.detach().cpu()]\nfor dt, t in zip(dts, schedule):\n    time  = inference_sched.pad_time(inf_size, t, DEVICE) #torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        vt = model(torch.cat([sample, time[:, None]], dim=-1))\n    sample = interpolant.step(vt, sample, dt, time)\n    trajectory.append(sample.detach().cpu())\n</pre> model = torch.nn.Sequential(             torch.nn.Linear(dim + 1, hidden_size),             torch.nn.SELU(),             torch.nn.Linear(hidden_size, hidden_size),             torch.nn.SELU(),             torch.nn.Linear(hidden_size, hidden_size),             torch.nn.SELU(),             torch.nn.Linear(hidden_size, dim),         ).to(DEVICE) inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) trajectory = [sample.detach().cpu()] for dt, t in zip(dts, schedule):     time  = inference_sched.pad_time(inf_size, t, DEVICE) #torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         vt = model(torch.cat([sample, time[:, None]], dim=-1))     sample = interpolant.step(vt, sample, dt, time)     trajectory.append(sample.detach().cpu()) In\u00a0[17]: Copied! <pre>plot_limit = 1024\ntraj = torch.stack(trajectory).cpu().detach().numpy()\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(0)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(1)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> plot_limit = 1024 traj = torch.stack(trajectory).cpu().detach().numpy()  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(0)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(1)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[18]: Copied! <pre>import math\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Network(nn.Module):\n    def __init__(\n        self, dim_in: int, dim_out: int, dim_hids: List[int],\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            TimeLinear(dim_in, dim_hids[0]),\n            *[TimeLinear(dim_hids[i-1], dim_hids[i]) for i in range(1, len(dim_hids))],\n            TimeLinear(dim_hids[-1], dim_out)\n        ])\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        for i, layer in enumerate(self.layers):\n            x = layer(x, t)\n            if i &lt; len(self.layers) - 1:\n                x = F.relu(x)\n        return x\n        \nclass TimeLinear(nn.Module):\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n\n        self.time_embedding = TimeEmbedding(dim_out)\n        self.fc = nn.Linear(dim_in, dim_out)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.fc(x)\n        alpha = self.time_embedding(t).view(-1, self.dim_out)\n        return alpha * x\n        \nclass TimeEmbedding(nn.Module):\n    # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n    def __init__(self, hidden_size, frequency_embedding_size=256):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period)\n            * torch.arange(start=0, end=half, dtype=torch.float32)\n            / half\n        ).to(device=t.device)\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat(\n                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n            )\n        return embedding\n\n    def forward(self, t: torch.Tensor):\n        if t.ndim == 0:\n            t = t.unsqueeze(-1)\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n</pre> import math from typing import List  import torch import torch.nn as nn import torch.nn.functional as F  class Network(nn.Module):     def __init__(         self, dim_in: int, dim_out: int, dim_hids: List[int],     ):         super().__init__()         self.layers = nn.ModuleList([             TimeLinear(dim_in, dim_hids[0]),             *[TimeLinear(dim_hids[i-1], dim_hids[i]) for i in range(1, len(dim_hids))],             TimeLinear(dim_hids[-1], dim_out)         ])      def forward(self, x: torch.Tensor, t: torch.Tensor):         for i, layer in enumerate(self.layers):             x = layer(x, t)             if i &lt; len(self.layers) - 1:                 x = F.relu(x)         return x          class TimeLinear(nn.Module):     def __init__(self, dim_in: int, dim_out: int):         super().__init__()         self.dim_in = dim_in         self.dim_out = dim_out          self.time_embedding = TimeEmbedding(dim_out)         self.fc = nn.Linear(dim_in, dim_out)      def forward(self, x: torch.Tensor, t: torch.Tensor):         x = self.fc(x)         alpha = self.time_embedding(t).view(-1, self.dim_out)         return alpha * x          class TimeEmbedding(nn.Module):     # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py     def __init__(self, hidden_size, frequency_embedding_size=256):         super().__init__()         self.mlp = nn.Sequential(             nn.Linear(frequency_embedding_size, hidden_size, bias=True),             nn.SiLU(),             nn.Linear(hidden_size, hidden_size, bias=True),         )         self.frequency_embedding_size = frequency_embedding_size      @staticmethod     def timestep_embedding(t, dim, max_period=10000):         \"\"\"         Create sinusoidal timestep embeddings.         :param t: a 1-D Tensor of N indices, one per batch element.                           These may be fractional.         :param dim: the dimension of the output.         :param max_period: controls the minimum frequency of the embeddings.         :return: an (N, D) Tensor of positional embeddings.         \"\"\"         # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py         half = dim // 2         freqs = torch.exp(             -math.log(max_period)             * torch.arange(start=0, end=half, dtype=torch.float32)             / half         ).to(device=t.device)         args = t[:, None].float() * freqs[None]         embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)         if dim % 2:             embedding = torch.cat(                 [embedding, torch.zeros_like(embedding[:, :1])], dim=-1             )         return embedding      def forward(self, t: torch.Tensor):         if t.ndim == 0:             t = t.unsqueeze(-1)         t_freq = self.timestep_embedding(t, self.frequency_embedding_size)         t_emb = self.mlp(t_freq)         return t_emb In\u00a0[19]: Copied! <pre>from bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import DDPM\nfrom bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule, DiscreteLinearNoiseSchedule\nfrom bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\nfrom bionemo.moco.distributions.prior import GaussianPrior\nDEVICE = \"cuda:0\"\nuniform_time = UniformTimeDistribution(discrete_time=True, nsteps = 1000)\nsimple_prior = GaussianPrior()\ninterpolant = DDPM(time_distribution=uniform_time, \n                            prior_distribution=simple_prior,\n                            prediction_type = \"noise\",\n                            noise_schedule = DiscreteLinearNoiseSchedule(nsteps = 1000),\n                            device=DEVICE)\n</pre> from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import DDPM from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule, DiscreteLinearNoiseSchedule from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule from bionemo.moco.distributions.prior import GaussianPrior DEVICE = \"cuda:0\" uniform_time = UniformTimeDistribution(discrete_time=True, nsteps = 1000) simple_prior = GaussianPrior() interpolant = DDPM(time_distribution=uniform_time,                              prior_distribution=simple_prior,                             prediction_type = \"noise\",                             noise_schedule = DiscreteLinearNoiseSchedule(nsteps = 1000),                             device=DEVICE) In\u00a0[20]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, \n                dim_out=dim, \n                dim_hids=[hidden_size]*num_hiddens)\noptimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\ninterpolant = interpolant.to_device(DEVICE)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = interpolant.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = interpolant.sample_time(batch_size)\n    xt = interpolant.interpolate(x1, t, x0)\n\n    eps = model(xt, t)\n    loss = interpolant.loss(eps, x0, t).mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k+1}: loss {loss.item():0.3f}\") \n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim,                  dim_out=dim,                  dim_hids=[hidden_size]*num_hiddens) optimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3) DEVICE = \"cuda\" model = model.to(DEVICE) interpolant = interpolant.to_device(DEVICE) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = interpolant.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = interpolant.sample_time(batch_size)     xt = interpolant.interpolate(x1, t, x0)      eps = model(xt, t)     loss = interpolant.loss(eps, x0, t).mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k+1}: loss {loss.item():0.3f}\")  <pre>1000: loss 0.337\n2000: loss 0.320\n3000: loss 0.260\n4000: loss 0.328\n5000: loss 0.324\n6000: loss 0.427\n7000: loss 0.254\n8000: loss 0.352\n9000: loss 0.365\n10000: loss 0.390\n11000: loss 0.332\n12000: loss 0.265\n13000: loss 0.362\n14000: loss 0.394\n15000: loss 0.405\n16000: loss 0.340\n17000: loss 0.326\n18000: loss 0.357\n19000: loss 0.330\n20000: loss 0.409\n</pre> In\u00a0[21]: Copied! <pre>x0 = interpolant.sample_prior(shape).to(DEVICE)\nx1 = sample_moons(batch_size).to(DEVICE)\nfor t in range(0, 900, 100):\n    tt = interpolant.sample_time(batch_size)*0 + t\n    out = interpolant.interpolate(x1, tt, x0)\n    plt.scatter(out[:, 0].cpu().detach(), out[:, 1].cpu().detach())\n    plt.title(f\"Time = {t}\")\n    plt.show()\n</pre> x0 = interpolant.sample_prior(shape).to(DEVICE) x1 = sample_moons(batch_size).to(DEVICE) for t in range(0, 900, 100):     tt = interpolant.sample_time(batch_size)*0 + t     out = interpolant.interpolate(x1, tt, x0)     plt.scatter(out[:, 0].cpu().detach(), out[:, 1].cpu().detach())     plt.title(f\"Time = {t}\")     plt.show() In\u00a0[22]: Copied! <pre>inf_size = 1024\nschedule = DiscreteLinearInferenceSchedule(nsteps = 1000, direction = \"diffusion\").generate_schedule(device= DEVICE)                                     \nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample]\nfor t in schedule:\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    vt = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step_noise(vt, full_t, sample)\n    trajectory.append(sample) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 schedule = DiscreteLinearInferenceSchedule(nsteps = 1000, direction = \"diffusion\").generate_schedule(device= DEVICE)                                      sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample] for t in schedule:     full_t  = torch.full((inf_size,), t).to(DEVICE)     vt = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step_noise(vt, full_t, sample)     trajectory.append(sample) # save the trajectory for plotting purposes  In\u00a0[23]: Copied! <pre>import matplotlib.pyplot as plt\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() <pre>/home/dreidenbach/mambaforge/envs/moco_bionemo/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[24]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nfor t in schedule:\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        eps_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step(eps_hat, full_t, sample)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] for t in schedule:     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         eps_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step(eps_hat, full_t, sample)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes  In\u00a0[25]: Copied! <pre>traj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[26]: Copied! <pre>inf_size = 1024\nschedule = DiscreteLinearInferenceSchedule(nsteps = 1000, direction = \"diffusion\").generate_schedule(device= DEVICE) \nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nfor t in schedule:\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        eps_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step_ddim(eps_hat, full_t, sample)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 schedule = DiscreteLinearInferenceSchedule(nsteps = 1000, direction = \"diffusion\").generate_schedule(device= DEVICE)  sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] for t in schedule:     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         eps_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step_ddim(eps_hat, full_t, sample)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes  In\u00a0[27]: Copied! <pre>traj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[28]: Copied! <pre>model = Network(dim_in=dim, \n                dim_out=dim, \n                dim_hids=[hidden_size]*num_hiddens).to(DEVICE)\ninf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE)\ntrajectory = [sample.detach().cpu()]\nfor t in schedule:\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        vt = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step_noise(vt, full_t, sample)\n    trajectory.append(sample.detach().cpu()) #\nplot_limit = 1024\ntraj = torch.stack(trajectory).cpu().detach().numpy()\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(0)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(1)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> model = Network(dim_in=dim,                  dim_out=dim,                  dim_hids=[hidden_size]*num_hiddens).to(DEVICE) inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) trajectory = [sample.detach().cpu()] for t in schedule:     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         vt = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step_noise(vt, full_t, sample)     trajectory.append(sample.detach().cpu()) # plot_limit = 1024 traj = torch.stack(trajectory).cpu().detach().numpy()  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(0)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(1)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[29]: Copied! <pre>from bionemo.moco.distributions.time.uniform import UniformTimeDistribution\nfrom bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM\nfrom bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule, DiscreteLinearNoiseSchedule\nfrom bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\nfrom bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\nDEVICE = \"cuda:0\"\nuniform_time = UniformTimeDistribution(discrete_time=True, nsteps = 1000)\nsimple_prior = GaussianPrior()\ninterpolant = DDPM(time_distribution=uniform_time, \n                            prior_distribution=simple_prior,\n                            prediction_type = \"data\",\n                            noise_schedule = DiscreteLinearNoiseSchedule(nsteps = 1000),\n                            device=DEVICE)\n</pre> from bionemo.moco.distributions.time.uniform import UniformTimeDistribution from bionemo.moco.interpolants.discrete_time.continuous.ddpm import DDPM from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule, DiscreteLinearNoiseSchedule from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior DEVICE = \"cuda:0\" uniform_time = UniformTimeDistribution(discrete_time=True, nsteps = 1000) simple_prior = GaussianPrior() interpolant = DDPM(time_distribution=uniform_time,                              prior_distribution=simple_prior,                             prediction_type = \"data\",                             noise_schedule = DiscreteLinearNoiseSchedule(nsteps = 1000),                             device=DEVICE) In\u00a0[30]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, \n                dim_out=dim, \n                dim_hids=[hidden_size]*num_hiddens)\noptimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\ninterpolant = interpolant.to_device(DEVICE)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = interpolant.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = interpolant.sample_time(batch_size)\n    xt = interpolant.interpolate(x1, t, x0)\n\n    x_hat = model(xt, t)\n    loss = interpolant.loss(x_hat, x1, t, weight_type=\"data_to_noise\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k+1}: loss {loss.item():0.3f}\") \n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim,                  dim_out=dim,                  dim_hids=[hidden_size]*num_hiddens) optimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3) DEVICE = \"cuda\" model = model.to(DEVICE) interpolant = interpolant.to_device(DEVICE) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = interpolant.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = interpolant.sample_time(batch_size)     xt = interpolant.interpolate(x1, t, x0)      x_hat = model(xt, t)     loss = interpolant.loss(x_hat, x1, t, weight_type=\"data_to_noise\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k+1}: loss {loss.item():0.3f}\")  <pre>1000: loss 0.908\n2000: loss 32.665\n3000: loss 0.371\n4000: loss 0.970\n5000: loss 0.434\n6000: loss 0.814\n7000: loss 0.599\n8000: loss 0.545\n9000: loss 0.594\n10000: loss 5.172\n11000: loss 0.415\n12000: loss 0.699\n13000: loss 0.400\n14000: loss 0.416\n15000: loss 0.904\n16000: loss 0.785\n17000: loss 0.428\n18000: loss 0.541\n19000: loss 0.346\n20000: loss 1.336\n</pre> In\u00a0[31]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nfor t in schedule:\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step(x_hat, full_t, sample)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] for t in schedule:     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step(x_hat, full_t, sample)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes  In\u00a0[32]: Copied! <pre>import matplotlib.pyplot as plt\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[33]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, \n                dim_out=dim, \n                dim_hids=[hidden_size]*num_hiddens)\noptimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\ninterpolant = interpolant.to_device(DEVICE)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = interpolant.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = interpolant.sample_time(batch_size)\n    xt = interpolant.interpolate(x1, t, x0)\n\n    x_hat = model(xt, t)\n    loss = interpolant.loss(x_hat, x1, t, weight_type=\"ones\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k+1}: loss {loss.item():0.3f}\") \n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim,                  dim_out=dim,                  dim_hids=[hidden_size]*num_hiddens) optimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3) DEVICE = \"cuda\" model = model.to(DEVICE) interpolant = interpolant.to_device(DEVICE) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = interpolant.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = interpolant.sample_time(batch_size)     xt = interpolant.interpolate(x1, t, x0)      x_hat = model(xt, t)     loss = interpolant.loss(x_hat, x1, t, weight_type=\"ones\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k+1}: loss {loss.item():0.3f}\")  <pre>1000: loss 2.489\n2000: loss 2.569\n3000: loss 2.848\n4000: loss 2.444\n5000: loss 2.644\n6000: loss 2.609\n7000: loss 2.766\n8000: loss 2.713\n9000: loss 2.555\n10000: loss 2.639\n11000: loss 2.778\n12000: loss 2.800\n13000: loss 2.509\n14000: loss 2.518\n15000: loss 2.562\n16000: loss 2.739\n17000: loss 2.990\n18000: loss 2.300\n19000: loss 2.318\n20000: loss 2.638\n</pre> In\u00a0[34]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nfor t in schedule:\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step(x_hat, full_t, sample)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] for t in schedule:     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step(x_hat, full_t, sample)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes  In\u00a0[35]: Copied! <pre>import matplotlib.pyplot as plt\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[36]: Copied! <pre>from bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.interpolants import VDM\nfrom bionemo.moco.schedules.noise.continuous_snr_transforms import CosineSNRTransform, LinearSNRTransform, LinearLogInterpolatedSNRTransform\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\nfrom bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior\nDEVICE = \"cuda:0\"\nuniform_time = UniformTimeDistribution(discrete_time=False)\nsimple_prior = GaussianPrior()\ninterpolant = VDM(time_distribution=uniform_time, \n            prior_distribution=simple_prior,\n            prediction_type = \"data\",\n            noise_schedule = LinearLogInterpolatedSNRTransform(),\n            device=DEVICE)\nschedule = LinearInferenceSchedule(nsteps = 1000, direction=\"diffusion\")\n</pre> from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.interpolants import VDM from bionemo.moco.schedules.noise.continuous_snr_transforms import CosineSNRTransform, LinearSNRTransform, LinearLogInterpolatedSNRTransform from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule from bionemo.moco.distributions.prior.continuous.gaussian import GaussianPrior DEVICE = \"cuda:0\" uniform_time = UniformTimeDistribution(discrete_time=False) simple_prior = GaussianPrior() interpolant = VDM(time_distribution=uniform_time,              prior_distribution=simple_prior,             prediction_type = \"data\",             noise_schedule = LinearLogInterpolatedSNRTransform(),             device=DEVICE) schedule = LinearInferenceSchedule(nsteps = 1000, direction=\"diffusion\") In\u00a0[37]: Copied! <pre># Place both the model and the interpolant on the same device\ndim = 2\nhidden_size = 128\nnum_hiddens = 3\nbatch_size = 256\nmodel = Network(dim_in=dim, \n                dim_out=dim, \n                dim_hids=[hidden_size]*num_hiddens)\nDEVICE = \"cuda\"\nmodel = model.to(DEVICE)\n</pre> # Place both the model and the interpolant on the same device dim = 2 hidden_size = 128 num_hiddens = 3 batch_size = 256 model = Network(dim_in=dim,                  dim_out=dim,                  dim_hids=[hidden_size]*num_hiddens) DEVICE = \"cuda\" model = model.to(DEVICE) In\u00a0[38]: Copied! <pre>optimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3)\nfor k in range(20000):\n    optimizer.zero_grad()\n    shape = (batch_size, dim)\n    x0 = interpolant.sample_prior(shape).to(DEVICE)\n    x1 = sample_moons(batch_size).to(DEVICE)\n\n    t = interpolant.sample_time(batch_size)\n    xt = interpolant.interpolate(x1, t, x0)\n\n    x_hat = model(xt, t)\n    loss = interpolant.loss(x_hat, x1, t, weight_type=\"ones\").mean()\n\n    loss.backward()\n    optimizer.step()\n\n    if (k + 1) % 1000 == 0:\n        print(f\"{k+1}: loss {loss.item():0.3f}\") \n</pre> optimizer = torch.optim.Adam(model.parameters(), lr = 1.e-3) for k in range(20000):     optimizer.zero_grad()     shape = (batch_size, dim)     x0 = interpolant.sample_prior(shape).to(DEVICE)     x1 = sample_moons(batch_size).to(DEVICE)      t = interpolant.sample_time(batch_size)     xt = interpolant.interpolate(x1, t, x0)      x_hat = model(xt, t)     loss = interpolant.loss(x_hat, x1, t, weight_type=\"ones\").mean()      loss.backward()     optimizer.step()      if (k + 1) % 1000 == 0:         print(f\"{k+1}: loss {loss.item():0.3f}\")  <pre>1000: loss 1.331\n2000: loss 1.067\n3000: loss 1.343\n4000: loss 1.291\n5000: loss 1.249\n6000: loss 0.954\n7000: loss 1.063\n8000: loss 1.179\n9000: loss 1.246\n10000: loss 1.641\n11000: loss 1.088\n12000: loss 1.208\n13000: loss 1.274\n14000: loss 0.927\n15000: loss 1.078\n16000: loss 1.109\n17000: loss 1.046\n18000: loss 1.235\n19000: loss 1.325\n20000: loss 1.159\n</pre> In\u00a0[39]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nfor dt, t in zip(dts, ts):\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step(x_hat, full_t, sample, dt)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] ts = schedule.generate_schedule() dts = schedule.discretize() for dt, t in zip(dts, ts):     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step(x_hat, full_t, sample, dt)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes  In\u00a0[40]: Copied! <pre>import matplotlib.pyplot as plt\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[41]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nfor dt, t in zip(dts, ts):\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step_ddim(x_hat, full_t, sample, dt)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] ts = schedule.generate_schedule() dts = schedule.discretize() for dt, t in zip(dts, ts):     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step_ddim(x_hat, full_t, sample, dt)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes  In\u00a0[42]: Copied! <pre>import matplotlib.pyplot as plt\ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> import matplotlib.pyplot as plt traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[43]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nfor dt, t in zip(dts, ts):\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    # sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)\n    sample = interpolant.step_ode(x_hat, full_t, sample, dt)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n    \ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] ts = schedule.generate_schedule() dts = schedule.discretize() for dt, t in zip(dts, ts):     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     # sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)     sample = interpolant.step_ode(x_hat, full_t, sample, dt)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes      traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[44]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nfor dt, t in zip(dts, ts):\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    # sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)\n    sample = interpolant.step_ode(x_hat, full_t, sample, dt, temperature = 1.5)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n    \ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\n# Assuming traj is your tensor and traj.shape = (N, 2000, 2)\n# where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] ts = schedule.generate_schedule() dts = schedule.discretize() for dt, t in zip(dts, ts):     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     # sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)     sample = interpolant.step_ode(x_hat, full_t, sample, dt, temperature = 1.5)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes      traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  # Assuming traj is your tensor and traj.shape = (N, 2000, 2) # where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[45]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nfor dt, t in zip(dts, ts):\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    # sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)\n    sample = interpolant.step_ode(x_hat, full_t, sample, dt, temperature = 0.5)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n    \ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] ts = schedule.generate_schedule() dts = schedule.discretize() for dt, t in zip(dts, ts):     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     # sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)     sample = interpolant.step_ode(x_hat, full_t, sample, dt, temperature = 0.5)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes      traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show() In\u00a0[46]: Copied! <pre>inf_size = 1024\nsample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise\ntrajectory = [sample.detach().cpu()]\nts = schedule.generate_schedule()\ndts = schedule.discretize()\nfor dt, t in zip(dts, ts):\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    with torch.no_grad():\n        x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model\n    sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)\n    # sample = interpolant.step_ode(x_hat, full_t, sample, dt)\n    trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes\n    \ntraj = torch.stack(trajectory).cpu().detach().numpy()\nplot_limit = 1024\n\nplt.figure(figsize=(6, 6))\n\n# Plot the first time point in black\nplt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, traj.shape[0]-1):\n    plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")\n\n# Plot the last time point in blue\nplt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nplt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow')\nplt.legend()\nplt.xticks([])\nplt.yticks([])\nplt.show()\n</pre> inf_size = 1024 sample = interpolant.sample_prior((inf_size, 2)).to(DEVICE) # Start with noise trajectory = [sample.detach().cpu()] ts = schedule.generate_schedule() dts = schedule.discretize() for dt, t in zip(dts, ts):     full_t  = torch.full((inf_size,), t).to(DEVICE)     with torch.no_grad():         x_hat = model(sample, full_t) # calculate the vector field based on the definition of the model     sample = interpolant.step_hybrid_sde(x_hat, full_t, sample, dt)     # sample = interpolant.step_ode(x_hat, full_t, sample, dt)     trajectory.append(sample.detach().cpu()) # save the trajectory for plotting purposes      traj = torch.stack(trajectory).cpu().detach().numpy() plot_limit = 1024  plt.figure(figsize=(6, 6))  # Plot the first time point in black plt.scatter(traj[0, :plot_limit, 0], traj[0, :plot_limit, 1], s=10, alpha=0.8, c=\"black\", label='Prior sample z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, traj.shape[0]-1):     plt.scatter(traj[i, :plot_limit, 0], traj[i, :plot_limit, 1], s=0.2, alpha=0.2, c=\"olive\")  # Plot the last time point in blue plt.scatter(traj[-1, :plot_limit, 0], traj[-1, :plot_limit, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly plt.scatter([], [], s=0.2, alpha=0.2, c=\"olive\", label='Flow') plt.legend() plt.xticks([]) plt.yticks([]) plt.show()"},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#building-generative-models-for-continuous-data-via-continuous-interpolants","title":"Building Generative Models for Continuous Data via Continuous Interpolants\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#task-setup","title":"Task Setup\u00b6","text":"<p>To demonstrate how Conditional Flow Matching works we use sklearn to sample from and create custom 2D distriubtions.</p> <p>To start we define our \"dataloader\" so to speak. This is the '''sample_moons''' function.</p> <p>Next we define a custom PriorDistribution to enable the conversion of 8 equidistance gaussians to the moon distribution above.</p>"},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#model-creation","title":"Model Creation\u00b6","text":"<p>Here we define a simple 4 layer MLP and define our optimizer</p>"},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#continuous-flow-matching-interpolant","title":"Continuous Flow Matching Interpolant\u00b6","text":"<p>Here we import our desired interpolant objects.</p> <p>The continuous flow matcher and the desired time distribution.</p>"},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#training-loop","title":"Training Loop\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#setting-up-generation","title":"Setting Up Generation\u00b6","text":"<p>Now we need to import the desired inference time schedule. This is what gives us the time values to iterate through to iteratively generate from our model.</p> <p>Here we show the output time schedule as well as the discretization between time points. We note that different inference time schedules may have different shapes resulting in non uniform dt</p>"},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#sample-from-the-trained-model","title":"Sample from the trained model\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#sample-from-underlying-score-model","title":"Sample from underlying score model\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#low-temperature-sampling-is-a-heuristic-unclear-what-effects-it-has-on-the-final-distribution-intuitively-it-cuts-tails-and-focuses-more-on-the-mode-in-practice-who-knows-exactly-whats-the-final-effect","title":"low temperature sampling is a heuristic, unclear what effects it has on the final distribution. Intuitively, it cuts tails and focuses more on the mode, in practice who knows exactly what's the final effect.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#gt_mode-is-a-hyperparameter-that-must-be-experimentally-chosen","title":"gt_mode is a hyperparameter that must be experimentally chosen\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#what-happens-if-you-just-sample-from-a-random-model","title":"What happens if you just sample from a random model?\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#now-lets-try-a-different-interpolant-type","title":"Now let's try a different Interpolant type\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#lets-create-an-architecture-that-has-a-formal-time-embedding-as-here-we-use-more-timesteps","title":"Let's create an architecture that has a formal time embedding as here we use more timesteps\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#ddpm-interpolant","title":"DDPM Interpolant\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#note-ddpm-must-be-used-with-a-gaussian-prior","title":"note DDPM must be used with a Gaussian Prior.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#train-the-model","title":"Train the Model\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#lets-vizualize-what-the-interpolation-looks-like-during-training-for-different-times","title":"Let's vizualize what the interpolation looks like during training for different times\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#create-the-inference-time-schedule-and-sample-from-the-model","title":"Create the inference time schedule and sample from the model\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#notice-that-his-yields-very-similar-results-to-using-the-underlying-score-function-in-the-stochastic-score-based-cfm-example","title":"Notice that his yields very similar results to using the underlying score function in the stochastic score based CFM example\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#notice-that-there-is-no-difference-whether-or-not-we-convert-the-predicted-noise-to-data-inside-thte-step-function","title":"Notice that there is no difference whether or not we convert the predicted noise to data inside thte .step() function\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#lets-try-other-cool-sampling-functions","title":"Let's try other cool sampling functions\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#what-happens-when-you-sample-from-an-untrained-model-with-ddpm","title":"What happens when you sample from an untrained model with DDPM\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#now-lets-switch-the-parameterization-of-ddpm-from-noise-to-data","title":"Now let's switch the parameterization of DDPM from noise to data\u00b6","text":"<p>Here instead of training the model to learn the noise we want to learn the raw data. Both options are valid and the choice of which depends on the underlying modeling task.</p>"},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#let-us-first-train-the-model-with-a-weight-such-that-it-is-theoretically-equivalent-to-the-simple-noise-matching-loss-see-equation-9-from-httpsarxivorgpdf220200512","title":"Let us first train the model with a weight such that it is theoretically equivalent to the simple noise matching loss. See Equation 9 from https://arxiv.org/pdf/2202.00512\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#now-let-us-train-with-no-loss-weighting-to-optimize-a-true-data-matching-loss-for-comparison","title":"Now let us train with no loss weighting to optimize a true data matching loss for comparison\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#the-choice-in-data-vs-noise-and-variance-schedule-are-hyperparameters-that-must-be-tuned-to-each-task","title":"The choice in data vs noise and variance schedule are hyperparameters that must be tuned to each task\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#many-of-these-choices-are-empirical-and-part-of-the-tuning-process-to-best-model-your-data-via-noise-data-or-even-velocity-prediction","title":"many of these choices are empirical and part of the tuning process to best model your data via noise, data, or even velocity prediction.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#now-lets-try-a-continuous-time-analog-interpolant-to-ddpm-called-vdm","title":"Now let's try a continuous time analog interpolant to DDPM called VDM\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#this-interpolant-was-used-in-chroma-and-is-described-in-great-detail-here-httpswwwbiorxivorgcontent10110120221201518682v1fullpdf","title":"This interpolant was used in Chroma and is described in great detail here https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1.full.pdf\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/continuous_data_interpolant_tutorial/#what-is-interesting-here-is-that-the-deterministic-sampling-of-ddim-best-recovers-the-flow-matching-ode-samples","title":"What is interesting here is that the deterministic sampling of DDIM best recovers the Flow Matching ODE samples\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/","title":"Building Generative Models for Discrete Data via Discrete Interpolants","text":"In\u00a0[1]: Copied! <pre>import math\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\ntorch.cuda.manual_seed(42)\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions.categorical import Categorical\nfrom tqdm import tqdm\n</pre> import math import os import time  import matplotlib.pyplot as plt import numpy as np import torch torch.cuda.manual_seed(42) import torch.nn as nn import torch.nn.functional as F from torch.distributions.categorical import Categorical from tqdm import tqdm In\u00a0[2]: Copied! <pre># training\nB = 32 # batch size\nD = 10 # dimension or sequence length, this is the number of discrete elements\nS = 2 # state space, binary so S=2\n# here we have a batch of 32 objects that consist of 10 binary variables. \n\nclass Model(nn.Module):\n    def __init__(self, D, S):\n        super().__init__()\n        self.embedding = nn.Embedding(S+1, 16)\n        self.net = nn.Sequential(\n            nn.Linear(17 * D, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, S*D),\n        )\n\n    def forward(self, x, t):\n        B, D = x.shape\n        x_emb = self.embedding(x) # (B, D, 16)\n        net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1) # (B, D * 17)\n        return self.net(net_input).reshape(B, D, S) # (B, D, S)\n</pre> # training B = 32 # batch size D = 10 # dimension or sequence length, this is the number of discrete elements S = 2 # state space, binary so S=2 # here we have a batch of 32 objects that consist of 10 binary variables.   class Model(nn.Module):     def __init__(self, D, S):         super().__init__()         self.embedding = nn.Embedding(S+1, 16)         self.net = nn.Sequential(             nn.Linear(17 * D, 128),             nn.ReLU(),             nn.Linear(128, 128),             nn.ReLU(),             nn.Linear(128, S*D),         )      def forward(self, x, t):         B, D = x.shape         x_emb = self.embedding(x) # (B, D, 16)         net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1) # (B, D * 17)         return self.net(net_input).reshape(B, D, S) # (B, D, S)  In\u00a0[3]: Copied! <pre>from bionemo.moco.distributions.prior import DiscreteUniformPrior\nfrom bionemo.moco.interpolants import DiscreteFlowMatcher\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\nB = 32 # batch size\nD = 10 # dimension\nS = 2 # state space\n\nDEVICE = \"cuda:0\"\nprior = DiscreteUniformPrior(num_classes=S)\ntime_distribution = UniformTimeDistribution()\ndfm = DiscreteFlowMatcher(time_distribution=time_distribution,\n                          prior_distribution=prior,\n                          device=DEVICE)\nschedule = LinearInferenceSchedule(nsteps = 1000)\n</pre> from bionemo.moco.distributions.prior import DiscreteUniformPrior from bionemo.moco.interpolants import DiscreteFlowMatcher from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule  B = 32 # batch size D = 10 # dimension S = 2 # state space  DEVICE = \"cuda:0\" prior = DiscreteUniformPrior(num_classes=S) time_distribution = UniformTimeDistribution() dfm = DiscreteFlowMatcher(time_distribution=time_distribution,                           prior_distribution=prior,                           device=DEVICE) schedule = LinearInferenceSchedule(nsteps = 1000) In\u00a0[4]: Copied! <pre>model = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n</pre> model = Model(D, S) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) In\u00a0[5]: Copied! <pre>model = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D+1, (B,))\n    x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    x0 = dfm.sample_prior(x1.shape) # B x D\n    t = dfm.sample_time(B)\n    xt = dfm.interpolate(x1, t, x0)\n    logits = model(xt, t) # (B, D, S)\n    loss = dfm.loss(logits, x1, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n</pre> model = model.to(DEVICE) losses = [] for _ in tqdm(range(50000)):     num_ones = torch.randint(0, D+1, (B,))     x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)     # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]     optimizer.zero_grad()     x0 = dfm.sample_prior(x1.shape) # B x D     t = dfm.sample_time(B)     xt = dfm.interpolate(x1, t, x0)     logits = model(xt, t) # (B, D, S)     loss = dfm.loss(logits, x1, t).mean()     loss.backward()     optimizer.step()     losses.append(loss.item()) <pre>  0%|          | 0/50000 [00:00&lt;?, ?it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [03:17&lt;00:00, 252.71it/s]\n</pre> In\u00a0[6]: Copied! <pre>plt.plot(losses, label='Training Loss', linestyle='-', color='blue', marker='o')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> plt.plot(losses, label='Training Loss', linestyle='-', color='blue', marker='o') plt.xlabel('Step') plt.ylabel('Loss') plt.title('Training Loss') plt.legend() plt.grid(True) plt.show() In\u00a0[7]: Copied! <pre>num_samples = 1000\nxt = dfm.sample_prior((num_samples, D))\nprint(xt.shape)\nts = schedule.generate_schedule(device=DEVICE)\ndts = schedule.discretize(device=DEVICE)\n</pre> num_samples = 1000 xt = dfm.sample_prior((num_samples, D)) print(xt.shape) ts = schedule.generate_schedule(device=DEVICE) dts = schedule.discretize(device=DEVICE) <pre>torch.Size([1000, 10])\n</pre> In\u00a0[8]: Copied! <pre>ts\n</pre> ts Out[8]: <pre>tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,\n        0.0090, 0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160, 0.0170,\n        0.0180, 0.0190, 0.0200, 0.0210, 0.0220, 0.0230, 0.0240, 0.0250, 0.0260,\n        0.0270, 0.0280, 0.0290, 0.0300, 0.0310, 0.0320, 0.0330, 0.0340, 0.0350,\n        0.0360, 0.0370, 0.0380, 0.0390, 0.0400, 0.0410, 0.0420, 0.0430, 0.0440,\n        0.0450, 0.0460, 0.0470, 0.0480, 0.0490, 0.0500, 0.0510, 0.0520, 0.0530,\n        0.0540, 0.0550, 0.0560, 0.0570, 0.0580, 0.0590, 0.0600, 0.0610, 0.0620,\n        0.0630, 0.0640, 0.0650, 0.0660, 0.0670, 0.0680, 0.0690, 0.0700, 0.0710,\n        0.0720, 0.0730, 0.0740, 0.0750, 0.0760, 0.0770, 0.0780, 0.0790, 0.0800,\n        0.0810, 0.0820, 0.0830, 0.0840, 0.0850, 0.0860, 0.0870, 0.0880, 0.0890,\n        0.0900, 0.0910, 0.0920, 0.0930, 0.0940, 0.0950, 0.0960, 0.0970, 0.0980,\n        0.0990, 0.1000, 0.1010, 0.1020, 0.1030, 0.1040, 0.1050, 0.1060, 0.1070,\n        0.1080, 0.1090, 0.1100, 0.1110, 0.1120, 0.1130, 0.1140, 0.1150, 0.1160,\n        0.1170, 0.1180, 0.1190, 0.1200, 0.1210, 0.1220, 0.1230, 0.1240, 0.1250,\n        0.1260, 0.1270, 0.1280, 0.1290, 0.1300, 0.1310, 0.1320, 0.1330, 0.1340,\n        0.1350, 0.1360, 0.1370, 0.1380, 0.1390, 0.1400, 0.1410, 0.1420, 0.1430,\n        0.1440, 0.1450, 0.1460, 0.1470, 0.1480, 0.1490, 0.1500, 0.1510, 0.1520,\n        0.1530, 0.1540, 0.1550, 0.1560, 0.1570, 0.1580, 0.1590, 0.1600, 0.1610,\n        0.1620, 0.1630, 0.1640, 0.1650, 0.1660, 0.1670, 0.1680, 0.1690, 0.1700,\n        0.1710, 0.1720, 0.1730, 0.1740, 0.1750, 0.1760, 0.1770, 0.1780, 0.1790,\n        0.1800, 0.1810, 0.1820, 0.1830, 0.1840, 0.1850, 0.1860, 0.1870, 0.1880,\n        0.1890, 0.1900, 0.1910, 0.1920, 0.1930, 0.1940, 0.1950, 0.1960, 0.1970,\n        0.1980, 0.1990, 0.2000, 0.2010, 0.2020, 0.2030, 0.2040, 0.2050, 0.2060,\n        0.2070, 0.2080, 0.2090, 0.2100, 0.2110, 0.2120, 0.2130, 0.2140, 0.2150,\n        0.2160, 0.2170, 0.2180, 0.2190, 0.2200, 0.2210, 0.2220, 0.2230, 0.2240,\n        0.2250, 0.2260, 0.2270, 0.2280, 0.2290, 0.2300, 0.2310, 0.2320, 0.2330,\n        0.2340, 0.2350, 0.2360, 0.2370, 0.2380, 0.2390, 0.2400, 0.2410, 0.2420,\n        0.2430, 0.2440, 0.2450, 0.2460, 0.2470, 0.2480, 0.2490, 0.2500, 0.2510,\n        0.2520, 0.2530, 0.2540, 0.2550, 0.2560, 0.2570, 0.2580, 0.2590, 0.2600,\n        0.2610, 0.2620, 0.2630, 0.2640, 0.2650, 0.2660, 0.2670, 0.2680, 0.2690,\n        0.2700, 0.2710, 0.2720, 0.2730, 0.2740, 0.2750, 0.2760, 0.2770, 0.2780,\n        0.2790, 0.2800, 0.2810, 0.2820, 0.2830, 0.2840, 0.2850, 0.2860, 0.2870,\n        0.2880, 0.2890, 0.2900, 0.2910, 0.2920, 0.2930, 0.2940, 0.2950, 0.2960,\n        0.2970, 0.2980, 0.2990, 0.3000, 0.3010, 0.3020, 0.3030, 0.3040, 0.3050,\n        0.3060, 0.3070, 0.3080, 0.3090, 0.3100, 0.3110, 0.3120, 0.3130, 0.3140,\n        0.3150, 0.3160, 0.3170, 0.3180, 0.3190, 0.3200, 0.3210, 0.3220, 0.3230,\n        0.3240, 0.3250, 0.3260, 0.3270, 0.3280, 0.3290, 0.3300, 0.3310, 0.3320,\n        0.3330, 0.3340, 0.3350, 0.3360, 0.3370, 0.3380, 0.3390, 0.3400, 0.3410,\n        0.3420, 0.3430, 0.3440, 0.3450, 0.3460, 0.3470, 0.3480, 0.3490, 0.3500,\n        0.3510, 0.3520, 0.3530, 0.3540, 0.3550, 0.3560, 0.3570, 0.3580, 0.3590,\n        0.3600, 0.3610, 0.3620, 0.3630, 0.3640, 0.3650, 0.3660, 0.3670, 0.3680,\n        0.3690, 0.3700, 0.3710, 0.3720, 0.3730, 0.3740, 0.3750, 0.3760, 0.3770,\n        0.3780, 0.3790, 0.3800, 0.3810, 0.3820, 0.3830, 0.3840, 0.3850, 0.3860,\n        0.3870, 0.3880, 0.3890, 0.3900, 0.3910, 0.3920, 0.3930, 0.3940, 0.3950,\n        0.3960, 0.3970, 0.3980, 0.3990, 0.4000, 0.4010, 0.4020, 0.4030, 0.4040,\n        0.4050, 0.4060, 0.4070, 0.4080, 0.4090, 0.4100, 0.4110, 0.4120, 0.4130,\n        0.4140, 0.4150, 0.4160, 0.4170, 0.4180, 0.4190, 0.4200, 0.4210, 0.4220,\n        0.4230, 0.4240, 0.4250, 0.4260, 0.4270, 0.4280, 0.4290, 0.4300, 0.4310,\n        0.4320, 0.4330, 0.4340, 0.4350, 0.4360, 0.4370, 0.4380, 0.4390, 0.4400,\n        0.4410, 0.4420, 0.4430, 0.4440, 0.4450, 0.4460, 0.4470, 0.4480, 0.4490,\n        0.4500, 0.4510, 0.4520, 0.4530, 0.4540, 0.4550, 0.4560, 0.4570, 0.4580,\n        0.4590, 0.4600, 0.4610, 0.4620, 0.4630, 0.4640, 0.4650, 0.4660, 0.4670,\n        0.4680, 0.4690, 0.4700, 0.4710, 0.4720, 0.4730, 0.4740, 0.4750, 0.4760,\n        0.4770, 0.4780, 0.4790, 0.4800, 0.4810, 0.4820, 0.4830, 0.4840, 0.4850,\n        0.4860, 0.4870, 0.4880, 0.4890, 0.4900, 0.4910, 0.4920, 0.4930, 0.4940,\n        0.4950, 0.4960, 0.4970, 0.4980, 0.4990, 0.5000, 0.5010, 0.5020, 0.5030,\n        0.5040, 0.5050, 0.5060, 0.5070, 0.5080, 0.5090, 0.5100, 0.5110, 0.5120,\n        0.5130, 0.5140, 0.5150, 0.5160, 0.5170, 0.5180, 0.5190, 0.5200, 0.5210,\n        0.5220, 0.5230, 0.5240, 0.5250, 0.5260, 0.5270, 0.5280, 0.5290, 0.5300,\n        0.5310, 0.5320, 0.5330, 0.5340, 0.5350, 0.5360, 0.5370, 0.5380, 0.5390,\n        0.5400, 0.5410, 0.5420, 0.5430, 0.5440, 0.5450, 0.5460, 0.5470, 0.5480,\n        0.5490, 0.5500, 0.5510, 0.5520, 0.5530, 0.5540, 0.5550, 0.5560, 0.5570,\n        0.5580, 0.5590, 0.5600, 0.5610, 0.5620, 0.5630, 0.5640, 0.5650, 0.5660,\n        0.5670, 0.5680, 0.5690, 0.5700, 0.5710, 0.5720, 0.5730, 0.5740, 0.5750,\n        0.5760, 0.5770, 0.5780, 0.5790, 0.5800, 0.5810, 0.5820, 0.5830, 0.5840,\n        0.5850, 0.5860, 0.5870, 0.5880, 0.5890, 0.5900, 0.5910, 0.5920, 0.5930,\n        0.5940, 0.5950, 0.5960, 0.5970, 0.5980, 0.5990, 0.6000, 0.6010, 0.6020,\n        0.6030, 0.6040, 0.6050, 0.6060, 0.6070, 0.6080, 0.6090, 0.6100, 0.6110,\n        0.6120, 0.6130, 0.6140, 0.6150, 0.6160, 0.6170, 0.6180, 0.6190, 0.6200,\n        0.6210, 0.6220, 0.6230, 0.6240, 0.6250, 0.6260, 0.6270, 0.6280, 0.6290,\n        0.6300, 0.6310, 0.6320, 0.6330, 0.6340, 0.6350, 0.6360, 0.6370, 0.6380,\n        0.6390, 0.6400, 0.6410, 0.6420, 0.6430, 0.6440, 0.6450, 0.6460, 0.6470,\n        0.6480, 0.6490, 0.6500, 0.6510, 0.6520, 0.6530, 0.6540, 0.6550, 0.6560,\n        0.6570, 0.6580, 0.6590, 0.6600, 0.6610, 0.6620, 0.6630, 0.6640, 0.6650,\n        0.6660, 0.6670, 0.6680, 0.6690, 0.6700, 0.6710, 0.6720, 0.6730, 0.6740,\n        0.6750, 0.6760, 0.6770, 0.6780, 0.6790, 0.6800, 0.6810, 0.6820, 0.6830,\n        0.6840, 0.6850, 0.6860, 0.6870, 0.6880, 0.6890, 0.6900, 0.6910, 0.6920,\n        0.6930, 0.6940, 0.6950, 0.6960, 0.6970, 0.6980, 0.6990, 0.7000, 0.7010,\n        0.7020, 0.7030, 0.7040, 0.7050, 0.7060, 0.7070, 0.7080, 0.7090, 0.7100,\n        0.7110, 0.7120, 0.7130, 0.7140, 0.7150, 0.7160, 0.7170, 0.7180, 0.7190,\n        0.7200, 0.7210, 0.7220, 0.7230, 0.7240, 0.7250, 0.7260, 0.7270, 0.7280,\n        0.7290, 0.7300, 0.7310, 0.7320, 0.7330, 0.7340, 0.7350, 0.7360, 0.7370,\n        0.7380, 0.7390, 0.7400, 0.7410, 0.7420, 0.7430, 0.7440, 0.7450, 0.7460,\n        0.7470, 0.7480, 0.7490, 0.7500, 0.7510, 0.7520, 0.7530, 0.7540, 0.7550,\n        0.7560, 0.7570, 0.7580, 0.7590, 0.7600, 0.7610, 0.7620, 0.7630, 0.7640,\n        0.7650, 0.7660, 0.7670, 0.7680, 0.7690, 0.7700, 0.7710, 0.7720, 0.7730,\n        0.7740, 0.7750, 0.7760, 0.7770, 0.7780, 0.7790, 0.7800, 0.7810, 0.7820,\n        0.7830, 0.7840, 0.7850, 0.7860, 0.7870, 0.7880, 0.7890, 0.7900, 0.7910,\n        0.7920, 0.7930, 0.7940, 0.7950, 0.7960, 0.7970, 0.7980, 0.7990, 0.8000,\n        0.8010, 0.8020, 0.8030, 0.8040, 0.8050, 0.8060, 0.8070, 0.8080, 0.8090,\n        0.8100, 0.8110, 0.8120, 0.8130, 0.8140, 0.8150, 0.8160, 0.8170, 0.8180,\n        0.8190, 0.8200, 0.8210, 0.8220, 0.8230, 0.8240, 0.8250, 0.8260, 0.8270,\n        0.8280, 0.8290, 0.8300, 0.8310, 0.8320, 0.8330, 0.8340, 0.8350, 0.8360,\n        0.8370, 0.8380, 0.8390, 0.8400, 0.8410, 0.8420, 0.8430, 0.8440, 0.8450,\n        0.8460, 0.8470, 0.8480, 0.8490, 0.8500, 0.8510, 0.8520, 0.8530, 0.8540,\n        0.8550, 0.8560, 0.8570, 0.8580, 0.8590, 0.8600, 0.8610, 0.8620, 0.8630,\n        0.8640, 0.8650, 0.8660, 0.8670, 0.8680, 0.8690, 0.8700, 0.8710, 0.8720,\n        0.8730, 0.8740, 0.8750, 0.8760, 0.8770, 0.8780, 0.8790, 0.8800, 0.8810,\n        0.8820, 0.8830, 0.8840, 0.8850, 0.8860, 0.8870, 0.8880, 0.8890, 0.8900,\n        0.8910, 0.8920, 0.8930, 0.8940, 0.8950, 0.8960, 0.8970, 0.8980, 0.8990,\n        0.9000, 0.9010, 0.9020, 0.9030, 0.9040, 0.9050, 0.9060, 0.9070, 0.9080,\n        0.9090, 0.9100, 0.9110, 0.9120, 0.9130, 0.9140, 0.9150, 0.9160, 0.9170,\n        0.9180, 0.9190, 0.9200, 0.9210, 0.9220, 0.9230, 0.9240, 0.9250, 0.9260,\n        0.9270, 0.9280, 0.9290, 0.9300, 0.9310, 0.9320, 0.9330, 0.9340, 0.9350,\n        0.9360, 0.9370, 0.9380, 0.9390, 0.9400, 0.9410, 0.9420, 0.9430, 0.9440,\n        0.9450, 0.9460, 0.9470, 0.9480, 0.9490, 0.9500, 0.9510, 0.9520, 0.9530,\n        0.9540, 0.9550, 0.9560, 0.9570, 0.9580, 0.9590, 0.9600, 0.9610, 0.9620,\n        0.9630, 0.9640, 0.9650, 0.9660, 0.9670, 0.9680, 0.9690, 0.9700, 0.9710,\n        0.9720, 0.9730, 0.9740, 0.9750, 0.9760, 0.9770, 0.9780, 0.9790, 0.9800,\n        0.9810, 0.9820, 0.9830, 0.9840, 0.9850, 0.9860, 0.9870, 0.9880, 0.9890,\n        0.9900, 0.9910, 0.9920, 0.9930, 0.9940, 0.9950, 0.9960, 0.9970, 0.9980,\n        0.9990], device='cuda:0')</pre> In\u00a0[9]: Copied! <pre>LinearInferenceSchedule(nsteps = 100, min_t=0, inclusive_end=False).generate_schedule()\n</pre> LinearInferenceSchedule(nsteps = 100, min_t=0, inclusive_end=False).generate_schedule() Out[9]: <pre>tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n        0.9900])</pre> In\u00a0[10]: Copied! <pre>for dt, t in zip(dts, ts):\n    t = schedule.pad_time(num_samples, t, DEVICE)\n    logits = model(xt, t)\n    xt = dfm.step(logits, t, xt, dt, stochasticity=0)\n</pre> for dt, t in zip(dts, ts):     t = schedule.pad_time(num_samples, t, DEVICE)     logits = model(xt, t)     xt = dfm.step(logits, t, xt, dt, stochasticity=0)  In\u00a0[11]: Copied! <pre>counts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D+2))\nplt.show()\n</pre> counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D+2)) plt.show() In\u00a0[12]: Copied! <pre>num_ones = torch.randint(0, D+1, (1000,))\nx1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long()\ncounts = x1.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D+2))\nplt.show()\n</pre> num_ones = torch.randint(0, D+1, (1000,)) x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long() counts = x1.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D+2)) plt.show() In\u00a0[13]: Copied! <pre>x0 = dfm.sample_prior((10000, D))\ncounts = x0.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D+2))\nplt.show()\n</pre> x0 = dfm.sample_prior((10000, D)) counts = x0.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D+2)) plt.show() In\u00a0[14]: Copied! <pre>from bionemo.moco.distributions.prior import DiscreteUniformPrior\nfrom bionemo.moco.interpolants import D3PM\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule\nfrom bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule\n\nB = 32 # batch size\nD = 10 # dimension\nS = 2 # state space\n\nDEVICE = \"cuda:0\"\nprior = DiscreteUniformPrior(num_classes=S)\ntime_distribution = UniformTimeDistribution(discrete_time = True, nsteps = 1000)\nnoise_schedule = DiscreteCosineNoiseSchedule(nsteps = 1000)\nd3pm = D3PM(time_distribution=time_distribution,\n                          prior_distribution=prior,\n                          noise_schedule = noise_schedule,\n                          device=DEVICE)\nschedule = DiscreteLinearInferenceSchedule(nsteps = 1000, direction=\"diffusion\", device=DEVICE)\n</pre> from bionemo.moco.distributions.prior import DiscreteUniformPrior from bionemo.moco.interpolants import D3PM from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.schedules.noise.discrete_noise_schedules import DiscreteCosineNoiseSchedule from bionemo.moco.schedules.inference_time_schedules import DiscreteLinearInferenceSchedule  B = 32 # batch size D = 10 # dimension S = 2 # state space  DEVICE = \"cuda:0\" prior = DiscreteUniformPrior(num_classes=S) time_distribution = UniformTimeDistribution(discrete_time = True, nsteps = 1000) noise_schedule = DiscreteCosineNoiseSchedule(nsteps = 1000) d3pm = D3PM(time_distribution=time_distribution,                           prior_distribution=prior,                           noise_schedule = noise_schedule,                           device=DEVICE) schedule = DiscreteLinearInferenceSchedule(nsteps = 1000, direction=\"diffusion\", device=DEVICE) In\u00a0[15]: Copied! <pre>model = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\nd3pm.terminal_distribution # here we can see as confirmation that the distribution we are diffusing from is binary\n</pre> model = Model(D, S) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) d3pm.terminal_distribution # here we can see as confirmation that the distribution we are diffusing from is binary Out[15]: <pre>tensor([0.5000, 0.5000])</pre> In\u00a0[16]: Copied! <pre>model = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D+1, (B,))\n    x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    # x0 = dfm.sample_prior(x1.shape) # B x D\n    t = d3pm.sample_time(B)\n    xt = d3pm.interpolate(x1, t)\n    logits = model(xt, t) # (B, D, S)\n    loss = d3pm.loss(logits, x1, xt, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n</pre> model = model.to(DEVICE) losses = [] for _ in tqdm(range(50000)):     num_ones = torch.randint(0, D+1, (B,))     x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)     # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]     optimizer.zero_grad()     # x0 = dfm.sample_prior(x1.shape) # B x D     t = d3pm.sample_time(B)     xt = d3pm.interpolate(x1, t)     logits = model(xt, t) # (B, D, S)     loss = d3pm.loss(logits, x1, xt, t).mean()     loss.backward()     optimizer.step()     losses.append(loss.item()) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [04:21&lt;00:00, 191.13it/s]\n</pre> In\u00a0[17]: Copied! <pre>plt.plot(losses, label='Training Loss', linestyle='-', color='blue', marker='o')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\nplt.grid(True)\nplt.ylim([0,1])\n# plt.yscale('log')\nplt.show()\n</pre> plt.plot(losses, label='Training Loss', linestyle='-', color='blue', marker='o') plt.xlabel('Step') plt.ylabel('Loss') plt.title('Training Loss') plt.legend() plt.grid(True) plt.ylim([0,1]) # plt.yscale('log') plt.show() In\u00a0[18]: Copied! <pre>ts = schedule.generate_schedule()\nnum_samples = 1000\nxt = d3pm.sample_prior((num_samples, D))\nfor t in ts:\n    t = torch.full((xt.shape[0],), t).to(DEVICE)\n    logits = model(xt, t)\n    xt = d3pm.step(logits, t, xt)\n</pre> ts = schedule.generate_schedule() num_samples = 1000 xt = d3pm.sample_prior((num_samples, D)) for t in ts:     t = torch.full((xt.shape[0],), t).to(DEVICE)     logits = model(xt, t)     xt = d3pm.step(logits, t, xt) In\u00a0[19]: Copied! <pre>counts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D+2))\nplt.show()\n</pre> counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D+2)) plt.show() In\u00a0[20]: Copied! <pre>xt = d3pm.sample_prior((num_samples, D))\ncounts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D+2))\nplt.show()\n</pre> xt = d3pm.sample_prior((num_samples, D)) counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D+2)) plt.show() In\u00a0[21]: Copied! <pre>from bionemo.moco.distributions.prior import DiscreteMaskedPrior\nfrom bionemo.moco.interpolants import MDLM\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\n\nDEVICE = \"cuda:0\"\nprior = DiscreteMaskedPrior(num_classes = 2, inclusive = False)\ntime_distribution = UniformTimeDistribution(discrete_time = False)\nnoise_schedule = CosineExpNoiseTransform()\nmdlm = MDLM(time_distribution=time_distribution,\n                          prior_distribution=prior,\n                          noise_schedule = noise_schedule,\n                          device=DEVICE)\nschedule = LinearInferenceSchedule(direction = \"diffusion\", nsteps = 1000)\n</pre> from bionemo.moco.distributions.prior import DiscreteMaskedPrior from bionemo.moco.interpolants import MDLM from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.schedules.noise.continuous_noise_transforms import CosineExpNoiseTransform from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule  DEVICE = \"cuda:0\" prior = DiscreteMaskedPrior(num_classes = 2, inclusive = False) time_distribution = UniformTimeDistribution(discrete_time = False) noise_schedule = CosineExpNoiseTransform() mdlm = MDLM(time_distribution=time_distribution,                           prior_distribution=prior,                           noise_schedule = noise_schedule,                           device=DEVICE) schedule = LinearInferenceSchedule(direction = \"diffusion\", nsteps = 1000) In\u00a0[22]: Copied! <pre>prior.num_classes # The inclusive flag allows us to chose whether or not to add a dimension\n</pre> prior.num_classes # The inclusive flag allows us to chose whether or not to add a dimension Out[22]: <pre>3</pre> In\u00a0[23]: Copied! <pre># training\nB = 32 # batch size\nD = 10 # dimension\nS = 3 # state space\n\nmodel = Model(D, S)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    \nmodel = model.to(DEVICE)\nlosses = []\nfor _ in tqdm(range(50000)):\n    num_ones = torch.randint(0, D+1, (B,))\n    x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)\n    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n    optimizer.zero_grad()\n    # x0 = dfm.sample_prior(x1.shape) # B x D\n    t = mdlm.sample_time(B)\n    xt = mdlm.interpolate(x1, t)\n    logits = model(xt, t) # (B, D, S)\n    loss = mdlm.loss(logits, x1, xt, t).mean()\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n</pre> # training B = 32 # batch size D = 10 # dimension S = 3 # state space  model = Model(D, S) optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)      model = model.to(DEVICE) losses = [] for _ in tqdm(range(50000)):     num_ones = torch.randint(0, D+1, (B,))     x1 = (torch.arange(D)[None, :] &lt; num_ones[:, None]).long().to(DEVICE)     # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]     optimizer.zero_grad()     # x0 = dfm.sample_prior(x1.shape) # B x D     t = mdlm.sample_time(B)     xt = mdlm.interpolate(x1, t)     logits = model(xt, t) # (B, D, S)     loss = mdlm.loss(logits, x1, xt, t).mean()     loss.backward()     optimizer.step()     losses.append(loss.item()) <pre>  7%|\u258b         | 3273/50000 [00:51&lt;12:03, 64.60it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [10:55&lt;00:00, 76.32it/s] \n</pre> In\u00a0[24]: Copied! <pre>plt.plot(losses, label='Training Loss', linestyle='-', color='blue', marker='o')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\nplt.grid(True)\nplt.ylim([0,1])\nplt.show()\n</pre> plt.plot(losses, label='Training Loss', linestyle='-', color='blue', marker='o') plt.xlabel('Step') plt.ylabel('Loss') plt.title('Training Loss') plt.legend() plt.grid(True) plt.ylim([0,1]) plt.show() In\u00a0[25]: Copied! <pre>num_samples = 1000\nxt = mdlm.sample_prior((num_samples, D))\ncounts = xt.flatten().cpu()\n\n# Compute frequency of each class index\nclass_counts = torch.bincount(counts)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.bar(range(len(class_counts)), class_counts.numpy(), color='red')\nplt.xlabel('Class Index')\nplt.ylabel('Frequency')\nplt.title('Discrete Distribution of Class Indices')\nplt.xticks(range(len(class_counts)))  # Set x-ticks to class indices\nplt.show()\n</pre> num_samples = 1000 xt = mdlm.sample_prior((num_samples, D)) counts = xt.flatten().cpu()  # Compute frequency of each class index class_counts = torch.bincount(counts)  # Plotting plt.figure(figsize=(8, 5)) plt.bar(range(len(class_counts)), class_counts.numpy(), color='red') plt.xlabel('Class Index') plt.ylabel('Frequency') plt.title('Discrete Distribution of Class Indices') plt.xticks(range(len(class_counts)))  # Set x-ticks to class indices plt.show()  In\u00a0[26]: Copied! <pre>ts = schedule.generate_schedule()\ndts = schedule.discretize()\nnum_samples = 1000\nxt = mdlm.sample_prior((num_samples, D))\nfor dt, t in zip(dts, ts):\n    t = torch.full((xt.shape[0],), t).to(DEVICE)\n    logits = model(xt, t)\n    xt = mdlm.step(logits, t, xt, dt)\n</pre> ts = schedule.generate_schedule() dts = schedule.discretize() num_samples = 1000 xt = mdlm.sample_prior((num_samples, D)) for dt, t in zip(dts, ts):     t = torch.full((xt.shape[0],), t).to(DEVICE)     logits = model(xt, t)     xt = mdlm.step(logits, t, xt, dt) In\u00a0[27]: Copied! <pre>counts = xt.flatten().cpu()\n\n# Compute frequency of each class index\nclass_counts = torch.bincount(counts)\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.bar(range(len(class_counts)), class_counts.numpy(), color='green')\nplt.xlabel('Class Index')\nplt.ylabel('Frequency')\nplt.title('Discrete Distribution of Class Indices')\nplt.xticks(range(len(class_counts)))  # Set x-ticks to class indices\nplt.show()\n</pre>  counts = xt.flatten().cpu()  # Compute frequency of each class index class_counts = torch.bincount(counts)  # Plotting plt.figure(figsize=(8, 5)) plt.bar(range(len(class_counts)), class_counts.numpy(), color='green') plt.xlabel('Class Index') plt.ylabel('Frequency') plt.title('Discrete Distribution of Class Indices') plt.xticks(range(len(class_counts)))  # Set x-ticks to class indices plt.show()  In\u00a0[28]: Copied! <pre>counts = xt.cpu().sum(dim=1).float()\nplt.hist(counts.numpy(), bins=range(D+2))\nplt.show()\n</pre> counts = xt.cpu().sum(dim=1).float() plt.hist(counts.numpy(), bins=range(D+2)) plt.show()"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#building-generative-models-for-discrete-data-via-discrete-interpolants","title":"Building Generative Models for Discrete Data via Discrete Interpolants\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#tutorial","title":"Tutorial\u00b6","text":"<p>This notebook walks through how to use 3 discrete data interpolants: (1) Discrete Flow Matching (2) Discrete Denoising Diffusion Probabilistic Models, and (3) Masked Diffusion Language Modeling</p>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#task","title":"Task\u00b6","text":"<p>here our object contains 10 binary elements with the goal distribution being a uniform distribution over the 10 elements.</p> <p>We initalize our interpolants with a binary uniform prior so on average each sample with have a value of 5 out of 10</p>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#define-the-model-architecture","title":"Define the Model Architecture\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#define-the-discret-flow-matching-interpolant","title":"Define the Discret Flow Matching Interpolant\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#train-dfm","title":"Train DFM\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#sample-from-dfm","title":"Sample from DFM\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#aside-on-low-temperature-sampling","title":"Aside on Low Temperature Sampling\u00b6","text":"<p>=====================================</p>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#temperature-in-sampling-distributions","title":"Temperature (<code>\u03c4</code>) in Sampling Distributions\u00b6","text":"<ul> <li>Definition: <code>\u03c4</code> is a hyperparameter that scales the logits (unnormalized log probabilities) of a categorical distribution before applying the softmax function to obtain the probabilities.</li> <li>Formulas:<ol> <li>Logits Scaling: <code>scaled_logits = logits / \u03c4</code></li> <li>Softmax: <code>probabilities = softmax(scaled_logits)</code></li> </ol> </li> </ul>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#effects-of-low-temperature-0-on-the-distribution","title":"Effects of Low Temperature (<code>\u03c4 \u2192 0</code>) on the Distribution\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#characteristics","title":"Characteristics\u00b6","text":"<ul> <li>Peakiness Increases (Higher Confidence)<ul> <li>Distribution becomes more peaked around the mode (most likely outcome).</li> <li>Model becomes more confident in its top prediction.</li> </ul> </li> <li>Less Exploration, More Exploitation<ul> <li>Model is less likely to sample from less confident (lower probability) areas.</li> <li>Exploits the most likely outcome rather than exploring possibilities.</li> </ul> </li> <li>Convergence to Argmax<ul> <li>Sampling converges to selecting the argmax of the logits (highest logit value).</li> <li>Model always chooses the single most likely outcome, with no randomness.</li> </ul> </li> <li>Reduced Entropy<ul> <li>Entropy decreases, indicating less uncertainty.</li> <li>Reflects the distribution's increased peakiness around a single outcome.</li> </ul> </li> </ul>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#visual-illustration","title":"Visual Illustration\u00b6","text":"Temperature Distribution Over A, B, C Entropy &amp; Randomness High (<code>\u03c4 \u226b 1</code>) ~{0.33, 0.33, 0.33} Higher Entropy, More Randomness Low (<code>\u03c4 \u2248 0</code>) ~{0.99, 0.005, 0.005} Lower Entropy, Less Randomness"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#practical-implications-of-low-temperature-sampling","title":"Practical Implications of Low Temperature Sampling\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#advantages","title":"Advantages\u00b6","text":"<ul> <li>Faster Convergence in Training<ul> <li>Beneficial when quick convergence to an optimal solution is desired.</li> </ul> </li> <li>Clearer \"Best\" Predictions<ul> <li>Useful when clear indications of the model's most confident choice are needed.</li> </ul> </li> </ul>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#disadvantages","title":"Disadvantages\u00b6","text":"<ul> <li>Overconfidence<ul> <li>May lead to overestimation of the model's certainty in predictions.</li> </ul> </li> <li>Lack of Diversity<ul> <li>Results in repetitive or less innovative outcomes in applications requiring diverse outputs (e.g., text generation, game playing).</li> </ul> </li> </ul>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#understanding-stochasticity","title":"Understanding Stochasticity\u00b6","text":"<p>====================================================================</p>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#stochasticity-term-explanation","title":"Stochasticity Term Explanation\u00b6","text":"<p>The <code>stochasticity</code> term is a hyperparameter influencing the amount of noise added.</p>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#mask-prior-code-snippet","title":"Mask Prior Code Snippet\u00b6","text":"<pre>step_prob = (\n    dt * x_1_pred_prob * ((1 + stochasticity * t) / (1 - t)) * xt_is_mask\n    + dt * (1 - xt_is_mask) * mask_one_hot.view(1, 1, -1) * stochasticity * (t + dt &lt; 1).float()\n)\n</pre>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#uniform-prior-code-snippet","title":"Uniform Prior Code Snippet\u00b6","text":"<pre>step_prob = (\n    dt * x_1_pred_prob * ((1 + stochasticity + stochasticity * (S - 1) * t) / (1 - t))\n    + dt * pt_x1_eq_xt_prob * stochasticity\n)\n</pre>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#effects-of-stochasticity","title":"Effects of Stochasticity\u00b6","text":"<ul> <li>Value Range: 0 to N</li> <li>Exploration-Exploitation Tradeoff: Balances exploring new sequences (high stochasticity) and exploiting the most likely sequence (low stochasticity)</li> <li>Controls Masking and Unmasking for Masked Prior: Increasing the stochasticity control the unmasking and re-masking ratio.</li> </ul>"},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#generated-dfm-samples","title":"Generated DFM Samples\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#ground-truth-distribution","title":"Ground Truth Distribution\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#discrete-uniform-prior-distribution","title":"Discrete Uniform Prior Distribution\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#we-see-that-with-dfm-we-are-able-to-approximate-the-ground-truth-distributionnow-lets-try-a-different-interpolant","title":"We see that with DFM we are able to approximate the ground truth distribution.Now let's try a different interpolant\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#d3pm-interpolant","title":"D3PM Interpolant\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#train-d3pm","title":"Train D3PM\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#sample-from-d3pm","title":"Sample from D3PM\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#d3pm-generated-distribution","title":"D3PM Generated Distribution\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#d3pm-prior-distribution","title":"D3PM Prior Distribution\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#now-lets-try-a-new-interpolant-and-a-new-prior","title":"Now let's try a new interpolant and a new prior\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#mdlm-interpolant","title":"MDLM Interpolant\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#train-mdlm","title":"Train MDLM\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#visualize-the-mask-prior","title":"Visualize the MASK Prior\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#sample-from-the-mdlm-trained-model","title":"Sample from the MDLM trained model\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#visualize-the-class-breakdown-green-and-generated-samples-blue","title":"Visualize the class breakdown (green) and generated samples (blue)\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#here-we-can-take-binary-data-and-rather-than-using-a-uniform-prior-introduce-a-mask-state-here-mdlm-trained-on-the-same-data-is-able-to-generate-the-desired-discrete-data-shown-ion-blue-although-starting-from-pure-mask-states-seen-in-red","title":"here we can take binary data and rather than using a uniform prior introduce a MASK state. Here MDLM trained on the same data is able to generate the desired discrete data shown ion blue although starting from pure MASK states seen in red.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/discrete_data_interpolant_tutorial/#these-3-cases-show-how-on-the-same-data-one-can-switch-between-various-diffusion-and-flow-matching-options-that-each-come-with-various-inference-time-sampling-abilities","title":"These 3 cases show how on the same data one can switch between various diffusion and flow matching options that each come with various inference time sampling abilities.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/","title":"Optimal Transport Samplers Tutorial","text":"In\u00a0[1]: Copied! <pre>import math\nimport os\nimport time\nimport copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom bionemo.moco.interpolants import EquivariantOTSampler, OTSampler\n\nfrom sklearn.datasets import make_moons\n</pre> import math import os import time import copy  import matplotlib.pyplot as plt import numpy as np import torch from bionemo.moco.interpolants import EquivariantOTSampler, OTSampler  from sklearn.datasets import make_moons In\u00a0[2]: Copied! <pre>def sample_moons(n, normalize = False):\n    x1, _ = make_moons(n_samples=n, noise=0.08)\n    x1 = torch.Tensor(x1)\n    x1 =  x1 * 3 - 1\n    if normalize:\n        x1 = (x1 - x1.mean(0))/x1.std(0) * 2\n    return x1\n\ndef sample_gaussian(n, dim = 2):\n    return torch.randn(n, dim)\n</pre> def sample_moons(n, normalize = False):     x1, _ = make_moons(n_samples=n, noise=0.08)     x1 = torch.Tensor(x1)     x1 =  x1 * 3 - 1     if normalize:         x1 = (x1 - x1.mean(0))/x1.std(0) * 2     return x1  def sample_gaussian(n, dim = 2):     return torch.randn(n, dim) In\u00a0[3]: Copied! <pre># Sample x0 and x1\nx1 = sample_moons(100, normalize=True).numpy()\nx0 = sample_gaussian(100).numpy()\n# Plot data points and linear interpolation\nplt.scatter(x1[:, 0], x1[:, 1], label='$x_0$')\nplt.scatter(x0[:, 0], x0[:, 1], label='$x_1$')\nx0 = np.asarray(x0)\nx1 = np.asarray(x1)\nfor i in range(len(x1)):\n    plt.plot([x0[i, 0], x1[i, 0]], [x0[i, 1], x1[i, 1]], color='k', alpha=0.2)\nplt.legend()\n</pre> # Sample x0 and x1 x1 = sample_moons(100, normalize=True).numpy() x0 = sample_gaussian(100).numpy() # Plot data points and linear interpolation plt.scatter(x1[:, 0], x1[:, 1], label='$x_0$') plt.scatter(x0[:, 0], x0[:, 1], label='$x_1$') x0 = np.asarray(x0) x1 = np.asarray(x1) for i in range(len(x1)):     plt.plot([x0[i, 0], x1[i, 0]], [x0[i, 1], x1[i, 1]], color='k', alpha=0.2) plt.legend() Out[3]: <pre>&lt;matplotlib.legend.Legend at 0x7690ce3f3d30&gt;</pre> In\u00a0[4]: Copied! <pre># Initialize the OTSampler\not_sampler = OTSampler(method=\"exact\", num_threads=1)\n# Sample new pairs from the OTSampler, mask is not used in this example\n# Replace is set to False, so no duplicates are allowed\n# Sort is set to \"x0\", so the order of output x0 is the same as input x0\not_sampled_x0, ot_sampled_x1, mask = ot_sampler.apply_augmentation(\n    torch.Tensor(x0), \n    torch.Tensor(x1), \n    mask=None, replace=False, sort=\"x0\")\n# Convert the sampled tensors to numpy arrays\not_sampled_x0 = ot_sampled_x0.numpy()\not_sampled_x1 = ot_sampled_x1.numpy()\n</pre> # Initialize the OTSampler ot_sampler = OTSampler(method=\"exact\", num_threads=1) # Sample new pairs from the OTSampler, mask is not used in this example # Replace is set to False, so no duplicates are allowed # Sort is set to \"x0\", so the order of output x0 is the same as input x0 ot_sampled_x0, ot_sampled_x1, mask = ot_sampler.apply_augmentation(     torch.Tensor(x0),      torch.Tensor(x1),      mask=None, replace=False, sort=\"x0\") # Convert the sampled tensors to numpy arrays ot_sampled_x0 = ot_sampled_x0.numpy() ot_sampled_x1 = ot_sampled_x1.numpy() In\u00a0[5]: Copied! <pre># Plot data points and linear interpolation\nplt.scatter(ot_sampled_x1[:, 0], ot_sampled_x1[:, 1], label='$x_0$')\nplt.scatter(ot_sampled_x0[:, 0], ot_sampled_x0[:, 1], label='$x_1$')\nfor i in range(len(x1)):\n    plt.plot(\n        [ot_sampled_x0[i, 0], ot_sampled_x1[i, 0]], \n        [ot_sampled_x0[i, 1], ot_sampled_x1[i, 1]], \n        color='k', alpha=0.2\n    )\nplt.legend()\n</pre> # Plot data points and linear interpolation plt.scatter(ot_sampled_x1[:, 0], ot_sampled_x1[:, 1], label='$x_0$') plt.scatter(ot_sampled_x0[:, 0], ot_sampled_x0[:, 1], label='$x_1$') for i in range(len(x1)):     plt.plot(         [ot_sampled_x0[i, 0], ot_sampled_x1[i, 0]],          [ot_sampled_x0[i, 1], ot_sampled_x1[i, 1]],          color='k', alpha=0.2     ) plt.legend() Out[5]: <pre>&lt;matplotlib.legend.Legend at 0x7690c6597b80&gt;</pre> In\u00a0[6]: Copied! <pre>from bionemo.moco.interpolants import ContinuousFlowMatcher\nfrom bionemo.moco.distributions.time import UniformTimeDistribution\nfrom bionemo.moco.distributions.prior import GaussianPrior\n\ndef trainCFM(use_ot=False):\n    # Initialize model, optimizer, and flow matcher\n    dim = 2\n    hidden_size = 64\n    batch_size = 256\n    model = torch.nn.Sequential(\n                torch.nn.Linear(dim + 1, hidden_size),\n                torch.nn.SELU(),\n                torch.nn.Linear(hidden_size, hidden_size),\n                torch.nn.SELU(),\n                torch.nn.Linear(hidden_size, hidden_size),\n                torch.nn.SELU(),\n                torch.nn.Linear(hidden_size, dim),\n            )\n    optimizer = torch.optim.Adam(model.parameters())\n\n    uniform_time = UniformTimeDistribution()\n    moon_prior = GaussianPrior()\n    sigma = 0.1\n    cfm = ContinuousFlowMatcher(time_distribution=uniform_time, \n                                prior_distribution=moon_prior, \n                                sigma=sigma, \n                                prediction_type=\"velocity\")\n\n    # Place both the model and the interpolant on the same device\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = model.to(DEVICE)\n    cfm = cfm.to_device(DEVICE)\n\n    for k in range(10000):\n        optimizer.zero_grad()\n        shape = (batch_size, dim)\n        x0 = cfm.sample_prior(shape).to(DEVICE)\n        x1 = sample_moons(batch_size, normalize=False).to(DEVICE)\n        if use_ot:\n            x0, x1, mask = ot_sampler.apply_augmentation(\n                x0, x1, \n                mask=None, replace=False, sort=\"x0\"\n            )\n        t = cfm.sample_time(batch_size)\n        xt = cfm.interpolate(x1, t, x0)\n        ut = cfm.calculate_target(x1, x0)\n\n        vt = model(torch.cat([xt, t[:, None]], dim=-1))\n        loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()\n\n        loss.backward()\n        optimizer.step()\n\n        if (k + 1) % 5000 == 0:\n            print(f\"{k+1}: loss {loss.item():0.3f}\") \n    return model, cfm\n</pre> from bionemo.moco.interpolants import ContinuousFlowMatcher from bionemo.moco.distributions.time import UniformTimeDistribution from bionemo.moco.distributions.prior import GaussianPrior  def trainCFM(use_ot=False):     # Initialize model, optimizer, and flow matcher     dim = 2     hidden_size = 64     batch_size = 256     model = torch.nn.Sequential(                 torch.nn.Linear(dim + 1, hidden_size),                 torch.nn.SELU(),                 torch.nn.Linear(hidden_size, hidden_size),                 torch.nn.SELU(),                 torch.nn.Linear(hidden_size, hidden_size),                 torch.nn.SELU(),                 torch.nn.Linear(hidden_size, dim),             )     optimizer = torch.optim.Adam(model.parameters())      uniform_time = UniformTimeDistribution()     moon_prior = GaussianPrior()     sigma = 0.1     cfm = ContinuousFlowMatcher(time_distribution=uniform_time,                                  prior_distribution=moon_prior,                                  sigma=sigma,                                  prediction_type=\"velocity\")      # Place both the model and the interpolant on the same device     DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"     model = model.to(DEVICE)     cfm = cfm.to_device(DEVICE)      for k in range(10000):         optimizer.zero_grad()         shape = (batch_size, dim)         x0 = cfm.sample_prior(shape).to(DEVICE)         x1 = sample_moons(batch_size, normalize=False).to(DEVICE)         if use_ot:             x0, x1, mask = ot_sampler.apply_augmentation(                 x0, x1,                  mask=None, replace=False, sort=\"x0\"             )         t = cfm.sample_time(batch_size)         xt = cfm.interpolate(x1, t, x0)         ut = cfm.calculate_target(x1, x0)          vt = model(torch.cat([xt, t[:, None]], dim=-1))         loss = cfm.loss(vt, ut, target_type=\"velocity\").mean()          loss.backward()         optimizer.step()          if (k + 1) % 5000 == 0:             print(f\"{k+1}: loss {loss.item():0.3f}\")      return model, cfm In\u00a0[7]: Copied! <pre># Train a model with OT\not_model, ot_cfm = trainCFM(use_ot=True)\n# Train a model without OT\nno_ot_model, no_ot_cfm = trainCFM(use_ot=False)\n</pre> # Train a model with OT ot_model, ot_cfm = trainCFM(use_ot=True) # Train a model without OT no_ot_model, no_ot_cfm = trainCFM(use_ot=False) <pre>5000: loss 0.064\n10000: loss 0.067\n5000: loss 2.570\n10000: loss 3.204\n</pre> In\u00a0[8]: Copied! <pre># Set up the sampling time schedule\nfrom bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ninference_sched = LinearInferenceSchedule(nsteps = 100)\nschedule = inference_sched.generate_schedule().to(DEVICE)\ndts = inference_sched.discretize().to(DEVICE)\n</pre> # Set up the sampling time schedule from bionemo.moco.schedules.inference_time_schedules import LinearInferenceSchedule DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" inference_sched = LinearInferenceSchedule(nsteps = 100) schedule = inference_sched.generate_schedule().to(DEVICE) dts = inference_sched.discretize().to(DEVICE) In\u00a0[9]: Copied! <pre># Sampling with the two trained models\ninf_size = 1024\not_sample = ot_cfm.sample_prior((inf_size, 2)) # Start with noise\nno_ot_sample = copy.deepcopy(ot_sample) # Ensure the same starting point for both models\not_sample, no_ot_sample = ot_sample.to(DEVICE), no_ot_sample.to(DEVICE)\not_trajectory, no_ot_trajectory = [ot_sample], [no_ot_sample]\nfor dt, t in zip(dts, schedule):\n    full_t  = torch.full((inf_size,), t).to(DEVICE)\n    ot_vt = ot_model(torch.cat([ot_sample, full_t[:, None]], dim=-1)) # calculate the vector field based on the definition of the model\n    ot_sample = ot_cfm.step(ot_vt, ot_sample, dt, full_t)\n    no_ot_vt = no_ot_model(torch.cat([no_ot_sample, full_t[:, None]], dim=-1)) # calculate the vector field based on the definition of the model\n    no_ot_sample = no_ot_cfm.step(no_ot_vt, no_ot_sample, dt, full_t)\n    ot_trajectory.append(ot_sample) # save the trajectory for plotting purposes\n    no_ot_trajectory.append(no_ot_sample) # save the trajectory for plotting purposes\n</pre> # Sampling with the two trained models inf_size = 1024 ot_sample = ot_cfm.sample_prior((inf_size, 2)) # Start with noise no_ot_sample = copy.deepcopy(ot_sample) # Ensure the same starting point for both models ot_sample, no_ot_sample = ot_sample.to(DEVICE), no_ot_sample.to(DEVICE) ot_trajectory, no_ot_trajectory = [ot_sample], [no_ot_sample] for dt, t in zip(dts, schedule):     full_t  = torch.full((inf_size,), t).to(DEVICE)     ot_vt = ot_model(torch.cat([ot_sample, full_t[:, None]], dim=-1)) # calculate the vector field based on the definition of the model     ot_sample = ot_cfm.step(ot_vt, ot_sample, dt, full_t)     no_ot_vt = no_ot_model(torch.cat([no_ot_sample, full_t[:, None]], dim=-1)) # calculate the vector field based on the definition of the model     no_ot_sample = no_ot_cfm.step(no_ot_vt, no_ot_sample, dt, full_t)     ot_trajectory.append(ot_sample) # save the trajectory for plotting purposes     no_ot_trajectory.append(no_ot_sample) # save the trajectory for plotting purposes In\u00a0[10]: Copied! <pre>ot_traj = torch.stack(ot_trajectory).cpu().detach().numpy()\nno_ot_traj = torch.stack(no_ot_trajectory).cpu().detach().numpy()\nn = 2000\n\n# Assuming traj is your tensor and traj.shape = (N, 2000, 2)\n# where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the first time point in black\nax[0].scatter(ot_traj[0, :n, 0], ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label='Prior z(S)')\nax[1].scatter(no_ot_traj[0, :n, 0], no_ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label='Prior z(S)')\n\n# Plot all the rest of the time points except the first and last in olive\nfor i in range(1, ot_traj.shape[0]-1):\n    ax[0].scatter(ot_traj[i, :n, 0], ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)\n    ax[1].scatter(no_ot_traj[i, :n, 0], no_ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)\n\n# Plot the last time point in blue\nax[0].scatter(ot_traj[-1, :n, 0], ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\nax[1].scatter(no_ot_traj[-1, :n, 0], no_ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label='z(0)')\n\n# Add a second legend for \"Flow\" since we can't label in the loop directly\nfor i in range(2):\n    ax[i].scatter([], [], s=2, alpha=1, c=\"olive\", label='Flow')\n    ax[i].legend()\n    # ax[i].set_aspect('equal')\n    ax[i].set_xticks([])\n    ax[i].set_yticks([])\n    ax[i].set_xlim(-5, 6)\n    ax[i].set_ylim(-4, 5)\n    if i == 0:\n        ax[i].set_title(\"With OT\")\n    else:\n        ax[i].set_title(\"Without OT\")\nplt.subplots_adjust(wspace=0.05)\nplt.show()\n</pre> ot_traj = torch.stack(ot_trajectory).cpu().detach().numpy() no_ot_traj = torch.stack(no_ot_trajectory).cpu().detach().numpy() n = 2000  # Assuming traj is your tensor and traj.shape = (N, 2000, 2) # where N is the number of time points, 2000 is the number of samples at each time point, and 2 is for the x and y coordinates.  fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # Plot the first time point in black ax[0].scatter(ot_traj[0, :n, 0], ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label='Prior z(S)') ax[1].scatter(no_ot_traj[0, :n, 0], no_ot_traj[0, :n, 1], s=10, alpha=0.8, c=\"black\", label='Prior z(S)')  # Plot all the rest of the time points except the first and last in olive for i in range(1, ot_traj.shape[0]-1):     ax[0].scatter(ot_traj[i, :n, 0], ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)     ax[1].scatter(no_ot_traj[i, :n, 0], no_ot_traj[i, :n, 1], s=0.2, alpha=0.2, c=\"olive\", zorder=1)  # Plot the last time point in blue ax[0].scatter(ot_traj[-1, :n, 0], ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label='z(0)') ax[1].scatter(no_ot_traj[-1, :n, 0], no_ot_traj[-1, :n, 1], s=4, alpha=1, c=\"blue\", label='z(0)')  # Add a second legend for \"Flow\" since we can't label in the loop directly for i in range(2):     ax[i].scatter([], [], s=2, alpha=1, c=\"olive\", label='Flow')     ax[i].legend()     # ax[i].set_aspect('equal')     ax[i].set_xticks([])     ax[i].set_yticks([])     ax[i].set_xlim(-5, 6)     ax[i].set_ylim(-4, 5)     if i == 0:         ax[i].set_title(\"With OT\")     else:         ax[i].set_title(\"Without OT\") plt.subplots_adjust(wspace=0.05) plt.show() In\u00a0[11]: Copied! <pre>first_points = no_ot_traj[0]\nlast_points = no_ot_traj[-1]\ndistances = ((last_points - first_points)**2).sum(-1)\naverage_distance = np.mean(distances)\n\nprint(f\"Average Distance between First and Last Points without OT: {average_distance.item()}\")\n\nfirst_points = ot_traj[0]\nlast_points = ot_traj[-1]\ndistances = ((last_points - first_points)**2).sum(-1)\naverage_distance = np.mean(distances)\n\nprint(f\"Average Distance between First and Last Points with OT: {average_distance.item()}\")\n</pre>  first_points = no_ot_traj[0] last_points = no_ot_traj[-1] distances = ((last_points - first_points)**2).sum(-1) average_distance = np.mean(distances)  print(f\"Average Distance between First and Last Points without OT: {average_distance.item()}\")  first_points = ot_traj[0] last_points = ot_traj[-1] distances = ((last_points - first_points)**2).sum(-1) average_distance = np.mean(distances)  print(f\"Average Distance between First and Last Points with OT: {average_distance.item()}\") <pre>Average Distance between First and Last Points without OT: 3.3887150287628174\nAverage Distance between First and Last Points with OT: 3.6937265396118164\n</pre> In\u00a0[12]: Copied! <pre>def sum_of_squared_distances(trajectory):\n    \"\"\"\n    Calculate the sum of squared distances from start to mid and mid to end of a trajectory.\n    \n    Parameters:\n    - trajectory: A numpy array of shape (N, D) where N is the number of points \n                  in the trajectory and D is the dimensionality of the space.\n                  \n    Returns:\n    - Sum of squared distances (start to mid + mid to end).\n    \"\"\"\n    mid_idx = len(trajectory) // 2\n    start_point = trajectory[0]\n    mid_point = trajectory[mid_idx]\n    end_point = trajectory[-1]\n    \n    start_to_mid_distance = np.linalg.norm(start_point - mid_point)\n    mid_to_end_distance = np.linalg.norm(mid_point - end_point)\n    \n    return start_to_mid_distance**2 + mid_to_end_distance**2\n\n# Calculate and print sum of squared distances for both trajectories\nno_ot_sum_squared_distance = sum_of_squared_distances(no_ot_traj)\not_sum_squared_distance = sum_of_squared_distances(ot_traj)\n\nprint(\"Sum of Squared Distances (start to mid + mid to end):\")\nprint(f\"Without OT: {no_ot_sum_squared_distance:.4f}\")\nprint(f\"With OT: {ot_sum_squared_distance:.4f}\")\n</pre> def sum_of_squared_distances(trajectory):     \"\"\"     Calculate the sum of squared distances from start to mid and mid to end of a trajectory.          Parameters:     - trajectory: A numpy array of shape (N, D) where N is the number of points                    in the trajectory and D is the dimensionality of the space.                        Returns:     - Sum of squared distances (start to mid + mid to end).     \"\"\"     mid_idx = len(trajectory) // 2     start_point = trajectory[0]     mid_point = trajectory[mid_idx]     end_point = trajectory[-1]          start_to_mid_distance = np.linalg.norm(start_point - mid_point)     mid_to_end_distance = np.linalg.norm(mid_point - end_point)          return start_to_mid_distance**2 + mid_to_end_distance**2  # Calculate and print sum of squared distances for both trajectories no_ot_sum_squared_distance = sum_of_squared_distances(no_ot_traj) ot_sum_squared_distance = sum_of_squared_distances(ot_traj)  print(\"Sum of Squared Distances (start to mid + mid to end):\") print(f\"Without OT: {no_ot_sum_squared_distance:.4f}\") print(f\"With OT: {ot_sum_squared_distance:.4f}\") <pre>Sum of Squared Distances (start to mid + mid to end):\nWithout OT: 2318.0609\nWith OT: 1895.0890\n</pre> In\u00a0[13]: Copied! <pre># Define helper functions\ndef rotation_matrix(angle):\n    theta = (angle/180.) * np.pi\n    c, s = np.cos(theta), np.sin(theta)\n    return np.array([[c, -s], [s, c]])\n\ndef rotate(x, angle):\n    R = rotation_matrix(angle)\n    return x @ R.T\n\ndef plot_quadrilateral(x, axis, color='C0', marker='o', label=None):\n    assert x.shape == (4, 2)\n    axis.scatter(\n        x[:, 0], x[:, 1], \n        c=color, marker=marker, linewidths=1, \n        edgecolors='k', zorder=2, label=label\n    )\n    for i in range(len(x)):\n        if i &lt; 3:\n            axis.plot([x[i, 0], x[i+1, 0]], [x[i, 1], x[i+1, 1]], c=color, zorder=1)\n        else:\n            axis.plot([x[i, 0], x[0, 0]], [x[i, 1], x[0, 1]], c=color, zorder=1)\n    return axis\n</pre> # Define helper functions def rotation_matrix(angle):     theta = (angle/180.) * np.pi     c, s = np.cos(theta), np.sin(theta)     return np.array([[c, -s], [s, c]])  def rotate(x, angle):     R = rotation_matrix(angle)     return x @ R.T  def plot_quadrilateral(x, axis, color='C0', marker='o', label=None):     assert x.shape == (4, 2)     axis.scatter(         x[:, 0], x[:, 1],          c=color, marker=marker, linewidths=1,          edgecolors='k', zorder=2, label=label     )     for i in range(len(x)):         if i &lt; 3:             axis.plot([x[i, 0], x[i+1, 0]], [x[i, 1], x[i+1, 1]], c=color, zorder=1)         else:             axis.plot([x[i, 0], x[0, 0]], [x[i, 1], x[0, 1]], c=color, zorder=1)     return axis In\u00a0[14]: Copied! <pre># Initialize \nk0 = np.array([\n    [[-2, 0], [0, 1], [2, 0], [0, -1]], # Rhombus\n    [[-1, 2], [-1, 4], [1, 4], [1, 2]], # Square\n])\nangles = [60, 25]\n\n# Rotate and shuffle samples in k0 to create k1\nk1 = np.array([rotate(k0[i], angles[i]) for i in [1, 0]])\nmarkers = ['o', 's']\n\n# Translate k0 and k1\nk0 = np.array(k0)-2\nk1 = np.array(k1)+2\n</pre> # Initialize  k0 = np.array([     [[-2, 0], [0, 1], [2, 0], [0, -1]], # Rhombus     [[-1, 2], [-1, 4], [1, 4], [1, 2]], # Square ]) angles = [60, 25]  # Rotate and shuffle samples in k0 to create k1 k1 = np.array([rotate(k0[i], angles[i]) for i in [1, 0]]) markers = ['o', 's']  # Translate k0 and k1 k0 = np.array(k0)-2 k1 = np.array(k1)+2 In\u00a0[15]: Copied! <pre># Plot k0 and k1\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nfor i in range(len(k0)):\n    plot_quadrilateral(k0[i], ax, color='C1', marker=markers[i], label='$k_0^%d$'%i)\n    plot_quadrilateral(k1[i], ax, color='C0', marker=markers[i], label='$k_1^%d$'%i)\n    # Calculate centroids of k0 and k1\n    centroid_k0 = np.mean(k0[i], axis=0)\n    centroid_k1 = np.mean(k1[i], axis=0)\n\n    # Plot a red line connecting the centroids\n    ax.plot(*zip(centroid_k0, centroid_k1), color='red', linewidth=1, linestyle='--')\nax.legend()\nax.set_aspect('equal', adjustable='box')\n</pre> # Plot k0 and k1 fig, ax = plt.subplots(1, 1, figsize=(5, 5)) for i in range(len(k0)):     plot_quadrilateral(k0[i], ax, color='C1', marker=markers[i], label='$k_0^%d$'%i)     plot_quadrilateral(k1[i], ax, color='C0', marker=markers[i], label='$k_1^%d$'%i)     # Calculate centroids of k0 and k1     centroid_k0 = np.mean(k0[i], axis=0)     centroid_k1 = np.mean(k1[i], axis=0)      # Plot a red line connecting the centroids     ax.plot(*zip(centroid_k0, centroid_k1), color='red', linewidth=1, linestyle='--') ax.legend() ax.set_aspect('equal', adjustable='box') In\u00a0[16]: Copied! <pre># Initialize the Kabsch OT Sampler\nkabsch_ot_sampler = EquivariantOTSampler(method=\"exact\", num_threads=1)\n# Sample new pairs from the EquivariantOTSampler, mask is not used in this example\n# Replace is set to False, so no duplicates are allowed\n# Sort is set to \"x0\", so the order of output x0 is the same as input x0\nkabsch_k0, kabsch_k1, mask = kabsch_ot_sampler.apply_augmentation(\n    torch.Tensor(k0), \n    torch.Tensor(k1), \n    mask=None, replace=False, sort=\"x0\")\n# Convert the sampled tensors to numpy arrays\nkabsch_k0 = kabsch_k0.numpy()\nkabsch_k1 = kabsch_k1.numpy()\n</pre> # Initialize the Kabsch OT Sampler kabsch_ot_sampler = EquivariantOTSampler(method=\"exact\", num_threads=1) # Sample new pairs from the EquivariantOTSampler, mask is not used in this example # Replace is set to False, so no duplicates are allowed # Sort is set to \"x0\", so the order of output x0 is the same as input x0 kabsch_k0, kabsch_k1, mask = kabsch_ot_sampler.apply_augmentation(     torch.Tensor(k0),      torch.Tensor(k1),      mask=None, replace=False, sort=\"x0\") # Convert the sampled tensors to numpy arrays kabsch_k0 = kabsch_k0.numpy() kabsch_k1 = kabsch_k1.numpy() In\u00a0[17]: Copied! <pre># Plot newly sampled k0 and k1, note that k1 is rotated to match k0\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nfor i in range(len(kabsch_k0)):\n    plot_quadrilateral(kabsch_k0[i], ax, color='C1', marker=markers[i], label='$k_0^%d$'%i)\n    plot_quadrilateral(kabsch_k1[i], ax, color='C0', marker=markers[i], label='$k_1^%d$'%i)\n    # Calculate centroids of k0 and k1\n    # Calculate centroids of k0 and k1\n    centroid_k0 = np.mean(kabsch_k0[i], axis=0)\n    centroid_k1 = np.mean(kabsch_k1[i], axis=0)\n\n    # Plot a red line connecting the centroids\n    ax.plot(*zip(centroid_k0, centroid_k1), color='red', linewidth=1, linestyle='--')\nax.legend()\nax.set_aspect('equal', adjustable='box')\n</pre> # Plot newly sampled k0 and k1, note that k1 is rotated to match k0 fig, ax = plt.subplots(1, 1, figsize=(5, 5)) for i in range(len(kabsch_k0)):     plot_quadrilateral(kabsch_k0[i], ax, color='C1', marker=markers[i], label='$k_0^%d$'%i)     plot_quadrilateral(kabsch_k1[i], ax, color='C0', marker=markers[i], label='$k_1^%d$'%i)     # Calculate centroids of k0 and k1     # Calculate centroids of k0 and k1     centroid_k0 = np.mean(kabsch_k0[i], axis=0)     centroid_k1 = np.mean(kabsch_k1[i], axis=0)      # Plot a red line connecting the centroids     ax.plot(*zip(centroid_k0, centroid_k1), color='red', linewidth=1, linestyle='--') ax.legend() ax.set_aspect('equal', adjustable='box')"},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#optimal-transport-samplers-tutorial","title":"Optimal Transport Samplers Tutorial\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#task-setup","title":"Task Setup\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#demonstrating-the-effectiveness-of-ot-sampler-and-kabsch-based-equivariant-ot-sampler","title":"Demonstrating the effectiveness of OT sampler and Kabsch-based Equivariant OT sampler\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#1-we-will-start-with-the-ot-sampler-the-ot-sampler-is-an-implementation-of-the-ot-cfm-algorithm-proposed-by-tong-et-al-for-a-batch-of-randomly-sampled-noise-mathrmx_0-and-data-mathrmx_1-the-ot-sampler-will-sample-x_0-x_1-pairs-based-on-their-euclidean-distances-we-will-demonstrate-how-to-use-the-ot-sampler-with-a-simple-2d-example","title":"1. We will start with the OT sampler. The OT sampler is an implementation of the \"OT-CFM\" algorithm proposed by Tong et. al. For a batch of randomly sampled noise ($\\mathrm{x}_0$) and data ($\\mathrm{x}_1$), the OT sampler will sample $(x_0, x_1)$ pairs based on their Euclidean distances. We will demonstrate how to use the OT sampler with a simple 2D example.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#11-sample-100-points-from-a-standard-gaussian-distribution-mathrmx_0-sim-pi_0-orange-colored-and-another-100-points-from-a-double-moon-shape-distribution-mathrmx_1-sim-pi_1-blue-colored-the-linear-interpolation-between-pairs-x_0i-x_1i-are-plotted-using-grey-lines","title":"1.1 Sample 100 points from a standard Gaussian distribution ($\\mathrm{x}_0 \\sim \\pi_0$, orange colored), and another 100 points from a double moon-shape distribution ($\\mathrm{x}_1 \\sim \\pi_1$, blue colored). The linear interpolation between pairs ($x_0^i, x_1^i$) are plotted using grey lines.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#12-initialize-the-ot-sampler-and-sample-new-x_0-x_1-pairs-to-minimize-the-transport-cost-of-the-entire-batch-the-linear-interpolation-between-new-pairs-x_0i-x_1i-are-plotted-using-grey-lines-we-can-see-that-there-are-less-crossover-of-interpolation-trajectories-and-the-transport-cost-has-been-reduced","title":"1.2 Initialize the OT sampler and sample new $(x_0, x_1)$ pairs to minimize the transport cost of the entire batch. The linear interpolation between new pairs ($x_0^i, x_1^i$) are plotted using grey lines. We can see that there are less crossover of interpolation trajectories and the transport cost has been reduced.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#13-lets-see-how-the-ot-can-help-in-conditional-flow-matching-training-we-will-train-two-models-one-with-ot-and-the-other-one-without-and-compare-the-flow-trajectory-during-sampling","title":"1.3 Let's see how the OT can help in conditional flow matching training. We will train two models, one with OT and the other one without, and compare the flow trajectory during sampling.\u00b6","text":"<p>Note the ContinuousFlowMatcher object can be initialized with any batch augmentation using the 'ot_type' parameter. For clarity we pull in our previosuly initialized OT Sampler.</p>"},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#14-visualization-of-flow-trajectories-predicted-by-the-two-models-with-ot-left-the-flow-trajectory-is-straighter-thus-less-transport-cost-comapred-to-without-ot-right","title":"1.4 Visualization of flow trajectories predicted by the two models. With OT (left), the flow trajectory is straighter, thus less transport cost comapred to without OT (right).\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#2-we-will-then-introduce-the-kabsch-ot-sampler-the-kabsch-ot-sampler-is-an-implementation-of-the-equivariant-ot-algorithm-klein-et-al-for-a-batch-of-randomly-sampled-noise-mathrmx_0-and-data-mathrmx_1-the-kabsch-ot-sampler-will-sample-x_0-x_1-pairs-based-on-the-rmsd-after-aligning-zero-centered-x_0-x_1-using-kabsch-algorithm-we-will-demonstrate-how-to-use-the-kabsch-ot-sampler-with-a-simple-2d-example","title":"2. We will then introduce the Kabsch OT sampler. The Kabsch OT sampler is an implementation of the \"Equivariant OT\" algorithm (Klein et al.). For a batch of randomly sampled noise ($\\mathrm{x}_0$) and data ($\\mathrm{x}_1$), the Kabsch OT sampler will sample $(x_0, x_1)$ pairs based on the RMSD after aligning zero-centered $(x_0, x_1)$ using Kabsch algorithm. We will demonstrate how to use the Kabsch OT sampler with a simple 2D example.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#21-initialize-mathrmk_0-which-contains-two-samples-k_00-is-a-rhombus-and-k_01-is-a-square-then-initialize-mathrmk_1-which-is-rotated-mathrmk_0-shuffle-the-order-of-mathrmk_1-so-k_10-is-rotated-square-and-k_11-is-rotated-rhombus-when-plotting-the-k_00-and-k_10-are-shown-with-circle-shaped-dots-while-k_01-and-k_11-are-shown-with-square-shaped-dots","title":"2.1 Initialize $\\mathrm{k}_0$ which contains two samples. $k_0^0$ is a rhombus and $k_0^1$ is a square. Then initialize $\\mathrm{k}_1$ which is rotated $\\mathrm{k}_0$. Shuffle the order of $\\mathrm{k}_1$ so $k_1^0$ is rotated square and $k_1^1$ is rotated rhombus. When plotting, the $k_0^0$ and $k_1^0$ are shown with circle-shaped dots while $k_0^1$ and $k_1^1$ are shown with square-shaped dots.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#we-see-that-we-have-arbitraility-set-up-a-mismatch-the-orange-rhombus-with-circle-dots-is-tied-to-the-blue-rotated-square-with-circle-dots-we-can-use-equivariantot-to-fix-this","title":"We see that we have arbitraility set up a mismatch. The orange rhombus with circle dots is tied to the blue rotated square with circle dots. We can use EquivariantOT to fix this.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#22-initialize-the-kabsch-based-equivariant-ot-sampler-and-sample-new-k_0-k_1-pairs-to-minimize-the-transport-cost-of-the-entire-batch-after-rotational-alignment-we-can-see-that-the-order-of-newly-sampled-mathrmk_1-has-changed-to-match-mathrmk_0-note-that-the-sampled-mathrmk_1-will-be-rotated-but-not-translated","title":"2.2 Initialize the Kabsch-based  Equivariant OT sampler and sample new $(k_0, k_1)$ pairs to minimize the transport cost of the entire batch after rotational alignment. We can see that the order of newly sampled $\\mathrm{k}_1$ has changed to match $\\mathrm{k}_0$. Note that the sampled $\\mathrm{k}_1$ will be rotated but not translated.\u00b6","text":""},{"location":"user-guide/examples/bionemo-moco/ot_sampler_tutorial/#if-you-wanted-to-align-with-respect-to-rotations-and-translations-you-could-center-your-data-or-augment-the-equivariantot-object","title":"If you wanted to align with respect to rotations and translations you could center your data or augment the EquivariantOT object\u00b6","text":""},{"location":"user-guide/examples/bionemo-scdl/example_notebook/","title":"Example notebook","text":"NOTE It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits. In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport pooch\nfrom torch.utils.data import DataLoader\n\nfrom bionemo.core import BIONEMO_CACHE_DIR\nfrom bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n</pre> import os import tempfile  import pooch from torch.utils.data import DataLoader  from bionemo.core import BIONEMO_CACHE_DIR from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset from bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch <p>First, copy the input data. This can be done by copying https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad to a directory named <code>hdf5s</code>.</p> In\u00a0[2]: Copied! <pre>input_data = pooch.retrieve(\n    'https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad',\n    path=BIONEMO_CACHE_DIR / \"hdf5s\",\n    known_hash='a0728e13a421bbcd6b2718e1d32f88d0d5c7cb92289331e3f14a59b7c513b3bc')\n</pre> input_data = pooch.retrieve(     'https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad',     path=BIONEMO_CACHE_DIR / \"hdf5s\",     known_hash='a0728e13a421bbcd6b2718e1d32f88d0d5c7cb92289331e3f14a59b7c513b3bc') In\u00a0[3]: Copied! <pre>#Create a SingleCellMemMapDataset\ndataset_temp_dir = tempfile.TemporaryDirectory()\ndataset_dir = os.path.join(dataset_temp_dir.name, \"97e_scmm\")\n\ndata = SingleCellMemMapDataset(dataset_dir, input_data)\n</pre> #Create a SingleCellMemMapDataset dataset_temp_dir = tempfile.TemporaryDirectory() dataset_dir = os.path.join(dataset_temp_dir.name, \"97e_scmm\")  data = SingleCellMemMapDataset(dataset_dir, input_data) In\u00a0[4]: Copied! <pre>#Save the dataset to the disk. \ndata.save()\n</pre> #Save the dataset to the disk.  data.save() Out[4]: <pre>True</pre> In\u00a0[5]: Copied! <pre>#Reload the data\nreloaded_data = SingleCellMemMapDataset(dataset_dir)\n</pre> #Reload the data reloaded_data = SingleCellMemMapDataset(dataset_dir) <p>There are various numbers of columns per observation. However, for a batch size of 1 the data does not need to be collated. It will then be outputted in a torch tensor of shape (1, 2, num_obs) The first row of lengh num_obs contains the column pointers, and the second row contains the corresponding values.</p> In\u00a0[6]: Copied! <pre>model = lambda x : x\n\ndataloader = DataLoader(data, batch_size=1, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</pre> model = lambda x : x  dataloader = DataLoader(data, batch_size=1, shuffle=True, collate_fn=collate_sparse_matrix_batch) n_epochs = 1 for e in range(n_epochs):     for batch in dataloader:         model(batch)  <pre>/home/pbinder/bionemo-framework/sub-packages/bionemo-scdl/src/bionemo/scdl/util/torch_dataloader_utils.py:39: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n  batch_sparse_tensor = torch.sparse_csr_tensor(batch_rows, batch_cols, batch_values, size=(len(batch), max_pointer))\n</pre> <p>The data can be collated with a batch size of 1 and must be collated with larger batch sizes. This will collate several sparse matrices into the CSR (Compressed Sparse Row) torch tensor format.</p> In\u00a0[7]: Copied! <pre>model = lambda x : x\n\ndataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</pre> model = lambda x : x  dataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch) n_epochs = 1 for e in range(n_epochs):     for batch in dataloader:         model(batch)  <p>For some applications, we might want to also use the features. These can be specified with get_row(index, return_features = True). By default, all features are returned, but the features can be specified with the feature_vars argument in get_row, which corresponds to a list of the feature names to return.</p> In\u00a0[21]: Copied! <pre>for index in range(len(data)):\n    model(data.get_row(index,return_features = True))\n</pre> for index in range(len(data)):     model(data.get_row(index,return_features = True))  <p>Alternatively, if there are multiple AnnData files, they can be converted into a single SingleCellMemMapDataset. If the hdf5 directory has one or more AnnData files, the SingleCellCollection class crawls the filesystem to recursively find AnnData files (with the h5ad extension). The code below is in scripts/convert_h5ad_to_scdl.py. It will create a new dataset at example_dataset. This can also be called with the convert_h5ad_to_scdl command.</p> In\u00a0[\u00a0]: Copied! <pre># path to dir holding hdf5s data\nhdf5s = BIONEMO_CACHE_DIR / \"hdf5s\"\n\n# path to output dir where SCDataset will be stored\noutput_dir = os.path.join('scdataset_output')\n</pre> # path to dir holding hdf5s data hdf5s = BIONEMO_CACHE_DIR / \"hdf5s\"  # path to output dir where SCDataset will be stored output_dir = os.path.join('scdataset_output') In\u00a0[\u00a0]: Copied! <pre>from bionemo.scdl.io.single_cell_collection import SingleCellCollection\nwith tempfile.TemporaryDirectory() as temp_dir:\n    coll = SingleCellCollection(temp_dir)\n    coll.load_h5ad_multi(hdf5s, max_workers=4, use_processes=True)\n    coll.flatten(output_dir, destroy_on_copy=True)\n</pre> from bionemo.scdl.io.single_cell_collection import SingleCellCollection with tempfile.TemporaryDirectory() as temp_dir:     coll = SingleCellCollection(temp_dir)     coll.load_h5ad_multi(hdf5s, max_workers=4, use_processes=True)     coll.flatten(output_dir, destroy_on_copy=True) In\u00a0[\u00a0]: Copied! <pre>dataset_temp_dir.cleanup()\n</pre> dataset_temp_dir.cleanup()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/getting-started/","title":"Getting Started","text":""},{"location":"user-guide/getting-started/#repository-structure","title":"Repository structure","text":""},{"location":"user-guide/getting-started/#high-level-overview","title":"High level overview","text":"<p>This repository is structured as a meta-package that collects together many python packages. We designed in this way because this is how we expect our users to use bionemo, as a package that they themselves import and use in their own projects. By structuring code like this ourselves we ensure that bionemo developers follow similar patterns to our end users.</p> <p>Each model is stored in its own <code>sub-packages</code>. Some examples of models include:</p> <ul> <li><code>sub-packages/bionemo-esm2</code>: ESM2 model</li> <li><code>sub-packages/bionemo-geneformer</code>: Geneformer</li> <li><code>sub-packages/bionemo-example_model</code>: A minimal example MNIST model that demonstrates how you can write a lightweight     megatron model that doesn't actually support any megatron parallelism, but should run fine as long as you only use     data parallelism to train.</li> </ul> <p>There are also useful utility packages, for example:</p> <ul> <li><code>sub-packages/bionemo-scdl</code>: Single Cell Dataloader (SCDL) provides a dataset implementation that can be used by downstream     single-cell models in the bionemo package.</li> <li><code>sub-packages/bionemo-testing</code>: a suite of utilities that are useful in testing, think <code>torch.testing</code> or <code>np.testing</code>.</li> </ul> <p>Finally some of the packages represent common functions and abstract base classes that expose APIs that are useful for interacting with <code>NeMo2</code>. Some examples of these include:</p> <ul> <li><code>sub-packages/bionemo-core</code>: mostly just high level APIs</li> <li><code>sub-packages/bionemo-llm</code>: ABCs for code that multiple large language models (eg BERT variants) share.</li> </ul> <p>Documentation source is stored in <code>docs/</code></p> <p>The script for building a local docker container is <code>./launch.sh</code> which has some useful commands including:</p> <ul> <li><code>./launch.sh build</code> to build the container</li> <li><code>./launch.sh run</code> to get into a running container with reasonable settings for data/code mounts etc.</li> </ul>"},{"location":"user-guide/getting-started/#more-detailed-structure-notes","title":"More detailed structure notes","text":"<pre><code>$ tree -C -I \"*.pyc\" -I \"test_data\" -I \"test_experiment\" -I \"test_finettune_experiment\" -I __pycache__ -I \"*.egg-info\" -I lightning_logs -I results -I data -I MNIST* -I 3rdparty\n.\n\u251c\u2500\u2500 CODE-REVIEW.md -&gt; docs/CODE-REVIEW.md\n\u251c\u2500\u2500 CODEOWNERS\n\u251c\u2500\u2500 CONTRIBUTING.md -&gt; docs/CONTRIBUTING.md\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 LICENSE\n\u2502   \u251c\u2500\u2500 license.txt\n\u2502   \u2514\u2500\u2500 third_party.txt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 VERSION\n\u251c\u2500\u2500 ci\n\u2502   \u2514\u2500\u2500 scripts\n\u2502       \u251c\u2500\u2500 nightly_test.sh\n\u2502       \u251c\u2500\u2500 pr_test.sh\n\u2502       \u2514\u2500\u2500 static_checks.sh\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 CODE-REVIEW.md\n\u2502   \u251c\u2500\u2500 CONTRIBUTING.md\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 docs\n\u2502   \u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 color-schemes.css\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 custom-material.css\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 fonts.css\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 images\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 favicon.png\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 logo-icon-black.svg\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 logo-white.svg\n\u2502   \u2502   \u251c\u2500\u2500 developer-guide\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 CODE-REVIEW.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 CONTRIBUTING.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 jupyter-notebooks.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2502   \u2514\u2500\u2500 user-guide\n\u2502   \u2502       \u2514\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 mkdocs.yml\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 scripts\n\u2502       \u2514\u2500\u2500 gen_ref_pages.py\n\u251c\u2500\u2500 launch.sh\n\u251c\u2500\u2500 license_header\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 requirements-cve.txt\n\u251c\u2500\u2500 requirements-dev.txt\n\u251c\u2500\u2500 requirements-test.txt\n\u251c\u2500\u2500 scripts   # \ud83d\udfe2 Temporary scripts that demonstrate how to run some of these programs. These will be replaced.\n\u2502   \u251c\u2500\u2500 artifact_paths.yaml\n\u2502   \u251c\u2500\u2500 download_artifacts.py\n\u2502   \u251c\u2500\u2500 gpt-pretrain.py\n\u2502   \u251c\u2500\u2500 protein\n\u2502   \u2502   \u2514\u2500\u2500 esm2\n\u2502           \u2514\u2500\u2500 esm2_dataset_perplexity.py\n# \ud83d\udfe2 All work goes into `sub-packages`\n#  Sub-packages represent individually installable subsets of the bionemo codebase. We recommend that you\n#  create new sub-packages to track your experiments and save any updated models or utilities that you need.\n\u251c\u2500\u2500 sub-packages\n\u2502   \u251c\u2500\u2500 bionemo-core  # \ud83d\udfe2 bionemo-core, and bionemo-llm represent top level sub-packages that do not depend on others\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src  # \ud83d\udfe2 All sub-packages have a `src` and a `test` sub-directory.\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 core\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 config.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 batching_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 dtypes.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 random_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests  # \ud83d\udfe2 Test files should be mirrored with `src` files, and have the same name other than `test_[file_name].py`\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u251c\u2500\u2500 core\n\u2502   \u2502           \u2514\u2500\u2500 pytorch\n\u2502   \u2502               \u2514\u2500\u2500 utils\n\u2502   \u2502                   \u2514\u2500\u2500 test_dtypes.py\n\u2502   \u251c\u2500\u2500 bionemo-esm2  # \ud83d\udfe2 The ESM2 model sub-package. This stores models and dataloaders necessary for pretraining and some example fine-tuning.\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 esm2\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 model\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 attention.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 embedding.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 lr_scheduler.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 model.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 esm2\n\u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502               \u251c\u2500\u2500 conftest.py\n\u2502   \u2502               \u2514\u2500\u2500 model\n\u2502   \u2502                   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_attention.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_embedding.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_lr_scheduler.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_model.py\n\u2502   \u251c\u2500\u2500 bionemo-example_model  # \ud83d\udfe2 a small example model that demonstrates how to write a megatron model from scratch and train on MNIST\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 example_model\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 lightning_basic.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 example_model\n\u2502   \u2502               \u2514\u2500\u2500 test_lightning_basic.py\n\u2502   \u251c\u2500\u2500 bionemo-fw  # \ud83d\udfe2 a meta-package that pulls together all other packages. A user can install this and get all of bionemo.\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 fw\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 fw\n\u2502   \u2502               \u2514\u2500\u2500 test_sub_package_imports.py\n\u2502   \u251c\u2500\u2500 bionemo-geneformer  # \ud83d\udfe2 geneformer sub-module\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 geneformer\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 finetune_token_regressor.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 tokenizer\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 gene_tokenizer.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 label2id_tokenizer.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 geneformer\n\u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502               \u251c\u2500\u2500 test_model.py\n\u2502   \u2502               \u251c\u2500\u2500 test_stop_and_go.py\n\u2502   \u2502               \u2514\u2500\u2500 test_transformer_specs.py\n\u2502   \u251c\u2500\u2500 bionemo-llm  # \ud83d\udfe2 shared model code for LLM style models, eg BERT variants, transformer variants, etc.\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 llm\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 lightning.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 biobert\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 lightning.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 model.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 testing_utils.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u2514\u2500\u2500 transformer_specs.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 layers.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 loss.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 datamodule_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 iomixin_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 logger_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 remote.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 weight_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 llm\n\u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502               \u251c\u2500\u2500 model\n\u2502   \u2502               \u2502   \u251c\u2500\u2500 biobert\n\u2502   \u2502               \u2502   \u2502   \u2514\u2500\u2500 test_transformer_specs.py\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_loss.py\n\u2502   \u2502               \u251c\u2500\u2500 test_lightning.py\n\u2502   \u2502               \u2514\u2500\u2500 utils\n\u2502   \u2502                   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_datamodule_utils.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_iomixin_utils.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_logger_utils.py\n\u2502   \u251c\u2500\u2500 bionemo-scdl  # \ud83d\udfe2\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 examples\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 example_notebook.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 scdl\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 single_cell_row_dataset.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 index\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 row_feature_index.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 io\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 single_cell_collection.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 single_cell_memmap_dataset.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 scripts\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 convert_h5ad_to_scdl.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 util\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 async_worker_queue.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 torch_dataloader_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 scdl\n\u2502   \u2502               \u251c\u2500\u2500 conftest.py\n\u2502   \u2502               \u251c\u2500\u2500 index\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_row_feature_index.py\n\u2502   \u2502               \u251c\u2500\u2500 io\n\u2502   \u2502               \u2502   \u251c\u2500\u2500 test_single_cell_collection.py\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_single_cell_memmap_dataset.py\n\u2502   \u2502               \u2514\u2500\u2500 util\n\u2502   \u2502                   \u251c\u2500\u2500 test_async_worker_queue.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_torch_dataloader_utils.py\n\u2502   \u251c\u2500\u2500 bionemo-testing\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 testing\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 callbacks.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 harnesses\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 stop_and_go.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 megatron_parallel_state_utils.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 testing_callbacks.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 testing\n\u2502   \u2502               \u2514\u2500\u2500 test_megatron_parallel_state_utils.py\n\u2502   \u2514\u2500\u2500 bionemo-webdatamodule\n\u2502       \u251c\u2500\u2500 LICENSE\n\u2502       \u251c\u2500\u2500 README.md\n\u2502       \u251c\u2500\u2500 pyproject.toml\n\u2502       \u251c\u2500\u2500 requirements.txt\n\u2502       \u251c\u2500\u2500 setup.py\n\u2502       \u251c\u2500\u2500 src\n\u2502       \u2502   \u2514\u2500\u2500 bionemo\n\u2502       \u2502       \u2514\u2500\u2500 webdatamodule\n\u2502       \u2502           \u251c\u2500\u2500 __init__.py\n\u2502       \u2502           \u251c\u2500\u2500 datamodule.py\n\u2502       \u2502           \u2514\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 tests\n\u2502           \u2514\u2500\u2500 bionemo\n\u2502               \u2514\u2500\u2500 webdatamodule\n\u2502                   \u251c\u2500\u2500 __init__.py\n\u2502                   \u251c\u2500\u2500 conftest.py\n\u2502                   \u2514\u2500\u2500 test_datamodule.py\n</code></pre>"},{"location":"user-guide/getting-started/#installation","title":"Installation","text":""},{"location":"user-guide/getting-started/#initializing-3rd-party-dependencies-as-git-submodules","title":"Initializing 3rd-party dependencies as git submodules","text":"<p>For development, the NeMo and Megatron-LM dependencies are vendored in the bionemo-2 repository workspace as git submodules. The pinned commits for these submodules represent the \"last-known-good\" versions of these packages that are confirmed to be working with bionemo2 (and those that are tested in CI).</p> <p>To initialize these sub-modules when cloning the repo, add the <code>--recursive</code> flag to the git clone command:</p> <pre><code>git clone --recursive git@github.com:NVIDIA/bionemo-framework.git\n</code></pre> <p>To download the pinned versions of these submodules within an existing git repository, run</p> <pre><code>git submodule update --init --recursive\n</code></pre>"},{"location":"user-guide/getting-started/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Hardware and Software Prerequisites</li> <li>Access and Startup</li> <li>Initialization Guide</li> <li>Development</li> <li>Training Models</li> </ul>"},{"location":"user-guide/getting-started/access-startup/","title":"Access and Startup","text":"<p>The BioNeMo Framework is free to use and easily accessible. The preferred method of accessing the software is through the BioNeMo Docker container, which provides a seamless and hassle-free way to develop and execute code. By using the Docker container, you can bypass the complexity of handling dependencies, ensuring that you have a consistent and reproducible environment for your projects.</p> <p>In this section of the documentation, we will guide you through the process of pulling the BioNeMo Docker container and setting up a local development environment. By following these steps, you will be able to quickly get started with the BioNeMo Framework and begin exploring its features and capabilities.</p>"},{"location":"user-guide/getting-started/access-startup/#access-the-bionemo-framework","title":"Access the BioNeMo Framework","text":""},{"location":"user-guide/getting-started/access-startup/#brevdev-access","title":"Brev.Dev Access","text":"<p>The BioNeMo Framework container can run in a brev.dev launchable: . It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credit. After launching the instance, launch a Terminal session in the Jupyter Lab UI. (Note: This links to the nightly release and may be out of sync with these docs.)</p>"},{"location":"user-guide/getting-started/access-startup/#ngc-account-and-api-key-configuration","title":"NGC Account and API Key Configuration","text":"<p>Another option to access the BioNeMo Framework container is to use a free NVIDIA GPU Cloud (NGC) account and an API key linked to that account.</p> <p>NGC is a portal of enterprise services, software, and support for artificial intelligence and high-performance computing (HPC) workloads. The BioNeMo Docker container is hosted on the NGC Container Registry. To pull and run a container from this registry, you will need to create a free NGC account and an API Key using the following steps:</p> <ol> <li>Create a free account on NGC and log in.</li> <li>At the top right, click on the User &gt; Setup &gt; Generate API Key, then click + Generate API Key and Confirm. Copy and store your API Key in a secure location.</li> </ol> <p>You can now view the BioNeMo Framework container at this direct link in the NGC Catalog or by searching the NGC Catalog for \u201cBioNeMo Framework\u201d. Feel free to explore the other resources available to you in the catalog.</p>"},{"location":"user-guide/getting-started/access-startup/#ngc-cli-configuration","title":"NGC CLI Configuration","text":"<p>The NGC Command Line Interface (CLI) is a command-line tool for managing resources in NGC, including datasets and model checkpoints. You can download the CLI on your local machine using the instructions on the NGC CLI website.</p> <p>Once you have installed the NGC CLI, run <code>ngc config set</code> at the command line to setup your NGC credentials:</p> <ul> <li>API key: Enter your API Key</li> <li>CLI output: Accept the default (ASCII format) by pressing <code>Enter</code></li> <li>org: Choose your preferred organization from the supplied list</li> <li>team: Choose the team to which you have been assigned from the supplied list</li> <li>ace : Choose an ACE, if applicable, otherwise press <code>Enter</code> to continue</li> </ul> <p>Note that the org and team are only relevant when pulling private containers/datasets from NGC created by you or your team. To access BioNeMo Framework, you can use the default value.</p>"},{"location":"user-guide/getting-started/access-startup/#startup-instructions","title":"Startup Instructions","text":"<p>BioNeMo is compatible with a wide variety of computing environments, including both local workstations, data centers, and Cloud Service Providers (CSPs) such as Amazon Web Services, Microsoft Azure, Google Cloud Platform, and Oracle Cloud Infrastructure, and NVIDIA\u2019s own DGX Cloud.</p>"},{"location":"user-guide/getting-started/access-startup/#running-the-container-on-a-local-machine","title":"Running the Container on a Local Machine","text":"<p>This section will provide instructions for running the BioNeMo Framework container on a local workstation. This process will involve the following steps:</p> <ol> <li>Logging into the NGC Container Registry (<code>nvcr.io</code>)</li> <li>Pulling the container from the registry</li> <li>Running a Jupyter Lab instance inside the container for local development</li> </ol>"},{"location":"user-guide/getting-started/access-startup/#pull-docker-container-from-ngc","title":"Pull Docker Container from NGC","text":"<p>Open a command prompt on your machine and enter the following:</p> <pre><code>docker login nvcr.io\n</code></pre> <p>This command will prompt you to enter your API key. Fill in the details as shown below. Note that you should enter the string <code>$oauthtoken</code> as your username. Replace the password (<code>&lt;YOUR_API_KEY&gt;</code>) with the API key that you generated in the NGC Account and API Key Configuration section above:</p> <pre><code>Username: $oauthtoken\nPassword: &lt;YOUR_API_KEY&gt;\n</code></pre> <p>You can now pull the BioNeMo Framework container using the following command:</p> <pre><code>docker pull nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre>"},{"location":"user-guide/getting-started/access-startup/#run-the-bionemo-framework-container","title":"Run the BioNeMo Framework Container","text":"<p>Now that you have pulled the BioNeMo Framework container, you can run it as you would a normal Docker container. For instance, to get basic shell access you can run the following command:</p> <pre><code>docker run --rm -it --gpus all \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  /bin/bash\n</code></pre> <p>Because BioNeMo is distributed as a Docker container, standard arguments can be passed to the <code>docker run</code> command to alter the behavior of the container and its interactions with the host system. For more information on these arguments, refer to the Docker documentation.</p> <p>In the next section, Initialization Guide, we will present some useful <code>docker run</code> command variants for common workflows.</p>"},{"location":"user-guide/getting-started/access-startup/#running-on-any-major-csp-with-the-nvidia-gpu-optimized-vmi","title":"Running on Any Major CSP with the NVIDIA GPU-Optimized VMI","text":"<p>The BioNeMo Framework container is supported on cloud-based GPU instances through the NVIDIA GPU-Optimized Virtual Machine Image (VMI), available for AWS, GCP, Azure, and OCI. NVIDIA VMIs are built on Ubuntu and provide a standardized operating system environment across cloud infrastructure for running NVIDIA GPU-accelerated software. These images are pre-configured with software dependencies such as NVIDIA GPU drivers, Docker, and the NVIDIA Container Toolkit. More details about NVIDIA VMIs can be found in the NGC Catalog.</p> <p>The general steps for launching the BioNeMo Framework container using a CSP are:</p> <ol> <li>Launch a GPU-equipped instance running the NVIDIA GPU-Optimized VMI on your preferred CSP. Follow the instructions for     launching a GPU-equipped instance provided by your CSP.</li> <li>Connect to the running instance using SSH and run the BioNeMo Framework container exactly as outlined in the     Running the Container on a Local Machine section on     the Access and Startup page.</li> </ol>"},{"location":"user-guide/getting-started/development/","title":"Development with BioNeMo","text":"<p>On this page, we will cover the organization of the codebase and the setup necessary for the two primary development workflows for users of the BioNeMo Framework: training and fine-tuning models. For both of these workflows, we recommend setting the <code>NGC_CLI_API_KEY</code> environment variable as discussed in the Initialization Guide. This environment variable is required by the script that will be used to download both model checkpoints and data from NGC to be used in these workflows.</p>"},{"location":"user-guide/getting-started/development/#bionemo-code-overview","title":"BioNeMo Code Overview","text":"<p>The BioNeMo codebase is structured as a meta-package that collects together many Python packages. We designed BioNeMo this way with the expectation that users will import and use BioNeMo in their own projects. By structuring code this way, we ensure that BioNeMo developers follow similar patterns to those we expect of our end users.</p> <p>Each model is stored in its own subdirectory of <code>sub-packages</code>. Some examples of models include:</p> <ul> <li><code>bionemo-esm2</code>: The ESM-2 model</li> <li><code>bionemo-geneformer</code>: The Geneformer model</li> <li><code>bionemo-example_model</code>: A minimal example MNIST model that demonstrates how you can write a lightweight     Megatron model that does not actually support any megatron parallelism, but should run fine as long as you only use     data parallelism to train.</li> </ul> <p>We also include useful utility packages, for example:</p> <ul> <li><code>bionemo-scdl</code>: Single Cell Dataloader (SCDL) provides a dataset implementation that can be used by     downstream single-cell models in the bionemo package.</li> <li><code>bionemo-testing</code>: A suite of utilities that are useful in testing, think <code>torch.testing</code> or <code>np.testing</code>.</li> </ul> <p>Finally some of the packages represent common functions and abstract base classes that expose APIs that are useful for interacting with <code>NeMo2</code>. Some examples of these include:</p> <ul> <li><code>bionemo-core</code>: High-level APIs</li> <li><code>bionemo-llm</code>: Abstract base classes for code that multiple large language models (eg BERT variants) share.</li> </ul>"},{"location":"user-guide/getting-started/development/#package-structure","title":"Package Structure","text":"<p>Within each of the Bionemo packages, a consistent structure is employed to facilitate organization and maintainability. The following components are present in each package:</p> <ul> <li><code>pyproject.toml</code>: Defines package metadata, including version, package name, and executable scripts to be installed.</li> <li><code>src</code>: Contains all source code for the package. Each package features a top-level <code>bionemo</code> folder, which serves     as the primary namespace for imports. During the build process, all <code>bionemo/*</code> source files are combined into a     single package, with unique subdirectory names appended to the <code>bionemo</code> directory.</li> <li><code>tests</code>: Houses all package tests. The convention for test files is to locate them in the same path as the     corresponding <code>src</code> file, but within the <code>tests</code> directory, with a <code>test_</code> prefix added to the test file name. For     example, to test a module-level file <code>src/bionemo/my_module</code>, a test file <code>tests/bionemo/test_my_module.py</code> should     be created. Similarly, to test a specific file <code>src/bionemo/my_module/my_file.py</code>, the test file should be named     <code>tests/bionemo/my_module/test_my_file.py</code>. Running <code>py.test sub-packages/my_package</code> will execute all tests within     the <code>tests</code> directory.</li> <li><code>examples</code>: Some packages include an <code>examples</code> directory containing Jupyter Notebook (<code>.ipynb</code>) files, which are     aggregated into the main documentation.</li> <li><code>README.md</code>: The core package README file serves as the primary documentation for each sub-package when uploaded     to PyPI.</li> <li><code>LICENSE</code>: For consistency, all Bionemo packages should utilize the Apache-2.0 license. By contributing code to     BioNeMo, you acknowledge permission for the code to be re-released under an Apache v2 license.</li> </ul>"},{"location":"user-guide/getting-started/development/#model-training-process","title":"Model Training Process","text":"<p>Note</p> <p>See also Training Models</p> <p>The process for pretraining models from BioNeMo involves running scripts located in the <code>scripts</code> directory. Each script exposes a Command-Line Interface (CLI) that contains and documents the options available for that model.</p> <p>To pretrain a model, you need to run the corresponding script with the required parameters. For example, to pretrain the ESM-2 and Geneformer models, you would call <code>train_esm2</code> and <code>train_geneformer</code> executables, respectively.</p> <p>The scripts provide various options that can be customized for pretraining, such as:</p> <ul> <li>Data directories and paths</li> <li>Model checkpoint paths</li> <li>Experiment names for tracking</li> <li>Number of GPUs and nodes</li> <li>Validation check intervals</li> <li>Number of dataset workers</li> <li>Number of steps</li> <li>Sequence lengths</li> <li>Micro-batch sizes</li> <li>Limit on validation batches</li> </ul> <p>You can specify these options when running the script using command-line arguments. For each of the available scripts, you can use the <code>--help</code> option for an explanation of the available options for that model.</p> <p>For more information on pretraining a model, refer to the ESM-2 Pretraining Tutorial.</p>"},{"location":"user-guide/getting-started/development/#fine-tuning","title":"Fine-Tuning","text":"<p>The model fine-tuning process involves downloading the required model checkpoints using the <code>download_bionemo_data</code> script. This script takes in the model name and version as arguments, along with the data source, which can be either <code>ngc</code> (default) or <code>pbss</code> for NVIDIA employees.</p> <p>To view a list of available resources (both model checkpoints and datasets), you can use the following command:</p> <pre><code>download_bionemo_data --list-resources\n</code></pre>"},{"location":"user-guide/getting-started/development/#step-1-download-data-and-checkpoints","title":"Step 1: Download Data and Checkpoints","text":"<p>To download the data and checkpoints, use the following command:</p> <pre><code>export DATA_SOURCE=\"ngc\"\nMODEL_CKPT=$(download_bionemo_data &lt;model_name&gt;/&lt;checkpoint_name&gt;:&lt;version&gt; --source $DATA_SOURCE);\n</code></pre> <p>Replace <code>&lt;model_name&gt;</code> with the desired model (for example, <code>esm2</code> or <code>geneformer</code>), <code>&lt;version&gt;</code> with the desired version, and <code>&lt;checkpoint_name&gt;</code> with the desired checkpoint name.</p> <p>Additionally, you can download available datasets from NGC using the following command, making similar substitutions as with the model checkpoint download command above:</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data &lt;model_name&gt;/testdata:&lt;version&gt; --source $DATA_SOURCE);\n</code></pre> <p>Alternatively, you can use your own data by configuring your container run with volume mounts as discussed in the Initialization Guide.</p>"},{"location":"user-guide/getting-started/development/#step-2-adapt-the-training-process","title":"Step 2: Adapt the Training Process","text":"<p>Fine-tuning may involve specifying a different combination of model and loss than was used to train the initial version of the model. The fine-tuning steps will be application-specific, but a general set of steps include:</p> <ol> <li>Prepare your dataset: Collect and prepare your dataset, including the sequence data and target values. This step is     crucial to ensure that your dataset is in a format that can be used for training.</li> <li>Create a custom dataset class: Define a custom dataset class that can handle your specific data format. This class should     be able to initialize the dataset and retrieve individual data points.</li> <li>Create a datamodule: Define a datamodule that prepares the data for training. This includes tasks such as data loading,     tokenization, and batching.</li> <li>Fine-tune the model: Use a pre-trained model as a starting point and fine-tune it on your dataset. This involves     adjusting the model's parameters to fit your specific task and dataset.</li> <li>Configure the fine-tuning: Set various hyperparameters for the fine-tuning process, such as the batch size, number of     training steps, and learning rate. These hyperparameters can significantly affect the performance of the fine-tuned     model.</li> <li>Run inference: Once the model is fine-tuned, use it to make predictions on new, unseen data.</li> </ol> <p>For more information on fine-tuning a model, refer to the ESM-2 Fine-tuning Tutorial.</p>"},{"location":"user-guide/getting-started/development/#advanced-developer-documentation","title":"Advanced Developer Documentation","text":"<p>For advanced development information (for example, developing the source code of BioNeMo), refer to the README found on the main page of the BioNeMo GitHub Repository.</p>"},{"location":"user-guide/getting-started/initialization-guide/","title":"Initialization Guide","text":"<p>Note</p> <p>Prior to beginning this section, you must confirm that your computing platform meets or exceeds the prerequisites outlined in the Hardware and Software Prerequisites page and that you have already pulled and verified that you can run the BioNeMo container as outlined in the Access and Startup page.</p> <p>At this point, you have successfully launched and run the Docker container. This section will guide you through setting up your host machine environment, suggest Docker commands for various common workflows, and explain helpful <code>docker run</code> options.</p>"},{"location":"user-guide/getting-started/initialization-guide/#setting-up-your-host-machine-environment","title":"Setting Up Your Host Machine Environment","text":"<p>To effectively use the BioNeMo Framework, we recommend an organized environment configuration and directory structure. Specifically, we recommend having several cache directories per project. These directories will contain project files such as data, model checkpoints, training scripts, and outputs such as logs and predictions. To facilitate container set up, we recommend storing the paths to these directories in a <code>.env</code> file that can be referenced at container runtime. Below, we suggest useful environment variables to define in this file.</p>"},{"location":"user-guide/getting-started/initialization-guide/#creating-a-env-file-for-first-time-setup","title":"Creating a .env File For First Time Setup","text":"<p>We recommend using a <code>.env</code> file in your local workspace to define environment variables. Specifically, the following variables are useful to include in your <code>.env</code> file:</p> <pre><code># Local Cache Directories\nLOCAL_RESULTS_PATH\nDOCKER_RESULTS_PATH\nLOCAL_DATA_PATH\nDOCKER_DATA_PATH\nLOCAL_MODELS_PATH\nDOCKER_MODELS_PATH\n\n# Desired Jupyter Port\nJUPYTER_PORT\n\n# NGC Configuration Settings\nNGC_CLI_API_KEY\nNGC_CLI_ORG\nNGC_CLI_TEAM\nNGC_CLI_FORMAT_TYPE\n\n# Weights and Biases API Key\nWANDB_API_KEY\n</code></pre> <p>For each of these variables, you can define them inside the <code>.env</code> file using <code>=</code>. For example, you can set the NGC API key using <code>NGC_CLI_API_KEY=&lt;your API key here&gt;</code>. You can then define these variables in your current shell using:</p> <pre><code>source .env\n</code></pre> <p>Running this command will make these variables available for use in the <code>docker run</code> command examples shown below.</p> <p>NGC Credentials Required for Data Download</p> <p>Some of the credentials in the above <code>.env</code> file are optional for specific workflows. However, if you intend to use data hosted on the NGC platform (for example, model checkpoints and example training data), you must define both NGC_CLI_API_KEY and NGC_CLI_ORG at container run time. The easiest way to ensure these variables are set is to use the <code>.env</code> file as shown here with your specific variable definitions.</p> <p>Refer to the list below for an explanation of each of these variables:</p> <ul> <li><code>LOCAL_RESULTS_PATH</code> and <code>DOCKER_RESULTS_PATH</code>: Paths for storing results, with <code>LOCAL</code> referring to the path on the     local machine and <code>DOCKER</code> referring to the path inside the Docker container.</li> <li><code>LOCAL_DATA_PATH</code> and <code>DOCKER_DATA_PATH</code>: Paths for storing data, again with <code>LOCAL</code> and <code>DOCKER</code> distinctions.</li> <li><code>LOCAL_MODELS_PATH</code> and <code>DOCKER_MODELS_PATH</code>: Paths for storing machine learning models, with the same local and     Docker differences.</li> <li><code>JUPYTER_PORT</code>: The port number for a Jupyter Lab server, default port is 8888.</li> <li><code>NGC_CLI_API_KEY</code>, <code>NGC_CLI_ORG</code>, <code>NGC_CLI_TEAM</code>, and <code>NGC_CLI_FORMAT_TYPE</code>: API key, organization, team, and format     type for the NVIDIA GPU Cloud (NGC) command-line interface (CLI).</li> <li><code>WANDB_API_KEY</code>: An API key for Weights and Biases (W&amp;B), a platform for machine learning experiment tracking and     visualization.</li> </ul> Weights and Biases Setup (WANDB_API_KEY, Optional) <p>Weights and Biases (W&amp;B) is a machine learning operations platform that provides tools and services to help machine learning practitioners build, train, and deploy models more efficiently. BioNeMo is built to work with W&amp;B and requires only simple setup steps to start tracking your experiments. To set up W&amp;B inside your container, follow the steps below:</p> <ol> <li>Sign up for an account at Weights and Biases.</li> <li>Setup your API Key with W&amp;B.</li> <li>Set the <code>WANDB_API_KEY</code> variable in your <code>.env</code> in the same way as you set the previous environment variable     above.</li> <li>Set the environment variable inside your container using the <code>-e</code> option, as shown in the next section.</li> </ol>"},{"location":"user-guide/getting-started/initialization-guide/#starting-the-bionemo-container-for-common-workflows","title":"Starting the BioNeMo Container for Common Workflows","text":"<p>Below we describe some common BioNeMo workflows, including how to setup and run the container in each case. Each of the following examples will assume that you have local workspace directories as defined in your <code>.env</code> file shown above that you will attach to the container via volume mounts.</p>"},{"location":"user-guide/getting-started/initialization-guide/#starting-a-shell-inside-the-container","title":"Starting a Shell Inside the Container","text":"<p>With a shell inside the BioNeMo Docker container, you can execute commands, edit files, and run applications as if you were working directly on the host machine. This self-contained environment allows you to work with your project's dependencies and configurations in isolation, ensuring consistent results and reproducibility. You can install packages, test and debug applications, and customize the environment to suit your needs.</p> <p>You can launch a Bash shell inside the BioNeMo container using the command below. Note that any files modified in the mounted directories while inside the container will persist on the host machine, but other modifications (such as installed software) will not.</p> <pre><code>docker run \\\n  --rm -it \\\n  --gpus all \\\n  --network host \\\n  --shm-size=4g \\\n  -e WANDB_API_KEY \\\n  -e NGC_CLI_API_KEY \\\n  -e NGC_CLI_ORG \\\n  -e NGC_CLI_TEAM \\\n  -e NGC_CLI_FORMAT_TYPE \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  /bin/bash\n</code></pre> <ul> <li><code>--rm</code>: Removes the container when it exits.</li> <li><code>-it</code>: Allocates a pseudo-TTY and keeps the container running in the foreground.</li> <li><code>--gpus all</code>: Allocates all available GPUs on the host machine.</li> <li><code>--network host</code>: Allows the container to use the host's network stack, effectively sharing the host's network     namespace and allowing the container to access the host's network interfaces directly.</li> <li><code>--shm-size=4g</code>: Sets the size of the shared memory (/dev/shm) in the container to 4 gigabytes, which can be useful for applications that rely heavily on shared memory.</li> <li><code>-e &lt;VARIABLE&gt;</code>: Sets the environment variable inside the container, taking the value set on the host machine.</li> <li><code>-v &lt;LOCAL DIRECTORY&gt;:&lt;DOCKER DIRECTORY&gt;</code>: Mounts a volume from the host machine to the container.</li> <li><code>nvcr.io/nvidia/clara/bionemo-framework:nightly</code>: The path to the Docker image to use.</li> <li><code>/bin/bash</code>: The command to run inside the container, which starts a Bash shell.</li> </ul>"},{"location":"user-guide/getting-started/initialization-guide/#running-a-model-training-script-inside-the-container","title":"Running a Model Training Script Inside the Container","text":"<p>Running a model training script inside the BioNeMo Docker container is the preferred workflow for model training. The container provides an encapsulated and reproducible training environment. By mounting a volume from the host machine, the output directory containing results such as logs and checkpoints can be persisted even after the container is removed. A training script can be run as in the example below. Replace <code>training.py</code> and option (for example, <code>--option1</code>) with the file name and relevant command line options, respectively.</p> <pre><code>docker run --rm -it --gpus all \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH\n</code></pre> <p>Many of the Docker run options are identical to the shell example above, with the exception of the command being run:</p> <ul> <li><code>python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH</code>: The command to run inside the container, which runs the <code>training.py</code> Python script with the specified command-line arguments.</li> </ul>"},{"location":"user-guide/getting-started/initialization-guide/#running-jupyter-lab-inside-the-container","title":"Running Jupyter Lab Inside the Container","text":"<p>By starting a Jupyter Lab instance inside the BioNeMo Framework container, users can leverage the container's optimized environment for machine learning workloads to accelerate their data science workflows, while also benefiting from the interactive and collaborative features of Jupyter Lab. This allows users to seamlessly transition between data preparation, model development, and visualization, all within a single, streamlined environment. You can then launch the container. We recommend running the container in a Jupyter Lab environment using the command below:</p> <pre><code>docker run --rm -d --gpus all \\\n  -p $JUPYTER_PORT:$JUPYTER_PORT \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:nightly \\\n  jupyter lab \\\n    --allow-root \\\n    --ip=* \\\n    --port=$JUPYTER_PORT \\\n    --no-browser \\\n    --NotebookApp.token='' \\\n    --NotebookApp.allow_origin='*' \\\n    --ContentsManager.allow_hidden=True \\\n    --notebook-dir=$DOCKER_RESULTS_PATH\n</code></pre> <p>Refer to the guide below for an explanation of the recommended Jupyter Lab options:</p> <ul> <li><code>jupyter lab ...</code>: The command to run inside the container, which starts a Jupyter Lab server. The options are:<ul> <li><code>--allow-root</code>: Allow the Jupyter Lab server to run as the root user.</li> <li><code>--ip=*</code>: Listen on all available network interfaces, which allows access from outside the container.</li> <li><code>--port=$JUPYTER_PORT</code>: Listen on port 8888.</li> <li><code>--no-browser</code>: Do not open a browser window automatically.</li> <li><code>--NotebookApp.token=''</code>: Set an empty token for the Jupyter Lab server (no authentication is required).</li> <li><code>--NotebookApp.allow_origin='*'</code>: Allow requests from any origin.</li> <li><code>--ContentsManager.allow_hidden=True</code>: Allow the contents manager to access hidden files and directories.</li> <li><code>--notebook-dir=$DOCKER_RESULTS_PATH</code>: Set the notebook directory to     <code>$DOCKER_RESULTS_PATH</code> inside the container.</li> </ul> </li> </ul>"},{"location":"user-guide/getting-started/initialization-guide/#common-docker-run-options","title":"Common <code>docker run</code> Options","text":"<p>Below we explain some common <code>docker run</code> options and how to use them as part of your BioNeMo development workflows.</p>"},{"location":"user-guide/getting-started/initialization-guide/#mounting-volumes-with-the-v-option","title":"Mounting Volumes with the <code>-v</code> Option","text":"<p>The <code>-v</code>  allows you to mount a host machine's directory as a volume inside the container. This enables data persistence even after the container is deleted or restarted. In the context of machine learning workflows, leveraging the <code>-v</code> option is essential for maintaining a local cache of datasets, model weights, and results on the host machine such that they can persist after the container terminates and be reused across container runs.</p> <p>Syntax:</p> <p><pre><code>docker run -v &lt;host_directory&gt;:&lt;container_directory&gt; &lt;image_name&gt;\n</code></pre> Example:</p> <pre><code>docker run -v /path/to/local/cache:/workspace/bionemo2/cache \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <p>In this example, the <code>/path/to/local/cache</code> directory on the host machine is mounted as a volume at <code>/workspace/bionemo2/cache</code> inside the container.</p>"},{"location":"user-guide/getting-started/initialization-guide/#setting-environment-variables-with-the-e-option","title":"Setting Environment Variables with the <code>-e</code> Option","text":"<p>The <code>-e</code> option allows you to set environment variables inside the container. You can use this option to define variables that will be available to the application running inside the container.</p> <p>Example:</p> <pre><code>docker run -e MY_VAR=value -e ANOTHER_VAR=another_value \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <ul> <li><code>-e MY_VAR=value</code> sets the <code>MY_VAR</code> environment variable to <code>value</code> inside the container.</li> <li><code>-e ANOTHER_VAR=another_value</code> sets the <code>ANOTHER_VAR</code> environment variable to <code>another_value</code> inside the container.</li> </ul> <p>You can set multiple environment variables by repeating the <code>-e</code> option. The values of these variables will be available to the application running inside the container, allowing you to customize its behavior.</p> <p>Note that you can also use shell variables and command substitutions to set environment variables dynamically. For example:</p> <pre><code>MY_EXTERNAL_VAR=external_value\ndocker run -e MY_INTERNAL_VAR=$MY_EXTERNAL_VAR \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <p>In this example, the <code>MY_INTERNAL_VAR</code> environment variable inside the container will be set to the value of the <code>MY_EXTERNAL_VAR</code> shell variable on the host machine.</p>"},{"location":"user-guide/getting-started/initialization-guide/#setting-user-and-group-ids-with-the-u-option","title":"Setting User and Group IDs with the <code>-u</code> Option","text":"<p>The <code>-u</code> option sets the user and group IDs to use for the container process. By matching the IDs of the user on the host machine, the user inside the container will have identical permissions for reading and writing files in the mounted volumes as the user that ran the command. You can use command substitutions to automatically retrieve your user and group IDs.</p> <p>Example:</p> <pre><code>docker run -u $(id -u):$(id -g) \\\n    nvcr.io/nvidia/clara/bionemo-framework:nightly\n</code></pre> <ul> <li><code>$(id -u)</code> is a command substitution that executes the id -u command and captures its output. <code>id -u</code> prints the     effective user ID of the current user.</li> <li><code>$(id -g)</code> is another command substitution that executes the <code>id -g</code> command and captures its output. <code>id -g</code> prints     the effective group ID of the current user.</li> </ul>"},{"location":"user-guide/getting-started/pre-reqs/","title":"Hardware and Software Prerequisites","text":"<p>Before you begin using the BioNeMo framework, please ensure the hardware and software prerequisites outlined below are met.</p>"},{"location":"user-guide/getting-started/pre-reqs/#hardware","title":"Hardware","text":"<p>The BioNeMo Framework is compatible with environments that have access to NVIDIA GPUs. <code>bfloat16</code> precision requires an Ampere generation GPU or higher (Compute Capability \u22658.0). You may be able to run BioNeMo on GPUs without <code>bfloat16</code>, but this use-case is not supported by the development team.</p>"},{"location":"user-guide/getting-started/pre-reqs/#gpu-support-matrix","title":"GPU Support Matrix","text":"<p>The following datacenter and desktop GPUs have Compute Capability \u22658.0 and are supported hardware for BioNeMo:</p> GPU Compute Capability Support H100 9.0 Full L4 8.9 Full L40 8.9 Full A100 8.0 Full A40 8.6 Full A30 8.0 Full A10 8.6 Full A16 8.6 Full A2 8.6 Full RTX 6000 8.9 Full RTX A6000 8.6 Full RTX A5000 8.6 Full RTX A4000 8.6 Full"},{"location":"user-guide/getting-started/pre-reqs/#software","title":"Software","text":"<p>The BioNeMo Framework is supported on x86 Linux systems.</p> <p>Please ensure that the following are installed in your desired execution environment:</p> <ul> <li>Appropriate GPU drivers (minimum version: 560; lower versions may be compatible, but cannot be guaranteed)</li> <li>Docker (with GPU support, Docker Engine 19.03 or above)</li> <li>NVIDIA Container Toolkit to allow Docker to access the GPUs</li> </ul>"},{"location":"user-guide/getting-started/training-models/","title":"Training Models","text":""},{"location":"user-guide/getting-started/training-models/#pydantic-configuration","title":"Pydantic Configuration","text":"<p>BioNeMo 2 provides two entrypoints for models with both argparse and pydantic. Both documented in the <code>Models</code> section below. Pydantic based configuration is designed to accept a configuration yaml file as input, along with context specific arguments (e.g., should we resume from existing checkpoints?). These YAML configs go through a Pydantic Validator, in this case referred to as <code>MainConfig</code>. This Config is composed of several other Pydantic models, see the class definition for details. To pre-populate a config with reasonable defaults for various standard models, we provide 'recipes.' These are simple methods that instantiate the config object and then serialize it to a YAML configuration file. From this file, you may either submit it directly, or modify the various parameters to meet your usecase. For example, Weights and biases, devices, precision, and dataset options are all extremely useful to modify. Then, you would submit this config for training.</p> <p>These two workflows are packaged as executables when esm2 or geneformer are installed with pip. These commands will appear as:</p> <pre><code>bionemo-geneformer-recipe\nbionemo-esm2-recipe\nbionemo-geneformer-train\nbionemo-esm2-train\n</code></pre>"},{"location":"user-guide/getting-started/training-models/#esm-2","title":"ESM-2","text":""},{"location":"user-guide/getting-started/training-models/#running","title":"Running","text":"<p>First off, we have a utility function for downloading full/test data and model checkpoints called <code>download_bionemo_data</code> that our following examples currently use. This will download the object if it is not already on your local system,  and then return the path either way. For example if you run this twice in a row, you should expect the second time you run it to return the path almost instantly.</p> <p>NOTE: NVIDIA employees should use <code>pbss</code> rather than <code>ngc</code> for the data source.</p> <pre><code>export MY_DATA_SOURCE=\"ngc\"\n</code></pre> <p>or for NVIDIA internal employees with new data etc:</p> <pre><code>export MY_DATA_SOURCE=\"pbss\"\n</code></pre> <pre><code># The fastest transformer engine environment variables in testing were the following two\nTEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \\\nESM2_650M_CKPT=$(download_bionemo_data esm2/650m:2.0 --source $MY_DATA_SOURCE); \\\n\ntrain_esm2     \\\n    --train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \\\n    --train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \\\n    --valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \\\n    --valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \\\n    --result-dir ./results     \\\n    --experiment-name test_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 1 \\\n    --num-steps 10 \\\n    --max-seq-length 1024 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --restore-from-checkpoint-path ${ESM2_650M_CKPT}\n</code></pre>"},{"location":"user-guide/getting-started/training-models/#running-with-pydantic-configs","title":"Running with Pydantic configs","text":"<p>Alternatively, we provide a validated and serialized configuration file entrypoint for executing the same workflow. These can be generated using the <code>bionemo-esm2-recipe</code> entrypoints. Recipes are available for 8m, 650m, and 3b ESM2 models. You may select which preset config to use by setting the <code>--recipe</code> parameter. The output is then a serialized configuration file that may be used in the associated <code>bionemo-esm2-train</code> commands.</p> <pre><code># The fastest transformer engine environment variables in testing were the following two\nTEST_DATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source $MY_DATA_SOURCE); \\\nbionemo-esm2-recipe \\\n--train-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet     \\\n--train-database-path ${TEST_DATA_DIR}/2024_03_sanity/train_sanity.db     \\\n--valid-cluster-path ${TEST_DATA_DIR}/2024_03_sanity/valid_clusters.parquet     \\\n--valid-database-path ${TEST_DATA_DIR}/2024_03_sanity/validation.db     \\\n--result-dir ./results     \\\n--dest my_config.yaml\\\n--recipe esm2_8m_recipe\n</code></pre> <p>\u26a0\ufe0f IMPORTANT: Inspect and edit the contents of the outputted my_config.yaml as you see fit</p> <p>NOTE: To continue training from an existing checkpoint, simply pass in the path --initial-ckpt-path to the recipe command. This will populate the YAML with the correct field to ensure pretraining is initialized from an existing checkpoint.</p> <p>To submit a training job with the passed config, first update the yaml file with any additional execution parameters of your choosing: number of devices, workers, steps, etc. Second, invoke our training entrypoint. To do this, we need three things:</p> <ul> <li>Configuration file, the YAML produced by the previous step</li> <li>Model config type, in this case the pretraining config. This will validate the arguments in the config YAML against     those required for pretraining. Alternatively, things like fine-tuning with custom task heads may be specified here.     This allows for mixing/matching Data Modules with various tasks.</li> <li>Data Config type, this specifies how to parse, validate, and prepare the DataModule. This may change depending on task, for example, pretraining ESM2 uses a protein cluster oriented sampling method. In the case of inference or fine-tuning a pretrained model, a simple fasta file may be sufficient. There is a one-to-one relationship between DataConfig types and DataModule types.</li> </ul> <p>\u26a0\ufe0f Warning: This setup does NO configuration of Weights and Biases. Edit your config YAML and populate it with your WandB details.</p> <pre><code>bionemo-esm2-train \\\n--data-config-cls bionemo.esm2.run.config_models.ESM2DataConfig \\\n--model-config-cls bionemo.esm2.run.config_models.ExposedESM2PretrainConfig \\\n--config my_config.yaml\n</code></pre> <p>NOTE: both data-config-cls and model-config-cls have default values corresponding to ESM2DataConfig and ExposedESM2PretrainingConfig</p> <p>DataConfigCls and ModelConfigCls can also refer to locally defined types by the user. As long as python knows how to import the specified path, they may be configured. For example, you may have a custom Dataset/DataModule that you would like to mix with an existing recipe. In this case, you define a DataConfig object with the generic specified as your DataModule type, and then pass in the config type to the training recipe.</p>"},{"location":"user-guide/getting-started/training-models/#geneformer","title":"Geneformer","text":""},{"location":"user-guide/getting-started/training-models/#running_1","title":"Running","text":"<p>Similar to ESM-2, you can download the dataset and checkpoint through our utility function.</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\nGENEFORMER_10M_CKPT=$(download_bionemo_data geneformer/10M_240530:2.0 --source $MY_DATA_SOURCE); \\\ntrain_geneformer     \\\n    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl    \\\n    --result-dir ./results     \\\n    --restore-from-checkpoint-path ${GENEFORMER_10M_CKPT} \\\n    --experiment-name test_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 0 \\\n    --num-steps 55 \\\n    --seq-length 128 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2\n</code></pre> <p>To fine-tune, you to specify a different combination of model and loss. Pass the path to the outputted config file from the previous step as the <code>--restore-from-checkpoint-path</code>, and also change <code>--training-model-config-class</code> to the newly created model-config-class.</p> <p>While no CLI option currently exists to hot swap in different data modules and processing functions now, you could copy the <code>sub-projects/bionemo-geneformer/geneformer/scripts/train_geneformer.py</code> and modify the DataModule class that gets initialized.</p> <p>Simple fine-tuning example (NOTE: please change <code>--restore-from-checkpoint-path</code> to be the checkpoint directory path that was output last by the previous train run)</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\ntrain_geneformer     \\\n    --data-dir ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl    \\\n    --result-dir ./results     \\\n    --experiment-name test_finettune_experiment     \\\n    --num-gpus 1  \\\n    --num-nodes 1 \\\n    --val-check-interval 10 \\\n    --num-dataset-workers 0 \\\n    --num-steps 55 \\\n    --seq-length 128 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --training-model-config-class FineTuneSeqLenBioBertConfig \\\n    --restore-from-checkpoint-path results/test_experiment/dev/checkpoints/test_experiment--val_loss=4.3506-epoch=1-last\n</code></pre>"},{"location":"user-guide/getting-started/training-models/#running-with-pydantic-configs_1","title":"Running with Pydantic configs","text":"<p>Alternatively, we provide a validated and serialized configuration file entrypoint for executing the same workflow. Recipes are available for 10m, and 106m geneformer models. Additionally we provide an example recipe of finetuning, where the objective is to 'regress' on token IDs rather than the traditional masked language model approach. In practice, you will likely need to implement your own DataModule, DataConfig, and Finetuning model. You can use the same overall approach, but with customizations for your task.</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data single_cell/testdata-20241203 --source $MY_DATA_SOURCE); \\\nbionemo-geneformer-recipe \\\n    --recipe 10m-pretrain \\\n    --dest my_config.json \\\n    --data-path ${TEST_DATA_DIR}/cellxgene_2023-12-15_small_processed_scdl \\\n    --result-dir ./results\n</code></pre> <p>\u26a0\ufe0f IMPORTANT: Inspect and edit the contents of the outputted my_config.yaml as you see fit</p> <p>NOTE: To pretrain from an existing checkpoint, simply pass in the path --initial-ckpt-path to the recipe command. This will populate the YAML with the correct field to ensure pretraining is initialized from an existing checkpoint.</p> <p>To submit a training job with the passed config, first update the yaml file with any additional execution parameters of your choosing: number of devices, workers, steps, etc. Second, invoke our training entrypoint. To do this, we need three things:</p> <ul> <li>Configuration file, the YAML produced by the previous step</li> <li>Model config type, in this case the pretraining config. This will validate the arguments in the config YAML against     those required for pretraining. Alternatively, things like fine-tuning with custom task heads may be specified here.     This allows for mixing/matching Data Modules with various tasks.</li> <li>Data Config type, this specifies how to parse, validate, and prepare the DataModule. This may change depending on task, for example, while fine-tuning you may want to use a custom Dataset/DataModule that includes PERTURB-seq. In this case, the default pretraining DataConfig and DataModule will be insufficient. See ESM2 for additional example usecases.</li> </ul> <p>\u26a0\ufe0f Warning: This setup does NO configuration of Weights and Biases. Edit your config YAML and populate it with your WandB details.</p> <pre><code>bionemo-geneformer-train \\\n--data-config-cls bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig \\\n--model-config-cls bionemo.geneformer.run.config_models.ExposedGeneformerPretrainConfig \\\n--config my_config.yaml\n</code></pre> <p>NOTE: both data-config-cls and model-config-cls have default values corresponding to GeneformerPretrainingDataConfig and ExposedGeneformerPretrainConfig</p> <p>DataConfigCls and ModelConfigCls can also refer to locally defined types by the user. As long as python knows how to import the specified path, they may be configured. For example, you may have a custom Dataset/DataModule that you would like to mix with an existing recipe. In this case, you define a DataConfig object with the generic specified as your DataModule type, and then pass in the config type to the training recipe.</p>"}]}